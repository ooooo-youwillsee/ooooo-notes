[{"categories":["源码分析 nacos 系列"],"content":"在 nacos 中，发布配置分为 http 和 grpc 两种方式，分别为 ","date":"2023-09-11 08:00:00","objectID":"/ooooo-notes/%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90-nacos-%E5%8F%91%E5%B8%83%E9%85%8D%E7%BD%AE/:0:0","tags":["nacos","source code","源码分析 nacos 系列"],"title":"源码分析 nacos 发布配置","uri":"/ooooo-notes/%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90-nacos-%E5%8F%91%E5%B8%83%E9%85%8D%E7%BD%AE/"},{"categories":["源码分析 nacos 系列"],"content":"在 nacos 中，手动创建 service，更新 service，删除 service，更新 instance，都是通过 raft 协议来实现的，所以来简单介绍下。 ","date":"2023-09-09 08:00:00","objectID":"/ooooo-notes/%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90-nacos-%E5%90%8C%E6%AD%A5%E6%9C%8D%E5%8A%A1%E5%92%8C%E5%AE%9E%E4%BE%8B%E5%85%83%E6%95%B0%E6%8D%AE%E4%BF%A1%E6%81%AF/:0:0","tags":["nacos","source code","源码分析 nacos 系列"],"title":"源码分析 nacos 同步服务和实例元数据信息","uri":"/ooooo-notes/%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90-nacos-%E5%90%8C%E6%AD%A5%E6%9C%8D%E5%8A%A1%E5%92%8C%E5%AE%9E%E4%BE%8B%E5%85%83%E6%95%B0%E6%8D%AE%E4%BF%A1%E6%81%AF/"},{"categories":["源码分析 nacos 系列"],"content":"service 元数据同步 源码位置: com.alibaba.nacos.naming.core.ServiceOperatorV2Impl // 下面这些方法最终都会调用 metadataOperateService 的 updateServiceMetadata 或者 deleteServiceMetadata // 创建 service public void create(Service service, ServiceMetadata metadata) throws NacosException { // 检查 service if (ServiceManager.getInstance().containSingleton(service)) { throw new NacosApiException(NacosException.INVALID_PARAM, ErrorCode.SERVICE_ALREADY_EXIST, String.format(\"specified service %s already exists!\", service.getGroupedServiceName())); } // raft 协议更新服务元数据 metadataOperateService.updateServiceMetadata(service, metadata); } // 更新 service @Override public void update(Service service, ServiceMetadata metadata) throws NacosException { // 检查 service if (!ServiceManager.getInstance().containSingleton(service)) { throw new NacosApiException(NacosException.INVALID_PARAM, ErrorCode.SERVICE_NOT_EXIST, String.format(\"service %s not found!\", service.getGroupedServiceName())); } // raft 协议更新服务元数据 metadataOperateService.updateServiceMetadata(service, metadata); } // 删除 service @Override public void delete(String namespaceId, String serviceName) throws NacosException { Service service = getServiceFromGroupedServiceName(namespaceId, serviceName, true); // raft 协议删除服务元数据 delete(service); } 源码位置: com.alibaba.nacos.naming.core.v2.metadata.NamingMetadataOperateService#updateServiceMetadata // 更新服务元数据 public void updateServiceMetadata(Service service, ServiceMetadata serviceMetadata) { MetadataOperation\u003cServiceMetadata\u003e operation = buildMetadataOperation(service); operation.setMetadata(serviceMetadata); // 构建 WriteRequest, 这里的 group 是 naming_service_metadata WriteRequest operationLog = WriteRequest.newBuilder().setGroup(Constants.SERVICE_METADATA) .setOperation(DataOperation.CHANGE.name()).setData(ByteString.copyFrom(serializer.serialize(operation))) .build(); // 提交元数据 submitMetadataOperation(operationLog); } // 删除服务元数据 public void delete(Service service) throws NacosException { // 检查 service if (!ServiceManager.getInstance().containSingleton(service)) { throw new NacosApiException(NacosException.INVALID_PARAM, ErrorCode.SERVICE_NOT_EXIST, String.format(\"service %s not found!\", service.getGroupedServiceName())); } // 删除 service，必须先注销所有的 instance if (!serviceStorage.getPushData(service).getHosts().isEmpty()) { throw new NacosApiException(NacosException.INVALID_PARAM, ErrorCode.SERVICE_DELETE_FAILURE, \"Service \" + service.getGroupedServiceName() + \" is not empty, can't be delete. Please unregister instance first\"); } // 删除服务元数据 metadataOperateService.deleteServiceMetadata(service); } 源码位置: com.alibaba.nacos.naming.core.v2.metadata.NamingMetadataOperateService#submitMetadataOperation // 提交元数据, 使用 raft 协议, 最后会被 ServiceMetadataProcessor#onApply 处理 private void submitMetadataOperation(WriteRequest operationLog) { try { Response response = cpProtocol.write(operationLog); if (!response.getSuccess()) { throw new NacosRuntimeException(NacosException.SERVER_ERROR, \"do metadata operation failed \" + response.getErrMsg()); } } catch (Exception e) { throw new NacosRuntimeException(NacosException.SERVER_ERROR, \"do metadata operation failed\", e); } } 源码位置: com.alibaba.nacos.naming.core.v2.metadata.ServiceMetadataProcessor#onApply @Override public Response onApply(WriteRequest request) { readLock.lock(); try { MetadataOperation\u003cServiceMetadata\u003e op = serializer.deserialize(request.getData().toByteArray(), processType); switch (DataOperation.valueOf(request.getOperation())) { case ADD: addClusterMetadataToService(op); break; case CHANGE: updateServiceMetadata(op); break; case DELETE: deleteServiceMetadata(op); break; default: return Response.newBuilder().setSuccess(false) .setErrMsg(\"Unsupported operation \" + request.getOperation()).build(); } return Response.newBuilder().setSuccess(true).build(); } catch (Exception e) { Loggers.RAFT.error(\"onApply {} service metadata operation failed. \", request.getOperation(), e); String errorMessage = null == e.getMessage() ? e.getClass().getName() : e.getMe","date":"2023-09-09 08:00:00","objectID":"/ooooo-notes/%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90-nacos-%E5%90%8C%E6%AD%A5%E6%9C%8D%E5%8A%A1%E5%92%8C%E5%AE%9E%E4%BE%8B%E5%85%83%E6%95%B0%E6%8D%AE%E4%BF%A1%E6%81%AF/:1:0","tags":["nacos","source code","源码分析 nacos 系列"],"title":"源码分析 nacos 同步服务和实例元数据信息","uri":"/ooooo-notes/%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90-nacos-%E5%90%8C%E6%AD%A5%E6%9C%8D%E5%8A%A1%E5%92%8C%E5%AE%9E%E4%BE%8B%E5%85%83%E6%95%B0%E6%8D%AE%E4%BF%A1%E6%81%AF/"},{"categories":["源码分析 nacos 系列"],"content":"instance 元数据同步 源码位置: com.alibaba.nacos.naming.core.InstanceOperatorClientImpl#updateInstance // 更新实例元数据 @Override public void updateInstance(String namespaceId, String serviceName, Instance instance) throws NacosException { NamingUtils.checkInstanceIsLegal(instance); Service service = getService(namespaceId, serviceName, instance.isEphemeral()); // 检查 service if (!ServiceManager.getInstance().containSingleton(service)) { throw new NacosApiException(NacosException.INVALID_PARAM, ErrorCode.INSTANCE_ERROR, \"service not found, namespace: \" + namespaceId + \", service: \" + service); } String metadataId = InstancePublishInfo .genMetadataId(instance.getIp(), instance.getPort(), instance.getClusterName()); // 更新实例元数据 metadataOperateService.updateInstanceMetadata(service, metadataId, buildMetadata(instance)); } 源码位置: com.alibaba.nacos.naming.core.v2.metadata.NamingMetadataOperateService#updateInstanceMetadata // 更新实例元数据 public void updateInstanceMetadata(Service service, String metadataId, InstanceMetadata instanceMetadata) { MetadataOperation\u003cInstanceMetadata\u003e operation = buildMetadataOperation(service); operation.setTag(metadataId); operation.setMetadata(instanceMetadata); // 构造 WriteRequest 请求，注意这里的 group 为 naming_instance_metadata WriteRequest operationLog = WriteRequest.newBuilder().setGroup(Constants.INSTANCE_METADATA) .setOperation(DataOperation.CHANGE.name()).setData(ByteString.copyFrom(serializer.serialize(operation))) .build(); // 提交元数据请求 submitMetadataOperation(operationLog); } 源码位置: com.alibaba.nacos.naming.core.v2.metadata.NamingMetadataOperateService#submitMetadataOperation // 提交元数据请求 private void submitMetadataOperation(WriteRequest operationLog) { try { // raft 提交请求，会被 InstanceMetadataProcessor#onApply 来处理 Response response = cpProtocol.write(operationLog); if (!response.getSuccess()) { throw new NacosRuntimeException(NacosException.SERVER_ERROR, \"do metadata operation failed \" + response.getErrMsg()); } } catch (Exception e) { throw new NacosRuntimeException(NacosException.SERVER_ERROR, \"do metadata operation failed\", e); } } 源码位置: com.alibaba.nacos.naming.core.v2.metadata.InstanceMetadataProcessor#onApply @Override public Response onApply(WriteRequest request) { readLock.lock(); try { MetadataOperation\u003cInstanceMetadata\u003e op = serializer.deserialize(request.getData().toByteArray(), processType); switch (DataOperation.valueOf(request.getOperation())) { case ADD: case CHANGE: updateInstanceMetadata(op); break; case DELETE: deleteInstanceMetadata(op); break; default: return Response.newBuilder().setSuccess(false) .setErrMsg(\"Unsupported operation \" + request.getOperation()).build(); } return Response.newBuilder().setSuccess(true).build(); } catch (Exception e) { Loggers.RAFT.error(\"onApply {} instance metadata operation failed. \", request.getOperation(), e); String errorMessage = null == e.getMessage() ? e.getClass().getName() : e.getMessage(); return Response.newBuilder().setSuccess(false).setErrMsg(errorMessage).build(); } finally { readLock.unlock(); } } ","date":"2023-09-09 08:00:00","objectID":"/ooooo-notes/%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90-nacos-%E5%90%8C%E6%AD%A5%E6%9C%8D%E5%8A%A1%E5%92%8C%E5%AE%9E%E4%BE%8B%E5%85%83%E6%95%B0%E6%8D%AE%E4%BF%A1%E6%81%AF/:2:0","tags":["nacos","source code","源码分析 nacos 系列"],"title":"源码分析 nacos 同步服务和实例元数据信息","uri":"/ooooo-notes/%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90-nacos-%E5%90%8C%E6%AD%A5%E6%9C%8D%E5%8A%A1%E5%92%8C%E5%AE%9E%E4%BE%8B%E5%85%83%E6%95%B0%E6%8D%AE%E4%BF%A1%E6%81%AF/"},{"categories":["随笔"],"content":"raft 节点在机器ip变动之后，可能出现选主不成功的问题。 ","date":"2023-09-08 08:00:00","objectID":"/ooooo-notes/raft-%E5%8D%8F%E8%AE%AE%E9%87%8D%E6%96%B0%E8%AE%BE%E7%BD%AE-ip/:0:0","tags":["raft"],"title":"raft 协议重新设置 ip","uri":"/ooooo-notes/raft-%E5%8D%8F%E8%AE%AE%E9%87%8D%E6%96%B0%E8%AE%BE%E7%BD%AE-ip/"},{"categories":["随笔"],"content":"解决方法 下面是 nacos 的 JRaft 解决方法 @Test public void test() throws IOException { String[] groupIds = {\"naming_instance_metadata\", \"naming_persistent_service_v2\", \"naming_service_metadata\"}; String logPath = \"/Users/ooooo/nacos/data/protocol/raft/%s/log\"; // 遍历 groupId for (String groupId : groupIds) { // logStorage LogStorage logStorage = new RocksDBLogStorage(String.format(logPath, groupId), new RaftOptions()); // logStorageOptions LogStorageOptions logStorageOptions = new LogStorageOptions(); logStorageOptions.setConfigurationManager(new ConfigurationManager()); logStorageOptions.setLogEntryCodecFactory(LogEntryV2CodecFactory.getInstance()); logStorageOptions.setGroupId(groupId); // 初始化 boolean init = logStorage.init(logStorageOptions); if (init) { // 获取最后一个 index long lastLogIndex = logStorage.getLastLogIndex(); System.out.println(lastLogIndex); // 新增配置 LogEntry logEntry = new LogEntry(); logEntry.setType(EnumOutter.EntryType.ENTRY_TYPE_CONFIGURATION); logEntry.setPeers(Collections.singletonList(PeerId.parsePeer(\"127.0.0.1:7848\"))); // 添加到日志中 boolean b = logStorage.appendEntry(logEntry); System.out.println(b); } } } ","date":"2023-09-08 08:00:00","objectID":"/ooooo-notes/raft-%E5%8D%8F%E8%AE%AE%E9%87%8D%E6%96%B0%E8%AE%BE%E7%BD%AE-ip/:1:0","tags":["raft"],"title":"raft 协议重新设置 ip","uri":"/ooooo-notes/raft-%E5%8D%8F%E8%AE%AE%E9%87%8D%E6%96%B0%E8%AE%BE%E7%BD%AE-ip/"},{"categories":["源码分析 nacos 系列"],"content":"raft 协议的初始化 源码位置: com.alibaba.nacos.core.distributed.raft.JRaftProtocol#init @Override public void init(RaftConfig config) { // 判断是否已经初始化 if (initialized.compareAndSet(false, true)) { this.raftConfig = config; // 这里是一个空方法 NotifyCenter.registerToSharePublisher(RaftEvent.class); // raftServer 初始化 this.raftServer.init(this.raftConfig); // raftServer 启动 this.raftServer.start(); // There is only one consumer to ensure that the internal consumption // is sequential and there is no concurrent competition // 监听 RaftEvent 事件 NotifyCenter.registerSubscriber(new Subscriber\u003cRaftEvent\u003e() { @Override public void onEvent(RaftEvent event) { Loggers.RAFT.info(\"This Raft event changes : {}\", event); final String groupId = event.getGroupId(); Map\u003cString, Map\u003cString, Object\u003e\u003e value = new HashMap\u003c\u003e(); Map\u003cString, Object\u003e properties = new HashMap\u003c\u003e(); final String leader = event.getLeader(); final Long term = event.getTerm(); final List\u003cString\u003e raftClusterInfo = event.getRaftClusterInfo(); final String errMsg = event.getErrMsg(); // Leader information needs to be selectively updated. If it is valid data, // the information in the protocol metadata is updated. MapUtil.putIfValNoEmpty(properties, MetadataKey.LEADER_META_DATA, leader); MapUtil.putIfValNoNull(properties, MetadataKey.TERM_META_DATA, term); MapUtil.putIfValNoEmpty(properties, MetadataKey.RAFT_GROUP_MEMBER, raftClusterInfo); MapUtil.putIfValNoEmpty(properties, MetadataKey.ERR_MSG, errMsg); value.put(groupId, properties); // 保存元数据 metaData.load(value); // The metadata information is injected into the metadata information of the node // 会发布 MembersChangeEvent 事件 injectProtocolMetaData(metaData); } @Override public Class\u003c? extends Event\u003e subscribeType() { return RaftEvent.class; } }); } } ","date":"2023-09-06 08:00:00","objectID":"/ooooo-notes/%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90-nacos-cp-%E5%8D%8F%E8%AE%AE-raft/:1:0","tags":["nacos","source code","源码分析 nacos 系列"],"title":"源码分析 nacos CP 协议 Raft","uri":"/ooooo-notes/%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90-nacos-cp-%E5%8D%8F%E8%AE%AE-raft/"},{"categories":["源码分析 nacos 系列"],"content":"raftServer 的初始化 源码位置: com.alibaba.nacos.core.distributed.raft.JRaftServer#init void init(RaftConfig config) { this.raftConfig = config; this.serializer = SerializeFactory.getDefault(); Loggers.RAFT.info(\"Initializes the Raft protocol, raft-config info : {}\", config); // 初始化 raft 线程池 RaftExecutor.init(config); // 解析配置 final String self = config.getSelfMember(); String[] info = InternetAddressUtil.splitIPPortStr(self); selfIp = info[0]; selfPort = Integer.parseInt(info[1]); localPeerId = PeerId.parsePeer(self); nodeOptions = new NodeOptions(); // Set the election timeout time. The default is 5 seconds. // 选举的超时时间 int electionTimeout = Math.max(ConvertUtils.toInt(config.getVal(RaftSysConstants.RAFT_ELECTION_TIMEOUT_MS), RaftSysConstants.DEFAULT_ELECTION_TIMEOUT), RaftSysConstants.DEFAULT_ELECTION_TIMEOUT); // 请求超时时间 rpcRequestTimeoutMs = ConvertUtils.toInt(raftConfig.getVal(RaftSysConstants.RAFT_RPC_REQUEST_TIMEOUT_MS), RaftSysConstants.DEFAULT_RAFT_RPC_REQUEST_TIMEOUT_MS); // 共享定时器 nodeOptions.setSharedElectionTimer(true); nodeOptions.setSharedVoteTimer(true); nodeOptions.setSharedStepDownTimer(true); nodeOptions.setSharedSnapshotTimer(true); nodeOptions.setElectionTimeoutMs(electionTimeout); // 配置 raft RaftOptions raftOptions = RaftOptionsBuilder.initRaftOptions(raftConfig); nodeOptions.setRaftOptions(raftOptions); // open jraft node metrics record function nodeOptions.setEnableMetrics(true); CliOptions cliOptions = new CliOptions(); // 初始化 raft 的 cliService this.cliService = RaftServiceFactory.createAndInitCliService(cliOptions); // cliService 的通信类, 从这个类中可以拿到 rpcClient this.cliClientService = (CliClientServiceImpl) ((CliServiceImpl) this.cliService).getCliClientService(); } ","date":"2023-09-06 08:00:00","objectID":"/ooooo-notes/%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90-nacos-cp-%E5%8D%8F%E8%AE%AE-raft/:2:0","tags":["nacos","source code","源码分析 nacos 系列"],"title":"源码分析 nacos CP 协议 Raft","uri":"/ooooo-notes/%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90-nacos-cp-%E5%8D%8F%E8%AE%AE-raft/"},{"categories":["源码分析 nacos 系列"],"content":"raftServer 的启动 源码位置: com.alibaba.nacos.core.distributed.raft.JRaftServer#start synchronized void start() { // 判断是否已经启动 if (!isStarted) { Loggers.RAFT.info(\"========= The raft protocol is starting... =========\"); try { // init raft group node // 初始化 raft 的节点列表 com.alipay.sofa.jraft.NodeManager raftNodeManager = com.alipay.sofa.jraft.NodeManager.getInstance(); for (String address : raftConfig.getMembers()) { PeerId peerId = PeerId.parsePeer(address); conf.addPeer(peerId); raftNodeManager.addAddress(peerId.getEndpoint()); } // 设置节点配置 nodeOptions.setInitialConf(conf); // 创建 rpcServer 和自定义请求处理器, 这个 server 在多个 raft group 中是共享的 rpcServer = JRaftUtils.initRpcServer(this, localPeerId); // rpcServer 初始化 if (!this.rpcServer.init(null)) { Loggers.RAFT.error(\"Fail to init [BaseRpcServer].\"); throw new RuntimeException(\"Fail to init [BaseRpcServer].\"); } // Initialize multi raft group service framework isStarted = true; // 创建 raftGroup createMultiRaftGroup(processors); Loggers.RAFT.info(\"========= The raft protocol start finished... =========\"); } catch (Exception e) { Loggers.RAFT.error(\"raft protocol start failure, cause: \", e); throw new JRaftException(e); } } } 源码位置: com.alibaba.nacos.core.distributed.raft.utils.JRaftUtils#initRpcServer // 创建 rpcServer 和自定义请求处理器 public static RpcServer initRpcServer(JRaftServer server, PeerId peerId) { GrpcRaftRpcFactory raftRpcFactory = (GrpcRaftRpcFactory) RpcFactoryHelper.rpcFactory(); // 注册 protobuf 序列化类 raftRpcFactory.registerProtobufSerializer(Log.class.getName(), Log.getDefaultInstance()); raftRpcFactory.registerProtobufSerializer(GetRequest.class.getName(), GetRequest.getDefaultInstance()); raftRpcFactory.registerProtobufSerializer(WriteRequest.class.getName(), WriteRequest.getDefaultInstance()); raftRpcFactory.registerProtobufSerializer(ReadRequest.class.getName(), ReadRequest.getDefaultInstance()); raftRpcFactory.registerProtobufSerializer(Response.class.getName(), Response.getDefaultInstance()); // 注册响应类 MarshallerRegistry registry = raftRpcFactory.getMarshallerRegistry(); registry.registerResponseInstance(Log.class.getName(), Response.getDefaultInstance()); registry.registerResponseInstance(GetRequest.class.getName(), Response.getDefaultInstance()); registry.registerResponseInstance(WriteRequest.class.getName(), Response.getDefaultInstance()); registry.registerResponseInstance(ReadRequest.class.getName(), Response.getDefaultInstance()); final RpcServer rpcServer = raftRpcFactory.createRpcServer(peerId.getEndpoint()); // 添加 raft 的 requestProcessor RaftRpcServerFactory.addRaftRequestProcessors(rpcServer, RaftExecutor.getRaftCoreExecutor(), RaftExecutor.getRaftCliServiceExecutor()); // 注册自定义的 requestProcessor rpcServer.registerProcessor(new NacosWriteRequestProcessor(server, SerializeFactory.getDefault())); rpcServer.registerProcessor(new NacosReadRequestProcessor(server, SerializeFactory.getDefault())); return rpcServer; } 源码位置: com.alibaba.nacos.core.distributed.raft.JRaftServer#createMultiRaftGroup // 创建 raftGroup // 从这个方法可以看出 // 每个 groupName 都会对应一个 processor，一个 NacosStateMachine，一个 Node。 synchronized void createMultiRaftGroup(Collection\u003cRequestProcessor4CP\u003e processors) { // There is no reason why the LogProcessor cannot be processed because of the synchronization if (!this.isStarted) { // 添加 processor this.processors.addAll(processors); return; } // raft 日志存储路径 final String parentPath = Paths.get(EnvUtil.getNacosHome(), \"data/protocol/raft\").toString(); // 遍历 processors for (RequestProcessor4CP processor : processors) { final String groupName = processor.group(); // 判断 group 是否重复 if (multiRaftGroup.containsKey(groupName)) { throw new DuplicateRaftGroupException(groupName); } // Ensure that each Raft Group has its own configuration and NodeOptions Configuration configuration = conf.copy(); NodeOptions copy = nodeOptions.copy(); // 初始化目录 JRaftUtils.initDirectory(parentPath, groupName, copy); // Here, the LogProcessor is passed into StateMachine, and when the StateMachine // triggers onApply, ","date":"2023-09-06 08:00:00","objectID":"/ooooo-notes/%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90-nacos-cp-%E5%8D%8F%E8%AE%AE-raft/:3:0","tags":["nacos","source code","源码分析 nacos 系列"],"title":"源码分析 nacos CP 协议 Raft","uri":"/ooooo-notes/%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90-nacos-cp-%E5%8D%8F%E8%AE%AE-raft/"},{"categories":["源码分析 nacos 系列"],"content":"raft 节点变更 源码位置: com.alibaba.nacos.core.distributed.raft.JRaftServer#registerSelfToCluster // 增加节点 void registerSelfToCluster(String groupId, PeerId selfIp, Configuration conf) { while (!isShutdown) { try { // 获取 groupId 对应的成员列表 List\u003cPeerId\u003e peerIds = cliService.getPeers(groupId, conf); if (peerIds.contains(selfIp)) { return; } // 添加自己的 ip 到集群中 Status status = cliService.addPeer(groupId, conf, selfIp); if (status.isOk()) { return; } Loggers.RAFT.warn(\"Failed to join the cluster, retry...\"); } catch (Exception e) { Loggers.RAFT.error(\"Failed to join the cluster, retry...\", e); } ThreadUtils.sleep(1_000L); } } 源码位置: com.alibaba.nacos.consistency.ConsistencyProtocol#memberChange // 删除节点 @Override public void memberChange(Set\u003cString\u003e addresses) { // 这里会重试 5 次 for (int i = 0; i \u003c 5; i++) { // 删除节点 if (this.raftServer.peerChange(jRaftMaintainService, addresses)) { return; } ThreadUtils.sleep(100L); } Loggers.RAFT.warn(\"peer removal failed\"); } boolean peerChange(JRaftMaintainService maintainService, Set\u003cString\u003e newPeers) { // This is only dealing with node deletion, the Raft protocol, where the node adds itself to the cluster when it starts up Set\u003cString\u003e oldPeers = new HashSet\u003c\u003e(this.raftConfig.getMembers()); this.raftConfig.setMembers(localPeerId.toString(), newPeers); oldPeers.removeAll(newPeers); // 检查节点是否有删除，为空，表示节点不变或者有新的节点加入 if (oldPeers.isEmpty()) { return true; } Set\u003cString\u003e waitRemove = oldPeers; AtomicInteger successCnt = new AtomicInteger(0); // 遍历 multiRaftGroup 来删除 multiRaftGroup.forEach(new BiConsumer\u003cString, RaftGroupTuple\u003e() { @Override public void accept(String group, RaftGroupTuple tuple) { Map\u003cString, String\u003e params = new HashMap\u003c\u003e(); params.put(JRaftConstants.GROUP_ID, group); params.put(JRaftConstants.COMMAND_NAME, JRaftConstants.REMOVE_PEERS); params.put(JRaftConstants.COMMAND_VALUE, StringUtils.join(waitRemove, StringUtils.COMMA)); // 执行删除命令, REMOVE_PEERS RestResult\u003cString\u003e result = maintainService.execute(params); if (result.ok()) { successCnt.incrementAndGet(); } else { Loggers.RAFT.error(\"Node removal failed : {}\", result); } } }); return successCnt.get() == multiRaftGroup.size(); } // 源码位置：com.alibaba.nacos.core.distributed.raft.utils.JRaftOps#REMOVE_PEERS REMOVE_PEERS(JRaftConstants.REMOVE_PEERS) { @Override public RestResult\u003cString\u003e execute(CliService cliService, String groupId, Node node, Map\u003cString, String\u003e args) { final Configuration conf = node.getOptions().getInitialConf(); final String peers = args.get(JRaftConstants.COMMAND_VALUE); // 遍历节点 for (String s : peers.split(\",\")) { List\u003cPeerId\u003e peerIds = cliService.getPeers(groupId, conf); final PeerId waitRemove = PeerId.parsePeer(s); // 不包含，则不需要删除 if (!peerIds.contains(waitRemove)) { continue; } // 删除节点 Status status = cliService.removePeer(groupId, conf, waitRemove); if (!status.isOk()) { return RestResultUtils.failed(status.getErrorMsg()); } } return RestResultUtils.success(); } }, ","date":"2023-09-06 08:00:00","objectID":"/ooooo-notes/%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90-nacos-cp-%E5%8D%8F%E8%AE%AE-raft/:4:0","tags":["nacos","source code","源码分析 nacos 系列"],"title":"源码分析 nacos CP 协议 Raft","uri":"/ooooo-notes/%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90-nacos-cp-%E5%8D%8F%E8%AE%AE-raft/"},{"categories":["源码分析 nacos 系列"],"content":"raft 的请求处理过程 这里以 writeRequest 为例，来说明 raft 是如何处理请求的。 源码位置: com.alibaba.nacos.naming.core.v2.service.impl.PersistentClientOperationServiceImpl#registerInstance // 客户端发起注册持久化实例 @Override public void registerInstance(Service service, Instance instance, String clientId) { Service singleton = ServiceManager.getInstance().getSingleton(service); if (singleton.isEphemeral()) { throw new NacosRuntimeException(NacosException.INVALID_PARAM, String.format(\"Current service %s is ephemeral service, can't register persistent instance.\", singleton.getGroupedServiceName())); } final InstanceStoreRequest request = new InstanceStoreRequest(); request.setService(service); request.setInstance(instance); request.setClientId(clientId); // 这里设置了 raft group, 等下会用到 final WriteRequest writeRequest = WriteRequest.newBuilder().setGroup(group()) .setData(ByteString.copyFrom(serializer.serialize(request))).setOperation(DataOperation.ADD.name()) .build(); try { // raftProtocol 来写请求 protocol.write(writeRequest); Loggers.RAFT.info(\"Client registered. service={}, clientId={}, instance={}\", service, instance, clientId); } catch (Exception e) { throw new NacosRuntimeException(NacosException.SERVER_ERROR, e); } } 源码位置: com.alibaba.nacos.core.distributed.raft.JRaftProtocol#write // raftProtocol 来写请求 @Override public Response write(WriteRequest request) throws Exception { // 异步请求 CompletableFuture\u003cResponse\u003e future = writeAsync(request); // Here you wait for 10 seconds, as long as possible, for the request to complete return future.get(10_000L, TimeUnit.MILLISECONDS); } @Override public CompletableFuture\u003cResponse\u003e writeAsync(WriteRequest request) { // raftServer 提交请求 return raftServer.commit(request.getGroup(), request, new CompletableFuture\u003c\u003e()); } 源码位置: com.alibaba.nacos.core.distributed.raft.JRaftServer#commit // raftServer 提交请求 public CompletableFuture\u003cResponse\u003e commit(final String group, final Message data, final CompletableFuture\u003cResponse\u003e future) { LoggerUtils.printIfDebugEnabled(Loggers.RAFT, \"data requested this time : {}\", data); // 通过 group 找到对应的 raft 配置 final RaftGroupTuple tuple = findTupleByGroup(group); if (tuple == null) { future.completeExceptionally(new IllegalArgumentException(\"No corresponding Raft Group found : \" + group)); return future; } // 包装 future 回调函数 FailoverClosureImpl closure = new FailoverClosureImpl(future); final Node node = tuple.node; if (node.isLeader()) { // The leader node directly applies this request // 如果是 leader，直接 apply 请求 applyOperation(node, data, closure); } else { // Forward to Leader for request processing // 如果不是 leader，把请求转发给 leader，后面 leader 继续 apply 请求 invokeToLeader(group, data, rpcRequestTimeoutMs, closure); } return future; } 源码位置: com.alibaba.nacos.core.distributed.raft.JRaftServer#applyOperation // leader 节点 apply 请求 public void applyOperation(Node node, Message data, FailoverClosure closure) { final Task task = new Task(); // 设置回调 task.setDone(new NacosClosure(data, status -\u003e { // 把响应设置给 closure, closure 就是 FailoverClosureImpl NacosClosure.NacosStatus nacosStatus = (NacosClosure.NacosStatus) status; closure.setThrowable(nacosStatus.getThrowable()); closure.setResponse(nacosStatus.getResponse()); closure.run(nacosStatus); })); // add request type field at the head of task data. // 封装请求，WriteRequest 或者 ReadRequest byte[] requestTypeFieldBytes = new byte[2]; requestTypeFieldBytes[0] = ProtoMessageUtil.REQUEST_TYPE_FIELD_TAG; if (data instanceof ReadRequest) { requestTypeFieldBytes[1] = ProtoMessageUtil.REQUEST_TYPE_READ; } else { requestTypeFieldBytes[1] = ProtoMessageUtil.REQUEST_TYPE_WRITE; } byte[] dataBytes = data.toByteArray(); // 设置数据 task.setData((ByteBuffer) ByteBuffer.allocate(requestTypeFieldBytes.length + dataBytes.length) .put(requestTypeFieldBytes).put(dataBytes).position(0)); // apply 请求，写入主节点日志，复制日志到从节点，超过半数节点成功，然后执行状态机 NacosStateMachine node.apply(task); } 源码位置: com.alibaba.nacos.core.distributed.raft.NacosStateMachine#onApply // 执行状态机 @Override public void onApply(Iterator iter) { int index = 0; ","date":"2023-09-06 08:00:00","objectID":"/ooooo-notes/%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90-nacos-cp-%E5%8D%8F%E8%AE%AE-raft/:5:0","tags":["nacos","source code","源码分析 nacos 系列"],"title":"源码分析 nacos CP 协议 Raft","uri":"/ooooo-notes/%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90-nacos-cp-%E5%8D%8F%E8%AE%AE-raft/"},{"categories":["源码分析 nacos 系列"],"content":"转发请求给 leader 源码位置: com.alibaba.nacos.core.distributed.raft.JRaftServer#invokeToLeader // 转发请求给 leader private void invokeToLeader(final String group, final Message request, final int timeoutMillis, FailoverClosure closure) { try { final Endpoint leaderIp = Optional.ofNullable(getLeader(group)) .orElseThrow(() -\u003e new NoLeaderException(group)).getEndpoint(); // 调用 cliClientService 来转发请求 cliClientService.getRpcClient().invokeAsync(leaderIp, request, new InvokeCallback() { @Override public void complete(Object o, Throwable ex) { if (Objects.nonNull(ex)) { closure.setThrowable(ex); closure.run(new Status(RaftError.UNKNOWN, ex.getMessage())); return; } if (!((Response)o).getSuccess()) { closure.setThrowable(new IllegalStateException(((Response) o).getErrMsg())); closure.run(new Status(RaftError.UNKNOWN, ((Response) o).getErrMsg())); return; } closure.setResponse((Response) o); closure.run(Status.OK()); } @Override public Executor executor() { return RaftExecutor.getRaftCliServiceExecutor(); } }, timeoutMillis); } catch (Exception e) { closure.setThrowable(e); closure.run(new Status(RaftError.UNKNOWN, e.toString())); } } 源码位置: com.alibaba.nacos.core.distributed.raft.processor.NacosWriteRequestProcessor // 接受 WriteRequest public class NacosWriteRequestProcessor extends AbstractProcessor implements RpcProcessor\u003cWriteRequest\u003e { private static final String INTEREST_NAME = WriteRequest.class.getName(); private final JRaftServer server; public NacosWriteRequestProcessor(JRaftServer server, Serializer serializer) { super(serializer); this.server = server; } @Override public void handleRequest(RpcContext rpcCtx, WriteRequest request) { // 处理请求 handleRequest(server, request.getGroup(), rpcCtx, request); } @Override public String interest() { return INTEREST_NAME; } } // 处理请求 protected void handleRequest(final JRaftServer server, final String group, final RpcContext rpcCtx, Message message) { try { ... // 如果是 leader 节点，就处理请求，否则返回错误 if (tuple.getNode().isLeader()) { execute(server, rpcCtx, message, tuple); } else { rpcCtx.sendResponse( Response.newBuilder().setSuccess(false).setErrMsg(\"Could not find leader : \" + group).build()); } } catch (Throwable e) { Loggers.RAFT.error(\"handleRequest has error : \", e); rpcCtx.sendResponse(Response.newBuilder().setSuccess(false).setErrMsg(e.toString()).build()); } } // 执行请求 protected void execute(JRaftServer server, final RpcContext asyncCtx, final Message message, final JRaftServer.RaftGroupTuple tuple) { ... // apply 请求，这个和上面的逻辑一样，不继续分析了 server.applyOperation(tuple.getNode(), message, closure); } ","date":"2023-09-06 08:00:00","objectID":"/ooooo-notes/%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90-nacos-cp-%E5%8D%8F%E8%AE%AE-raft/:6:0","tags":["nacos","source code","源码分析 nacos 系列"],"title":"源码分析 nacos CP 协议 Raft","uri":"/ooooo-notes/%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90-nacos-cp-%E5%8D%8F%E8%AE%AE-raft/"},{"categories":["源码分析 nacos 系列"],"content":"raft 处理 ReadRequest 源码位置: com.alibaba.nacos.core.distributed.raft.JRaftServer#get // raft 处理 ReadRequest CompletableFuture\u003cResponse\u003e get(final ReadRequest request) { final String group = request.getGroup(); CompletableFuture\u003cResponse\u003e future = new CompletableFuture\u003c\u003e(); // 检查 group 是否存在 final RaftGroupTuple tuple = findTupleByGroup(group); if (Objects.isNull(tuple)) { future.completeExceptionally(new NoSuchRaftGroupException(group)); return future; } final Node node = tuple.node; final RequestProcessor processor = tuple.processor; try { // raft 协议的一致性读，如果返回成功，可以确保数据是一致的，直接本地处理就可以 node.readIndex(BytesUtil.EMPTY_BYTES, new ReadIndexClosure() { @Override public void run(Status status, long index, byte[] reqCtx) { if (status.isOk()) { try { Response response = processor.onRequest(request); future.complete(response); } catch (Throwable t) { MetricsMonitor.raftReadIndexFailed(); future.completeExceptionally(new ConsistencyException( \"The conformance protocol is temporarily unavailable for reading\", t)); } return; } // 返回错误，从 leader 中读取数据 MetricsMonitor.raftReadIndexFailed(); Loggers.RAFT.error(\"ReadIndex has error : {}, go to Leader read.\", status.getErrorMsg()); MetricsMonitor.raftReadFromLeader(); readFromLeader(request, future); } }); return future; } catch (Throwable e) { MetricsMonitor.raftReadFromLeader(); Loggers.RAFT.warn(\"Raft linear read failed, go to Leader read logic : {}\", e.toString()); // run raft read readFromLeader(request, future); return future; } } // 从 leader 中读取数据 public void readFromLeader(final ReadRequest request, final CompletableFuture\u003cResponse\u003e future) { commit(request.getGroup(), request, future); } // 提交请求, 这个方法上面已经解析过了 public CompletableFuture\u003cResponse\u003e commit(final String group, final Message data, final CompletableFuture\u003cResponse\u003e future) { LoggerUtils.printIfDebugEnabled(Loggers.RAFT, \"data requested this time : {}\", data); final RaftGroupTuple tuple = findTupleByGroup(group); if (tuple == null) { future.completeExceptionally(new IllegalArgumentException(\"No corresponding Raft Group found : \" + group)); return future; } FailoverClosureImpl closure = new FailoverClosureImpl(future); final Node node = tuple.node; if (node.isLeader()) { // The leader node directly applies this request applyOperation(node, data, closure); } else { // Forward to Leader for request processing invokeToLeader(group, data, rpcRequestTimeoutMs, closure); } return future; } ","date":"2023-09-06 08:00:00","objectID":"/ooooo-notes/%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90-nacos-cp-%E5%8D%8F%E8%AE%AE-raft/:7:0","tags":["nacos","source code","源码分析 nacos 系列"],"title":"源码分析 nacos CP 协议 Raft","uri":"/ooooo-notes/%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90-nacos-cp-%E5%8D%8F%E8%AE%AE-raft/"},{"categories":["源码分析 nacos 系列"],"content":"nacos 中的 raft 用法 com.alibaba.nacos.naming.core.v2.service.impl.PersistentClientOperationServiceImpl: 客户端注册实例 com.alibaba.nacos.naming.consistency.persistent.impl.PersistentServiceProcessor: naming 模块，配置管理 com.alibaba.nacos.config.server.service.repository.embedded.DistributedDatabaseOperateImpl: 内嵌数据库 ","date":"2023-09-06 08:00:00","objectID":"/ooooo-notes/%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90-nacos-cp-%E5%8D%8F%E8%AE%AE-raft/:8:0","tags":["nacos","source code","源码分析 nacos 系列"],"title":"源码分析 nacos CP 协议 Raft","uri":"/ooooo-notes/%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90-nacos-cp-%E5%8D%8F%E8%AE%AE-raft/"},{"categories":["源码分析 nacos 系列"],"content":"在 nacos 中，集群成员分为静态加载和动态加载，静态加载就是读取 cluster.conf 文件，动态加载就是从一个接口中获取。 ","date":"2023-09-05 08:00:00","objectID":"/ooooo-notes/%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90-nacos-%E9%9B%86%E7%BE%A4%E6%88%90%E5%91%98%E7%AE%A1%E7%90%86/:0:0","tags":["nacos","source code","源码分析 nacos 系列"],"title":"源码分析 nacos 集群成员管理","uri":"/ooooo-notes/%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90-nacos-%E9%9B%86%E7%BE%A4%E6%88%90%E5%91%98%E7%AE%A1%E7%90%86/"},{"categories":["源码分析 nacos 系列"],"content":"集群成员加载的入口 源码位置: com.alibaba.nacos.core.cluster.ServerMemberManager#initAndStartLookup private void initAndStartLookup() throws NacosException { // 查找对应的 lookup, lookup 的实现有 file, standalone, address this.lookup = LookupFactory.createLookUp(this); // 是否使用 address lookup isUseAddressServer = this.lookup.useAddressServer(); // 启动 lookup this.lookup.start(); } 源码位置: com.alibaba.nacos.core.cluster.lookup.LookupFactory#createLookUp // 查找对应的 lookup public static MemberLookup createLookUp(ServerMemberManager memberManager) throws NacosException { // 不是 standalone 模式 if (!EnvUtil.getStandaloneMode()) { // lookupType 默认为空 String lookupType = EnvUtil.getProperty(LOOKUP_MODE_TYPE); // 根据 lookupType 来选择 LookupType type = chooseLookup(lookupType); LOOK_UP = find(type); currentLookupType = type; } else { // standalone lookup LOOK_UP = new StandaloneMemberLookup(); } // 注入 memberManager LOOK_UP.injectMemberManager(memberManager); Loggers.CLUSTER.info(\"Current addressing mode selection : {}\", LOOK_UP.getClass().getSimpleName()); return LOOK_UP; } 源码位置: com.alibaba.nacos.core.cluster.lookup.LookupFactory#chooseLookup // 根据 lookupType 来选择 private static LookupType chooseLookup(String lookupType) { // lookupType 不为空，则返回 if (StringUtils.isNotBlank(lookupType)) { LookupType type = LookupType.sourceOf(lookupType); if (Objects.nonNull(type)) { return type; } } // 判断 cluster.conf 是否存在，如果存在就是 file lookup File file = new File(EnvUtil.getClusterConfFilePath()); if (file.exists() || StringUtils.isNotBlank(EnvUtil.getMemberList())) { return LookupType.FILE_CONFIG; } // 默认返回 address lookup return LookupType.ADDRESS_SERVER; } ","date":"2023-09-05 08:00:00","objectID":"/ooooo-notes/%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90-nacos-%E9%9B%86%E7%BE%A4%E6%88%90%E5%91%98%E7%AE%A1%E7%90%86/:1:0","tags":["nacos","source code","源码分析 nacos 系列"],"title":"源码分析 nacos 集群成员管理","uri":"/ooooo-notes/%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90-nacos-%E9%9B%86%E7%BE%A4%E6%88%90%E5%91%98%E7%AE%A1%E7%90%86/"},{"categories":["源码分析 nacos 系列"],"content":"standalone lookup 源码位置: com.alibaba.nacos.core.cluster.lookup.StandaloneMemberLookup // 这个类，就是添加了自己的 url public class StandaloneMemberLookup extends AbstractMemberLookup { @Override public void doStart() { String url = EnvUtil.getLocalAddress(); // 发布 MembersChangeEvent 事件 afterLookup(MemberUtil.readServerConf(Collections.singletonList(url))); } @Override protected void doDestroy() throws NacosException { } @Override public boolean useAddressServer() { return false; } } ","date":"2023-09-05 08:00:00","objectID":"/ooooo-notes/%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90-nacos-%E9%9B%86%E7%BE%A4%E6%88%90%E5%91%98%E7%AE%A1%E7%90%86/:2:0","tags":["nacos","source code","源码分析 nacos 系列"],"title":"源码分析 nacos 集群成员管理","uri":"/ooooo-notes/%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90-nacos-%E9%9B%86%E7%BE%A4%E6%88%90%E5%91%98%E7%AE%A1%E7%90%86/"},{"categories":["源码分析 nacos 系列"],"content":"file lookup 源码位置: com.alibaba.nacos.core.cluster.lookup.FileConfigMemberLookup#doStart @Override public void doStart() throws NacosException { // 读取文件, 然后发布 MembersChangeEvent 事件 readClusterConfFromDisk(); // Use the inotify mechanism to monitor file changes and automatically // trigger the reading of cluster.conf try { // 动态监听配置文件 WatchFileCenter.registerWatcher(EnvUtil.getConfPath(), watcher); } catch (Throwable e) { Loggers.CLUSTER.error(\"An exception occurred in the launch file monitor : {}\", e.getMessage()); } } // 监听器，会监听 cluster.conf 文件，如果变动，就重新读取文件 private FileWatcher watcher = new FileWatcher() { @Override public void onChange(FileChangeEvent event) { readClusterConfFromDisk(); } @Override public boolean interest(String context) { return StringUtils.contains(context, DEFAULT_SEARCH_SEQ); } }; ","date":"2023-09-05 08:00:00","objectID":"/ooooo-notes/%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90-nacos-%E9%9B%86%E7%BE%A4%E6%88%90%E5%91%98%E7%AE%A1%E7%90%86/:3:0","tags":["nacos","source code","源码分析 nacos 系列"],"title":"源码分析 nacos 集群成员管理","uri":"/ooooo-notes/%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90-nacos-%E9%9B%86%E7%BE%A4%E6%88%90%E5%91%98%E7%AE%A1%E7%90%86/"},{"categories":["源码分析 nacos 系列"],"content":"address lookup 源码位置: com.alibaba.nacos.core.cluster.lookup.AddressServerMemberLookup#doStart @Override public void doStart() throws NacosException { // 最大的是失败个数 this.maxFailCount = Integer.parseInt(EnvUtil.getProperty(HEALTH_CHECK_FAIL_COUNT_PROPERTY, DEFAULT_HEALTH_CHECK_FAIL_COUNT)); // 初始化发现地址 initAddressSys(); // 运行，获取地址 run(); } 源码位置: com.alibaba.nacos.core.cluster.lookup.AddressServerMemberLookup#initAddressSys // 初始化发现地址 // 获取域名，端口，然后拼装为地址 private void initAddressSys() { String envDomainName = System.getenv(ADDRESS_SERVER_DOMAIN_ENV); if (StringUtils.isBlank(envDomainName)) { domainName = EnvUtil.getProperty(ADDRESS_SERVER_DOMAIN_PROPERTY, DEFAULT_SERVER_DOMAIN); } else { domainName = envDomainName; } String envAddressPort = System.getenv(ADDRESS_SERVER_PORT_ENV); if (StringUtils.isBlank(envAddressPort)) { addressPort = EnvUtil.getProperty(ADDRESS_SERVER_PORT_PROPERTY, DEFAULT_SERVER_POINT); } else { addressPort = envAddressPort; } String envAddressUrl = System.getenv(ADDRESS_SERVER_URL_ENV); if (StringUtils.isBlank(envAddressUrl)) { addressUrl = EnvUtil.getProperty(ADDRESS_SERVER_URL_PROPERTY, EnvUtil.getContextPath() + \"/\" + \"serverlist\"); } else { addressUrl = envAddressUrl; } addressServerUrl = HTTP_PREFIX + domainName + \":\" + addressPort + addressUrl; envIdUrl = HTTP_PREFIX + domainName + \":\" + addressPort + \"/env\"; Loggers.CORE.info(\"ServerListService address-server port:\" + addressPort); Loggers.CORE.info(\"ADDRESS_SERVER_URL:\" + addressServerUrl); } 源码位置: com.alibaba.nacos.core.cluster.lookup.AddressServerMemberLookup#run // 运行，获取地址 private void run() throws NacosException { // With the address server, you need to perform a synchronous member node pull at startup // Repeat three times, successfully jump out boolean success = false; Throwable ex = null; int maxRetry = EnvUtil.getProperty(ADDRESS_SERVER_RETRY_PROPERTY, Integer.class, DEFAULT_SERVER_RETRY_TIME); // 重试次数 for (int i = 0; i \u003c maxRetry; i++) { try { // 获取集群成员地址, 发布 MembersChangeEvent 事件 syncFromAddressUrl(); success = true; break; } catch (Throwable e) { ex = e; Loggers.CLUSTER.error(\"[serverlist] exception, error : {}\", ExceptionUtil.getAllExceptionMsg(ex)); } } if (!success) { throw new NacosException(NacosException.SERVER_ERROR, ex); } // 定时调用 GlobalExecutor.scheduleByCommon(new AddressServerSyncTask(), DEFAULT_SYNC_TASK_DELAY_MS); } ","date":"2023-09-05 08:00:00","objectID":"/ooooo-notes/%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90-nacos-%E9%9B%86%E7%BE%A4%E6%88%90%E5%91%98%E7%AE%A1%E7%90%86/:4:0","tags":["nacos","source code","源码分析 nacos 系列"],"title":"源码分析 nacos 集群成员管理","uri":"/ooooo-notes/%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90-nacos-%E9%9B%86%E7%BE%A4%E6%88%90%E5%91%98%E7%AE%A1%E7%90%86/"},{"categories":["源码分析 nacos 系列"],"content":" nacos 基于 2.2.4 版本 nacos 对于临时实例注册，采用的是 AP 协议，我们看看是怎么设计的。 ","date":"2023-08-29 08:00:00","objectID":"/ooooo-notes/%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90-nacos-ap-%E5%8D%8F%E8%AE%AE-distro/:0:0","tags":["nacos","source code","源码分析 nacos 系列"],"title":"源码分析 nacos AP 协议 Distro","uri":"/ooooo-notes/%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90-nacos-ap-%E5%8D%8F%E8%AE%AE-distro/"},{"categories":["源码分析 nacos 系列"],"content":"DistroProtocol 初始化 源码位置: com.alibaba.nacos.core.distributed.distro.DistroProtocol#DistroProtocol // DistroProtocol 构造函数 public DistroProtocol(ServerMemberManager memberManager, DistroComponentHolder distroComponentHolder, DistroTaskEngineHolder distroTaskEngineHolder) { this.memberManager = memberManager; this.distroComponentHolder = distroComponentHolder; this.distroTaskEngineHolder = distroTaskEngineHolder; // 开始定时任务 startDistroTask(); } // 开始定时任务 private void startDistroTask() { // standalone 表示单个节点，不用开启定时任务 if (EnvUtil.getStandaloneMode()) { isInitialized = true; return; } // 定时任务，同步当前节点的数据给其他节点, 最终会执行 DistroVerifyTimedTask startVerifyTask(); // 定时任务，从其他节点加载数据，最终会执行 DistroLoadDataTask startLoadTask(); } ","date":"2023-08-29 08:00:00","objectID":"/ooooo-notes/%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90-nacos-ap-%E5%8D%8F%E8%AE%AE-distro/:1:0","tags":["nacos","source code","源码分析 nacos 系列"],"title":"源码分析 nacos AP 协议 Distro","uri":"/ooooo-notes/%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90-nacos-ap-%E5%8D%8F%E8%AE%AE-distro/"},{"categories":["源码分析 nacos 系列"],"content":"DistroVerifyTimedTask 同步节点数据来续约 DistroVerifyTimedTask 源码位置: com.alibaba.nacos.core.distributed.distro.task.verify.DistroVerifyTimedTask // 同步当前节点的数据给其他节点, 这个类很重要 // 在 distro 协议中，每个节点只会处理部分数据, 数据的版本要通过定时任务来发送给其他节点进行续约， // 否则 client 下一次请求到其他节点，因为数据没有定时续约，会导致这个数据会过期删除. public class DistroVerifyTimedTask implements Runnable { ... @Override public void run() { try { // 获取其他节点列表 List\u003cMember\u003e targetServer = serverMemberManager.allMembersWithoutSelf(); if (Loggers.DISTRO.isDebugEnabled()) { Loggers.DISTRO.debug(\"server list is: {}\", targetServer); } // 根据 type 来同步数据 for (String each : distroComponentHolder.getDataStorageTypes()) { verifyForDataStorage(each, targetServer); } } catch (Exception e) { Loggers.DISTRO.error(\"[DISTRO-FAILED] verify task failed.\", e); } } private void verifyForDataStorage(String type, List\u003cMember\u003e targetServer) { // DistroDataStorage 数据存储，目前只有一个实现类 DistroClientDataProcessor DistroDataStorage dataStorage = distroComponentHolder.findDataStorage(type); if (!dataStorage.isFinishInitial()) { Loggers.DISTRO.warn(\"data storage {} has not finished initial step, do not send verify data\", dataStorage.getClass().getSimpleName()); return; } // 获取当前节点的数据，很重要 List\u003cDistroData\u003e verifyData = dataStorage.getVerifyData(); if (null == verifyData || verifyData.isEmpty()) { return; } // 同步给其他节点 for (Member member : targetServer) { DistroTransportAgent agent = distroComponentHolder.findTransportAgent(type); if (null == agent) { continue; } // 同步数据, 执行 DistroVerifyExecuteTask executeTaskExecuteEngine.addTask(member.getAddress() + type, new DistroVerifyExecuteTask(agent, verifyData, member.getAddress(), type)); } } } 源码位置: com.alibaba.nacos.naming.consistency.ephemeral.distro.v2.DistroClientDataProcessor#getVerifyData // 获取当前节点的数据 public List\u003cDistroData\u003e getVerifyData() { List\u003cDistroData\u003e result = null; for (String each : clientManager.allClientId()) { Client client = clientManager.getClient(each); if (null == client || !client.isEphemeral()) { continue; } // 是当前节点的数据 if (clientManager.isResponsibleClient(client)) { // clientId 和 reversion 来校验数据，并进行续约 DistroClientVerifyInfo verifyData = new DistroClientVerifyInfo(client.getClientId(), client.getRevision()); DistroKey distroKey = new DistroKey(client.getClientId(), TYPE); DistroData data = new DistroData(distroKey, ApplicationUtils.getBean(Serializer.class).serialize(verifyData)); data.setType(DataOperation.VERIFY); if (result == null) { result = new LinkedList\u003c\u003e(); } result.add(data); } } return result; } 源码位置: com.alibaba.nacos.core.distributed.distro.task.verify.DistroVerifyExecuteTask#run // 同步数据 // 数据会包装为 DistroDataRequest, 会被 DistroDataRequestHandler 处理 @Override public void run() { for (DistroData each : verifyData) { try { if (transportAgent.supportCallbackTransport()) { doSyncVerifyDataWithCallback(each); } else { doSyncVerifyData(each); } } catch (Exception e) { Loggers.DISTRO .error(\"[DISTRO-FAILED] verify data for type {} to {} failed.\", resourceType, targetServer, e); } } } 源码位置: com.alibaba.nacos.naming.remote.rpc.handler.DistroDataRequestHandler#handle // DistroDataRequestHandler 处理 DistroDataRequest // 这些方法都是委托 DistroProtocol 类来完成具体的调用 @Override public DistroDataResponse handle(DistroDataRequest request, RequestMeta meta) throws NacosException { try { switch (request.getDataOperation()) { case VERIFY: return handleVerify(request.getDistroData(), meta); case SNAPSHOT: return handleSnapshot(); case ADD: case CHANGE: case DELETE: return handleSyncData(request.getDistroData()); case QUERY: return handleQueryData(request.getDistroData()); default: return new DistroDataResponse(); } } catch (Exception e) { Loggers.DISTRO.error(\"[DISTRO-FAILED] distro handle with exception\", e); DistroDataResponse result = new DistroDataResponse(); result.setErrorCode(ResponseCode.FAIL.getCode()); result.setMessage(\"handle distro request with exception\"); return result; } } // 处理 VERIFY 请求 private DistroDataResponse handleVerify(DistroData distroData, RequestMeta meta) { DistroDataResponse re","date":"2023-08-29 08:00:00","objectID":"/ooooo-notes/%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90-nacos-ap-%E5%8D%8F%E8%AE%AE-distro/:2:0","tags":["nacos","source code","源码分析 nacos 系列"],"title":"源码分析 nacos AP 协议 Distro","uri":"/ooooo-notes/%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90-nacos-ap-%E5%8D%8F%E8%AE%AE-distro/"},{"categories":["源码分析 nacos 系列"],"content":"DistroLoadDataTask 加载数据 源码位置: com.alibaba.nacos.core.distributed.distro.task.load.DistroLoadDataTask#run @Override public void run() { try { load(); if (!checkCompleted()) { GlobalExecutor.submitLoadDataTask(this, distroConfig.getLoadDataRetryDelayMillis()); } else { loadCallback.onSuccess(); Loggers.DISTRO.info(\"[DISTRO-INIT] load snapshot data success\"); } } catch (Exception e) { loadCallback.onFailed(e); Loggers.DISTRO.error(\"[DISTRO-INIT] load snapshot data failed. \", e); } } // 加载节点数据 private void load() throws Exception { // 检查节点列表 while (memberManager.allMembersWithoutSelf().isEmpty()) { Loggers.DISTRO.info(\"[DISTRO-INIT] waiting server list init...\"); TimeUnit.SECONDS.sleep(1); } while (distroComponentHolder.getDataStorageTypes().isEmpty()) { Loggers.DISTRO.info(\"[DISTRO-INIT] waiting distro data storage register...\"); TimeUnit.SECONDS.sleep(1); } // 从远端加载快照数据, 用于服务快速启动 for (String each : distroComponentHolder.getDataStorageTypes()) { if (!loadCompletedMap.containsKey(each) || !loadCompletedMap.get(each)) { loadCompletedMap.put(each, loadAllDataSnapshotFromRemote(each)); } } } ","date":"2023-08-29 08:00:00","objectID":"/ooooo-notes/%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90-nacos-ap-%E5%8D%8F%E8%AE%AE-distro/:3:0","tags":["nacos","source code","源码分析 nacos 系列"],"title":"源码分析 nacos AP 协议 Distro","uri":"/ooooo-notes/%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90-nacos-ap-%E5%8D%8F%E8%AE%AE-distro/"},{"categories":["源码分析 nacos 系列"],"content":"DistroFilter 拦截请求 源码位置: com.alibaba.nacos.naming.web.DistroFilter#doFilter // DistroFilter 会拦截所有的请求 @Override public void doFilter(ServletRequest servletRequest, ServletResponse servletResponse, FilterChain filterChain) throws IOException, ServletException { ReuseHttpServletRequest req = new ReuseHttpServletRequest((HttpServletRequest) servletRequest); HttpServletResponse resp = (HttpServletResponse) servletResponse; String urlString = req.getRequestURI(); if (StringUtils.isNotBlank(req.getQueryString())) { urlString += \"?\" + req.getQueryString(); } try { // 获取请求对应的方法 Method method = controllerMethodsCache.getMethod(req); String path = new URI(req.getRequestURI()).getPath(); if (method == null) { throw new NoSuchMethodException(req.getMethod() + \" \" + path); } // 方法是否有 @CanDistro 注解，没有就直接放行，不处理 if (!method.isAnnotationPresent(CanDistro.class)) { filterChain.doFilter(req, resp); return; } // 获取请求参数中的 ip 和 port String distroTag = distroTagGenerator.getResponsibleTag(req); // 当前节点是否响应该请求，如果是，直接放行，这个很重要, 后面继续解析 if (distroMapper.responsible(distroTag)) { filterChain.doFilter(req, resp); return; } // proxy request to other server if necessary: String userAgent = req.getHeader(HttpHeaderConsts.USER_AGENT_HEADER); // 判断必须是 client 的请求，不能是 server 之间的请求 if (StringUtils.isNotBlank(userAgent) \u0026\u0026 userAgent.contains(UtilsAndCommons.NACOS_SERVER_HEADER)) { // This request is sent from peer server, should not be redirected again: Loggers.SRV_LOG.error(\"receive invalid redirect request from peer {}\", req.getRemoteAddr()); resp.sendError(HttpServletResponse.SC_BAD_REQUEST, \"receive invalid redirect request from peer \" + req.getRemoteAddr()); return; } // 获取转发节点, 根据 ip:port 的 hash 值对 serverList.size() 取余来计算 final String targetServer = distroMapper.mapSrv(distroTag); List\u003cString\u003e headerList = new ArrayList\u003c\u003e(16); Enumeration\u003cString\u003e headers = req.getHeaderNames(); while (headers.hasMoreElements()) { String headerName = headers.nextElement(); headerList.add(headerName); headerList.add(req.getHeader(headerName)); } final String body = IoUtils.toString(req.getInputStream(), StandardCharsets.UTF_8.name()); final Map\u003cString, String\u003e paramsValue = HttpClient.translateParameterMap(req.getParameterMap()); // 用 HttpClient 来转发请求到对应的节点上 RestResult\u003cString\u003e result = HttpClient .request(HTTP_PREFIX + targetServer + req.getRequestURI(), headerList, paramsValue, body, PROXY_CONNECT_TIMEOUT, PROXY_READ_TIMEOUT, StandardCharsets.UTF_8.name(), req.getMethod()); String data = result.ok() ? result.getData() : result.getMessage(); try { // 响应客户端请求 WebUtils.response(resp, data, result.getCode()); } catch (Exception ignore) { Loggers.SRV_LOG.warn(\"[DISTRO-FILTER] request failed: \" + distroMapper.mapSrv(distroTag) + urlString); } } catch (AccessControlException e) { resp.sendError(HttpServletResponse.SC_FORBIDDEN, \"access denied: \" + ExceptionUtil.getAllExceptionMsg(e)); } catch (NoSuchMethodException e) { resp.sendError(HttpServletResponse.SC_NOT_IMPLEMENTED, \"no such api:\" + req.getMethod() + \":\" + req.getRequestURI()); } catch (Exception e) { Loggers.SRV_LOG.warn(\"[DISTRO-FILTER] Server failed: \", e); resp.sendError(HttpServletResponse.SC_INTERNAL_SERVER_ERROR, \"Server failed, \" + ExceptionUtil.getAllExceptionMsg(e)); } } 源码位置: com.alibaba.nacos.naming.core.DistroMapper#responsible // 当前节点是否响应该请求 public boolean responsible(String responsibleTag) { final List\u003cString\u003e servers = healthyList; if (!switchDomain.isDistroEnabled() || EnvUtil.getStandaloneMode()) { return true; } if (CollectionUtils.isEmpty(servers)) { // means distro config is not ready yet return false; } // 当前节点地址找不到，不转发请求 String localAddress = EnvUtil.getLocalAddress(); int index = servers.indexOf(localAddress); int lastIndex = servers.lastIndexOf(localAddress); if (lastIndex \u003c 0 || index \u003c 0) { return true; } // 获取 hash 值，然后取余 int target = distroHash(responsibleTag) % servers.size(); return target \u003e= index \u0026\u0026 target \u003c= lastIndex; } ","date":"2023-08-29 08:00:00","objectID":"/ooooo-notes/%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90-nacos-ap-%E5%8D%8F%E8%AE%AE-distro/:4:0","tags":["nacos","source code","源码分析 nacos 系列"],"title":"源码分析 nacos AP 协议 Distro","uri":"/ooooo-notes/%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90-nacos-ap-%E5%8D%8F%E8%AE%AE-distro/"},{"categories":["源码分析 nacos 系列"],"content":" nacos 基于 2.2.4 版本 nacos 基于 grpc 的长连接来实现 client 和 server 的通信。 在有多个 server 端时，最初开始 client 的连接会均匀分布在 server 端，当重新上线 server 时，这时候 client 的连接会偏移到其他 server 端，这样会造成 server 端请求负载不均匀。 connection ","date":"2023-08-28 08:00:00","objectID":"/ooooo-notes/%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90-nacos-%E8%BF%9E%E6%8E%A5%E7%AE%A1%E7%90%86/:0:0","tags":["nacos","source code","源码分析 nacos 系列"],"title":"源码分析 nacos 连接管理","uri":"/ooooo-notes/%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90-nacos-%E8%BF%9E%E6%8E%A5%E7%AE%A1%E7%90%86/"},{"categories":["源码分析 nacos 系列"],"content":"client 发起连接 源码位置: com.alibaba.nacos.common.remote.client.grpc.GrpcClient#connectToServer // GrpcClient 发送 ConnectionSetupRequest 请求，建立连接 @Override public Connection connectToServer(ServerInfo serverInfo) { try { ... int port = serverInfo.getServerPort() + rpcPortOffset(); ManagedChannel managedChannel = createNewManagedChannel(serverInfo.getServerIp(), port); RequestGrpc.RequestFutureStub newChannelStubTemp = createNewChannelStub(managedChannel); if (newChannelStubTemp != null) { // 检查连接 Response response = serverCheck(serverInfo.getServerIp(), port, newChannelStubTemp); if (response == null || !(response instanceof ServerCheckResponse)) { shuntDownChannel(managedChannel); return null; } BiRequestStreamGrpc.BiRequestStreamStub biRequestStreamStub = BiRequestStreamGrpc.newStub( newChannelStubTemp.getChannel()); GrpcConnection grpcConn = new GrpcConnection(serverInfo, grpcExecutor); grpcConn.setConnectionId(((ServerCheckResponse) response).getConnectionId()); //create stream request and bind connection event to this connection. StreamObserver\u003cPayload\u003e payloadStreamObserver = bindRequestStream(biRequestStreamStub, grpcConn); // stream observer to send response to server grpcConn.setPayloadStreamObserver(payloadStreamObserver); grpcConn.setGrpcFutureServiceStub(newChannelStubTemp); grpcConn.setChannel(managedChannel); //send a setup request. // 发送 ConnectionSetupRequest 请求，建立连接 ConnectionSetupRequest conSetupRequest = new ConnectionSetupRequest(); ... grpcConn.sendRequest(conSetupRequest); //wait to register connection setup Thread.sleep(100L); return grpcConn; } return null; } catch (Exception e) { LOGGER.error(\"[{}]Fail to connect to server!,error={}\", GrpcClient.this.getName(), e); } return null; } ","date":"2023-08-28 08:00:00","objectID":"/ooooo-notes/%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90-nacos-%E8%BF%9E%E6%8E%A5%E7%AE%A1%E7%90%86/:1:0","tags":["nacos","source code","源码分析 nacos 系列"],"title":"源码分析 nacos 连接管理","uri":"/ooooo-notes/%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90-nacos-%E8%BF%9E%E6%8E%A5%E7%AE%A1%E7%90%86/"},{"categories":["源码分析 nacos 系列"],"content":"server 接受连接 源码位置: com.alibaba.nacos.core.remote.grpc.GrpcBiStreamRequestAcceptor#requestBiStream // GrpcBiStreamRequestAcceptor 处理 client 请求 @Override public void onNext(Payload payload) { ... // 处理 ConnectionSetupRequest 请求 if (parseObj instanceof ConnectionSetupRequest) { ConnectionSetupRequest setUpRequest = (ConnectionSetupRequest) parseObj; Map\u003cString, String\u003e labels = setUpRequest.getLabels(); String appName = \"-\"; if (labels != null \u0026\u0026 labels.containsKey(Constants.APPNAME)) { appName = labels.get(Constants.APPNAME); } ConnectionMeta metaInfo = new ConnectionMeta(connectionId, payload.getMetadata().getClientIp(), remoteIp, remotePort, localPort, ConnectionType.GRPC.getType(), setUpRequest.getClientVersion(), appName, setUpRequest.getLabels()); metaInfo.setTenant(setUpRequest.getTenant()); Connection connection = new GrpcConnection(metaInfo, responseObserver, GrpcServerConstants.CONTEXT_KEY_CHANNEL.get()); connection.setAbilities(setUpRequest.getAbilities()); boolean rejectSdkOnStarting = metaInfo.isSdkSource() \u0026\u0026 !ApplicationUtils.isStarted(); // 注册 connectionId 和 connection if (rejectSdkOnStarting || !connectionManager.register(connectionId, connection)) { //Not register to the connection manager if current server is over limit or server is starting. try { Loggers.REMOTE_DIGEST.warn(\"[{}]Connection register fail,reason:{}\", connectionId, rejectSdkOnStarting ? \" server is not started\" : \" server is over limited.\"); connection.request(new ConnectResetRequest(), 3000L); connection.close(); } catch (Exception e) { //Do nothing. if (connectionManager.traced(clientIp)) { Loggers.REMOTE_DIGEST .warn(\"[{}]Send connect reset request error,error={}\", connectionId, e); } } } ... } } 源码位置: com.alibaba.nacos.core.remote.ConnectionManager#register // 注册 connectionId 和 connection public synchronized boolean register(String connectionId, Connection connection) { // 判断是否连接 if (connection.isConnected()) { String clientIp = connection.getMetaInfo().clientIp; if (connections.containsKey(connectionId)) { return true; } if (checkLimit(connection)) { return false; } if (traced(clientIp)) { connection.setTraced(true); } // 添加 connection connections.put(connectionId, connection); if (!connectionForClientIp.containsKey(clientIp)) { connectionForClientIp.put(clientIp, new AtomicInteger(0)); } // 计算 clientIp 的连接数，这个数值可以供我们判断 是否需要 reloadClient (后面会介绍这个 http 请求) connectionForClientIp.get(clientIp).getAndIncrement(); // connection 回调函数 clientConnectionEventListenerRegistry.notifyClientConnected(connection); LOGGER.info(\"new connection registered successfully, connectionId = {},connection={} \", connectionId, connection); return true; } return false; } 源码位置: com.alibaba.nacos.core.remote.ClientConnectionEventListenerRegistry#notifyClientConnected // ClientConnectionEventListenerRegistry 通过 registerClientConnectionEventListener 方法来注册 // ClientConnectionEventListener 的实现类有 ConnectionBasedClientManager 和 RpcAckCallbackInitorOrCleaner, 它们都在父类中注册了 // connection 回调函数 public void notifyClientConnected(final Connection connection) { for (ClientConnectionEventListener clientConnectionEventListener : clientConnectionEventListeners) { try { clientConnectionEventListener.clientConnected(connection); } catch (Throwable throwable) { Loggers.REMOTE .info(\"[NotifyClientConnected] failed for listener {}\", clientConnectionEventListener.getName(), throwable); } } } // connection 回调函数 public void notifyClientDisConnected(final Connection connection) { for (ClientConnectionEventListener clientConnectionEventListener : clientConnectionEventListeners) { try { clientConnectionEventListener.clientDisConnected(connection); } catch (Throwable throwable) { Loggers.REMOTE.info(\"[NotifyClientDisConnected] failed for listener {}\", clientConnectionEventListener.getName(), throwable); } } } // 注册 listener public void registerClientConnectionEventListener(ClientConnectionEventListener listener) { Loggers.REMOTE.info(\"[ClientConnectionEventListenerRegistry] registry listener - \" + liste","date":"2023-08-28 08:00:00","objectID":"/ooooo-notes/%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90-nacos-%E8%BF%9E%E6%8E%A5%E7%AE%A1%E7%90%86/:2:0","tags":["nacos","source code","源码分析 nacos 系列"],"title":"源码分析 nacos 连接管理","uri":"/ooooo-notes/%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90-nacos-%E8%BF%9E%E6%8E%A5%E7%AE%A1%E7%90%86/"},{"categories":["源码分析 nacos 系列"],"content":"reloadClient 重置连接 源码位置: com.alibaba.nacos.core.controller.ServerLoaderController#reloadSingle @Secured(resource = Commons.NACOS_CORE_CONTEXT_V2 + \"/loader\", action = ActionTypes.WRITE) @GetMapping(\"/reloadClient\") public ResponseEntity\u003cString\u003e reloadSingle(@RequestParam String connectionId, @RequestParam(value = \"redirectAddress\", required = false) String redirectAddress) { // 发送 ConnectResetRequest 请求，重置客户端 connectionManager.loadSingle(connectionId, redirectAddress); return ResponseEntity.ok().body(\"success\"); } 源码位置: com.alibaba.nacos.core.remote.ConnectionManager#loadSingle // 发送 ConnectResetRequest 请求，重置客户端 public void loadSingle(String connectionId, String redirectAddress) { Connection connection = getConnection(connectionId); if (connection != null) { // isSdkSource 表示是 nacos 客户端 if (connection.getMetaInfo().isSdkSource()) { ConnectResetRequest connectResetRequest = new ConnectResetRequest(); if (StringUtils.isNotBlank(redirectAddress) \u0026\u0026 redirectAddress.contains(Constants.COLON)) { String[] split = redirectAddress.split(Constants.COLON); connectResetRequest.setServerIp(split[0]); connectResetRequest.setServerPort(split[1]); } try { // 发送 connectResetRequest 请求给客户端，会被 ConnectResetRequestHandler 处理 connection.request(connectResetRequest, 3000L); } catch (ConnectionAlreadyClosedException e) { // 发送异常，说明这个连接已经断开了，所以注销 connectionId unregister(connectionId); } catch (Exception e) { LOGGER.error(\"error occurs when expel connection, connectionId: {} \", connectionId, e); } } } } 源码位置: com.alibaba.nacos.common.remote.client.RpcClient.ConnectResetRequestHandler // ConnectResetRequestHandler 处理 ConnectResetRequest 请求 // 在 RpcClient 的 start 方法中添加了 ServerRequestHandler class ConnectResetRequestHandler implements ServerRequestHandler { @Override public Response requestReply(Request request) { if (request instanceof ConnectResetRequest) { try { synchronized (RpcClient.this) { if (isRunning()) { ConnectResetRequest connectResetRequest = (ConnectResetRequest) request; if (StringUtils.isNotBlank(connectResetRequest.getServerIp())) { ServerInfo serverInfo = resolveServerInfo( connectResetRequest.getServerIp() + Constants.COLON + connectResetRequest.getServerPort()); // 指定 serverInfo 变换 sever switchServerAsync(serverInfo, false); } else { // 变换 sever switchServerAsync(); } } } } catch (Exception e) { LoggerUtils.printIfErrorEnabled(LOGGER, \"[{}] Switch server error, {}\", rpcClientConfig.name(), e); } return new ConnectResetResponse(); } return null; } } ","date":"2023-08-28 08:00:00","objectID":"/ooooo-notes/%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90-nacos-%E8%BF%9E%E6%8E%A5%E7%AE%A1%E7%90%86/:3:0","tags":["nacos","source code","源码分析 nacos 系列"],"title":"源码分析 nacos 连接管理","uri":"/ooooo-notes/%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90-nacos-%E8%BF%9E%E6%8E%A5%E7%AE%A1%E7%90%86/"},{"categories":["源码分析 nacos 系列"],"content":"server 断开连接检查 源码位置: com.alibaba.nacos.core.remote.grpc.AddressTransportFilter#transportTerminated // AddressTransportFilter 在 BaseGrpcServer 的 startServer 方法中注册 @Override public void transportTerminated(Attributes transportAttrs) { // 获取 connectionId String connectionId = null; try { connectionId = transportAttrs.get(ATTR_TRANS_KEY_CONN_ID); } catch (Exception e) { // Ignore } if (StringUtils.isNotBlank(connectionId)) { Loggers.REMOTE_DIGEST .info(\"Connection transportTerminated,connectionId = {} \", connectionId); // 注销 connectionId, 回调 clientConnectionEventListener 接口 connectionManager.unregister(connectionId); } } ","date":"2023-08-28 08:00:00","objectID":"/ooooo-notes/%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90-nacos-%E8%BF%9E%E6%8E%A5%E7%AE%A1%E7%90%86/:4:0","tags":["nacos","source code","源码分析 nacos 系列"],"title":"源码分析 nacos 连接管理","uri":"/ooooo-notes/%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90-nacos-%E8%BF%9E%E6%8E%A5%E7%AE%A1%E7%90%86/"},{"categories":["源码分析 nacos 系列"],"content":" nacos 基于 2.2.4 版本 这里的 client 是指 nacos SDK，也就是模块 nacos-client. ","date":"2023-08-27 08:00:00","objectID":"/ooooo-notes/%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90-nacos-client-%E8%AE%A2%E9%98%85%E6%9C%8D%E5%8A%A1/:0:0","tags":["nacos","source code","源码分析 nacos 系列"],"title":"源码分析 nacos client 订阅服务","uri":"/ooooo-notes/%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90-nacos-client-%E8%AE%A2%E9%98%85%E6%9C%8D%E5%8A%A1/"},{"categories":["源码分析 nacos 系列"],"content":"订阅服务的主流程 源码位置: com.alibaba.nacos.client.naming.NacosNamingService#subscribe // NacosNamingService 订阅服务 @Override public void subscribe(String serviceName, String groupName, List\u003cString\u003e clusters, EventListener listener) throws NacosException { if (null == listener) { return; } String clusterString = StringUtils.join(clusters, \",\"); // 监听服务改变的回调函数，changeNotifier 订阅了 InstancesChangeEvent 事件 changeNotifier.registerListener(groupName, serviceName, clusterString, listener); // clientProxy 的实现类为 NamingClientProxyDelegate clientProxy.subscribe(serviceName, groupName, clusterString); } 源码位置: com.alibaba.nacos.client.naming.remote.NamingClientProxyDelegate#subscribe // NamingClientProxyDelegate 订阅服务 @Override public ServiceInfo subscribe(String serviceName, String groupName, String clusters) throws NacosException { NAMING_LOGGER.info(\"[SUBSCRIBE-SERVICE] service:{}, group:{}, clusters:{} \", serviceName, groupName, clusters); String serviceNameWithGroup = NamingUtils.getGroupedName(serviceName, groupName); String serviceKey = ServiceInfo.getKey(serviceNameWithGroup, clusters); // 注册 UpdateTask, 发送 http 请求来全量更新, 这个后面说 serviceInfoUpdateService.scheduleUpdateIfAbsent(serviceName, groupName, clusters); ServiceInfo result = serviceInfoHolder.getServiceInfoMap().get(serviceKey); if (null == result || !isSubscribed(serviceName, groupName, clusters)) { // grpc 订阅服务, 返回 serviceInfo result = grpcClientProxy.subscribe(serviceName, groupName, clusters); } // 处理 serviceInfo, 发布 InstancesChangeEvent 事件 serviceInfoHolder.processServiceInfo(result); return result; } 源码位置: com.alibaba.nacos.client.naming.remote.gprc.NamingGrpcClientProxy#subscribe // grpc 订阅服务 @Override public ServiceInfo subscribe(String serviceName, String groupName, String clusters) throws NacosException { if (NAMING_LOGGER.isDebugEnabled()) { NAMING_LOGGER.debug(\"[GRPC-SUBSCRIBE] service:{}, group:{}, cluster:{} \", serviceName, groupName, clusters); } // 标记服务要订阅，在 redoService 的定时任务中重新订阅 redoService.cacheSubscriberForRedo(serviceName, groupName, clusters); // 订阅服务 return doSubscribe(serviceName, groupName, clusters); } // redoService.cacheSubscriberForRedo public void cacheSubscriberForRedo(String serviceName, String groupName, String cluster) { String key = ServiceInfo.getKey(NamingUtils.getGroupedName(serviceName, groupName), cluster); SubscriberRedoData redoData = SubscriberRedoData.build(serviceName, groupName, cluster); synchronized (subscribes) { // 标记订阅 subscribes.put(key, redoData); } } // 订阅服务 public ServiceInfo doSubscribe(String serviceName, String groupName, String clusters) throws NacosException { SubscribeServiceRequest request = new SubscribeServiceRequest(namespaceId, groupName, serviceName, clusters, true); // 发送 SubscribeServiceRequest 请求，会被 SubscribeServiceRequestHandler 处理 SubscribeServiceResponse response = requestToServer(request, SubscribeServiceResponse.class); // 标记服务已订阅 redoService.subscriberRegistered(serviceName, groupName, clusters); return response.getServiceInfo(); } 源码位置: com.alibaba.nacos.naming.remote.rpc.handler.SubscribeServiceRequestHandler#handle // SubscribeServiceRequestHandler 处理请求 @Override @Secured(action = ActionTypes.READ) public SubscribeServiceResponse handle(SubscribeServiceRequest request, RequestMeta meta) throws NacosException { String namespaceId = request.getNamespace(); String serviceName = request.getServiceName(); String groupName = request.getGroupName(); String app = request.getHeader(\"app\", \"unknown\"); String groupedServiceName = NamingUtils.getGroupedName(serviceName, groupName); Service service = Service.newService(namespaceId, groupName, serviceName, true); // 把订阅的信息包装为 Subscriber 对象 Subscriber subscriber = new Subscriber(meta.getClientIp(), meta.getClientVersion(), app, meta.getClientIp(), namespaceId, groupedServiceName, 0, request.getClusters()); // 第一次订阅要返回对应的 serviceInfo ServiceInfo serviceInfo = ServiceUtil.selectInstancesWithHealthyProtection(serviceStorage.getData(service), metadataManager.getServiceMetadata(servic","date":"2023-08-27 08:00:00","objectID":"/ooooo-notes/%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90-nacos-client-%E8%AE%A2%E9%98%85%E6%9C%8D%E5%8A%A1/:1:0","tags":["nacos","source code","源码分析 nacos 系列"],"title":"源码分析 nacos client 订阅服务","uri":"/ooooo-notes/%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90-nacos-client-%E8%AE%A2%E9%98%85%E6%9C%8D%E5%8A%A1/"},{"categories":["源码分析 nacos 系列"],"content":"grpc 订阅处理 源码位置: com.alibaba.nacos.client.naming.remote.gprc.NamingGrpcClientProxy#start // start 方法在 NamingGrpcClientProxy 构造函数中调用 private void start(ServerListFactory serverListFactory, ServiceInfoHolder serviceInfoHolder) throws NacosException { // serverListFactory 来选择服务 rpcClient.serverListFactory(serverListFactory); // 监听 connectionEvent 事件 rpcClient.registerConnectionListener(redoService); // client 处理 server 请求, 重点看 NamingPushRequestHandler rpcClient.registerServerRequestHandler(new NamingPushRequestHandler(serviceInfoHolder)); rpcClient.start(); // 注册事件订阅 NotifyCenter.registerSubscriber(this); } 源码位置: com.alibaba.nacos.client.naming.remote.gprc.NamingPushRequestHandler#requestReply // client 处理 server 请求 @Override public Response requestReply(Request request) { if (request instanceof NotifySubscriberRequest) { // 服务实例变动了，服务端推送 serviceInfo NotifySubscriberRequest notifyRequest = (NotifySubscriberRequest) request; // 处理 serviceInfo, 发布 InstancesChangeEvent 事件 serviceInfoHolder.processServiceInfo(notifyRequest.getServiceInfo()); return new NotifySubscriberResponse(); } return null; } 源码位置: com.alibaba.nacos.client.naming.event.InstancesChangeNotifier#onEvent // InstancesChangeNotifier 监听 InstancesChangeEvent 事件 @Override public void onEvent(InstancesChangeEvent event) { String key = ServiceInfo .getKey(NamingUtils.getGroupedName(event.getServiceName(), event.getGroupName()), event.getClusters()); ConcurrentHashSet\u003cEventListener\u003e eventListeners = listenerMap.get(key); if (CollectionUtils.isEmpty(eventListeners)) { return; } for (final EventListener listener : eventListeners) { // 遍历回调函数 final com.alibaba.nacos.api.naming.listener.Event namingEvent = transferToNamingEvent(event); if (listener instanceof AbstractEventListener \u0026\u0026 ((AbstractEventListener) listener).getExecutor() != null) { ((AbstractEventListener) listener).getExecutor().execute(() -\u003e listener.onEvent(namingEvent)); } else { listener.onEvent(namingEvent); } } } ","date":"2023-08-27 08:00:00","objectID":"/ooooo-notes/%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90-nacos-client-%E8%AE%A2%E9%98%85%E6%9C%8D%E5%8A%A1/:2:0","tags":["nacos","source code","源码分析 nacos 系列"],"title":"源码分析 nacos client 订阅服务","uri":"/ooooo-notes/%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90-nacos-client-%E8%AE%A2%E9%98%85%E6%9C%8D%E5%8A%A1/"},{"categories":["源码分析 nacos 系列"],"content":"UpdateTask 全量更新 源码位置: com.alibaba.nacos.client.naming.core.ServiceInfoUpdateService.UpdateTask#run // UpdateTask 定时拉取全量的 instances @Override public void run() { long delayTime = DEFAULT_DELAY; try { // 判断是否订阅服务 if (!changeNotifier.isSubscribed(groupName, serviceName, clusters) \u0026\u0026 !futureMap.containsKey( serviceKey)) { NAMING_LOGGER.info(\"update task is stopped, service:{}, clusters:{}\", groupedServiceName, clusters); isCancel = true; return; } ServiceInfo serviceObj = serviceInfoHolder.getServiceInfoMap().get(serviceKey); // 第一次拉取 if (serviceObj == null) { serviceObj = namingClientProxy.queryInstancesOfService(serviceName, groupName, clusters, 0, false); serviceInfoHolder.processServiceInfo(serviceObj); lastRefTime = serviceObj.getLastRefTime(); return; } // 判断过期时间，然后再拉取 if (serviceObj.getLastRefTime() \u003c= lastRefTime) { serviceObj = namingClientProxy.queryInstancesOfService(serviceName, groupName, clusters, 0, false); serviceInfoHolder.processServiceInfo(serviceObj); } lastRefTime = serviceObj.getLastRefTime(); if (CollectionUtils.isEmpty(serviceObj.getHosts())) { incFailCount(); return; } // TODO multiple time can be configured. // 更新延时时间 delayTime = serviceObj.getCacheMillis() * DEFAULT_UPDATE_CACHE_TIME_MULTIPLE; resetFailCount(); } catch (NacosException e) { handleNacosException(e); } catch (Throwable e) { handleUnknownException(e); } finally { if (!isCancel) { // 下一次拉取任务 executor.schedule(this, Math.min(delayTime \u003c\u003c failCount, DEFAULT_DELAY * 60), TimeUnit.MILLISECONDS); } } } ","date":"2023-08-27 08:00:00","objectID":"/ooooo-notes/%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90-nacos-client-%E8%AE%A2%E9%98%85%E6%9C%8D%E5%8A%A1/:3:0","tags":["nacos","source code","源码分析 nacos 系列"],"title":"源码分析 nacos client 订阅服务","uri":"/ooooo-notes/%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90-nacos-client-%E8%AE%A2%E9%98%85%E6%9C%8D%E5%8A%A1/"},{"categories":["源码分析 nacos 系列"],"content":" nacos 基于 2.2.4 版本 这里的 client 是指 nacos SDK，也就是模块 nacos-client. ","date":"2023-08-26 08:00:00","objectID":"/ooooo-notes/%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90-nacos-client-%E6%B3%A8%E9%94%80%E5%AE%9E%E4%BE%8B/:0:0","tags":["nacos","source code","源码分析 nacos 系列"],"title":"源码分析 nacos client 注销实例","uri":"/ooooo-notes/%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90-nacos-client-%E6%B3%A8%E9%94%80%E5%AE%9E%E4%BE%8B/"},{"categories":["源码分析 nacos 系列"],"content":"注销实例的主流程 源码位置: com.alibaba.nacos.client.naming.NacosNamingService#deregisterInstance // 入口类: NacosNamingService @Override public void deregisterInstance(String serviceName, String groupName, Instance instance) throws NacosException { // clientProxy 的实现类为 NamingClientProxyDelegate clientProxy.deregisterService(serviceName, groupName, instance); } 源码位置: com.alibaba.nacos.client.naming.remote.NamingClientProxyDelegate#deregisterService // NamingClientProxyDelegate 注销实例 // 临时实例, grpcClientProxy // 持久化实例, httpClientProxy @Override public void deregisterService(String serviceName, String groupName, Instance instance) throws NacosException { getExecuteClientProxy(instance).deregisterService(serviceName, groupName, instance); } // 根据是否为临时实例来获取 clientProxy private NamingClientProxy getExecuteClientProxy(Instance instance) { return instance.isEphemeral() ? grpcClientProxy : httpClientProxy; } ","date":"2023-08-26 08:00:00","objectID":"/ooooo-notes/%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90-nacos-client-%E6%B3%A8%E9%94%80%E5%AE%9E%E4%BE%8B/:1:0","tags":["nacos","source code","源码分析 nacos 系列"],"title":"源码分析 nacos client 注销实例","uri":"/ooooo-notes/%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90-nacos-client-%E6%B3%A8%E9%94%80%E5%AE%9E%E4%BE%8B/"},{"categories":["源码分析 nacos 系列"],"content":"注销临时实例 源码位置: com.alibaba.nacos.client.naming.remote.gprc.NamingGrpcClientProxy#deregisterService @Override public void deregisterService(String serviceName, String groupName, Instance instance) throws NacosException { NAMING_LOGGER .info(\"[DEREGISTER-SERVICE] {} deregistering service {} with instance: {}\", namespaceId, serviceName, instance); // 标记 instance 要注销，可以在 redoService 定时任务重试 redoService.instanceDeregister(serviceName, groupName); // 注销实例 doDeregisterService(serviceName, groupName, instance); } // redoService.instanceDeregister public void instanceDeregister(String serviceName, String groupName) { String key = NamingUtils.getGroupedName(serviceName, groupName); synchronized (registeredInstances) { InstanceRedoData redoData = registeredInstances.get(key); if (null != redoData) { // 设置注销中 redoData.setUnregistering(true); // 设置最终状态 redoData.setExpectedRegistered(false); } } } // 注销实例 public void doDeregisterService(String serviceName, String groupName, Instance instance) throws NacosException { InstanceRequest request = new InstanceRequest(namespaceId, serviceName, groupName, NamingRemoteConstants.DE_REGISTER_INSTANCE, instance); // 发送 InstanceRequest 请求，会被 InstanceRequestHandler 处理 requestToServer(request, Response.class); // 标记 instance 已经注销 redoService.instanceDeregistered(serviceName, groupName); } 源码位置: com.alibaba.nacos.naming.remote.rpc.handler.InstanceRequestHandler#handle // InstanceRequestHandler 处理请求 @Override @Secured(action = ActionTypes.WRITE) public InstanceResponse handle(InstanceRequest request, RequestMeta meta) throws NacosException { Service service = Service .newService(request.getNamespace(), request.getGroupName(), request.getServiceName(), true); switch (request.getType()) { ... case NamingRemoteConstants.DE_REGISTER_INSTANCE: return deregisterInstance(service, request, meta); } } // 注销实例 private InstanceResponse deregisterInstance(Service service, InstanceRequest request, RequestMeta meta) { // 这个逻辑在【注销实例】章节中分析过了 clientOperationService.deregisterInstance(service, request.getInstance(), meta.getConnectionId()); NotifyCenter.publishEvent(new DeregisterInstanceTraceEvent(System.currentTimeMillis(), meta.getClientIp(), true, DeregisterInstanceReason.REQUEST, service.getNamespace(), service.getGroup(), service.getName(), request.getInstance().getIp(), request.getInstance().getPort())); return new InstanceResponse(NamingRemoteConstants.DE_REGISTER_INSTANCE); } ","date":"2023-08-26 08:00:00","objectID":"/ooooo-notes/%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90-nacos-client-%E6%B3%A8%E9%94%80%E5%AE%9E%E4%BE%8B/:2:0","tags":["nacos","source code","源码分析 nacos 系列"],"title":"源码分析 nacos client 注销实例","uri":"/ooooo-notes/%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90-nacos-client-%E6%B3%A8%E9%94%80%E5%AE%9E%E4%BE%8B/"},{"categories":["源码分析 nacos 系列"],"content":"注销持久化实例 源码位置: com.alibaba.nacos.client.naming.remote.http.NamingHttpClientProxy#deregisterService // 发送一个 http 请求 @Override public void deregisterService(String serviceName, String groupName, Instance instance) throws NacosException { NAMING_LOGGER .info(\"[DEREGISTER-SERVICE] {} deregistering service {} with instance: {}\", namespaceId, serviceName, instance); if (instance.isEphemeral()) { return; } final Map\u003cString, String\u003e params = new HashMap\u003c\u003e(16); params.put(CommonParams.NAMESPACE_ID, namespaceId); params.put(CommonParams.SERVICE_NAME, NamingUtils.getGroupedName(serviceName, groupName)); params.put(CommonParams.CLUSTER_NAME, instance.getClusterName()); params.put(IP_PARAM, instance.getIp()); params.put(PORT_PARAM, String.valueOf(instance.getPort())); params.put(EPHEMERAL_PARAM, String.valueOf(instance.isEphemeral())); reqApi(UtilAndComs.nacosUrlInstance, params, HttpMethod.DELETE); } ","date":"2023-08-26 08:00:00","objectID":"/ooooo-notes/%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90-nacos-client-%E6%B3%A8%E9%94%80%E5%AE%9E%E4%BE%8B/:3:0","tags":["nacos","source code","源码分析 nacos 系列"],"title":"源码分析 nacos client 注销实例","uri":"/ooooo-notes/%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90-nacos-client-%E6%B3%A8%E9%94%80%E5%AE%9E%E4%BE%8B/"},{"categories":["源码分析 nacos 系列"],"content":" nacos 基于 2.2.4 版本 这里的 client 是指 nacos SDK，也就是模块 nacos-client. ","date":"2023-08-25 08:00:00","objectID":"/ooooo-notes/%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90-nacos-client-%E6%B3%A8%E5%86%8C%E5%AE%9E%E4%BE%8B/:0:0","tags":["nacos","source code","源码分析 nacos 系列"],"title":"源码分析 nacos client 注册实例","uri":"/ooooo-notes/%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90-nacos-client-%E6%B3%A8%E5%86%8C%E5%AE%9E%E4%BE%8B/"},{"categories":["源码分析 nacos 系列"],"content":"注册实例的主流程 源码位置: com.alibaba.nacos.client.naming.NacosNamingService // NacosNamingService 注册实例，最后由 NamingClientProxyDelegate 来注册。 @Override public void registerInstance(String serviceName, String groupName, Instance instance) throws NacosException { // 检查参数 NamingUtils.checkInstanceIsLegal(instance); // 注册实例，clientProxy 实现类为 NamingClientProxyDelegate，具体分为 httpClientProxy 和 grpcClientProxy clientProxy.registerService(serviceName, groupName, instance); } 源码位置: com.alibaba.nacos.client.naming.remote.NamingClientProxyDelegate#registerService // 根据是否为临时实例选择对应的实现来注册实例 @Override public void registerService(String serviceName, String groupName, Instance instance) throws NacosException { getExecuteClientProxy(instance).registerService(serviceName, groupName, instance); } // 临时实例就是 grpcClientProxy, 因为 grpc 可以通过长连接来维持，连接断开，说明临时实例注销了 // 持久化实例就是 httpClientProxy, 因为持久化实例只需要一次请求就可以了，后续不需要维持 private NamingClientProxy getExecuteClientProxy(Instance instance) { return instance.isEphemeral() ? grpcClientProxy : httpClientProxy; } ","date":"2023-08-25 08:00:00","objectID":"/ooooo-notes/%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90-nacos-client-%E6%B3%A8%E5%86%8C%E5%AE%9E%E4%BE%8B/:1:0","tags":["nacos","source code","源码分析 nacos 系列"],"title":"源码分析 nacos client 注册实例","uri":"/ooooo-notes/%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90-nacos-client-%E6%B3%A8%E5%86%8C%E5%AE%9E%E4%BE%8B/"},{"categories":["源码分析 nacos 系列"],"content":"注册临时实例 源码位置: com.alibaba.nacos.client.naming.remote.gprc.NamingGrpcClientProxy#registerService @Override public void registerService(String serviceName, String groupName, Instance instance) throws NacosException { NAMING_LOGGER.info(\"[REGISTER-SERVICE] {} registering service {} with instance {}\", namespaceId, serviceName, instance); // 标记这个 instance 要注册，在连接断开之后通过 redoService 的定时任务重新注册 redoService.cacheInstanceForRedo(serviceName, groupName, instance); // 注册实例 doRegisterService(serviceName, groupName, instance); } // redoService.cacheInstanceForRedo public void cacheInstanceForRedo(String serviceName, String groupName, Instance instance) { String key = NamingUtils.getGroupedName(serviceName, groupName); InstanceRedoData redoData = InstanceRedoData.build(serviceName, groupName, instance); synchronized (registeredInstances) { registeredInstances.put(key, redoData); } } // 注册实例 public void doRegisterService(String serviceName, String groupName, Instance instance) throws NacosException { InstanceRequest request = new InstanceRequest(namespaceId, serviceName, groupName, NamingRemoteConstants.REGISTER_INSTANCE, instance); // 发送 InstanceRequest 请求，会被 InstanceRequestHandler 处理 requestToServer(request, Response.class); // 标记 instance 已经注册过 redoService.instanceRegistered(serviceName, groupName); } 源码位置: com.alibaba.nacos.naming.remote.rpc.handler.InstanceRequestHandler#handle // InstanceRequestHandler 处理请求 @Override @Secured(action = ActionTypes.WRITE) public InstanceResponse handle(InstanceRequest request, RequestMeta meta) throws NacosException { Service service = Service .newService(request.getNamespace(), request.getGroupName(), request.getServiceName(), true); switch (request.getType()) { case NamingRemoteConstants.REGISTER_INSTANCE: return registerInstance(service, request, meta); ... } } private InstanceResponse registerInstance(Service service, InstanceRequest request, RequestMeta meta) throws NacosException { // 注册实例，这个逻辑在【注册实例】章节分析过了 clientOperationService.registerInstance(service, request.getInstance(), meta.getConnectionId()); NotifyCenter.publishEvent(new RegisterInstanceTraceEvent(System.currentTimeMillis(), meta.getClientIp(), true, service.getNamespace(), service.getGroup(), service.getName(), request.getInstance().getIp(), request.getInstance().getPort())); return new InstanceResponse(NamingRemoteConstants.REGISTER_INSTANCE); } ","date":"2023-08-25 08:00:00","objectID":"/ooooo-notes/%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90-nacos-client-%E6%B3%A8%E5%86%8C%E5%AE%9E%E4%BE%8B/:2:0","tags":["nacos","source code","源码分析 nacos 系列"],"title":"源码分析 nacos client 注册实例","uri":"/ooooo-notes/%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90-nacos-client-%E6%B3%A8%E5%86%8C%E5%AE%9E%E4%BE%8B/"},{"categories":["源码分析 nacos 系列"],"content":"注册持久化实例 源码位置: com.alibaba.nacos.client.naming.remote.http.NamingHttpClientProxy#registerService // NamingHttpClientProxy 注册实例 // 发送 http 请求, 会被 com.alibaba.nacos.naming.controllers.InstanceController#register 处理, 这个逻辑在【注册实例】章节分析过了 public void registerService(String serviceName, String groupName, Instance instance) throws NacosException { NAMING_LOGGER.info(\"[REGISTER-SERVICE] {} registering service {} with instance: {}\", namespaceId, serviceName, instance); String groupedServiceName = NamingUtils.getGroupedName(serviceName, groupName); if (instance.isEphemeral()) { throw new UnsupportedOperationException( \"Do not support register ephemeral instances by HTTP, please use gRPC replaced.\"); } final Map\u003cString, String\u003e params = new HashMap\u003c\u003e(32); params.put(CommonParams.NAMESPACE_ID, namespaceId); params.put(CommonParams.SERVICE_NAME, groupedServiceName); params.put(CommonParams.GROUP_NAME, groupName); params.put(CommonParams.CLUSTER_NAME, instance.getClusterName()); params.put(IP_PARAM, instance.getIp()); params.put(PORT_PARAM, String.valueOf(instance.getPort())); params.put(WEIGHT_PARAM, String.valueOf(instance.getWeight())); params.put(REGISTER_ENABLE_PARAM, String.valueOf(instance.isEnabled())); params.put(HEALTHY_PARAM, String.valueOf(instance.isHealthy())); params.put(EPHEMERAL_PARAM, String.valueOf(instance.isEphemeral())); params.put(META_PARAM, JacksonUtils.toJson(instance.getMetadata())); reqApi(UtilAndComs.nacosUrlInstance, params, HttpMethod.POST); } ","date":"2023-08-25 08:00:00","objectID":"/ooooo-notes/%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90-nacos-client-%E6%B3%A8%E5%86%8C%E5%AE%9E%E4%BE%8B/:3:0","tags":["nacos","source code","源码分析 nacos 系列"],"title":"源码分析 nacos client 注册实例","uri":"/ooooo-notes/%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90-nacos-client-%E6%B3%A8%E5%86%8C%E5%AE%9E%E4%BE%8B/"},{"categories":["源码分析 nacos 系列"],"content":"redoService 定时任务 源码位置: com.alibaba.nacos.client.naming.remote.gprc.redo.NamingGrpcRedoService // NamingGrpcRedoService 构造函数 public NamingGrpcRedoService(NamingGrpcClientProxy clientProxy) { this.redoExecutor = new ScheduledThreadPoolExecutor(REDO_THREAD, new NameThreadFactory(REDO_THREAD_NAME)); // 定时调度 RedoScheduledTask this.redoExecutor.scheduleWithFixedDelay(new RedoScheduledTask(clientProxy, this), DEFAULT_REDO_DELAY, DEFAULT_REDO_DELAY, TimeUnit.MILLISECONDS); } 源码位置: com.alibaba.nacos.client.naming.remote.gprc.redo.RedoScheduledTask#run // RedoScheduledTask 定时任务 @Override public void run() { // 判断是否已连接，通过 NamingGrpcRedoService#onConnected 来改变状态 if (!redoService.isConnected()) { LogUtils.NAMING_LOGGER.warn(\"Grpc Connection is disconnect, skip current redo task\"); return; } // 因为 grpc 连接可能会断，所以需要重新注册实例和订阅服务 try { // 重新注册实例 redoForInstances(); // 重新订阅服务 redoForSubscribes(); } catch (Exception e) { LogUtils.NAMING_LOGGER.warn(\"Redo task run with unexpected exception: \", e); } } ","date":"2023-08-25 08:00:00","objectID":"/ooooo-notes/%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90-nacos-client-%E6%B3%A8%E5%86%8C%E5%AE%9E%E4%BE%8B/:4:0","tags":["nacos","source code","源码分析 nacos 系列"],"title":"源码分析 nacos client 注册实例","uri":"/ooooo-notes/%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90-nacos-client-%E6%B3%A8%E5%86%8C%E5%AE%9E%E4%BE%8B/"},{"categories":["源码分析 nacos 系列"],"content":" nacos 基于 2.2.4 版本 nacos 的 grpc client 使用的是生成的代码，位置在 com.alibaba.nacos.api.grpc.auto ","date":"2023-08-24 08:00:00","objectID":"/ooooo-notes/%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90-nacos-grpc-client-%E8%AE%BE%E8%AE%A1/:0:0","tags":["nacos","source code","源码分析 nacos 系列"],"title":"源码分析 nacos grpc client 设计","uri":"/ooooo-notes/%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90-nacos-grpc-client-%E8%AE%BE%E8%AE%A1/"},{"categories":["源码分析 nacos 系列"],"content":"client 的启动 源码位置: com.alibaba.nacos.common.remote.client.RpcClient#start // RpcClient 是父类，完成了基本功能，比如重试、连接事件 public final void start() throws NacosException { // 初始化状态变为启动中状态 boolean success = rpcClientStatus.compareAndSet(RpcClientStatus.INITIALIZED, RpcClientStatus.STARTING); if (!success) { return; } // 初始化客户端事件线程池 clientEventExecutor = new ScheduledThreadPoolExecutor(2, r -\u003e { ... }); // connection event consumer. // 连接事件 clientEventExecutor.submit(() -\u003e { while (!clientEventExecutor.isTerminated() \u0026\u0026 !clientEventExecutor.isShutdown()) { ConnectionEvent take; try { take = eventLinkedBlockingQueue.take(); if (take.isConnected()) { notifyConnected(); } else if (take.isDisConnected()) { notifyDisConnected(); } } catch (Throwable e) { // Do nothing } } }); // 重连事件 clientEventExecutor.submit(() -\u003e { while (true) { try { if (isShutdown()) { break; } ReconnectContext reconnectContext = reconnectionSignal.poll(rpcClientConfig.connectionKeepAlive(), TimeUnit.MILLISECONDS); if (reconnectContext == null) { // check alive time. // 进行健康检查，发送 HealthCheckRequest 请求 if (System.currentTimeMillis() - lastActiveTimeStamp \u003e= rpcClientConfig.connectionKeepAlive()) { boolean isHealthy = healthCheck(); if (!isHealthy) { // 判断当前连接 if (currentConnection == null) { continue; } LoggerUtils.printIfInfoEnabled(LOGGER, \"[{}] Server healthy check fail, currentConnection = {}\", rpcClientConfig.name(), currentConnection.getConnectionId()); RpcClientStatus rpcClientStatus = RpcClient.this.rpcClientStatus.get(); // 已经关闭了，无需检查了 if (RpcClientStatus.SHUTDOWN.equals(rpcClientStatus)) { break; } boolean statusFLowSuccess = RpcClient.this.rpcClientStatus.compareAndSet( rpcClientStatus, RpcClientStatus.UNHEALTHY); if (statusFLowSuccess) { reconnectContext = new ReconnectContext(null, false); } else { continue; } } else { lastActiveTimeStamp = System.currentTimeMillis(); continue; } } else { continue; } } // 检查服务是否已经删除 if (reconnectContext.serverInfo != null) { // clear recommend server if server is not in server list. boolean serverExist = false; for (String server : getServerListFactory().getServerList()) { ServerInfo serverInfo = resolveServerInfo(server); if (serverInfo.getServerIp().equals(reconnectContext.serverInfo.getServerIp())) { serverExist = true; reconnectContext.serverInfo.serverPort = serverInfo.serverPort; break; } } if (!serverExist) { LoggerUtils.printIfInfoEnabled(LOGGER, \"[{}] Recommend server is not in server list, ignore recommend server {}\", rpcClientConfig.name(), reconnectContext.serverInfo.getAddress()); // 赋值为 null，会挑选下一个服务来进行重连 reconnectContext.serverInfo = null; } } // 进行重连 reconnect(reconnectContext.serverInfo, reconnectContext.onRequestFail); } catch (Throwable throwable) { // Do nothing } } }); // connect to server, try to connect to server sync retryTimes times, async starting if failed. Connection connectToServer = null; rpcClientStatus.set(RpcClientStatus.STARTING); // 重试 int startUpRetryTimes = rpcClientConfig.retryTimes(); while (startUpRetryTimes \u003e 0 \u0026\u0026 connectToServer == null) { try { startUpRetryTimes--; // 获取下一个 serverInfo，因为 nacos 的地址可以配置多个，或者配置一个 http 地址来动态获取 ServerInfo serverInfo = nextRpcServer(); LoggerUtils.printIfInfoEnabled(LOGGER, \"[{}] Try to connect to server on start up, server: {}\", rpcClientConfig.name(), serverInfo); // 连接服务，由子类来实现，接下来继续看 connectToServer = connectToServer(serverInfo); } catch (Throwable e) { LoggerUtils.printIfWarnEnabled(LOGGER, \"[{}] Fail to connect to server on start up, error message = {}, start up retry times left: {}\", rpcClientConfig.name(), e.getMessage(), startUpRetryTimes, e); } } // 向 eventLinkedBlockingQueue 队列中添加 ConnectionEvent, 产生连接事件 if (connectToServer != null) { LoggerUtils.printIfInfoEnabled(LOGGER, \"[{}] Success to connect to server [{}] on start up, connectionId = {}\", rpcClientConfig.name(), connectToServer.serverInfo.getAddress(), connectToServer.getConnectionId()); // 设置当前连接 this.currentConnection = connectToServer; rpcClientStatus.set(RpcClientStatus.RUNNING); eventLinke","date":"2023-08-24 08:00:00","objectID":"/ooooo-notes/%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90-nacos-grpc-client-%E8%AE%BE%E8%AE%A1/:1:0","tags":["nacos","source code","源码分析 nacos 系列"],"title":"源码分析 nacos grpc client 设计","uri":"/ooooo-notes/%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90-nacos-grpc-client-%E8%AE%BE%E8%AE%A1/"},{"categories":["源码分析 nacos 系列"],"content":"连接 grpc server 源码位置: com.alibaba.nacos.common.remote.client.grpc.GrpcClient#connectToServer // 子类实现连接 grpc server // 1. 发送 ServerCheckRequest 请求来检查服务, 会返回 connectionId // 2. 发送 ConnectionSetupRequest 请求来注册 connection // 3. 最后包装为 GrpcConnection @Override public Connection connectToServer(ServerInfo serverInfo) { try { if (grpcExecutor == null) { this.grpcExecutor = createGrpcExecutor(serverInfo.getServerIp()); } // 计算端口偏移，对于 sdkClient 来说，就是 8848 + 1000 = 9848 // 如果用 nginx 来做代理，9848 端口也需要代理 int port = serverInfo.getServerPort() + rpcPortOffset(); ManagedChannel managedChannel = createNewManagedChannel(serverInfo.getServerIp(), port); // 单一请求 RequestGrpc.RequestFutureStub newChannelStubTemp = createNewChannelStub(managedChannel); if (newChannelStubTemp != null) { // 发送 ServerCheckRequest 请求，会被 GrpcRequestAcceptor#request 处理 Response response = serverCheck(serverInfo.getServerIp(), port, newChannelStubTemp); if (response == null || !(response instanceof ServerCheckResponse)) { shuntDownChannel(managedChannel); return null; } // 流式请求 BiRequestStreamGrpc.BiRequestStreamStub biRequestStreamStub = BiRequestStreamGrpc.newStub( newChannelStubTemp.getChannel()); GrpcConnection grpcConn = new GrpcConnection(serverInfo, grpcExecutor); grpcConn.setConnectionId(((ServerCheckResponse) response).getConnectionId()); //create stream request and bind connection event to this connection. // client 流式处理请求 StreamObserver\u003cPayload\u003e payloadStreamObserver = bindRequestStream(biRequestStreamStub, grpcConn); // stream observer to send response to server grpcConn.setPayloadStreamObserver(payloadStreamObserver); grpcConn.setGrpcFutureServiceStub(newChannelStubTemp); grpcConn.setChannel(managedChannel); //send a setup request. // 发送 ConnectionSetupRequest 请求，会被 GrpcBiStreamRequestAcceptor#requestBiStream 处理，注册 connection ConnectionSetupRequest conSetupRequest = new ConnectionSetupRequest(); conSetupRequest.setClientVersion(VersionUtils.getFullClientVersion()); conSetupRequest.setLabels(super.getLabels()); conSetupRequest.setAbilities(super.clientAbilities); conSetupRequest.setTenant(super.getTenant()); grpcConn.sendRequest(conSetupRequest); //wait to register connection setup Thread.sleep(100L); return grpcConn; } return null; } catch (Exception e) { LOGGER.error(\"[{}]Fail to connect to server!,error={}\", GrpcClient.this.getName(), e); } return null; } 源码位置: com.alibaba.nacos.common.remote.client.grpc.GrpcClient#bindRequestStream // client 流式处理请求 // onNext: 处理请求 // onError 和 onCompleted 来变换 grpc server private StreamObserver\u003cPayload\u003e bindRequestStream(final BiRequestStreamGrpc.BiRequestStreamStub streamStub, final GrpcConnection grpcConn) { return streamStub.requestBiStream(new StreamObserver\u003cPayload\u003e() { @Override public void onNext(Payload payload) { LoggerUtils.printIfDebugEnabled(LOGGER, \"[{}]Stream server request receive, original info: {}\", grpcConn.getConnectionId(), payload.toString()); try { Object parseBody = GrpcUtils.parse(payload); final Request request = (Request) parseBody; if (request != null) { try { // 处理服务端请求 Response response = handleServerRequest(request); if (response != null) { response.setRequestId(request.getRequestId()); // 响应 sendResponse(response); } else { LOGGER.warn(\"[{}]Fail to process server request, ackId-\u003e{}\", grpcConn.getConnectionId(), request.getRequestId()); } } catch (Exception e) { LoggerUtils.printIfErrorEnabled(LOGGER, \"[{}]Handle server request exception: {}\", grpcConn.getConnectionId(), payload.toString(), e.getMessage()); Response errResponse = ErrorResponse.build(NacosException.CLIENT_ERROR, \"Handle server request error\"); errResponse.setRequestId(request.getRequestId()); sendResponse(errResponse); } } } catch (Exception e) { LoggerUtils.printIfErrorEnabled(LOGGER, \"[{}]Error to process server push response: {}\", grpcConn.getConnectionId(), payload.getBody().getValue().toStringUtf8()); } } @Override public void onError(Throwable throwable) { boolean isRunning = isRunning(); boolean isAbandon = grpcConn.isAbandon","date":"2023-08-24 08:00:00","objectID":"/ooooo-notes/%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90-nacos-grpc-client-%E8%AE%BE%E8%AE%A1/:2:0","tags":["nacos","source code","源码分析 nacos 系列"],"title":"源码分析 nacos grpc client 设计","uri":"/ooooo-notes/%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90-nacos-grpc-client-%E8%AE%BE%E8%AE%A1/"},{"categories":["源码分析 nacos 系列"],"content":" nacos 基于 2.2.4 版本 nacos 在 2.0 版本中引入了 grpc，用来处理http连接数过多的问题，所以有必要看看nacos 是怎么使用 grpc的，这样方便我们理清整个请求流程。 在 nacos 中只定义了两个通用的请求模型，一个是 request-response, 另外一个就是基于双向流的 request-response. ","date":"2023-08-23 08:00:00","objectID":"/ooooo-notes/%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90-nacos-grpc-server-%E8%AE%BE%E8%AE%A1/:0:0","tags":["nacos","source code","源码分析 nacos 系列"],"title":"源码分析 nacos grpc server 设计","uri":"/ooooo-notes/%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90-nacos-grpc-server-%E8%AE%BE%E8%AE%A1/"},{"categories":["源码分析 nacos 系列"],"content":"grpc server 的启动 所有的 grpc 服务都基于这个 BaseGrpcServer ，子类通过不同配置来定制服务，比如端口、超时时间 源码位置: com.alibaba.nacos.core.remote.grpc.BaseGrpcServer // 在父类中调用 start 方法，来执行 startServer 方法 @Override public void startServer() throws Exception { final MutableHandlerRegistry handlerRegistry = new MutableHandlerRegistry(); // 添加 rpc 请求 addServices(handlerRegistry, new GrpcConnectionInterceptor()); NettyServerBuilder builder = NettyServerBuilder.forPort(getServicePort()).executor(getRpcExecutor()); // 配置 tls if (grpcServerConfig.getEnableTls()) { if (grpcServerConfig.getCompatibility()) { builder.protocolNegotiator(new OptionalTlsProtocolNegotiator(getSslContextBuilder())); } else { builder.sslContext(getSslContextBuilder()); } } // 配置 grpc server 的参数 server = builder.maxInboundMessageSize(getMaxInboundMessageSize()).fallbackHandlerRegistry(handlerRegistry) .compressorRegistry(CompressorRegistry.getDefaultInstance()) .decompressorRegistry(DecompressorRegistry.getDefaultInstance()) // 连接的过滤器，设置了一些属性，比如 ATTR_TRANS_KEY_CONN_ID .addTransportFilter(new AddressTransportFilter(connectionManager)) .keepAliveTime(getKeepAliveTime(), TimeUnit.MILLISECONDS) .keepAliveTimeout(getKeepAliveTimeout(), TimeUnit.MILLISECONDS) .permitKeepAliveTime(getPermitKeepAliveTime(), TimeUnit.MILLISECONDS) .build(); // 启动 grpc server server.start(); } // 添加 rpc 请求 // 这里添加了两个通用的请求模型，payload -\u003e payload , stream payload \u003c-\u003e stream payload // 所有的请求都会由 grpcCommonRequestAcceptor 和 grpcBiStreamRequestAcceptor 来处理，接下来看看是怎么处理请求的 private void addServices(MutableHandlerRegistry handlerRegistry, ServerInterceptor... serverInterceptor) { // unary common call register. final MethodDescriptor\u003cPayload, Payload\u003e unaryPayloadMethod = MethodDescriptor.\u003cPayload, Payload\u003enewBuilder() .setType(MethodDescriptor.MethodType.UNARY) .setFullMethodName(MethodDescriptor.generateFullMethodName(GrpcServerConstants.REQUEST_SERVICE_NAME, GrpcServerConstants.REQUEST_METHOD_NAME)) .setRequestMarshaller(ProtoUtils.marshaller(Payload.getDefaultInstance())) .setResponseMarshaller(ProtoUtils.marshaller(Payload.getDefaultInstance())).build(); // 定义 payload -\u003e payload final ServerCallHandler\u003cPayload, Payload\u003e payloadHandler = ServerCalls .asyncUnaryCall((request, responseObserver) -\u003e grpcCommonRequestAcceptor.request(request, responseObserver)); final ServerServiceDefinition serviceDefOfUnaryPayload = ServerServiceDefinition.builder( GrpcServerConstants.REQUEST_SERVICE_NAME) .addMethod(unaryPayloadMethod, payloadHandler).build(); handlerRegistry.addService(ServerInterceptors.intercept(serviceDefOfUnaryPayload, serverInterceptor)); // bi stream register. // 定义 stream payload \u003c-\u003e stream payload final ServerCallHandler\u003cPayload, Payload\u003e biStreamHandler = ServerCalls.asyncBidiStreamingCall( (responseObserver) -\u003e grpcBiStreamRequestAcceptor.requestBiStream(responseObserver)); final MethodDescriptor\u003cPayload, Payload\u003e biStreamMethod = MethodDescriptor.\u003cPayload, Payload\u003enewBuilder() .setType(MethodDescriptor.MethodType.BIDI_STREAMING).setFullMethodName(MethodDescriptor .generateFullMethodName(GrpcServerConstants.REQUEST_BI_STREAM_SERVICE_NAME, GrpcServerConstants.REQUEST_BI_STREAM_METHOD_NAME)) .setRequestMarshaller(ProtoUtils.marshaller(Payload.newBuilder().build())) .setResponseMarshaller(ProtoUtils.marshaller(Payload.getDefaultInstance())).build(); final ServerServiceDefinition serviceDefOfBiStream = ServerServiceDefinition .builder(GrpcServerConstants.REQUEST_BI_STREAM_SERVICE_NAME).addMethod(biStreamMethod, biStreamHandler).build(); handlerRegistry.addService(ServerInterceptors.intercept(serviceDefOfBiStream, serverInterceptor)); } ","date":"2023-08-23 08:00:00","objectID":"/ooooo-notes/%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90-nacos-grpc-server-%E8%AE%BE%E8%AE%A1/:1:0","tags":["nacos","source code","源码分析 nacos 系列"],"title":"源码分析 nacos grpc server 设计","uri":"/ooooo-notes/%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90-nacos-grpc-server-%E8%AE%BE%E8%AE%A1/"},{"categories":["源码分析 nacos 系列"],"content":"GrpcRequestAcceptor 处理单一请求 源码位置: com.alibaba.nacos.core.remote.grpc.GrpcRequestAcceptor // GrpcRequestAcceptor 处理请求 // 请求的逻辑比较清楚，最终由 RequestHandler 来处理请求 @Override public void request(Payload grpcRequest, StreamObserver\u003cPayload\u003e responseObserver) { // trace 请求 traceIfNecessary(grpcRequest, true); String type = grpcRequest.getMetadata().getType(); //server is on starting. // server 正在启动中， 返回错误 if (!ApplicationUtils.isStarted()) { Payload payloadResponse = GrpcUtils.convert( ErrorResponse.build(NacosException.INVALID_SERVER_STATUS, \"Server is starting,please try later.\")); traceIfNecessary(payloadResponse, false); responseObserver.onNext(payloadResponse); responseObserver.onCompleted(); return; } // server check. // 检查请求处理，在客户端启动时，会发送 ServerCheckRequest 请求 if (ServerCheckRequest.class.getSimpleName().equals(type)) { Payload serverCheckResponseP = GrpcUtils.convert(new ServerCheckResponse(GrpcServerConstants.CONTEXT_KEY_CONN_ID.get())); traceIfNecessary(serverCheckResponseP, false); responseObserver.onNext(serverCheckResponseP); responseObserver.onCompleted(); return; } // 根据 type 来获取 handler，这里最重要 RequestHandler requestHandler = requestHandlerRegistry.getByRequestType(type); //no handler found. // 没有找到对应的 handler，返回错误 if (requestHandler == null) { Loggers.REMOTE_DIGEST.warn(String.format(\"[%s] No handler for request type : %s :\", \"grpc\", type)); Payload payloadResponse = GrpcUtils .convert(ErrorResponse.build(NacosException.NO_HANDLER, \"RequestHandler Not Found\")); traceIfNecessary(payloadResponse, false); responseObserver.onNext(payloadResponse); responseObserver.onCompleted(); return; } //check connection status. // 检查连接状态, 在客户端启动时，会发送 ConnectionSetupRequest 请求来创建 Connection String connectionId = GrpcServerConstants.CONTEXT_KEY_CONN_ID.get(); boolean requestValid = connectionManager.checkValid(connectionId); if (!requestValid) { Loggers.REMOTE_DIGEST .warn(\"[{}] Invalid connection Id ,connection [{}] is un registered ,\", \"grpc\", connectionId); Payload payloadResponse = GrpcUtils .convert(ErrorResponse.build(NacosException.UN_REGISTER, \"Connection is unregistered.\")); traceIfNecessary(payloadResponse, false); responseObserver.onNext(payloadResponse); responseObserver.onCompleted(); return; } Object parseObj = null; try { // 根据 grpcRequest 中的 type 来进行 json 反序列化 parseObj = GrpcUtils.parse(grpcRequest); } catch (Exception e) { Loggers.REMOTE_DIGEST .warn(\"[{}] Invalid request receive from connection [{}] ,error={}\", \"grpc\", connectionId, e); Payload payloadResponse = GrpcUtils.convert(ErrorResponse.build(NacosException.BAD_GATEWAY, e.getMessage())); traceIfNecessary(payloadResponse, false); responseObserver.onNext(payloadResponse); responseObserver.onCompleted(); return; } // 无效的请求，返回错误 if (parseObj == null) { Loggers.REMOTE_DIGEST.warn(\"[{}] Invalid request receive ,parse request is null\", connectionId); Payload payloadResponse = GrpcUtils .convert(ErrorResponse.build(NacosException.BAD_GATEWAY, \"Invalid request\")); traceIfNecessary(payloadResponse, false); responseObserver.onNext(payloadResponse); responseObserver.onCompleted(); return; } // 只能是 request 对象 if (!(parseObj instanceof Request)) { Loggers.REMOTE_DIGEST .warn(\"[{}] Invalid request receive ,parsed payload is not a request,parseObj={}\", connectionId, parseObj); Payload payloadResponse = GrpcUtils .convert(ErrorResponse.build(NacosException.BAD_GATEWAY, \"Invalid request\")); traceIfNecessary(payloadResponse, false); responseObserver.onNext(payloadResponse); responseObserver.onCompleted(); return; } Request request = (Request) parseObj; try { // 获取对应的 connection，设置 requestMeta Connection connection = connectionManager.getConnection(GrpcServerConstants.CONTEXT_KEY_CONN_ID.get()); RequestMeta requestMeta = new RequestMeta(); requestMeta.setClientIp(connection.getMetaInfo().getClientIp()); requestMeta.setConnectionId(GrpcServerConstants.CONTEXT_KEY_CONN_ID.get()); requestMeta.setClientVersion(connection.getMetaInfo().getVersion()); requestMeta.setLabels(connection.getMetaInf","date":"2023-08-23 08:00:00","objectID":"/ooooo-notes/%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90-nacos-grpc-server-%E8%AE%BE%E8%AE%A1/:2:0","tags":["nacos","source code","源码分析 nacos 系列"],"title":"源码分析 nacos grpc server 设计","uri":"/ooooo-notes/%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90-nacos-grpc-server-%E8%AE%BE%E8%AE%A1/"},{"categories":["源码分析 nacos 系列"],"content":"GrpcBiStreamRequestAcceptor 处理流式请求 源码位置: com.alibaba.nacos.core.remote.grpc.GrpcBiStreamRequestAcceptor // GrpcBiStreamRequestAcceptor 处理流式请求 // 处理逻辑比较清楚，主要分为 onNext(正常请求)、onError(错误请求)、onCompleted(关闭请求) // 在流式处理中，没有 requestHandler 来处理请求， // 这是因为流式请求主要是 服务端发送数据给客户端，客户端接受后发送 ack response @Override public StreamObserver\u003cPayload\u003e requestBiStream(StreamObserver\u003cPayload\u003e responseObserver) { StreamObserver\u003cPayload\u003e streamObserver = new StreamObserver\u003cPayload\u003e() { final String connectionId = GrpcServerConstants.CONTEXT_KEY_CONN_ID.get(); final Integer localPort = GrpcServerConstants.CONTEXT_KEY_CONN_LOCAL_PORT.get(); final int remotePort = GrpcServerConstants.CONTEXT_KEY_CONN_REMOTE_PORT.get(); String remoteIp = GrpcServerConstants.CONTEXT_KEY_CONN_REMOTE_IP.get(); String clientIp = \"\"; @Override public void onNext(Payload payload) { // 获取客户端的 ip clientIp = payload.getMetadata().getClientIp(); traceDetailIfNecessary(payload); Object parseObj; try { // 反序列化对象 parseObj = GrpcUtils.parse(payload); } catch (Throwable throwable) { Loggers.REMOTE_DIGEST .warn(\"[{}]Grpc request bi stream,payload parse error={}\", connectionId, throwable); return; } // 请求对象为 null，不处理 if (parseObj == null) { Loggers.REMOTE_DIGEST .warn(\"[{}]Grpc request bi stream,payload parse null ,body={},meta={}\", connectionId, payload.getBody().getValue().toStringUtf8(), payload.getMetadata()); return; } // 处理 ConnectionSetupRequest 请求，客户端启动时会发送这个请求 if (parseObj instanceof ConnectionSetupRequest) { ConnectionSetupRequest setUpRequest = (ConnectionSetupRequest) parseObj; Map\u003cString, String\u003e labels = setUpRequest.getLabels(); String appName = \"-\"; if (labels != null \u0026\u0026 labels.containsKey(Constants.APPNAME)) { appName = labels.get(Constants.APPNAME); } ConnectionMeta metaInfo = new ConnectionMeta(connectionId, payload.getMetadata().getClientIp(), remoteIp, remotePort, localPort, ConnectionType.GRPC.getType(), setUpRequest.getClientVersion(), appName, setUpRequest.getLabels()); metaInfo.setTenant(setUpRequest.getTenant()); // 新建 connection Connection connection = new GrpcConnection(metaInfo, responseObserver, GrpcServerConstants.CONTEXT_KEY_CHANNEL.get()); connection.setAbilities(setUpRequest.getAbilities()); boolean rejectSdkOnStarting = metaInfo.isSdkSource() \u0026\u0026 !ApplicationUtils.isStarted(); // 注册 connection 对象, 如果不成功，则关闭连接 if (rejectSdkOnStarting || !connectionManager.register(connectionId, connection)) { //Not register to the connection manager if current server is over limit or server is starting. try { Loggers.REMOTE_DIGEST.warn(\"[{}]Connection register fail,reason:{}\", connectionId, rejectSdkOnStarting ? \" server is not started\" : \" server is over limited.\"); connection.request(new ConnectResetRequest(), 3000L); connection.close(); } catch (Exception e) { //Do nothing. if (connectionManager.traced(clientIp)) { Loggers.REMOTE_DIGEST .warn(\"[{}]Send connect reset request error,error={}\", connectionId, e); } } } } else if (parseObj instanceof Response) { // 处理 response 请求，请求可能需要 ack Response response = (Response) parseObj; if (connectionManager.traced(clientIp)) { Loggers.REMOTE_DIGEST .warn(\"[{}]Receive response of server request ,response={}\", connectionId, response); } // ack 通知 RpcAckCallbackSynchronizer.ackNotify(connectionId, response); connectionManager.refreshActiveTime(connectionId); } else { Loggers.REMOTE_DIGEST .warn(\"[{}]Grpc request bi stream,unknown payload receive ,parseObj={}\", connectionId, parseObj); } } @Override public void onError(Throwable t) { if (connectionManager.traced(clientIp)) { Loggers.REMOTE_DIGEST.warn(\"[{}]Bi stream on error,error={}\", connectionId, t); } if (responseObserver instanceof ServerCallStreamObserver) { ServerCallStreamObserver serverCallStreamObserver = ((ServerCallStreamObserver) responseObserver); if (serverCallStreamObserver.isCancelled()) { //client close the stream. } else { try { serverCallStreamObserver.onCompleted(); } catch (Throwable throwable) { //ignore } } } } @Override public void onCompleted() { if (connectionMa","date":"2023-08-23 08:00:00","objectID":"/ooooo-notes/%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90-nacos-grpc-server-%E8%AE%BE%E8%AE%A1/:3:0","tags":["nacos","source code","源码分析 nacos 系列"],"title":"源码分析 nacos grpc server 设计","uri":"/ooooo-notes/%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90-nacos-grpc-server-%E8%AE%BE%E8%AE%A1/"},{"categories":["源码分析 nacos 系列"],"content":" nacos 基于 2.2.4 版本 ","date":"2023-08-22 08:00:00","objectID":"/ooooo-notes/%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90-nacos-%E4%BB%BB%E5%8A%A1%E6%89%A7%E8%A1%8C%E5%99%A8%E7%9A%84%E8%AE%BE%E8%AE%A1/:0:0","tags":["nacos","source code","源码分析 nacos 系列"],"title":"源码分析 nacos 任务执行器的设计","uri":"/ooooo-notes/%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90-nacos-%E4%BB%BB%E5%8A%A1%E6%89%A7%E8%A1%8C%E5%99%A8%E7%9A%84%E8%AE%BE%E8%AE%A1/"},{"categories":["源码分析 nacos 系列"],"content":"任务执行器的设计 在 nacos 中，所有的任务都实现了 NacosTask 接口，所有的任务执行器都实现了 NacosTaskExecuteEngine 接口。 接下来看看这两个接口的设计 // NacosTask public interface NacosTask { /** * Judge Whether this nacos task should do. * * @return true means the nacos task should be done, otherwise false */ // 对于立即执行的任务，默认 return true. // 对于延时执行的任务，判断 System.currentTimeMillis() - this.lastProcessTime \u003e= this.taskInterval. boolean shouldProcess(); } // NacosTaskExecuteEngine // 从接口中可以看出 // 1. addTask, 一个 key 对应一个 task // 2. addProcessor, 一个 key 对应一个 processor, 有默认的 processor public interface NacosTaskExecuteEngine\u003cT extends NacosTask\u003e extends Closeable { /** * Get Task size in execute engine. * * @return size of task */ int size(); /** * Whether the execute engine is empty. * * @return true if the execute engine has no task to do, otherwise false */ boolean isEmpty(); /** * Add task processor {@link NacosTaskProcessor} for execute engine. * * @param key key of task * @param taskProcessor task processor */ void addProcessor(Object key, NacosTaskProcessor taskProcessor); /** * Remove task processor {@link NacosTaskProcessor} form execute engine for key. * * @param key key of task */ void removeProcessor(Object key); /** * Try to get {@link NacosTaskProcessor} by key, if non-exist, will return default processor. * * @param key key of task * @return task processor for task key or default processor if task processor for task key non-exist */ NacosTaskProcessor getProcessor(Object key); /** * Get all processor key. * * @return collection of processors */ Collection\u003cObject\u003e getAllProcessorKey(); /** * Set default task processor. If do not find task processor by task key, use this default processor to process * task. * * @param defaultTaskProcessor default task processor */ void setDefaultTaskProcessor(NacosTaskProcessor defaultTaskProcessor); /** * Add task into execute pool. * * @param key key of task * @param task task */ void addTask(Object key, T task); /** * Remove task. * * @param key key of task * @return nacos task */ T removeTask(Object key); /** * Get all task keys. * * @return collection of task keys. */ Collection\u003cObject\u003e getAllTaskKeys(); } ","date":"2023-08-22 08:00:00","objectID":"/ooooo-notes/%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90-nacos-%E4%BB%BB%E5%8A%A1%E6%89%A7%E8%A1%8C%E5%99%A8%E7%9A%84%E8%AE%BE%E8%AE%A1/:1:0","tags":["nacos","source code","源码分析 nacos 系列"],"title":"源码分析 nacos 任务执行器的设计","uri":"/ooooo-notes/%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90-nacos-%E4%BB%BB%E5%8A%A1%E6%89%A7%E8%A1%8C%E5%99%A8%E7%9A%84%E8%AE%BE%E8%AE%A1/"},{"categories":["源码分析 nacos 系列"],"content":"具体实现类(举例) PushDelayTask 和 PushDelayTaskExecuteEngine 源码位置: com.alibaba.nacos.naming.push.v2.task.PushDelayTask // PushDelayTask 的构造方法 // delay: 延时的事件 // targetClient: 推送的 clientId public PushDelayTask(Service service, long delay, String targetClient) { this.service = service; this.pushToAll = false; this.targetClients = new HashSet\u003c\u003e(1); this.targetClients.add(targetClient); setTaskInterval(delay); // 设置上一次处理时间，用来判断是否过期 setLastProcessTime(System.currentTimeMillis()); } // 每一个延时任务都会有 merge 方法, 用来合并相同的 task, 这样可以更高效的处理任务 // 比如因为客户端重试，发起了两个一样的 task，经过 merge 之后，处理一个就行。 @Override public void merge(AbstractDelayTask task) { if (!(task instanceof PushDelayTask)) { return; } PushDelayTask oldTask = (PushDelayTask) task; if (isPushToAll() || oldTask.isPushToAll()) { pushToAll = true; targetClients = null; } else { targetClients.addAll(oldTask.getTargetClients()); } setLastProcessTime(Math.min(getLastProcessTime(), task.getLastProcessTime())); Loggers.PUSH.info(\"[PUSH] Task merge for {}\", service); } // shouldProcess 方法在父类上面, 判断当前任务都是过期 @Override public boolean shouldProcess() { return (System.currentTimeMillis() - this.lastProcessTime \u003e= this.taskInterval); } 源码位置: com.alibaba.nacos.naming.push.v2.task.PushDelayTaskExecuteEngine#PushDelayTaskExecuteEngine // PushDelayTaskExecuteEngine 的初始化 public PushDelayTaskExecuteEngine(ClientManager clientManager, ClientServiceIndexesManager indexesManager, ServiceStorage serviceStorage, NamingMetadataManager metadataManager, PushExecutor pushExecutor, SwitchDomain switchDomain) { ... // 设置默认的processor, 用来处理任务, 这里没有特殊的 processor setDefaultTaskProcessor(new PushDelayTaskProcessor(this)); } // 在父类 NacosDelayTaskExecuteEngine 中用单一的线程池来启动，然后处理任务 public NacosDelayTaskExecuteEngine(String name, int initCapacity, Logger logger, long processInterval) { ... tasks = new ConcurrentHashMap\u003c\u003e(initCapacity); // 线程池 processingExecutor = ExecutorFactory.newSingleScheduledExecutorService(new NameThreadFactory(name)); // 最后调用自己的方法来处理任务 processingExecutor .scheduleWithFixedDelay(new ProcessRunnable(), processInterval, processInterval, TimeUnit.MILLISECONDS); } // 处理任务 private class ProcessRunnable implements Runnable { @Override public void run() { try { processTasks(); } catch (Throwable e) { getEngineLog().error(e.toString(), e); } } } // 在父类 NacosDelayTaskExecuteEngine 中 // com.alibaba.nacos.common.task.engine.NacosDelayTaskExecuteEngine#processTasks protected void processTasks() { // 获取所有的 key，因为 addTask 方法，所以每一个 task 都会关联到一个 key Collection\u003cObject\u003e keys = getAllTaskKeys(); for (Object taskKey : keys) { // 判断 task 是否到期，每个 task 创建时都是指定 taskInterval AbstractDelayTask task = removeTask(taskKey); if (null == task) { continue; } // 获取相应的 processor 来处理，一般来说就是默认的 processor, 比如 PushDelayTaskProcessor, 下面会说这个类 NacosTaskProcessor processor = getProcessor(taskKey); if (null == processor) { getEngineLog().error(\"processor not found for task, so discarded. \" + task); continue; } try { // 处理 task，如果处理失败，重新添加 task // ReAdd task if process failed if (!processor.process(task)) { retryFailedTask(taskKey, task); } } catch (Throwable e) { getEngineLog().error(\"Nacos task execute error \", e); retryFailedTask(taskKey, task); } } } // 重新添加 task private void retryFailedTask(Object key, AbstractDelayTask task) { task.setLastProcessTime(System.currentTimeMillis()); addTask(key, task); } 源码位置: com.alibaba.nacos.naming.push.v2.task.PushDelayTaskExecuteEngine.PushDelayTaskProcessor // PushDelayTaskProcessor 处理 PushDelayTask, 重新包装为 PushExecuteTask, 然后放入线程池中运行 private static class PushDelayTaskProcessor implements NacosTaskProcessor { private final PushDelayTaskExecuteEngine executeEngine; public PushDelayTaskProcessor(PushDelayTaskExecuteEngine executeEngine) { this.executeEngine = executeEngine; } @Override public boolean process(NacosTask task) { PushDelayTask pushDelayTask = (PushDelayTask) task; Service service = pushDelayTask.getService(); NamingExecuteTaskDispatcher.getInstance() .dispatchAndExecuteTask(service, new PushExecuteTask(servi","date":"2023-08-22 08:00:00","objectID":"/ooooo-notes/%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90-nacos-%E4%BB%BB%E5%8A%A1%E6%89%A7%E8%A1%8C%E5%99%A8%E7%9A%84%E8%AE%BE%E8%AE%A1/:2:0","tags":["nacos","source code","源码分析 nacos 系列"],"title":"源码分析 nacos 任务执行器的设计","uri":"/ooooo-notes/%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90-nacos-%E4%BB%BB%E5%8A%A1%E6%89%A7%E8%A1%8C%E5%99%A8%E7%9A%84%E8%AE%BE%E8%AE%A1/"},{"categories":["源码分析 netty 系列"],"content":"时间轮的用法 // 默认间隔时间为 100 毫秒 Timer timer = new HashedWheelTimer(); Timeout timeout = timer.newTimeout(new TimerTask() { @Override public void run(Timeout timeout) throws Exception { System.out.println(\"run\"); } }, 10, TimeUnit.SECONDS); timer.stop(); ","date":"2023-08-22 08:00:00","objectID":"/ooooo-notes/%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90-netty-%E6%97%B6%E9%97%B4%E8%BD%AE/:1:0","tags":["netty","source code","源码分析 netty 系列"],"title":"源码分析 netty 时间轮","uri":"/ooooo-notes/%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90-netty-%E6%97%B6%E9%97%B4%E8%BD%AE/"},{"categories":["源码分析 netty 系列"],"content":"时间轮的原理 有一个环形数组，每个格子都是一个队列，每隔 interval 时间，指针就会移动到下一格，然后把队列中的任务拿出来判断是否到期并执行。 timer ","date":"2023-08-22 08:00:00","objectID":"/ooooo-notes/%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90-netty-%E6%97%B6%E9%97%B4%E8%BD%AE/:2:0","tags":["netty","source code","源码分析 netty 系列"],"title":"源码分析 netty 时间轮","uri":"/ooooo-notes/%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90-netty-%E6%97%B6%E9%97%B4%E8%BD%AE/"},{"categories":["源码分析 netty 系列"],"content":"netty 的时间轮 源码位置: io.netty.util.HashedWheelTimer#HashedWheelTimer // 先来看看构造函数 // threadFactory: 可以用来指定线程的名称 // tickDuration, unit : 间隔时间 // ticksPerWheel: 环形数组的大小, 也就是时间轮的大小 // leakDetection: 检查资源，默认开启 // maxPendingTimeouts: 定时任务上限个数 // taskExecutor: 任务执行器，默认是同步执行，可以自定义来异步执行 public HashedWheelTimer( ThreadFactory threadFactory, long tickDuration, TimeUnit unit, int ticksPerWheel, boolean leakDetection, long maxPendingTimeouts, Executor taskExecutor) { checkNotNull(threadFactory, \"threadFactory\"); checkNotNull(unit, \"unit\"); checkPositive(tickDuration, \"tickDuration\"); checkPositive(ticksPerWheel, \"ticksPerWheel\"); this.taskExecutor = checkNotNull(taskExecutor, \"taskExecutor\"); // Normalize ticksPerWheel to power of two and initialize the wheel. // 创建时间轮, 其大小是 2 的倍数，最接近于 ticksPerWheel wheel = createWheel(ticksPerWheel); mask = wheel.length - 1; // Convert tickDuration to nanos. long duration = unit.toNanos(tickDuration); // Prevent overflow. if (duration \u003e= Long.MAX_VALUE / wheel.length) { throw new IllegalArgumentException(String.format( \"tickDuration: %d (expected: 0 \u003c tickDuration in nanos \u003c %d\", tickDuration, Long.MAX_VALUE / wheel.length)); } if (duration \u003c MILLISECOND_NANOS) { logger.warn(\"Configured tickDuration {} smaller than {}, using 1ms.\", tickDuration, MILLISECOND_NANOS); this.tickDuration = MILLISECOND_NANOS; } else { this.tickDuration = duration; } // 创建 work 线程，来进行 sleep workerThread = threadFactory.newThread(worker); leak = leakDetection || !workerThread.isDaemon() ? leakDetector.track(this) : null; this.maxPendingTimeouts = maxPendingTimeouts; // 时间轮不能开启太多，因为每个时间轮都要 work 线程来轮询 if (INSTANCE_COUNTER.incrementAndGet() \u003e INSTANCE_COUNT_LIMIT \u0026\u0026 WARNED_TOO_MANY_INSTANCES.compareAndSet(false, true)) { reportTooManyInstances(); } } // 创建时间轮, 实际就是一个数组，然后通过 i++ % size 来达到环形数组 private static HashedWheelBucket[] createWheel(int ticksPerWheel) { //ticksPerWheel may not be greater than 2^30 checkInRange(ticksPerWheel, 1, 1073741824, \"ticksPerWheel\"); // ticksPerWheel 是 2 的倍数 ticksPerWheel = normalizeTicksPerWheel(ticksPerWheel); HashedWheelBucket[] wheel = new HashedWheelBucket[ticksPerWheel]; for (int i = 0; i \u003c wheel.length; i ++) { wheel[i] = new HashedWheelBucket(); } return wheel; } 源码位置: io.netty.util.HashedWheelTimer#newTimeout // 添加一个定时任务 @Override public Timeout newTimeout(TimerTask task, long delay, TimeUnit unit) { checkNotNull(task, \"task\"); checkNotNull(unit, \"unit\"); long pendingTimeoutsCount = pendingTimeouts.incrementAndGet(); // 判断定时任务的个数 if (maxPendingTimeouts \u003e 0 \u0026\u0026 pendingTimeoutsCount \u003e maxPendingTimeouts) { pendingTimeouts.decrementAndGet(); throw new RejectedExecutionException(\"Number of pending timeouts (\" + pendingTimeoutsCount + \") is greater than or equal to maximum allowed pending \" + \"timeouts (\" + maxPendingTimeouts + \")\"); } // 启动 work 线程，这里是懒加载，一定是有定时任务添加，才会启动 start(); // Add the timeout to the timeout queue which will be processed on the next tick. // During processing all the queued HashedWheelTimeouts will be added to the correct HashedWheelBucket. // deadline 表示要等待的间隔时间 long deadline = System.nanoTime() + unit.toNanos(delay) - startTime; // Guard against overflow. if (delay \u003e 0 \u0026\u0026 deadline \u003c 0) { deadline = Long.MAX_VALUE; } // 创建 timeout, 然后添加到 timeouts 队列中，之后 work 线程会从队列中获取 HashedWheelTimeout timeout = new HashedWheelTimeout(this, task, deadline); timeouts.add(timeout); return timeout; } 源码位置: io.netty.util.HashedWheelTimer#start // 启动 work 线程 public void start() { // 判断状态 switch (WORKER_STATE_UPDATER.get(this)) { case WORKER_STATE_INIT: // 可能多线程启动，所以 cas 判断 if (WORKER_STATE_UPDATER.compareAndSet(this, WORKER_STATE_INIT, WORKER_STATE_STARTED)) { // 启动 work 线程，执行 run 方法 workerThread.start(); } break; case WORKER_STATE_STARTED: break; case WORKER_STATE_SHUTDOWN: throw new IllegalStateException(\"cannot be started once stopped\"); default: throw new Error(\"Invalid WorkerState\"); } // Wait until the startTime is initialized by the worker. while (startTime == 0) { try { startTimeInitialized.aw","date":"2023-08-22 08:00:00","objectID":"/ooooo-notes/%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90-netty-%E6%97%B6%E9%97%B4%E8%BD%AE/:3:0","tags":["netty","source code","源码分析 netty 系列"],"title":"源码分析 netty 时间轮","uri":"/ooooo-notes/%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90-netty-%E6%97%B6%E9%97%B4%E8%BD%AE/"},{"categories":["源码分析 nacos 系列"],"content":" nacos 基于 2.2.4 版本 nacos 订阅服务主要分为 http+udp 和 grpc 这两种方式，这两者的内部调用方法都是一样的，这里主要分析 http+udp 的方式。 ","date":"2023-08-21 08:00:00","objectID":"/ooooo-notes/%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90-nacos-%E8%AE%A2%E9%98%85%E6%9C%8D%E5%8A%A1/:0:0","tags":["nacos","source code","源码分析 nacos 系列"],"title":"源码分析 nacos 订阅服务","uri":"/ooooo-notes/%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90-nacos-%E8%AE%A2%E9%98%85%E6%9C%8D%E5%8A%A1/"},{"categories":["源码分析 nacos 系列"],"content":"订阅服务的 curl curl --location 'localhost:8848/nacos/v2/ns/instance/list?serviceName=test' ","date":"2023-08-21 08:00:00","objectID":"/ooooo-notes/%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90-nacos-%E8%AE%A2%E9%98%85%E6%9C%8D%E5%8A%A1/:1:0","tags":["nacos","source code","源码分析 nacos 系列"],"title":"源码分析 nacos 订阅服务","uri":"/ooooo-notes/%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90-nacos-%E8%AE%A2%E9%98%85%E6%9C%8D%E5%8A%A1/"},{"categories":["源码分析 nacos 系列"],"content":"订阅服务的主流程 源码位置: com.alibaba.nacos.naming.controllers.v2.InstanceControllerV2#list // 处理请求 public Result\u003cServiceInfo\u003e list(@RequestParam(value = \"namespaceId\", defaultValue = Constants.DEFAULT_NAMESPACE_ID) String namespaceId, @RequestParam(value = \"groupName\", defaultValue = Constants.DEFAULT_GROUP) String groupName, @RequestParam(\"serviceName\") String serviceName, @RequestParam(value = \"clusterName\", defaultValue = StringUtils.EMPTY) String clusterName, @RequestParam(value = \"ip\", defaultValue = StringUtils.EMPTY) String ip, @RequestParam(value = \"port\", defaultValue = \"0\") Integer port, @RequestParam(value = \"healthyOnly\", defaultValue = \"false\") Boolean healthyOnly, @RequestParam(value = \"app\", defaultValue = StringUtils.EMPTY) String app, @RequestHeader(value = HttpHeaderConsts.USER_AGENT_HEADER, required = false) String userAgent, @RequestHeader(value = HttpHeaderConsts.CLIENT_VERSION_HEADER, required = false) String clientVersion) { if (StringUtils.isEmpty(userAgent)) { userAgent = StringUtils.defaultIfEmpty(clientVersion, StringUtils.EMPTY); } String compositeServiceName = NamingUtils.getGroupedName(serviceName, groupName); // 根据 ip 和 port 来进行 udp 推送 Subscriber subscriber = new Subscriber(ip + \":\" + port, userAgent, app, ip, namespaceId, compositeServiceName, port, clusterName); // 获取所有的实例 return Result.success(instanceServiceV2.listInstance(namespaceId, compositeServiceName, subscriber, clusterName, healthyOnly)); } 源码位置: com.alibaba.nacos.naming.core.InstanceOperatorClientImpl#listInstance // 获取所有的实例 @Override public ServiceInfo listInstance(String namespaceId, String serviceName, Subscriber subscriber, String cluster, boolean healthOnly) { Service service = getService(namespaceId, serviceName, true); // For adapt 1.X subscribe logic if (subscriber.getPort() \u003e 0 \u0026\u0026 pushService.canEnablePush(subscriber.getAgent())) { // clientId = address + ID_DELIMITER + ephemeral, 这个很重要，用来判断是不是 udp push String clientId = IpPortBasedClient.getClientId(subscriber.getAddrStr(), true); // 根据 udp 的 ip 和 port 来创建 client createIpPortClientIfAbsent(clientId); // 添加订阅, 实现类只有一个，就是 EphemeralClientOperationServiceImpl，接下来分析这个 clientOperationService.subscribeService(service, subscriber, clientId); } ServiceInfo serviceInfo = serviceStorage.getData(service); ServiceMetadata serviceMetadata = metadataManager.getServiceMetadata(service).orElse(null); // 根据条件来筛选最终的 instances ServiceInfo result = ServiceUtil .selectInstancesWithHealthyProtection(serviceInfo, serviceMetadata, cluster, healthOnly, true, subscriber.getIp()); // adapt for v1.x sdk result.setName(NamingUtils.getGroupedName(result.getName(), result.getGroupName())); return result; } 源码位置: com.alibaba.nacos.naming.core.v2.service.impl.EphemeralClientOperationServiceImpl#subscribeService @Override public void subscribeService(Service service, Subscriber subscriber, String clientId) { // 获取单例的 service Service singleton = ServiceManager.getInstance().getSingletonIfExist(service).orElse(service); Client client = clientManager.getClient(clientId); if (!clientIsLegal(client, clientId)) { return; } // client 添加 service 和 subscriber client.addServiceSubscriber(singleton, subscriber); // 设置更新时间，以防被过期定时任务清理了 client.setLastUpdatedTime(); // 发布 ClientSubscribeServiceEvent NotifyCenter.publishEvent(new ClientOperationEvent.ClientSubscribeServiceEvent(singleton, clientId)); } 源码位置: com.alibaba.nacos.naming.core.v2.index.ClientServiceIndexesManager#handleClientOperation // ClientServiceIndexesManager 监听 ClientSubscribeServiceEvent 事件 private void handleClientOperation(ClientOperationEvent event) { Service service = event.getService(); String clientId = event.getClientId(); if (event instanceof ClientOperationEvent.ClientRegisterServiceEvent) { addPublisherIndexes(service, clientId); } else if (event instanceof ClientOperationEvent.ClientDeregisterServiceEvent) { removePublisherIndexes(service, clientId); } else if (event instanceof ClientOperationEvent.ClientSubscribeServiceEvent) { // 添加 service 对应的 client","date":"2023-08-21 08:00:00","objectID":"/ooooo-notes/%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90-nacos-%E8%AE%A2%E9%98%85%E6%9C%8D%E5%8A%A1/:2:0","tags":["nacos","source code","源码分析 nacos 系列"],"title":"源码分析 nacos 订阅服务","uri":"/ooooo-notes/%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90-nacos-%E8%AE%A2%E9%98%85%E6%9C%8D%E5%8A%A1/"},{"categories":["源码分析 nacos 系列"],"content":"延时任务推送 每个 PushDelayTask 到期之后，都会被 PushDelayTaskProcessor 来处理，重新包装为 PushExecuteTask. 源码位置: `` private static class PushDelayTaskProcessor implements NacosTaskProcessor { private final PushDelayTaskExecuteEngine executeEngine; public PushDelayTaskProcessor(PushDelayTaskExecuteEngine executeEngine) { this.executeEngine = executeEngine; } @Override public boolean process(NacosTask task) { PushDelayTask pushDelayTask = (PushDelayTask) task; Service service = pushDelayTask.getService(); // 丢任务到线程池来执行 NamingExecuteTaskDispatcher.getInstance() .dispatchAndExecuteTask(service, new PushExecuteTask(service, executeEngine, pushDelayTask)); return true; } } 源码位置: com.alibaba.nacos.naming.push.v2.task.PushExecuteTask#run // 执行 push 任务 public void run() { try { PushDataWrapper wrapper = generatePushData(); ClientManager clientManager = delayTaskEngine.getClientManager(); // 获取所有推送的 clientId for (String each : getTargetClientIds()) { Client client = clientManager.getClient(each); if (null == client) { // means this client has disconnect continue; } // 获取 service 对应的 subscriber Subscriber subscriber = client.getSubscriber(service); // skip if null if (subscriber == null) { continue; } // 执行具体的 push, 接下来看看是如何获取对应的 pushExecutor delayTaskEngine.getPushExecutor().doPushWithCallback(each, subscriber, wrapper, new ServicePushCallback(each, subscriber, wrapper.getOriginalData(), delayTask.isPushToAll())); } } catch (Exception e) { Loggers.PUSH.error(\"Push task for service\" + service.getGroupedServiceName() + \" execute failed \", e); delayTaskEngine.addTask(service, new PushDelayTask(service, 1000L)); } } 源码位置: com.alibaba.nacos.naming.push.v2.executor.PushExecutorDelegate#doPushWithCallback @Override public void doPushWithCallback(String clientId, Subscriber subscriber, PushDataWrapper data, NamingPushCallback callBack) { getPushExecuteService(clientId, subscriber).doPushWithCallback(clientId, subscriber, data, callBack); } private PushExecutor getPushExecuteService(String clientId, Subscriber subscriber) { Optional\u003cSpiPushExecutor\u003e result = SpiImplPushExecutorHolder.getInstance() .findPushExecutorSpiImpl(clientId, subscriber); if (result.isPresent()) { return result.get(); } // 获取对应的 pushExecuteService, 之前的 clientId = address + ID_DELIMITER + ephemeral // use nacos default push executor return clientId.contains(IpPortBasedClient.ID_DELIMITER) ? udpPushExecuteService : rpcPushExecuteService; } // udpPushExecuteService 的逻辑比较简单，就是发送一个 udp 的数据包，这里不继续分析了。 // rpcPushExecuteService 的逻辑就是发送一个 rpc 的数据包，后面的文章详说 ","date":"2023-08-21 08:00:00","objectID":"/ooooo-notes/%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90-nacos-%E8%AE%A2%E9%98%85%E6%9C%8D%E5%8A%A1/:3:0","tags":["nacos","source code","源码分析 nacos 系列"],"title":"源码分析 nacos 订阅服务","uri":"/ooooo-notes/%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90-nacos-%E8%AE%A2%E9%98%85%E6%9C%8D%E5%8A%A1/"},{"categories":["源码分析 nacos 系列"],"content":" nacos 基于 2.2.4 版本 ","date":"2023-08-20 08:00:00","objectID":"/ooooo-notes/%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90-nacos-%E6%B3%A8%E9%94%80%E5%AE%9E%E4%BE%8B/:0:0","tags":["nacos","source code","源码分析 nacos 系列"],"title":"源码分析 nacos 注销实例","uri":"/ooooo-notes/%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90-nacos-%E6%B3%A8%E9%94%80%E5%AE%9E%E4%BE%8B/"},{"categories":["源码分析 nacos 系列"],"content":"注销实例的 curl curl --location --request DELETE 'http://localhost:8848/nacos/v2/ns/instance' \\ --header 'Content-Type: application/x-www-form-urlencoded' \\ --data-urlencode 'serviceName=test' \\ --data-urlencode 'ip=1.2.3.4' \\ --data-urlencode 'port=80' ","date":"2023-08-20 08:00:00","objectID":"/ooooo-notes/%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90-nacos-%E6%B3%A8%E9%94%80%E5%AE%9E%E4%BE%8B/:1:0","tags":["nacos","source code","源码分析 nacos 系列"],"title":"源码分析 nacos 注销实例","uri":"/ooooo-notes/%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90-nacos-%E6%B3%A8%E9%94%80%E5%AE%9E%E4%BE%8B/"},{"categories":["源码分析 nacos 系列"],"content":"注销实例的主流程 源码位置: com.alibaba.nacos.naming.controllers.v2.InstanceControllerV2#deregister public Result\u003cString\u003e deregister(InstanceForm instanceForm) throws NacosException { // check param instanceForm.validate(); checkWeight(instanceForm.getWeight()); // build instance Instance instance = buildInstance(instanceForm); // 移除 instance instanceServiceV2.removeInstance(instanceForm.getNamespaceId(), buildCompositeServiceName(instanceForm), instance); // 发布 DeregisterInstanceTraceEvent 事件 NotifyCenter.publishEvent(new DeregisterInstanceTraceEvent(System.currentTimeMillis(), \"\", false, DeregisterInstanceReason.REQUEST, instanceForm.getNamespaceId(), instanceForm.getGroupName(), instanceForm.getServiceName(), instance.getIp(), instance.getPort())); return Result.success(\"ok\"); } 源码位置: com.alibaba.nacos.naming.core.InstanceOperatorClientImpl#removeInstance @Override public void removeInstance(String namespaceId, String serviceName, Instance instance) { // 判断 instance 是否已经注册过, 如果没有，则不用处理 boolean ephemeral = instance.isEphemeral(); String clientId = IpPortBasedClient.getClientId(instance.toInetAddr(), ephemeral); if (!clientManager.contains(clientId)) { Loggers.SRV_LOG.warn(\"remove instance from non-exist client: {}\", clientId); return; } Service service = getService(namespaceId, serviceName, ephemeral); // 注销实例，如果是临时实例，EphemeralClientOperationServiceImpl，如果是持久化实例，PersistentClientOperationServiceImpl clientOperationService.deregisterInstance(service, instance, clientId); } ","date":"2023-08-20 08:00:00","objectID":"/ooooo-notes/%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90-nacos-%E6%B3%A8%E9%94%80%E5%AE%9E%E4%BE%8B/:2:0","tags":["nacos","source code","源码分析 nacos 系列"],"title":"源码分析 nacos 注销实例","uri":"/ooooo-notes/%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90-nacos-%E6%B3%A8%E9%94%80%E5%AE%9E%E4%BE%8B/"},{"categories":["源码分析 nacos 系列"],"content":"临时实例注销 源码位置: com.alibaba.nacos.naming.core.v2.service.impl.EphemeralClientOperationServiceImpl#deregisterInstance @Override public void deregisterInstance(Service service, Instance instance, String clientId) { // 判断 service 是否存在 if (!ServiceManager.getInstance().containSingleton(service)) { Loggers.SRV_LOG.warn(\"remove instance from non-exist service: {}\", service); return; } Service singleton = ServiceManager.getInstance().getSingleton(service); Client client = clientManager.getClient(clientId); if (!clientIsLegal(client, clientId)) { return; } // 移除内存中的 instance 对象，这里会发布 ClientChangedEvent 事件，这个很重要 InstancePublishInfo removedInstance = client.removeServiceInstance(singleton); client.setLastUpdatedTime(); client.recalculateRevision(); if (null != removedInstance) { // 发布 ClientDeregisterServiceEvent 事件 NotifyCenter.publishEvent(new ClientOperationEvent.ClientDeregisterServiceEvent(singleton, clientId)); // 发布 InstanceMetadataEvent 事件 NotifyCenter.publishEvent( new MetadataEvent.InstanceMetadataEvent(singleton, removedInstance.getMetadataId(), true)); } } 源码位置: com.alibaba.nacos.naming.consistency.ephemeral.distro.v2.DistroClientDataProcessor#syncToAllServer // DistroClientDataProcessor 接受 ClientChangedEvent, 负责同步数据给其他节点 private void syncToAllServer(ClientEvent event) { Client client = event.getClient(); // Only ephemeral data sync by Distro, persist client should sync by raft. if (null == client || !client.isEphemeral() || !clientManager.isResponsibleClient(client)) { return; } if (event instanceof ClientEvent.ClientDisconnectEvent) { DistroKey distroKey = new DistroKey(client.getClientId(), TYPE); distroProtocol.sync(distroKey, DataOperation.DELETE); } else if (event instanceof ClientEvent.ClientChangedEvent) { DistroKey distroKey = new DistroKey(client.getClientId(), TYPE); distroProtocol.sync(distroKey, DataOperation.CHANGE); } } 源码位置: com.alibaba.nacos.naming.core.v2.index.ClientServiceIndexesManager#handleClientOperation // ClientServiceIndexesManager 会监听 ClientDeregisterServiceEvent 事件 private void handleClientOperation(ClientOperationEvent event) { Service service = event.getService(); String clientId = event.getClientId(); if (event instanceof ClientOperationEvent.ClientRegisterServiceEvent) { addPublisherIndexes(service, clientId); } else if (event instanceof ClientOperationEvent.ClientDeregisterServiceEvent) { // 移除 service 的 clientId removePublisherIndexes(service, clientId); } else if (event instanceof ClientOperationEvent.ClientSubscribeServiceEvent) { addSubscriberIndexes(service, clientId); } else if (event instanceof ClientOperationEvent.ClientUnsubscribeServiceEvent) { removeSubscriberIndexes(service, clientId); } } private void removePublisherIndexes(Service service, String clientId) { publisherIndexes.computeIfPresent(service, (s, ids) -\u003e { ids.remove(clientId); // 发布 ServiceChangedEvent 事件 NotifyCenter.publishEvent(new ServiceEvent.ServiceChangedEvent(service, true)); return ids.isEmpty() ? null : ids; }); } 源码位置: com.alibaba.nacos.naming.push.v2.NamingSubscriberServiceV2Impl#onEvent // NamingSubscriberServiceV2Impl 会监听 ServiceChangedEvent 事件 @Override public void onEvent(Event event) { if (event instanceof ServiceEvent.ServiceChangedEvent) { // If service changed, push to all subscribers. // 注销 instance， 必须推送给所有的订阅者 ServiceEvent.ServiceChangedEvent serviceChangedEvent = (ServiceEvent.ServiceChangedEvent) event; Service service = serviceChangedEvent.getService(); delayTaskEngine.addTask(service, new PushDelayTask(service, PushConfig.getInstance().getPushTaskDelay())); MetricsMonitor.incrementServiceChangeCount(service.getNamespace(), service.getGroup(), service.getName()); } else if (event instanceof ServiceEvent.ServiceSubscribedEvent) { // If service is subscribed by one client, only push this client. ServiceEvent.ServiceSubscribedEvent subscribedEvent = (ServiceEvent.ServiceSubscribedEvent) event; Service service = subscribedEvent.getService(); delayTaskEngine.addTask(service, new PushDelayTask(service,","date":"2023-08-20 08:00:00","objectID":"/ooooo-notes/%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90-nacos-%E6%B3%A8%E9%94%80%E5%AE%9E%E4%BE%8B/:3:0","tags":["nacos","source code","源码分析 nacos 系列"],"title":"源码分析 nacos 注销实例","uri":"/ooooo-notes/%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90-nacos-%E6%B3%A8%E9%94%80%E5%AE%9E%E4%BE%8B/"},{"categories":["源码分析 nacos 系列"],"content":"持久化实例注销 源码位置: com.alibaba.nacos.naming.core.v2.service.impl.PersistentClientOperationServiceImpl#deregisterInstance @Override public void deregisterInstance(Service service, Instance instance, String clientId) { final InstanceStoreRequest request = new InstanceStoreRequest(); request.setService(service); request.setInstance(instance); request.setClientId(clientId); // 注意这里的 group，在构造函数中进行注册对应的 processor final WriteRequest writeRequest = WriteRequest.newBuilder().setGroup(group()) .setData(ByteString.copyFrom(serializer.serialize(request))).setOperation(DataOperation.DELETE.name()) .build(); try { // 由 CPProtcol 写入请求到本地，然后同步到其他节点，最后应用状态机 protocol.write(writeRequest); Loggers.RAFT.info(\"Client unregistered. service={}, clientId={}, instance={}\", service, instance, clientId); } catch (Exception e) { throw new NacosRuntimeException(NacosException.SERVER_ERROR, e); } } // 构造函数中注册 requestProcessor, 这个可以分组的 public PersistentClientOperationServiceImpl(final PersistentIpPortClientManager clientManager) { this.clientManager = clientManager; this.protocol = ApplicationUtils.getBean(ProtocolManager.class).getCpProtocol(); this.protocol.addRequestProcessors(Collections.singletonList(this)); } // 处理状态机 @Override public Response onApply(WriteRequest request) { final Lock lock = readLock; lock.lock(); try { final InstanceStoreRequest instanceRequest = serializer.deserialize(request.getData().toByteArray()); final DataOperation operation = DataOperation.valueOf(request.getOperation()); switch (operation) { case ADD: onInstanceRegister(instanceRequest.service, instanceRequest.instance, instanceRequest.getClientId()); break; case DELETE: // 注销实例 onInstanceDeregister(instanceRequest.service, instanceRequest.getClientId()); break; case CHANGE: if (instanceAndServiceExist(instanceRequest)) { onInstanceRegister(instanceRequest.service, instanceRequest.instance, instanceRequest.getClientId()); } break; default: return Response.newBuilder().setSuccess(false).setErrMsg(\"unsupport operation : \" + operation) .build(); } return Response.newBuilder().setSuccess(true).build(); } catch (Exception e) { Loggers.RAFT.warn(\"Persistent client operation failed. \", e); return Response.newBuilder().setSuccess(false) .setErrMsg(\"Persistent client operation failed. \" + e.getMessage()).build(); } finally { lock.unlock(); } } // 注销实例， 这里的逻辑和临时实例注销的逻辑是一样的，所以就不继续解析了 private void onInstanceDeregister(Service service, String clientId) { Service singleton = ServiceManager.getInstance().getSingleton(service); Client client = clientManager.getClient(clientId); if (client == null) { Loggers.RAFT.warn(\"client not exist onInstanceDeregister, clientId : {} \", clientId); return; } // 移除内存的 instance，发布 ClientChangedEvent 事件 client.removeServiceInstance(singleton); client.setLastUpdatedTime(); if (client.getAllPublishedService().isEmpty()) { clientManager.clientDisconnected(clientId); } // 发布 ClientDeregisterServiceEvent 事件 NotifyCenter.publishEvent(new ClientOperationEvent.ClientDeregisterServiceEvent(singleton, clientId)); } ","date":"2023-08-20 08:00:00","objectID":"/ooooo-notes/%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90-nacos-%E6%B3%A8%E9%94%80%E5%AE%9E%E4%BE%8B/:4:0","tags":["nacos","source code","源码分析 nacos 系列"],"title":"源码分析 nacos 注销实例","uri":"/ooooo-notes/%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90-nacos-%E6%B3%A8%E9%94%80%E5%AE%9E%E4%BE%8B/"},{"categories":["源码分析 nacos 系列"],"content":" nacos 基于 2.2.4 版本 ","date":"2023-08-19 08:00:00","objectID":"/ooooo-notes/%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90-nacos-%E6%B3%A8%E5%86%8C%E5%AE%9E%E4%BE%8B/:0:0","tags":["nacos","source code","源码分析 nacos 系列"],"title":"源码分析 nacos 注册实例","uri":"/ooooo-notes/%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90-nacos-%E6%B3%A8%E5%86%8C%E5%AE%9E%E4%BE%8B/"},{"categories":["源码分析 nacos 系列"],"content":"注册实例的 curl curl --location 'http://localhost:8848/nacos/v2/ns/instance' \\ --header 'Content-Type: application/x-www-form-urlencoded' \\ --data-urlencode 'serviceName=test' \\ --data-urlencode 'ip=1.2.3.4' \\ --data-urlencode 'port=80' ","date":"2023-08-19 08:00:00","objectID":"/ooooo-notes/%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90-nacos-%E6%B3%A8%E5%86%8C%E5%AE%9E%E4%BE%8B/:1:0","tags":["nacos","source code","源码分析 nacos 系列"],"title":"源码分析 nacos 注册实例","uri":"/ooooo-notes/%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90-nacos-%E6%B3%A8%E5%86%8C%E5%AE%9E%E4%BE%8B/"},{"categories":["源码分析 nacos 系列"],"content":"注册实例的主流程 源码位置: com.alibaba.nacos.naming.controllers.v2.InstanceControllerV2#register public Result\u003cString\u003e register(InstanceForm instanceForm) throws NacosException { // check param instanceForm.validate(); checkWeight(instanceForm.getWeight()); // build instance Instance instance = buildInstance(instanceForm); // 注册实例 instanceServiceV2.registerInstance(instanceForm.getNamespaceId(), buildCompositeServiceName(instanceForm), instance); // 发布 traceEvent NotifyCenter.publishEvent(new RegisterInstanceTraceEvent(System.currentTimeMillis(), \"\", false, instanceForm.getNamespaceId(), instanceForm.getGroupName(), instanceForm.getServiceName(), instance.getIp(), instance.getPort())); return Result.success(\"ok\"); } 源码位置: com.alibaba.nacos.naming.core.InstanceOperatorClientImpl#registerInstance public void registerInstance(String namespaceId, String serviceName, Instance instance) throws NacosException { NamingUtils.checkInstanceIsLegal(instance); boolean ephemeral = instance.isEphemeral(); String clientId = IpPortBasedClient.getClientId(instance.toInetAddr(), ephemeral); // 创建 client createIpPortClientIfAbsent(clientId); // 构建 service 对象，在 nacos2.0 中，临时属性在 service 上, instance 的临时属性已经没有了 Service service = getService(namespaceId, serviceName, ephemeral); // 具体实现类负责注册，如果是临时实例，EphemeralClientOperationServiceImpl，如果是持久化实例，PersistentClientOperationServiceImpl clientOperationService.registerInstance(service, instance, clientId); } ","date":"2023-08-19 08:00:00","objectID":"/ooooo-notes/%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90-nacos-%E6%B3%A8%E5%86%8C%E5%AE%9E%E4%BE%8B/:2:0","tags":["nacos","source code","源码分析 nacos 系列"],"title":"源码分析 nacos 注册实例","uri":"/ooooo-notes/%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90-nacos-%E6%B3%A8%E5%86%8C%E5%AE%9E%E4%BE%8B/"},{"categories":["源码分析 nacos 系列"],"content":"临时实例注册 源码位置: com.alibaba.nacos.naming.core.v2.service.impl.EphemeralClientOperationServiceImpl#registerInstance @Override public void registerInstance(Service service, Instance instance, String clientId) throws NacosException { NamingUtils.checkInstanceIsLegal(instance); // 获得单例的 service，如果没有就会注册 Service singleton = ServiceManager.getInstance().getSingleton(service); if (!singleton.isEphemeral()) { throw new NacosRuntimeException(NacosException.INVALID_PARAM, String.format(\"Current service %s is persistent service, can't register ephemeral instance.\", singleton.getGroupedServiceName())); } // 获取 client，并检查 client Client client = clientManager.getClient(clientId); if (!clientIsLegal(client, clientId)) { return; } // InstancePublishInfo 就是 nacos 内部实例 InstancePublishInfo instanceInfo = getPublishInfo(instance); // 添加 service 和 instance，这里会发布 ClientChangedEvent 事件，非常重要 client.addServiceInstance(singleton, instanceInfo); client.setLastUpdatedTime(); client.recalculateRevision(); // 发布 ClientRegisterServiceEvent 事件 NotifyCenter.publishEvent(new ClientOperationEvent.ClientRegisterServiceEvent(singleton, clientId)); // 发布 InstanceMetadataEvent 事件 NotifyCenter .publishEvent(new MetadataEvent.InstanceMetadataEvent(singleton, instanceInfo.getMetadataId(), false)); } 源码位置: com.alibaba.nacos.naming.consistency.ephemeral.distro.v2.DistroClientDataProcessor#syncToAllServer // DistroClientDataProcessor 会监听 ClientChangedEvent 事件 private void syncToAllServer(ClientEvent event) { Client client = event.getClient(); // Only ephemeral data sync by Distro, persist client should sync by raft. if (null == client || !client.isEphemeral() || !clientManager.isResponsibleClient(client)) { return; } if (event instanceof ClientEvent.ClientDisconnectEvent) { DistroKey distroKey = new DistroKey(client.getClientId(), TYPE); distroProtocol.sync(distroKey, DataOperation.DELETE); } else if (event instanceof ClientEvent.ClientChangedEvent) { DistroKey distroKey = new DistroKey(client.getClientId(), TYPE); // 同步到其他节点 distroProtocol.sync(distroKey, DataOperation.CHANGE); } } 源码位置: com.alibaba.nacos.naming.core.v2.index.ClientServiceIndexesManager#handleClientOperation // ClientServiceIndexesManager 会监听 ClientRegisterServiceEvent 事件 private void handleClientOperation(ClientOperationEvent event) { Service service = event.getService(); String clientId = event.getClientId(); if (event instanceof ClientOperationEvent.ClientRegisterServiceEvent) { // 添加 client 的 publishIndex addPublisherIndexes(service, clientId); } else if (event instanceof ClientOperationEvent.ClientDeregisterServiceEvent) { removePublisherIndexes(service, clientId); } else if (event instanceof ClientOperationEvent.ClientSubscribeServiceEvent) { addSubscriberIndexes(service, clientId); } else if (event instanceof ClientOperationEvent.ClientUnsubscribeServiceEvent) { removeSubscriberIndexes(service, clientId); } } private void addPublisherIndexes(Service service, String clientId) { // service 和 clientId 是一对多的关系 publisherIndexes.computeIfAbsent(service, key -\u003e new ConcurrentHashSet\u003c\u003e()); publisherIndexes.get(service).add(clientId); // 发布 ServiceChangedEvent 事件 NotifyCenter.publishEvent(new ServiceEvent.ServiceChangedEvent(service, true)); } 源码位置: com.alibaba.nacos.naming.push.v2.NamingSubscriberServiceV2Impl#onEvent // NamingSubscriberServiceV2Impl 监听 ServiceChangedEvent public void onEvent(Event event) { if (event instanceof ServiceEvent.ServiceChangedEvent) { // If service changed, push to all subscribers. // service 下的 instance 改变之后，要推送给所有的订阅者 ServiceEvent.ServiceChangedEvent serviceChangedEvent = (ServiceEvent.ServiceChangedEvent) event; Service service = serviceChangedEvent.getService(); delayTaskEngine.addTask(service, new PushDelayTask(service, PushConfig.getInstance().getPushTaskDelay())); MetricsMonitor.incrementServiceChangeCount(service.getNamespace(), service.getGroup(), service.getName()); } else if (event instanceof ServiceEvent.ServiceSubscribedEvent) { // If service is subscribed by one cl","date":"2023-08-19 08:00:00","objectID":"/ooooo-notes/%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90-nacos-%E6%B3%A8%E5%86%8C%E5%AE%9E%E4%BE%8B/:3:0","tags":["nacos","source code","源码分析 nacos 系列"],"title":"源码分析 nacos 注册实例","uri":"/ooooo-notes/%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90-nacos-%E6%B3%A8%E5%86%8C%E5%AE%9E%E4%BE%8B/"},{"categories":["源码分析 nacos 系列"],"content":"持久化实例注册 源码位置: com.alibaba.nacos.naming.core.v2.service.impl.PersistentClientOperationServiceImpl#registerInstance @Override public void registerInstance(Service service, Instance instance, String clientId) { // 和临时实例注册一样，获取单例的 service Service singleton = ServiceManager.getInstance().getSingleton(service); if (singleton.isEphemeral()) { throw new NacosRuntimeException(NacosException.INVALID_PARAM, String.format(\"Current service %s is ephemeral service, can't register persistent instance.\", singleton.getGroupedServiceName())); } // 包装为 writeRequest 对象 final InstanceStoreRequest request = new InstanceStoreRequest(); request.setService(service); request.setInstance(instance); request.setClientId(clientId); // 这里设置了 group，在构造函数中会初始化 group 的 RequestProcessor final WriteRequest writeRequest = WriteRequest.newBuilder().setGroup(group()) .setData(ByteString.copyFrom(serializer.serialize(request))).setOperation(DataOperation.ADD.name()) .build(); try { // CPProtocol 负责写请求，同步到其他的节点，然后应用状态机 protocol.write(writeRequest); Loggers.RAFT.info(\"Client registered. service={}, clientId={}, instance={}\", service, instance, clientId); } catch (Exception e) { throw new NacosRuntimeException(NacosException.SERVER_ERROR, e); } } // 构造函数中，初始化话了 public PersistentClientOperationServiceImpl(final PersistentIpPortClientManager clientManager) { this.clientManager = clientManager; this.protocol = ApplicationUtils.getBean(ProtocolManager.class).getCpProtocol(); // 自己负责来处理 apply WriteRequest this.protocol.addRequestProcessors(Collections.singletonList(this)); } // 应用 raft 的状态机 // protocol.write(writeRequest) 之后, 就会回调这个方法 @Override public Response onApply(WriteRequest request) { final Lock lock = readLock; lock.lock(); try { final InstanceStoreRequest instanceRequest = serializer.deserialize(request.getData().toByteArray()); final DataOperation operation = DataOperation.valueOf(request.getOperation()); switch (operation) { case ADD: // 处理实例注册 onInstanceRegister(instanceRequest.service, instanceRequest.instance, instanceRequest.getClientId()); break; case DELETE: onInstanceDeregister(instanceRequest.service, instanceRequest.getClientId()); break; case CHANGE: if (instanceAndServiceExist(instanceRequest)) { onInstanceRegister(instanceRequest.service, instanceRequest.instance, instanceRequest.getClientId()); } break; default: return Response.newBuilder().setSuccess(false).setErrMsg(\"unsupport operation : \" + operation) .build(); } return Response.newBuilder().setSuccess(true).build(); } catch (Exception e) { Loggers.RAFT.warn(\"Persistent client operation failed. \", e); return Response.newBuilder().setSuccess(false) .setErrMsg(\"Persistent client operation failed. \" + e.getMessage()).build(); } finally { lock.unlock(); } } // 处理实例注册, 基本和临时实例注册一样, 后面就不重复分析了 private void onInstanceRegister(Service service, Instance instance, String clientId) { // 获取 service 和 client Service singleton = ServiceManager.getInstance().getSingleton(service); if (!clientManager.contains(clientId)) { clientManager.clientConnected(clientId, new ClientAttributes()); } Client client = clientManager.getClient(clientId); InstancePublishInfo instancePublishInfo = getPublishInfo(instance); // 添加 service 和 instance，发布 ClientChangedEvent 事件 client.addServiceInstance(singleton, instancePublishInfo); client.setLastUpdatedTime(); // 发布 ClientRegisterServiceEvent 事件 NotifyCenter.publishEvent(new ClientOperationEvent.ClientRegisterServiceEvent(singleton, clientId)); } ","date":"2023-08-19 08:00:00","objectID":"/ooooo-notes/%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90-nacos-%E6%B3%A8%E5%86%8C%E5%AE%9E%E4%BE%8B/:4:0","tags":["nacos","source code","源码分析 nacos 系列"],"title":"源码分析 nacos 注册实例","uri":"/ooooo-notes/%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90-nacos-%E6%B3%A8%E5%86%8C%E5%AE%9E%E4%BE%8B/"},{"categories":["源码分析 nacos 系列"],"content":" nacos 基于 2.2.4 版本 ","date":"2023-08-18 08:00:00","objectID":"/ooooo-notes/%E6%90%AD%E5%BB%BA-nacos-%E6%BA%90%E7%A0%81%E8%B0%83%E8%AF%95%E7%8E%AF%E5%A2%83/:0:0","tags":["nacos","source code","源码分析 nacos 系列"],"title":"搭建 nacos 源码调试环境","uri":"/ooooo-notes/%E6%90%AD%E5%BB%BA-nacos-%E6%BA%90%E7%A0%81%E8%B0%83%E8%AF%95%E7%8E%AF%E5%A2%83/"},{"categories":["源码分析 nacos 系列"],"content":"下载源码和编译 git clone git@github.com:alibaba/nacos.git mvn clean install -U -DskipTests ","date":"2023-08-18 08:00:00","objectID":"/ooooo-notes/%E6%90%AD%E5%BB%BA-nacos-%E6%BA%90%E7%A0%81%E8%B0%83%E8%AF%95%E7%8E%AF%E5%A2%83/:1:0","tags":["nacos","source code","源码分析 nacos 系列"],"title":"搭建 nacos 源码调试环境","uri":"/ooooo-notes/%E6%90%AD%E5%BB%BA-nacos-%E6%BA%90%E7%A0%81%E8%B0%83%E8%AF%95%E7%8E%AF%E5%A2%83/"},{"categories":["源码分析 nacos 系列"],"content":"配置环境 参考 startup.sh 文件，添加相应的 jvm 和 program 的参数。 添加 jvm 参数，-Dnacos.standalone=true, 单机启动 添加 jvm 参数，-Dnacos.home=/Users/ooooo/Code/Demo/nacos/distribution, 集群启动 添加 program 参数，--spring.config.additional-location=/Users/ooooo/Code/Demo/nacos/distribution/conf/application.properties 配置 cluster.conf，添加自己机器的 ip 配置 application.properties, 添加数据库相关配置，脚本位置在 /Users/ooooo/Code/Demo/nacos/distribution/conf/mysql-schema.sql 相关截图如下： run cluster-config application-properties ","date":"2023-08-18 08:00:00","objectID":"/ooooo-notes/%E6%90%AD%E5%BB%BA-nacos-%E6%BA%90%E7%A0%81%E8%B0%83%E8%AF%95%E7%8E%AF%E5%A2%83/:2:0","tags":["nacos","source code","源码分析 nacos 系列"],"title":"搭建 nacos 源码调试环境","uri":"/ooooo-notes/%E6%90%AD%E5%BB%BA-nacos-%E6%BA%90%E7%A0%81%E8%B0%83%E8%AF%95%E7%8E%AF%E5%A2%83/"},{"categories":["源码分析 nacos 系列"],"content":"启动 log ","date":"2023-08-18 08:00:00","objectID":"/ooooo-notes/%E6%90%AD%E5%BB%BA-nacos-%E6%BA%90%E7%A0%81%E8%B0%83%E8%AF%95%E7%8E%AF%E5%A2%83/:3:0","tags":["nacos","source code","源码分析 nacos 系列"],"title":"搭建 nacos 源码调试环境","uri":"/ooooo-notes/%E6%90%AD%E5%BB%BA-nacos-%E6%BA%90%E7%A0%81%E8%B0%83%E8%AF%95%E7%8E%AF%E5%A2%83/"},{"categories":["随笔"],"content":"1. goreleaser 的简单说明 # install goreleaser brew install goreleaser # init goreleaser, create .goreleaser.yml goreleaser init # available commands goreleaser build --clean goreleaser release --snapshot --clean ","date":"2023-08-01 08:00:00","objectID":"/ooooo-notes/github-%E4%B8%8A%E4%BD%BF%E7%94%A8-goreleaser/:1:0","tags":["github","tools","goreleaser"],"title":"github 上使用 goreleaser","uri":"/ooooo-notes/github-%E4%B8%8A%E4%BD%BF%E7%94%A8-goreleaser/"},{"categories":["随笔"],"content":"2. .goreleaser.yml 示例文件 # This is an example .goreleaser.yml file with some sensible defaults. # Make sure to check the documentation at https://goreleaser.com before: hooks: # You may remove this if you don't use go modules. - go mod tidy # you may remove this if you don't need go generate # - go generate ./... builds: - id: http-tunnel-client binary: http-tunnel-client main: ./cmd/http-tunnel-client env: - CGO_ENABLED=0 goos: - linux - windows - darwin goarch: - amd64 - arm64 - id: http-tunnel-server binary: http-tunnel-server main: ./cmd/http-tunnel-server env: - CGO_ENABLED=0 goos: - linux - windows - darwin goarch: - amd64 - arm64 archives: - format: tar.gz # this name template makes the OS and Arch compatible with the results of uname. name_template: \u003e- {{ .ProjectName }}_ {{- .Version }}_ {{- title .Os }}_ {{- .Arch }} {{- if .Arm }}v{{ .Arm }}{{ end }} # use zip for windows archives format_overrides: - goos: windows format: zip checksum: name_template: 'checksums.txt' snapshot: name_template: \"{{ incpatch .Version }}-next\" changelog: sort: asc filters: exclude: - '^docs:' - '^test:' # The lines beneath this are called `modelines`. See `:help modeline` # Feel free to remove those if you don't want/use them. # yaml-language-server: $schema=https://goreleaser.com/static/schema.json # vim: set ts=2 sw=2 tw=0 fo=cnqoj ","date":"2023-08-01 08:00:00","objectID":"/ooooo-notes/github-%E4%B8%8A%E4%BD%BF%E7%94%A8-goreleaser/:2:0","tags":["github","tools","goreleaser"],"title":"github 上使用 goreleaser","uri":"/ooooo-notes/github-%E4%B8%8A%E4%BD%BF%E7%94%A8-goreleaser/"},{"categories":["随笔"],"content":"3. github action 配置 文件路径：.github/workflows/goreleaser.yml name: goreleaser on: push: tags: - '*.*.*' # Trigger the workflow by mannually workflow_dispatch: jobs: goreleaser: runs-on: ubuntu-latest steps: - name: Checkout uses: actions/checkout@v3 with: fetch-depth: 0 - name: Set up Go uses: actions/setup-go@v4 with: go-version: '1.20' - name: Run GoReleaser uses: goreleaser/goreleaser-action@v4 with: version: latest args: release --clean env: GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }} ","date":"2023-08-01 08:00:00","objectID":"/ooooo-notes/github-%E4%B8%8A%E4%BD%BF%E7%94%A8-goreleaser/:3:0","tags":["github","tools","goreleaser"],"title":"github 上使用 goreleaser","uri":"/ooooo-notes/github-%E4%B8%8A%E4%BD%BF%E7%94%A8-goreleaser/"},{"categories":["随笔"],"content":"4. github token read write permisson ","date":"2023-08-01 08:00:00","objectID":"/ooooo-notes/github-%E4%B8%8A%E4%BD%BF%E7%94%A8-goreleaser/:4:0","tags":["github","tools","goreleaser"],"title":"github 上使用 goreleaser","uri":"/ooooo-notes/github-%E4%B8%8A%E4%BD%BF%E7%94%A8-goreleaser/"},{"categories":["随笔"],"content":"1. 搭建 kafka 环境 这里使用 docker 来搭建。 docker-compose.yml 配置如下，客户端端口:9094 version: \"3\" services: kafka: image: 'bitnami/kafka:latest' ports: - '9092:9092' - '9094:9094' environment: - KAFKA_CFG_NODE_ID=0 - KAFKA_CFG_PROCESS_ROLES=controller,broker - KAFKA_CFG_LISTENERS=PLAINTEXT://:9092,CONTROLLER://:9093,EXTERNAL://:9094 - KAFKA_CFG_ADVERTISED_LISTENERS=PLAINTEXT://kafka:9092,EXTERNAL://localhost:9094 - KAFKA_CFG_LISTENER_SECURITY_PROTOCOL_MAP=CONTROLLER:PLAINTEXT,EXTERNAL:SASL_PLAINTEXT,PLAINTEXT:PLAINTEXT - KAFKA_CFG_CONTROLLER_QUORUM_VOTERS=0@kafka:9093 - KAFKA_CFG_CONTROLLER_LISTENER_NAMES=CONTROLLER - KAFKA_CLIENT_USERS=user - KAFKA_CLIENT_PASSWORDS=password - KAFKA_CLIENT_LISTENER_NAME=SASL_PLAINTEXT 启动kafka docker-compose up -d ","date":"2023-08-01 08:00:00","objectID":"/ooooo-notes/kafka-%E7%9A%84-sasl-%E8%AE%A4%E8%AF%81/:1:0","tags":["kafka"],"title":"kafka 的 SASL 认证","uri":"/ooooo-notes/kafka-%E7%9A%84-sasl-%E8%AE%A4%E8%AF%81/"},{"categories":["随笔"],"content":"2. 客户端连接配置 spring: kafka: bootstrap-servers: localhost:9094 properties: security.protocol: SASL_PLAINTEXT sasl.jaas.config: org.apache.kafka.common.security.scram.ScramLoginModule required username='user' password='password'; sasl.mechanism: SCRAM-SHA-512 ","date":"2023-08-01 08:00:00","objectID":"/ooooo-notes/kafka-%E7%9A%84-sasl-%E8%AE%A4%E8%AF%81/:2:0","tags":["kafka"],"title":"kafka 的 SASL 认证","uri":"/ooooo-notes/kafka-%E7%9A%84-sasl-%E8%AE%A4%E8%AF%81/"},{"categories":["随笔"],"content":"3. 参考 docker kafka kafka sasl ","date":"2023-08-01 08:00:00","objectID":"/ooooo-notes/kafka-%E7%9A%84-sasl-%E8%AE%A4%E8%AF%81/:3:0","tags":["kafka"],"title":"kafka 的 SASL 认证","uri":"/ooooo-notes/kafka-%E7%9A%84-sasl-%E8%AE%A4%E8%AF%81/"},{"categories":["随笔"],"content":" websocket ","date":"2023-07-31 08:00:00","objectID":"/ooooo-notes/protocols/:0:0","tags":["protocol"],"title":"protocols","uri":"/ooooo-notes/protocols/"},{"categories":["随笔"],"content":"1. HttpHelloWorldServerHandler 为啥需要使用 SimpleChannelInboundHandler ? HttpObject 的子类有 LastHttpContent, HttpContent, HttpData， 它需要手动调用 release()。 ","date":"2023-07-30 08:00:00","objectID":"/ooooo-notes/%E4%BD%BF%E7%94%A8-netty-%E7%9A%84%E6%B3%A8%E6%84%8F%E7%82%B9/:1:0","tags":["netty"],"title":"使用 netty 的注意点","uri":"/ooooo-notes/%E4%BD%BF%E7%94%A8-netty-%E7%9A%84%E6%B3%A8%E6%84%8F%E7%82%B9/"},{"categories":["随笔"],"content":"1. 代码 在自定义封装 MQ 时，要注意 producer 和 consumer 的初始化时机，否则会出现 consumer 占用 consumerQueue 的情况 @Slf4j public class RocketMQHandler extends AbstractMQHandler { private final RocketMQConfig config; private final Supplier\u003cDefaultMQProducer\u003e producer; private final Supplier\u003cDefaultLitePullConsumer\u003e consumer; public RocketMQHandler(RocketMQProperties properties, RocketMQConfig config) { this.config = config; // 这里要延迟初始化，否则启动 consumer 占用 consumerQueue this.producer = SingletonSupplier.of(() -\u003e createProducer(properties, config)); this.consumer = SingletonSupplier.of(() -\u003e createConsumer(properties, config)); } @SneakyThrows @Override public void send(String message) { Message m = new Message(config.getDestinationName(), message.getBytes(StandardCharsets.UTF_8)); if (log.isTraceEnabled()) { log.trace(\"{} send message: {}\", RocketMQHandler.this.getClass().getSimpleName(), message); } producer.get().send(m); } @Override public void receive(MQListener listener) { ConsumerTask task = new ConsumerTask(listener); executorService.schedule(task, PULL_PERIOD, TimeUnit.MILLISECONDS); } @AllArgsConstructor private class ConsumerTask implements Runnable { private MQListener listener; @Override public void run() { List\u003cMessageExt\u003e msgs = consumer.get().poll(PULL_PERIOD); if (CollectionUtils.isNotEmpty(msgs)) { msgs.forEach(m -\u003e { String message = new String(m.getBody(), StandardCharsets.UTF_8); if (log.isTraceEnabled()) { log.trace(\"{} receive message: {}\", RocketMQHandler.this.getClass().getSimpleName(), message); } listener.onMessage(message); }); } executorService.schedule(this, PULL_PERIOD, TimeUnit.MILLISECONDS); } } @SneakyThrows private DefaultMQProducer createProducer(RocketMQProperties properties, RocketMQConfig config) { DefaultMQProducer producer = new DefaultMQProducer(config.getProducerGroup()); producer.setNamesrvAddr(properties.getNamesrvAddr()); producer.start(); return producer; } @SneakyThrows private DefaultLitePullConsumer createConsumer(RocketMQProperties properties, RocketMQConfig config) { DefaultLitePullConsumer consumer = new DefaultLitePullConsumer(config.getConsumerGroup()); consumer.setNamesrvAddr(properties.getNamesrvAddr()); switch (config.getConsumeMode()) { case P2P: consumer.setMessageModel(MessageModel.CLUSTERING); break; case BROADCAST: consumer.setMessageModel(MessageModel.BROADCASTING); break; } consumer.subscribe(config.getDestinationName(), \"*\"); consumer.start(); return consumer; } } ","date":"2023-07-07 08:00:00","objectID":"/ooooo-notes/rocketmq-%E7%9A%84-litepullconsumer-%E4%BD%BF%E7%94%A8/:1:0","tags":["spring","rocketmq"],"title":"rocketmq 的 LitePullConsumer 使用","uri":"/ooooo-notes/rocketmq-%E7%9A%84-litepullconsumer-%E4%BD%BF%E7%94%A8/"},{"categories":["随笔"],"content":"1. 配置 dubbo: application: parameters: registry-type: service registries: a: address: nacos://172.16.1.104:7848 group: DUBBO_SERVICE_GROUP parameters: namespace: a b: address: nacos://172.16.1.104:7848 group: DUBBO_SERVICE_GROUP parameters: namespace: b ","date":"2023-07-03 08:00:00","objectID":"/ooooo-notes/dubbo3-%E5%A4%9A%E6%B3%A8%E5%86%8C%E4%B8%AD%E5%BF%83%E7%9A%84%E5%B0%8Fbug/:1:0","tags":["dubbo"],"title":"dubbo3 多注册中心的小 bug","uri":"/ooooo-notes/dubbo3-%E5%A4%9A%E6%B3%A8%E5%86%8C%E4%B8%AD%E5%BF%83%E7%9A%84%E5%B0%8Fbug/"},{"categories":["随笔"],"content":"2. 问题 只会注册到一个 namespace 中 ","date":"2023-07-03 08:00:00","objectID":"/ooooo-notes/dubbo3-%E5%A4%9A%E6%B3%A8%E5%86%8C%E4%B8%AD%E5%BF%83%E7%9A%84%E5%B0%8Fbug/:2:0","tags":["dubbo"],"title":"dubbo3 多注册中心的小 bug","uri":"/ooooo-notes/dubbo3-%E5%A4%9A%E6%B3%A8%E5%86%8C%E4%B8%AD%E5%BF%83%E7%9A%84%E5%B0%8Fbug/"},{"categories":["随笔"],"content":"3. github dubbo issue ","date":"2023-07-03 08:00:00","objectID":"/ooooo-notes/dubbo3-%E5%A4%9A%E6%B3%A8%E5%86%8C%E4%B8%AD%E5%BF%83%E7%9A%84%E5%B0%8Fbug/:3:0","tags":["dubbo"],"title":"dubbo3 多注册中心的小 bug","uri":"/ooooo-notes/dubbo3-%E5%A4%9A%E6%B3%A8%E5%86%8C%E4%B8%AD%E5%BF%83%E7%9A%84%E5%B0%8Fbug/"},{"categories":["随笔"],"content":"1. 问题 在真实的使用过程中，可能不同的 mapper 接口使用的 sqlSessionFactory 不一样。就比如下面这个例子。 // 这个注解虽然可以指定 sqlSessionFactory, 但是最终使用的 configuration 对象是同一份。 @MapperScan(\"com.ooooo.**.mapper1\") @MapperScan(\"com.ooooo.**.mapper2\") public class DemoApplication { public static void main(String[] args) { SpringApplication.run(DemoApplication.class, args); } } ","date":"2023-06-08 08:00:00","objectID":"/ooooo-notes/mybatis-plus-%E7%9A%84%E8%87%AA%E5%AE%9A%E4%B9%89-mapper/:1:0","tags":["spring","mybatis"],"title":"mybatis-plus 的自定义 mapper","uri":"/ooooo-notes/mybatis-plus-%E7%9A%84%E8%87%AA%E5%AE%9A%E4%B9%89-mapper/"},{"categories":["随笔"],"content":"2. 解决方式 可以使用 MapperFactoryBean 来扩展，下面我给出相应的示例代码。 @Configuration public class ComponentConfigMybatiPlusConfiguration { @Getter public static SqlSessionFactory sqlSessionFactory; @Bean public MapperFactoryBean\u003cComponentTreeMapper\u003e componentTreeMapper() { return createMapper(ComponentTreeMapper.class); } private \u003cT\u003e MapperFactoryBean\u003cT\u003e createMapper(Class\u003cT\u003e clazz) { MapperFactoryBean\u003cT\u003e factoryBean = new MapperFactoryBean\u003c\u003e(clazz); factoryBean.setSqlSessionFactory(sqlSessionFactory()); return factoryBean; } private synchronized SqlSessionFactory sqlSessionFactory() { if (sqlSessionFactory != null) { return sqlSessionFactory; } // 使用默认的dataSource DataSource dataSource = SpringUtil.getBean(DataSource.class); Environment environment = new Environment(COMPONENT_CONFIG, new SpringManagedTransactionFactory(), dataSource); // build configuration MybatisConfiguration configuration = new MybatisConfiguration(); configuration.setEnvironment(environment); configuration.setCacheEnabled(false); configuration.setLocalCacheScope(LocalCacheScope.STATEMENT); setInterceptors(configuration); setMapperLocations(configuration, new String[]{\"classpath*:/mapper/**/*.xml\"}); setGlobalConfig(configuration); // factory sqlSessionFactory = new MybatisSqlSessionFactoryBuilder().build(configuration); return sqlSessionFactory; } private void setInterceptors(MybatisConfiguration configuration) { MybatisPlusInterceptor mybatisPlusInterceptor = new MybatisPlusInterceptor(); mybatisPlusInterceptor.addInnerInterceptor(new PaginationInnerInterceptor()); configuration.addInterceptor(mybatisPlusInterceptor); } private void setMapperLocations(MybatisConfiguration configuration, String[] mapperLocations) { ResourcePatternResolver resourceResolver = new PathMatchingResourcePatternResolver(); for (String mapperLocation : mapperLocations) { try { Resource[] resources = resourceResolver.getResources(mapperLocation); for (Resource resource : resources) { if (resource.exists()) { XMLMapperBuilder xmlMapperBuilder = new XMLMapperBuilder(resource.getInputStream(), configuration, resource.toString(), configuration.getSqlFragments()); xmlMapperBuilder.parse(); } } } catch (IOException ignored) { } } } private void setGlobalConfig(MybatisConfiguration configuration) { GlobalConfig globalConfig = GlobalConfigUtils.getGlobalConfig(configuration); // MetaObjectHandler globalConfig.setMetaObjectHandler(new ComponentMetaObjectHandler()); } } ","date":"2023-06-08 08:00:00","objectID":"/ooooo-notes/mybatis-plus-%E7%9A%84%E8%87%AA%E5%AE%9A%E4%B9%89-mapper/:2:0","tags":["spring","mybatis"],"title":"mybatis-plus 的自定义 mapper","uri":"/ooooo-notes/mybatis-plus-%E7%9A%84%E8%87%AA%E5%AE%9A%E4%B9%89-mapper/"},{"categories":["随笔"],"content":"1. 在 docker 上安装 harbor # 下载harbor wget https://github.com/goharbor/harbor/releases/download/v2.8.1/harbor-offline-installer-v2.8.1.tgz # 生成CA秘钥 openssl genrsa -out ca.key 4096 # 生成CA证书 openssl req -x509 -new -nodes -sha512 -days 3650 \\ -subj \"/C=CN/ST=Beijing/L=Beijing/O=example/OU=Personal/CN=yourdomain.com\" \\ -key ca.key \\ -out ca.crt # 生成秘钥 openssl genrsa -out yourdomain.com.key 4096 # 生成证书请求 openssl req -sha512 -new \\ -subj \"/C=CN/ST=Beijing/L=Beijing/O=example/OU=Personal/CN=yourdomain.com\" \\ -key yourdomain.com.key \\ -out yourdomain.com.csr # 生成证书 cat \u003e v3.ext \u003c\u003c-EOF authorityKeyIdentifier=keyid,issuer basicConstraints=CA:FALSE keyUsage = digitalSignature, nonRepudiation, keyEncipherment, dataEncipherment extendedKeyUsage = serverAuth subjectAltName = @alt_names [alt_names] DNS.1=yourdomain.com DNS.2=yourdomain DNS.3=hostname EOF openssl x509 -req -sha512 -days 3650 \\ -extfile v3.ext \\ -CA ca.crt -CAkey ca.key -CAcreateserial \\ -in yourdomain.com.csr \\ -out yourdomain.com.crt # 复制到harbor中， /data/cert/ 是harbor的证书目录 cp yourdomain.com.crt /data/cert/ cp yourdomain.com.key /data/cert/ # 转换为cert格式, 给docker使用 openssl x509 -inform PEM -in yourdomain.com.crt -out yourdomain.com.cert # 复制到docker中，这里是双向tls cp yourdomain.com.cert /etc/docker/certs.d/yourdomain.com/ cp yourdomain.com.key /etc/docker/certs.d/yourdomain.com/ cp ca.crt /etc/docker/certs.d/yourdomain.com/ # 重启docker,加载证书 systemctl restart docker # 执行harbor脚本，启动harbor ./prepare # 关闭 harbor docker-compose down -v # 启动 harbor docker-compose up -d # 验证docker docker login yourdomain.com harbor官方文档 ","date":"2023-06-02 08:00:00","objectID":"/ooooo-notes/%E5%AE%89%E8%A3%85-harbor/:1:0","tags":["docker","harbor"],"title":"安装 harbor","uri":"/ooooo-notes/%E5%AE%89%E8%A3%85-harbor/"},{"categories":["随笔"],"content":"2. containerd 配置 harbor # 复制证书到containerd mkdir /etc/containerd/yourdomain.com cp ca.crt /etc/containerd/yourdomain.com/ # 配置containerd vim /etc/containerd/config.toml #配置endpoint连接地址 [plugins.\"io.containerd.grpc.v1.cri\".registry.mirrors] [plugins.\"io.containerd.grpc.v1.cri\".registry.mirrors.\"yourdomain.com\"] endpoint = [\"https://yourdomain.com\"] #配置ca文件路径和用户名密码 [plugins.\"io.containerd.grpc.v1.cri\".registry.configs] [plugins.\"io.containerd.grpc.v1.cri\".registry.configs.\"yourdomain.com\".tls] ca_file = \"/etc/containerd/yourdomain.com/ca.crt\" [plugins.\"io.containerd.grpc.v1.cri\".registry.configs.\"yourdomain.com\".auth] username = \"admin\" password = \"Harbor12345\" 博客 ","date":"2023-06-02 08:00:00","objectID":"/ooooo-notes/%E5%AE%89%E8%A3%85-harbor/:2:0","tags":["docker","harbor"],"title":"安装 harbor","uri":"/ooooo-notes/%E5%AE%89%E8%A3%85-harbor/"},{"categories":["随笔"],"content":"1. nacos 的端口 nacos 的 http 端口为 8848，但是 nacos2.0 之后使用 grpc 端口, 而且是偏移量计算的，所以使用 nginx 代理就有坑。 对于 spring 来说, 可以配置多个地址 spring: cloud: nacos: server-addr: 172.168.0.101:8848,172.168.0.102:8848,172.168.0.103:8848 ","date":"2023-06-01 08:00:00","objectID":"/ooooo-notes/nacos-%E9%9B%86%E7%BE%A4%E7%94%A8-nginx-%E4%BB%A3%E7%90%86%E9%97%AE%E9%A2%98/:1:0","tags":["nacos","spring"],"title":"nacos 集群用 nginx 代理问题","uri":"/ooooo-notes/nacos-%E9%9B%86%E7%BE%A4%E7%94%A8-nginx-%E4%BB%A3%E7%90%86%E9%97%AE%E9%A2%98/"},{"categories":["随笔"],"content":"2. 源码 发送请求的方法 com.alibaba.nacos.common.remote.client.grpc.GrpcConnection#request @Override public Response request(Request request, long timeouts) throws NacosException { Payload grpcRequest = GrpcUtils.convert(request); // 发送请求 ListenableFuture\u003cPayload\u003e requestFuture = grpcFutureServiceStub.request(grpcRequest); Payload grpcResponse; try { grpcResponse = requestFuture.get(timeouts, TimeUnit.MILLISECONDS); } catch (Exception e) { throw new NacosException(NacosException.SERVER_ERROR, e); } return (Response) GrpcUtils.parse(grpcResponse); } 创建 grpc客户端 的方法 com.alibaba.nacos.common.remote.client.grpc.GrpcClient#connectToServer @Override public Connection connectToServer(ServerInfo serverInfo) { try { if (grpcExecutor == null) { this.grpcExecutor = createGrpcExecutor(serverInfo.getServerIp()); } // 这里就是计算端口的逻辑， serverPort 默认为 8848， rpcPortOffset 为 1000 int port = serverInfo.getServerPort() + rpcPortOffset(); ManagedChannel managedChannel = createNewManagedChannel(serverInfo.getServerIp(), port); // 新建 RequestGrpc.RequestFutureStub newChannelStubTemp = createNewChannelStub(managedChannel); if (newChannelStubTemp != null) { Response response = serverCheck(serverInfo.getServerIp(), port, newChannelStubTemp); if (response == null || !(response instanceof ServerCheckResponse)) { shuntDownChannel(managedChannel); return null; } BiRequestStreamGrpc.BiRequestStreamStub biRequestStreamStub = BiRequestStreamGrpc .newStub(newChannelStubTemp.getChannel()); GrpcConnection grpcConn = new GrpcConnection(serverInfo, grpcExecutor); grpcConn.setConnectionId(((ServerCheckResponse) response).getConnectionId()); //create stream request and bind connection event to this connection. StreamObserver\u003cPayload\u003e payloadStreamObserver = bindRequestStream(biRequestStreamStub, grpcConn); // stream observer to send response to server grpcConn.setPayloadStreamObserver(payloadStreamObserver); grpcConn.setGrpcFutureServiceStub(newChannelStubTemp); grpcConn.setChannel(managedChannel); //send a setup request. ConnectionSetupRequest conSetupRequest = new ConnectionSetupRequest(); conSetupRequest.setClientVersion(VersionUtils.getFullClientVersion()); conSetupRequest.setLabels(super.getLabels()); conSetupRequest.setAbilities(super.clientAbilities); conSetupRequest.setTenant(super.getTenant()); grpcConn.sendRequest(conSetupRequest); //wait to register connection setup Thread.sleep(100L); return grpcConn; } return null; } catch (Exception e) { LOGGER.error(\"[{}]Fail to connect to server!,error={}\", GrpcClient.this.getName(), e); } return null; } ","date":"2023-06-01 08:00:00","objectID":"/ooooo-notes/nacos-%E9%9B%86%E7%BE%A4%E7%94%A8-nginx-%E4%BB%A3%E7%90%86%E9%97%AE%E9%A2%98/:2:0","tags":["nacos","spring"],"title":"nacos 集群用 nginx 代理问题","uri":"/ooooo-notes/nacos-%E9%9B%86%E7%BE%A4%E7%94%A8-nginx-%E4%BB%A3%E7%90%86%E9%97%AE%E9%A2%98/"},{"categories":["随笔"],"content":"3. 参考 官方文档-端口说明 ","date":"2023-06-01 08:00:00","objectID":"/ooooo-notes/nacos-%E9%9B%86%E7%BE%A4%E7%94%A8-nginx-%E4%BB%A3%E7%90%86%E9%97%AE%E9%A2%98/:3:0","tags":["nacos","spring"],"title":"nacos 集群用 nginx 代理问题","uri":"/ooooo-notes/nacos-%E9%9B%86%E7%BE%A4%E7%94%A8-nginx-%E4%BB%A3%E7%90%86%E9%97%AE%E9%A2%98/"},{"categories":["随笔"],"content":" 我在 chrome 安装了 SwitchyOmega 代理插件，导致资源加载有问题。 检查 clash 代理，刷新 dns 配置，试试全局代理 可以增加 SwitchyOmega 配置 ","date":"2023-05-27 08:00:00","objectID":"/ooooo-notes/github-page-%E5%9B%BE%E7%89%87%E6%97%A0%E6%B3%95%E8%AE%BF%E9%97%AE/:0:0","tags":["resolution"],"title":"github page 图片无法访问","uri":"/ooooo-notes/github-page-%E5%9B%BE%E7%89%87%E6%97%A0%E6%B3%95%E8%AE%BF%E9%97%AE/"},{"categories":["随笔"],"content":" -n 以数字显示 -X 显示包体 -i 指定网卡 -w 写入文件 -c 包的个数 ","date":"2023-05-24 08:00:00","objectID":"/ooooo-notes/tcpdump-%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/:0:0","tags":["linux"],"title":"tcpdump 常用命令","uri":"/ooooo-notes/tcpdump-%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/"},{"categories":["随笔"],"content":"1. 指定端口 tcpdump -n -X -i any port 1234 -w 1.cap ","date":"2023-05-24 08:00:00","objectID":"/ooooo-notes/tcpdump-%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/:1:0","tags":["linux"],"title":"tcpdump 常用命令","uri":"/ooooo-notes/tcpdump-%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/"},{"categories":["随笔"],"content":"2. 指定主机 tcpdump -n -X -i any host 192.168.0.101 -w 1.cap ","date":"2023-05-24 08:00:00","objectID":"/ooooo-notes/tcpdump-%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/:2:0","tags":["linux"],"title":"tcpdump 常用命令","uri":"/ooooo-notes/tcpdump-%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/"},{"categories":["随笔"],"content":"3. 其他 # 监视指定主机和端口的数据包 tcpdump -i ens33 port 8080 and host node1 # 监视指定网络的数据包，如本机与192.168网段通信的数据包，\"-c 10\"表示只抓取10个包 tcpdump -i ens33 -c 10 net 192.168 # 抓取ping包 tcpdump -c 5 -nn -i eth0 icmp and src 192.168.100.62 ","date":"2023-05-24 08:00:00","objectID":"/ooooo-notes/tcpdump-%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/:3:0","tags":["linux"],"title":"tcpdump 常用命令","uri":"/ooooo-notes/tcpdump-%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/"},{"categories":["随笔"],"content":"3.参考 tcpdump说明 ","date":"2023-05-24 08:00:00","objectID":"/ooooo-notes/tcpdump-%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/:4:0","tags":["linux"],"title":"tcpdump 常用命令","uri":"/ooooo-notes/tcpdump-%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/"},{"categories":["随笔"],"content":"1. 检查本机的 dns 配置 # 建议不要配置 search，除非你自己明确, 可用的 dns 域名，如 8.8.8.8 # 修改后，centos系统不需要重启 NetworkManager, 重启可能被覆盖 cat /etc/resolv.conf # 重启 kubelet systemctl restart kubelet # 重启 k8s pod kubectl rollout restart deploy ","date":"2023-05-19 08:00:00","objectID":"/ooooo-notes/k8s-%E4%B8%AD%E7%9A%84-dns-%E9%97%AE%E9%A2%98/:1:0","tags":["resolution","k8s","dns","cloud native"],"title":"k8s 中的 dns 问题","uri":"/ooooo-notes/k8s-%E4%B8%AD%E7%9A%84-dns-%E9%97%AE%E9%A2%98/"},{"categories":["随笔"],"content":"2. 检查 pod 的 dns 配置 # 进入容器中 kubectl debug -it some-pod --image=busybox -- sh # 在容器中查看 dns 配置, 这里一定要是 coredns 的 clusterIP, 如果不对，检查 kubelet 的 dns 配置 cat /etc/resolv.conf # 在容器中查看 hosts 配置 cat /etc/hosts ","date":"2023-05-19 08:00:00","objectID":"/ooooo-notes/k8s-%E4%B8%AD%E7%9A%84-dns-%E9%97%AE%E9%A2%98/:2:0","tags":["resolution","k8s","dns","cloud native"],"title":"k8s 中的 dns 问题","uri":"/ooooo-notes/k8s-%E4%B8%AD%E7%9A%84-dns-%E9%97%AE%E9%A2%98/"},{"categories":["随笔"],"content":"3. 检查并配置 coredns 配置 # 检查配置 kubectl get cm coredns -n kube-system -oyaml # 自定义配置，如添加 hosts 配置 kubectl apply -f - \u003c\u003cEOF apiVersion: v1 data: Corefile: | .:53 { errors health { lameduck 5s } ready kubernetes cluster.local in-addr.arpa ip6.arpa { pods insecure fallthrough in-addr.arpa ip6.arpa ttl 30 } prometheus :9153 # 添加 hosts 配置 hosts { 172.16.1.36 git.abc.com fallthrough } # 不转发到 /etc/resolv.conf forward . 8.8.8.8 cache 30 loop reload loadbalance } kind: ConfigMap metadata: name: coredns namespace: kube-system EOF ","date":"2023-05-19 08:00:00","objectID":"/ooooo-notes/k8s-%E4%B8%AD%E7%9A%84-dns-%E9%97%AE%E9%A2%98/:3:0","tags":["resolution","k8s","dns","cloud native"],"title":"k8s 中的 dns 问题","uri":"/ooooo-notes/k8s-%E4%B8%AD%E7%9A%84-dns-%E9%97%AE%E9%A2%98/"},{"categories":["随笔"],"content":"4. 问题现象 tekton的 pod dns 显示错误，重新设置主机的 /etc/resolv.conf, 重启 kubelet， 重启 tekton. ","date":"2023-05-19 08:00:00","objectID":"/ooooo-notes/k8s-%E4%B8%AD%E7%9A%84-dns-%E9%97%AE%E9%A2%98/:4:0","tags":["resolution","k8s","dns","cloud native"],"title":"k8s 中的 dns 问题","uri":"/ooooo-notes/k8s-%E4%B8%AD%E7%9A%84-dns-%E9%97%AE%E9%A2%98/"},{"categories":["随笔"],"content":" # open target dir cd /etc/netplan # edit config file, you must creat if not exist sudo vim 00-installer-config.yaml # network device setting template network: ethernets: ens33: #dhcp4: yes dhcp4: no addresses: - 192.168.130.129/24 routes: - to: default via: 192.168.130.2 #nameservers: # addresses: [192.168.130.2] version: 2 # netplan apply sudo netplan apply # restart reboot ","date":"2023-04-01 08:00:00","objectID":"/ooooo-notes/ubuntu-add-static-ip/:0:0","tags":["ubuntu","resolution"],"title":"ubuntu add static ip","uri":"/ooooo-notes/ubuntu-add-static-ip/"},{"categories":["随笔"],"content":"1. 检查 kubelet 的 cri 在 k8s 中，由 kubelet 来拉取节点，而 kubelet 又借用了 cri 来操作容器和镜像. # 查看 kubelet 的启动参数, 其中的 --container-runtime-endpoint 就是 cri ps aux | grep kubelet # 我这里使用的是 containerd ","date":"2023-03-21 08:00:00","objectID":"/ooooo-notes/k8s-%E6%8B%89%E5%8F%96%E9%95%9C%E5%83%8F%E6%85%A2/:1:0","tags":["k8s","containerd"],"title":"k8s 拉取镜像慢","uri":"/ooooo-notes/k8s-%E6%8B%89%E5%8F%96%E9%95%9C%E5%83%8F%E6%85%A2/"},{"categories":["随笔"],"content":"2. 设置 containerd 代理 # 检查 containerd 服务的 unit 文件, 其中 Loaded 属性就是文件位置 systemctl status containerd # 编辑 containerd.service 文件，我这里的文件位置是 /lib/systemd/system/containerd.service vim /lib/systemd/system/containerd.service # 在 [service] 下添加环境变量 [Service] Environment=HTTP_PROXY=http://ooooo:10800 Environment=HTTPS_PROXY=http://ooooo:10800 # 重启 containerd sudo systemctl daemon-reload sudo systemctl restart containerd ","date":"2023-03-21 08:00:00","objectID":"/ooooo-notes/k8s-%E6%8B%89%E5%8F%96%E9%95%9C%E5%83%8F%E6%85%A2/:2:0","tags":["k8s","containerd"],"title":"k8s 拉取镜像慢","uri":"/ooooo-notes/k8s-%E6%8B%89%E5%8F%96%E9%95%9C%E5%83%8F%E6%85%A2/"},{"categories":["随笔"],"content":"3. 参考 cri proxy systemd environment ","date":"2023-03-21 08:00:00","objectID":"/ooooo-notes/k8s-%E6%8B%89%E5%8F%96%E9%95%9C%E5%83%8F%E6%85%A2/:3:0","tags":["k8s","containerd"],"title":"k8s 拉取镜像慢","uri":"/ooooo-notes/k8s-%E6%8B%89%E5%8F%96%E9%95%9C%E5%83%8F%E6%85%A2/"},{"categories":["随笔"],"content":"1. 文件夹权限 Volume 为 hostPath, 要注意文件夹权限， chmod 777 /data ","date":"2023-03-21 08:00:00","objectID":"/ooooo-notes/k8s-%E7%9A%84%E5%B0%8F%E9%97%AE%E9%A2%98/:1:0","tags":["k8s","cloud native"],"title":"k8s 的小问题","uri":"/ooooo-notes/k8s-%E7%9A%84%E5%B0%8F%E9%97%AE%E9%A2%98/"},{"categories":["随笔"],"content":"1. go list all 一直加载 go list all 命令需要发送请求，导致连接超时。 goland 配置代理 ","date":"2023-03-20 08:00:00","objectID":"/ooooo-notes/goland-%E6%89%93%E5%BC%80-go-%E9%A1%B9%E7%9B%AE%E4%B8%80%E7%9B%B4-loading/:1:0","tags":["ide","goland"],"title":"goland 打开 go 项目一直 loading","uri":"/ooooo-notes/goland-%E6%89%93%E5%BC%80-go-%E9%A1%B9%E7%9B%AE%E4%B8%80%E7%9B%B4-loading/"},{"categories":["随笔"],"content":" vim /etc/docker/daemon.json # 添加以下配置 { \"registry-mirrors\": [ \"https://hub-mirror.c.163.com\", \"https://mirror.baidubce.com\" ] } # 重启docker sudo systemctl daemon-reload sudo systemctl restart docker ","date":"2023-03-18 08:00:00","objectID":"/ooooo-notes/docker-%E8%AE%BE%E7%BD%AE%E9%95%9C%E5%83%8F%E6%BA%90/:0:0","tags":["docker"],"title":"docker 设置镜像源","uri":"/ooooo-notes/docker-%E8%AE%BE%E7%BD%AE%E9%95%9C%E5%83%8F%E6%BA%90/"},{"categories":["随笔"],"content":"1. 解决方法 设置区域和语言 ","date":"2023-03-17 08:00:00","objectID":"/ooooo-notes/ubuntu-%E6%89%93%E4%B8%8D%E5%BC%80%E7%BB%88%E7%AB%AF/:1:0","tags":["ubuntu","resolution"],"title":"ubuntu 打不开终端","uri":"/ooooo-notes/ubuntu-%E6%89%93%E4%B8%8D%E5%BC%80%E7%BB%88%E7%AB%AF/"},{"categories":["随笔"],"content":"2. 参考 博客 ","date":"2023-03-17 08:00:00","objectID":"/ooooo-notes/ubuntu-%E6%89%93%E4%B8%8D%E5%BC%80%E7%BB%88%E7%AB%AF/:2:0","tags":["ubuntu","resolution"],"title":"ubuntu 打不开终端","uri":"/ooooo-notes/ubuntu-%E6%89%93%E4%B8%8D%E5%BC%80%E7%BB%88%E7%AB%AF/"},{"categories":["随笔"],"content":"1. enable enhanced keyboard ","date":"2023-03-17 08:00:00","objectID":"/ooooo-notes/vmware-%E4%B8%80%E4%BA%9B%E8%AE%BE%E7%BD%AE/:1:0","tags":["vmware","resolution"],"title":"vmware 一些设置","uri":"/ooooo-notes/vmware-%E4%B8%80%E4%BA%9B%E8%AE%BE%E7%BD%AE/"},{"categories":["随笔"],"content":"2. enable back/forward mouse buttons in vmware path : somepath/Virtual Machines/Ubuntu 64-bit/*.vmx usb.generic.allowHID = \"TRUE\" mouse.vusb.enable = \"TRUE\" reference Back / Forward mouse buttons do not work in VMWare ","date":"2023-03-17 08:00:00","objectID":"/ooooo-notes/vmware-%E4%B8%80%E4%BA%9B%E8%AE%BE%E7%BD%AE/:2:0","tags":["vmware","resolution"],"title":"vmware 一些设置","uri":"/ooooo-notes/vmware-%E4%B8%80%E4%BA%9B%E8%AE%BE%E7%BD%AE/"},{"categories":["随笔"],"content":"怎么使用 h2 数据库。 ","date":"2023-03-15 08:00:00","objectID":"/ooooo-notes/h2-%E6%95%B0%E6%8D%AE%E5%BA%93%E4%BD%BF%E7%94%A8/:0:0","tags":["java"],"title":"h2 数据库使用","uri":"/ooooo-notes/h2-%E6%95%B0%E6%8D%AE%E5%BA%93%E4%BD%BF%E7%94%A8/"},{"categories":["随笔"],"content":"1. 引入依赖 dependencies { api('p6spy:p6spy') api('com.h2database:h2') } ","date":"2023-03-15 08:00:00","objectID":"/ooooo-notes/h2-%E6%95%B0%E6%8D%AE%E5%BA%93%E4%BD%BF%E7%94%A8/:1:0","tags":["java"],"title":"h2 数据库使用","uri":"/ooooo-notes/h2-%E6%95%B0%E6%8D%AE%E5%BA%93%E4%BD%BF%E7%94%A8/"},{"categories":["随笔"],"content":"2. 以内存的方式使用 # spring boot 配置 spring: datasource: driverClassName: com.p6spy.engine.spy.P6SpyDriver url: jdbc:p6spy:h2:mem:test;DB_CLOSE_DELAY=1000 ","date":"2023-03-15 08:00:00","objectID":"/ooooo-notes/h2-%E6%95%B0%E6%8D%AE%E5%BA%93%E4%BD%BF%E7%94%A8/:2:0","tags":["java"],"title":"h2 数据库使用","uri":"/ooooo-notes/h2-%E6%95%B0%E6%8D%AE%E5%BA%93%E4%BD%BF%E7%94%A8/"},{"categories":["随笔"],"content":"3. 以进程的方式使用 # 启动 h2 数据库 java -cp h2*.jar org.h2.tools.Server -ifNotExists # 启动 h2 console (可选) java -cp h2*.jar org.h2.tools.Console # 连接配置，会自动创建文件 url: jdbc:h2:tcp://localhost/~/test ","date":"2023-03-15 08:00:00","objectID":"/ooooo-notes/h2-%E6%95%B0%E6%8D%AE%E5%BA%93%E4%BD%BF%E7%94%A8/:3:0","tags":["java"],"title":"h2 数据库使用","uri":"/ooooo-notes/h2-%E6%95%B0%E6%8D%AE%E5%BA%93%E4%BD%BF%E7%94%A8/"},{"categories":["随笔"],"content":"4. 参考 官方文档 ","date":"2023-03-15 08:00:00","objectID":"/ooooo-notes/h2-%E6%95%B0%E6%8D%AE%E5%BA%93%E4%BD%BF%E7%94%A8/:4:0","tags":["java"],"title":"h2 数据库使用","uri":"/ooooo-notes/h2-%E6%95%B0%E6%8D%AE%E5%BA%93%E4%BD%BF%E7%94%A8/"},{"categories":["随笔"],"content":"相关命令 sudo apt install build-essential manpages-dev software-properties-common sudo add-apt-repository ppa:ubuntu-toolchain-r/test sudo apt update \u0026\u0026 sudo apt install gcc-11 g++-11 1. sudo apt update \u0026\u0026 sudo apt upgrade gcc libfontconfig1-dev systemtap-sdt-dev libx11-dev sudo apt-get install libx11-dev libxext-dev libxrender-dev libxrandr-dev libxtst-dev libxt-dev sudo apt-get install libcups2-dev sudo apt-get install libasound2-dev bash configure --build=x86_64-unknown-linux-gnu --enable-debug --with-jvm-variants=server --enable-dtrace bash configure --enable-debug --with-jvm-variants=server bash configure --enable-debug --with-jvm-variants=server --with-toolchain-type=gcc --with-boot-jdk=C:/Users/ooooo/Development/Jdk/jdk17 ","date":"2023-02-01 08:00:00","objectID":"/ooooo-notes/openjdk-build/:1:0","tags":["jdk"],"title":"openjdk build","uri":"/ooooo-notes/openjdk-build/"},{"categories":["随笔"],"content":"2. 参考 深入理解Java虚拟机（第3版） jdk build ","date":"2023-02-01 08:00:00","objectID":"/ooooo-notes/openjdk-build/:2:0","tags":["jdk"],"title":"openjdk build","uri":"/ooooo-notes/openjdk-build/"},{"categories":["随笔"],"content":"1. 检查 cgroup 的版本 # check if cgroup is supported cat /proc/filesystems | grep cgroup # check cgroup version cat /proc/mounts | grep cgroup ","date":"2023-01-29 08:00:00","objectID":"/ooooo-notes/linux-%E4%B8%AD%E7%9A%84-cgroup-%E6%9C%BA%E5%88%B6/:1:0","tags":["linux"],"title":"linux 中的 cgroup 机制","uri":"/ooooo-notes/linux-%E4%B8%AD%E7%9A%84-cgroup-%E6%9C%BA%E5%88%B6/"},{"categories":["随笔"],"content":"2. cgroup v2 操作 # create new dir cd /sys/fs/cgroup mkdir test # creat loop.sh for testing cpu quota vim loop.sh while : do : done # lunch loop.sh, generate pid -\u003e 2584068 nohup sh loop.sh \u0026 # echo pid to cgroup.procs echo 2584068 \u003e test/cgroup.procs # set cpu, at lease 0.1 echo 1000 10000 \u003e test/cpu.max # check top # recovery all kill 2584068 rmdir test 参考： 博客 ","date":"2023-01-29 08:00:00","objectID":"/ooooo-notes/linux-%E4%B8%AD%E7%9A%84-cgroup-%E6%9C%BA%E5%88%B6/:2:0","tags":["linux"],"title":"linux 中的 cgroup 机制","uri":"/ooooo-notes/linux-%E4%B8%AD%E7%9A%84-cgroup-%E6%9C%BA%E5%88%B6/"},{"categories":["随笔"],"content":"1. 准备数据 # create new schema create schema test; use test; # create table test create table user ( id int primary key, age int ); alter table user add index age_idx (age); # insert some test data insert into user values (3, 10), (5, 20), (8, 30); ","date":"2023-01-28 08:00:00","objectID":"/ooooo-notes/mysql-%E9%97%B4%E9%9A%99%E9%94%81/:1:0","tags":["mysql"],"title":"mysql 间隙锁","uri":"/ooooo-notes/mysql-%E9%97%B4%E9%9A%99%E9%94%81/"},{"categories":["随笔"],"content":"2. 间隙锁测试 ","date":"2023-01-28 08:00:00","objectID":"/ooooo-notes/mysql-%E9%97%B4%E9%9A%99%E9%94%81/:2:0","tags":["mysql"],"title":"mysql 间隙锁","uri":"/ooooo-notes/mysql-%E9%97%B4%E9%9A%99%E9%94%81/"},{"categories":["随笔"],"content":"1. 使用主键索引，指定行存在 # session 1, the row is exist for id = 3 , so it doesn't lock. begin; select * from user where id = 3 for update; # session 2, execute successful. begin; insert into user value (1, 20); ","date":"2023-01-28 08:00:00","objectID":"/ooooo-notes/mysql-%E9%97%B4%E9%9A%99%E9%94%81/:2:1","tags":["mysql"],"title":"mysql 间隙锁","uri":"/ooooo-notes/mysql-%E9%97%B4%E9%9A%99%E9%94%81/"},{"categories":["随笔"],"content":"2. 使用主键索引，指定行不存在 # session 1, the row isn't exist for id = 2, so it locks range (,3] begin; select * from user where id = 2 for update; # session 2, execute block. begin; insert into user value (1, 20); ","date":"2023-01-28 08:00:00","objectID":"/ooooo-notes/mysql-%E9%97%B4%E9%9A%99%E9%94%81/:2:2","tags":["mysql"],"title":"mysql 间隙锁","uri":"/ooooo-notes/mysql-%E9%97%B4%E9%9A%99%E9%94%81/"},{"categories":["随笔"],"content":"3. 使用主键索引，范围查找 # session 1, it locks range [1,5] begin; select * from user where id \u003e= 1 and id \u003c= 5 for update; # session 2, execute block begin; insert into user value (2, 20); ","date":"2023-01-28 08:00:00","objectID":"/ooooo-notes/mysql-%E9%97%B4%E9%9A%99%E9%94%81/:2:3","tags":["mysql"],"title":"mysql 间隙锁","uri":"/ooooo-notes/mysql-%E9%97%B4%E9%9A%99%E9%94%81/"},{"categories":["随笔"],"content":"4. 使用二级索引，指定行存在 # session 1, it locks range [3,8] begin; select * from user where age = 20 for update; # session 2, execute block. begin; insert into user value (4, 20); ","date":"2023-01-28 08:00:00","objectID":"/ooooo-notes/mysql-%E9%97%B4%E9%9A%99%E9%94%81/:2:4","tags":["mysql"],"title":"mysql 间隙锁","uri":"/ooooo-notes/mysql-%E9%97%B4%E9%9A%99%E9%94%81/"},{"categories":["随笔"],"content":"5. 使用二级索引，指定行不存在 # session 1, it locks range [3,5] begin; select * from user where age = 15 for update; # session 2, execute block. begin; insert into user value (4, 20); ","date":"2023-01-28 08:00:00","objectID":"/ooooo-notes/mysql-%E9%97%B4%E9%9A%99%E9%94%81/:2:5","tags":["mysql"],"title":"mysql 间隙锁","uri":"/ooooo-notes/mysql-%E9%97%B4%E9%9A%99%E9%94%81/"},{"categories":["随笔"],"content":"6. 使用二级索引，范围查询 # session 1, it locks range [3,8] begin; select * from user where age \u003e= 12 and age \u003c= 28 for update; # session 2, execute block. begin; insert into user value (4, 20); ","date":"2023-01-28 08:00:00","objectID":"/ooooo-notes/mysql-%E9%97%B4%E9%9A%99%E9%94%81/:2:6","tags":["mysql"],"title":"mysql 间隙锁","uri":"/ooooo-notes/mysql-%E9%97%B4%E9%9A%99%E9%94%81/"},{"categories":["随笔"],"content":"7. 结论 使用主键索引，行存在时，才只会锁定这一行。 其他情况都是使用范围锁定 ","date":"2023-01-28 08:00:00","objectID":"/ooooo-notes/mysql-%E9%97%B4%E9%9A%99%E9%94%81/:2:7","tags":["mysql"],"title":"mysql 间隙锁","uri":"/ooooo-notes/mysql-%E9%97%B4%E9%9A%99%E9%94%81/"},{"categories":["随笔"],"content":"3. 恢复数据 drop schame test; ","date":"2023-01-28 08:00:00","objectID":"/ooooo-notes/mysql-%E9%97%B4%E9%9A%99%E9%94%81/:3:0","tags":["mysql"],"title":"mysql 间隙锁","uri":"/ooooo-notes/mysql-%E9%97%B4%E9%9A%99%E9%94%81/"},{"categories":["随笔"],"content":"1. aufs 存储驱动 Ubuntu 22.04 LTS 不支持 aufs 文件系统 参考： ubuntu官方文档 ","date":"2023-01-24 08:00:00","objectID":"/ooooo-notes/docker-%E5%AD%98%E5%82%A8%E9%A9%B1%E5%8A%A8/:1:0","tags":["docker","storage"],"title":"docker 存储驱动","uri":"/ooooo-notes/docker-%E5%AD%98%E5%82%A8%E9%A9%B1%E5%8A%A8/"},{"categories":["随笔"],"content":"2. overlay2 存储驱动 # creat dir mkdir lower upper work mnt # mount lower upper work to mnt mount -t overlay -o lowerdir=lower,upperdir=upper,workdir=work none mnt # testing echo 1 \u003e lower/1 mkdir lower/2 mkdir upper/3 ll mnt # recovery all setting umount mnt rm -rf lower upper work mnt 参考： 文档 linux文档 ","date":"2023-01-24 08:00:00","objectID":"/ooooo-notes/docker-%E5%AD%98%E5%82%A8%E9%A9%B1%E5%8A%A8/:2:0","tags":["docker","storage"],"title":"docker 存储驱动","uri":"/ooooo-notes/docker-%E5%AD%98%E5%82%A8%E9%A9%B1%E5%8A%A8/"},{"categories":["随笔"],"content":"这篇文章主要简述 docker 中的 bridge 网络驱动是如何工作的。 ","date":"2023-01-20 08:00:00","objectID":"/ooooo-notes/docker-%E5%8D%95%E4%B8%BB%E6%9C%BA%E7%BD%91%E8%B7%AF/:0:0","tags":["docker","network"],"title":"docker 单主机网络","uri":"/ooooo-notes/docker-%E5%8D%95%E4%B8%BB%E6%9C%BA%E7%BD%91%E8%B7%AF/"},{"categories":["随笔"],"content":"1. 测试一，veth1 (ns1) — veth2 (ns2) # create ns1, ns2 ip netns add ns1 ip netns add ns2 # create veth1, veth2 ip link add veth1 type veth peer name veth2 # set veth1 for ns1, set veth2 for ns2 ip link set dev veth1 netns ns1 ip link set dev veth2 netns ns2 # set veth1 ip, set veth2 ip ip netns exec ns1 ip addr add 172.16.0.1/24 dev veth1 ip netns exec ns2 ip addr add 172.16.0.2/24 dev veth2 # set veth1 up, set veth2 up ip netns exec ns1 ip link set dev veth1 up ip netns exec ns2 ip link set dev veth2 up # show ip address ip netns exec ns1 ip addr ip netns exec ns2 ip addr # test for ping ip netns exec ns1 ping 172.16.0.2 ip netns exec ns2 ping 172.16.0.1 # recovery all setting ip netns delete ns1 ip netns delete ns2 ","date":"2023-01-20 08:00:00","objectID":"/ooooo-notes/docker-%E5%8D%95%E4%B8%BB%E6%9C%BA%E7%BD%91%E8%B7%AF/:1:0","tags":["docker","network"],"title":"docker 单主机网络","uri":"/ooooo-notes/docker-%E5%8D%95%E4%B8%BB%E6%9C%BA%E7%BD%91%E8%B7%AF/"},{"categories":["随笔"],"content":"2. 测试二，veth0 (bridge0) — veth1 (ns1) # create ns1 ip netns add ns1 # create veth0, veth1 ip link add veth0 type veth peer name veth1 # create bridge0 ip link add bridge0 type bridge # set veth1 for ns1 ip link set dev veth1 netns ns1 # set veth0 for bridge0 ip link set dev veth0 master bridge0 # set veth0 ip address ip addr add 172.16.0.1/24 dev veth0 # set bridge0 ip address ip addr add 172.16.0.0/24 dev bridge0 # set veth1 ip address ip netns exec ns1 ip addr add 172.16.0.2/24 dev veth1 # set veth0, veth1, bridge0 up ip link set dev veth0 up ip link set dev bridge0 up ip netns exec ns1 ip link set dev veth1 up # delete veth0 route ip route del 172.16.0.0/24 dev veth0 # show ip address ip addr ip netns exec ns1 ip addr # test for ping ping 172.16.0.2 ip netns exec ns1 ping 172.16.0.1 # recovery all setting ip netns del ns1 ip link del bridge0 ","date":"2023-01-20 08:00:00","objectID":"/ooooo-notes/docker-%E5%8D%95%E4%B8%BB%E6%9C%BA%E7%BD%91%E8%B7%AF/:2:0","tags":["docker","network"],"title":"docker 单主机网络","uri":"/ooooo-notes/docker-%E5%8D%95%E4%B8%BB%E6%9C%BA%E7%BD%91%E8%B7%AF/"},{"categories":["计划"],"content":"0、持续学习者 Talk is cheap. Show me the code. 英语比编程简单。 学习和实践要平衡。 学会和时间做朋友。 学会投资，学会理财。 学会先做减法，再做加法。 学英语很重要，学英语很重要，学英语很重要。 说明： ⭕ 进行中 ✅ 已完成 ❌ 已废弃 ❓ 有必要 ❗ 重要性 📝 记笔记 🖊 写代码 ","date":"2023-01-01 09:00:00","objectID":"/ooooo-notes/2023%E5%B9%B4%E8%AE%A1%E5%88%92/:1:0","tags":["learning"],"title":"2023年学习计划","uri":"/ooooo-notes/2023%E5%B9%B4%E8%AE%A1%E5%88%92/"},{"categories":["计划"],"content":"1、关于英语 《新概念二》 ⭕ ","date":"2023-01-01 09:00:00","objectID":"/ooooo-notes/2023%E5%B9%B4%E8%AE%A1%E5%88%92/:2:0","tags":["learning"],"title":"2023年学习计划","uri":"/ooooo-notes/2023%E5%B9%B4%E8%AE%A1%E5%88%92/"},{"categories":["计划"],"content":"2、关于技术 计划 🎉： 只记录自己认为有用的笔记。 ","date":"2023-01-01 09:00:00","objectID":"/ooooo-notes/2023%E5%B9%B4%E8%AE%A1%E5%88%92/:3:0","tags":["learning"],"title":"2023年学习计划","uri":"/ooooo-notes/2023%E5%B9%B4%E8%AE%A1%E5%88%92/"},{"categories":["计划"],"content":"1、书籍 0️⃣1️⃣. 《深入理解 Kafka：核心设计与实践原理》 0️⃣2️⃣. 《分布式一致性算法开发实战》 0️⃣3️⃣. 《Go Web 编程》 ✅ 0️⃣4️⃣. 《Effective C++》 0️⃣5️⃣. 《More Effective C++》 0️⃣6️⃣. 《深度探索C++对象模型》 0️⃣7️⃣. 《Go语言设计与实现》 0️⃣8️⃣. 《Vim实用技巧（第2版）》 ⭕ 0️⃣9️⃣. 《RocketMQ技术内幕 第二版》 ⭕ 1️⃣0️⃣. 《云原生服务网格Istio：原理、实践、架构与源码解析》 ✅ 1️⃣1️⃣. 《MySQL技术内幕》 1️⃣2️⃣. 《深入解析Java虚拟机HotSpot》 1️⃣3️⃣. 《Rust权威指南》 ✅ 1️⃣4️⃣. 《深入剖析Kubernetes》 1️⃣5️⃣. 《Kubernetes编程》 ⭕ 1️⃣6️⃣. 《Kubernetes设计模式》 1️⃣7️⃣. 《深入剖析Java虚拟机》 1️⃣8️⃣. 《算法训练营：海量图解+竞赛刷题（入门篇》 ⭕ 1️⃣9️⃣. 《算法训练营：海量图解+竞赛刷题（进阶篇）》 2️⃣0️⃣. 《TCP/IP详解 卷1：协议》 ⭕ 2️⃣1️⃣. 《自己动手写Docker》 ✅ 2️⃣2️⃣. 《UNIX网络编程 卷1：套接字联网API（第3版）》 2️⃣3️⃣. 《Docker——容器与容器云（第2版）》 ✅ 2️⃣4️⃣. 《高性能MySQL（第4版）》 ✅ 2️⃣5️⃣. 《Kubernetes网络权威指南：基础、原理与实践）》 ✅ 2️⃣6️⃣. 《Kubernetes Operator开发进阶》 2️⃣7️⃣. 《Kafka权威指南（第2版）》 2️⃣8️⃣. 《操作系统导论》 ⭕ 2️⃣9️⃣. 《Go语言底层原理剖析》 ⭕ 3️⃣0️⃣. 《Kubernetes进阶实战（第2版）》 ✅ ","date":"2023-01-01 09:00:00","objectID":"/ooooo-notes/2023%E5%B9%B4%E8%AE%A1%E5%88%92/:3:1","tags":["learning"],"title":"2023年学习计划","uri":"/ooooo-notes/2023%E5%B9%B4%E8%AE%A1%E5%88%92/"},{"categories":["计划"],"content":"2、文档 0️⃣1️⃣. 《深入拆解 Java 虚拟机》 0️⃣2️⃣. 从 0 开始带你成为JVM实战高手 ⭕ 0️⃣3️⃣. Go 语言项目开发实战 ⭕ 0️⃣4️⃣. Redis 源码剖析与实战 ✅ 0️⃣5️⃣. 深入 C 语言和程序运行原理 ","date":"2023-01-01 09:00:00","objectID":"/ooooo-notes/2023%E5%B9%B4%E8%AE%A1%E5%88%92/:3:2","tags":["learning"],"title":"2023年学习计划","uri":"/ooooo-notes/2023%E5%B9%B4%E8%AE%A1%E5%88%92/"},{"categories":["计划"],"content":"3. 源码 0️⃣1️⃣. 《rocketmq 源码》 ⭕ 0️⃣2️⃣. 《kubernetes 源码》 ⭕ 0️⃣3️⃣. 《istio 源码》 ⭕ 0️⃣4️⃣. 《etcd 源码》 0️⃣5️⃣. 《dubbo 源码》 ⭕ 0️⃣6️⃣. 《arthas》 0️⃣7️⃣. 《nsq 源码》 0️⃣8️⃣. 《eventing 源码》 0️⃣9️⃣. 《serving 源码》 1️⃣0️⃣. 《grpc-go 源码》 ⭕ ","date":"2023-01-01 09:00:00","objectID":"/ooooo-notes/2023%E5%B9%B4%E8%AE%A1%E5%88%92/:3:3","tags":["learning"],"title":"2023年学习计划","uri":"/ooooo-notes/2023%E5%B9%B4%E8%AE%A1%E5%88%92/"},{"categories":["计划"],"content":"4、视频 0️⃣1️⃣. 《玩转算法系列–图论精讲》 0️⃣2️⃣. 《玩转算法面试》 ⭕ ","date":"2023-01-01 09:00:00","objectID":"/ooooo-notes/2023%E5%B9%B4%E8%AE%A1%E5%88%92/:3:4","tags":["learning"],"title":"2023年学习计划","uri":"/ooooo-notes/2023%E5%B9%B4%E8%AE%A1%E5%88%92/"},{"categories":["计划"],"content":"4、关于其他 ","date":"2023-01-01 09:00:00","objectID":"/ooooo-notes/2023%E5%B9%B4%E8%AE%A1%E5%88%92/:4:0","tags":["learning"],"title":"2023年学习计划","uri":"/ooooo-notes/2023%E5%B9%B4%E8%AE%A1%E5%88%92/"},{"categories":["计划"],"content":"1、书籍 🎉 0️⃣1️⃣. 《卓有成效的工程师》 ✅ 0️⃣2️⃣. 《非暴力沟通》 ⭕ 0️⃣3️⃣. 《原则》 0️⃣4️⃣. 《刻意练习》 ⭕ 0️⃣5️⃣. 《关键对话》 0️⃣6️⃣. 《当下的启蒙》 0️⃣7️⃣. 《亲密关系：通往灵魂的桥梁》 ⭕ 0️⃣8️⃣. 《人性的弱点》 0️⃣9️⃣. 《数据密集型应用系统设计》 1️⃣0️⃣. 《当我谈跑步时，我谈些什么》 ","date":"2023-01-01 09:00:00","objectID":"/ooooo-notes/2023%E5%B9%B4%E8%AE%A1%E5%88%92/:4:1","tags":["learning"],"title":"2023年学习计划","uri":"/ooooo-notes/2023%E5%B9%B4%E8%AE%A1%E5%88%92/"},{"categories":["计划"],"content":"2、尝试 🎉 0️⃣1️⃣. 学会使用尤克里里弹奏 ","date":"2023-01-01 09:00:00","objectID":"/ooooo-notes/2023%E5%B9%B4%E8%AE%A1%E5%88%92/:4:2","tags":["learning"],"title":"2023年学习计划","uri":"/ooooo-notes/2023%E5%B9%B4%E8%AE%A1%E5%88%92/"},{"categories":["计划"],"content":"3、了解 🎉 0️⃣1️⃣. 暂无 ","date":"2023-01-01 09:00:00","objectID":"/ooooo-notes/2023%E5%B9%B4%E8%AE%A1%E5%88%92/:4:3","tags":["learning"],"title":"2023年学习计划","uri":"/ooooo-notes/2023%E5%B9%B4%E8%AE%A1%E5%88%92/"},{"categories":["计划"],"content":"4、娱乐 0️⃣1️⃣. 《奇遇人生 第一季》 0️⃣2️⃣. 《一本好书 1》 0️⃣3️⃣. 《一本好书 2》 0️⃣4️⃣. 《天气之子》 ","date":"2023-01-01 09:00:00","objectID":"/ooooo-notes/2023%E5%B9%B4%E8%AE%A1%E5%88%92/:4:4","tags":["learning"],"title":"2023年学习计划","uri":"/ooooo-notes/2023%E5%B9%B4%E8%AE%A1%E5%88%92/"},{"categories":["随笔"],"content":"1. 前置条件 安装 docker，必须配置 docker 代理，否则 build 失败。 参考 下载 istio 源码。 安装 go 和 dlv 工具。参考 ","date":"2022-12-19 09:00:00","objectID":"/ooooo-notes/%E6%90%AD%E5%BB%BA-istio-%E6%BA%90%E7%A0%81%E8%B0%83%E8%AF%95%E7%8E%AF%E5%A2%83/:1:0","tags":["istio","cloud native"],"title":"搭建 istio 源码调试环境","uri":"/ooooo-notes/%E6%90%AD%E5%BB%BA-istio-%E6%BA%90%E7%A0%81%E8%B0%83%E8%AF%95%E7%8E%AF%E5%A2%83/"},{"categories":["随笔"],"content":"2. 设置环境变量 # docker 地址 export HUB=\"docker.io/youwillsee\" # istio 的源码目录 export ISTIO=/root/code/istio # docker 的 tag export TAG=1.17-debug ","date":"2022-12-19 09:00:00","objectID":"/ooooo-notes/%E6%90%AD%E5%BB%BA-istio-%E6%BA%90%E7%A0%81%E8%B0%83%E8%AF%95%E7%8E%AF%E5%A2%83/:2:0","tags":["istio","cloud native"],"title":"搭建 istio 源码调试环境","uri":"/ooooo-notes/%E6%90%AD%E5%BB%BA-istio-%E6%BA%90%E7%A0%81%E8%B0%83%E8%AF%95%E7%8E%AF%E5%A2%83/"},{"categories":["随笔"],"content":"3. build istio # 构建 debug 的版本，会输出在 out 目录下 make DEBUG=1 build # 构建 debug 的版本，推到本地的 docker 中 make DEBUG=1 docker # 推送到远端的 docker 中 make docker.push # 清理 make clean 参考 istio-devlopment istio-code-base ","date":"2022-12-19 09:00:00","objectID":"/ooooo-notes/%E6%90%AD%E5%BB%BA-istio-%E6%BA%90%E7%A0%81%E8%B0%83%E8%AF%95%E7%8E%AF%E5%A2%83/:3:0","tags":["istio","cloud native"],"title":"搭建 istio 源码调试环境","uri":"/ooooo-notes/%E6%90%AD%E5%BB%BA-istio-%E6%BA%90%E7%A0%81%E8%B0%83%E8%AF%95%E7%8E%AF%E5%A2%83/"},{"categories":["随笔"],"content":"4. dlv 连接 # 找到 pid ps -ef | grep pilot-discovery # attach pid dlv --listen=:2345 --headless=true --api-version=2 --accept-multiclient attach 172965 # 使用 IDE 远程连接 GOland -\u003e go remote ","date":"2022-12-19 09:00:00","objectID":"/ooooo-notes/%E6%90%AD%E5%BB%BA-istio-%E6%BA%90%E7%A0%81%E8%B0%83%E8%AF%95%E7%8E%AF%E5%A2%83/:4:0","tags":["istio","cloud native"],"title":"搭建 istio 源码调试环境","uri":"/ooooo-notes/%E6%90%AD%E5%BB%BA-istio-%E6%BA%90%E7%A0%81%E8%B0%83%E8%AF%95%E7%8E%AF%E5%A2%83/"},{"categories":["随笔"],"content":"5. bind dlv to pilot (optional) Dockerfile.pilot # BASE_DISTRIBUTION is used to switch between the old base distribution and distroless base images ARG BASE_DISTRIBUTION=debug # Version is the base image version from the TLD Makefile ARG BASE_VERSION=latest ARG ISTIO_BASE_REGISTRY=gcr.io/istio-release # The following section is used as base image if BASE_DISTRIBUTION=debug FROM ${ISTIO_BASE_REGISTRY}/base:${BASE_VERSION} as debug # The following section is used as base image if BASE_DISTRIBUTION=distroless FROM ${ISTIO_BASE_REGISTRY}/distroless:${BASE_VERSION} as distroless # Add dlv FROM golang:1.20 AS build-dlv ENV GOPROXY=https://goproxy.io,direct RUN go install github.com/go-delve/delve/cmd/dlv@latest # This will build the final image based on either debug or distroless from above # hadolint ignore=DL3006 FROM ${BASE_DISTRIBUTION:-debug} ARG TARGETARCH COPY ${TARGETARCH:-amd64}/pilot-discovery /usr/local/bin/pilot-discovery # Copy templates for bootstrap generation. COPY envoy_bootstrap.json /var/lib/istio/envoy/envoy_bootstrap_tmpl.json COPY gcp_envoy_bootstrap.json /var/lib/istio/envoy/gcp_envoy_bootstrap_tmpl.json COPY --from=build-dlv /go/bin/dlv / USER 1337:1337 ENTRYPOINT [\"/dlv\", \"--listen=:1234\", \"--headless=true\", \"--api-version=2\", \"--accept-multiclient\", \"exec\", \"/usr/local/bin/pilot-discovery\", \"--\"] #ENTRYPOINT [\"/usr/local/bin/pilot-discovery\"] ","date":"2022-12-19 09:00:00","objectID":"/ooooo-notes/%E6%90%AD%E5%BB%BA-istio-%E6%BA%90%E7%A0%81%E8%B0%83%E8%AF%95%E7%8E%AF%E5%A2%83/:5:0","tags":["istio","cloud native"],"title":"搭建 istio 源码调试环境","uri":"/ooooo-notes/%E6%90%AD%E5%BB%BA-istio-%E6%BA%90%E7%A0%81%E8%B0%83%E8%AF%95%E7%8E%AF%E5%A2%83/"},{"categories":["随笔"],"content":"1. 配置 docker 代理 # 创建配置目录 mkdir -p /etc/systemd/system/docker.service.d # 创建配置文件 vim /etc/systemd/system/docker.service.d/http-proxy.conf # 配置文件内容 [Service] Environment=\"HTTP_PROXY=http://ooooo:10800\" Environment=\"HTTPS_PROXY=http://ooooo:10800\" # 重启 docker systemctl daemon-reload \u0026\u0026 systemctl restart docker # 查看配置是否生效 systemctl show --property=Environment docker ","date":"2022-12-18 09:00:00","objectID":"/ooooo-notes/%E8%AE%BE%E7%BD%AE-docker-%E4%BB%A3%E7%90%86/:1:0","tags":["docker","cloud native"],"title":"设置 docker 代理","uri":"/ooooo-notes/%E8%AE%BE%E7%BD%AE-docker-%E4%BB%A3%E7%90%86/"},{"categories":["微信文章"],"content":"1. 实现队列 代码： 使用 head 和 tail 来实现单链表 单链表涉及到两个节点，每次都要判断中间状态 这里使用的是 AtomicReference 来实现的，也可以使用 unsafe 来实现，有兴趣的可以尝试下 这里使用 curTail.next 进行 CAS 来指定下一个节点, 很少这么使用，后面再详细说说 public class LinkedQueue\u003cE\u003e { private final Node\u003cE\u003e dummy = new Node\u003c\u003e(null, null); private final AtomicReference\u003cNode\u003cE\u003e\u003e head = new AtomicReference\u003c\u003e(dummy); private final AtomicReference\u003cNode\u003cE\u003e\u003e tail = new AtomicReference\u003c\u003e(dummy); public boolean put(E item) { Node\u003cE\u003e newNode = new Node\u003c\u003e(item, null); while (true) { Node\u003cE\u003e curTail = tail.get(); Node\u003cE\u003e tailNext = curTail.next.get(); if (curTail == tail.get()) { if (tailNext != null) { // 队列处于中间状态，推进尾节点 tail.compareAndSet(curTail, tailNext); } else { // 处于稳定状态，尝试插入新节点 if (curTail.next.compareAndSet(null, newNode)) { // 插入操作成功，尝试推进尾节点 tail.compareAndSet(curTail, newNode); return true; } } } } } public E take() { while (true) { if (head.get() == tail.get()) { return null; } Node\u003cE\u003e oldHead = head.get(); Node\u003cE\u003e newHead = oldHead.next.get(); // 队列处于中间状态，可能另外一个线程已经 CAS 成功， 只剩下一个元素 dummy 了 if (newHead == null) { return null; } if (head.compareAndSet(oldHead, newHead)) { oldHead.next = null; return oldHead.item; } } } private static class Node\u003cE\u003e { private final E item; private AtomicReference\u003cNode\u003cE\u003e\u003e next; public Node(E item, Node\u003cE\u003e next) { this.item = item; this.next = new AtomicReference\u003c\u003e(next); } } } ","date":"2022-11-16 08:00:00","objectID":"/ooooo-notes/%E5%9C%A8-java-%E4%B8%AD%E4%BD%BF%E7%94%A8-cas-%E6%9D%A5%E5%AE%9E%E7%8E%B0%E9%98%9F%E5%88%97/:1:0","tags":["java"],"title":"在 java 中使用 CAS 来实现队列","uri":"/ooooo-notes/%E5%9C%A8-java-%E4%B8%AD%E4%BD%BF%E7%94%A8-cas-%E6%9D%A5%E5%AE%9E%E7%8E%B0%E9%98%9F%E5%88%97/"},{"categories":["微信文章"],"content":"2. 代码实现位置 github 地址 ","date":"2022-11-16 08:00:00","objectID":"/ooooo-notes/%E5%9C%A8-java-%E4%B8%AD%E4%BD%BF%E7%94%A8-cas-%E6%9D%A5%E5%AE%9E%E7%8E%B0%E9%98%9F%E5%88%97/:2:0","tags":["java"],"title":"在 java 中使用 CAS 来实现队列","uri":"/ooooo-notes/%E5%9C%A8-java-%E4%B8%AD%E4%BD%BF%E7%94%A8-cas-%E6%9D%A5%E5%AE%9E%E7%8E%B0%E9%98%9F%E5%88%97/"},{"categories":["微信文章"],"content":"1. 使用 Lock 来实现 Semaphore 代码： Semaphore 的功能就是允许同时有几个线程操作 acquire 方法，permit 会减一，如果为 0，则线程需要等待 release 方法，permit 会加一，唤醒等待的线程 public class SemaphoreOnLock { private final ReentrantLock lock = new ReentrantLock(); private final Condition condition = lock.newCondition(); private int permit; public SemaphoreOnLock(int permit) { this.permit = permit; } /** * 获取锁 */ public void acquire() { lock.lock(); try { while (permit \u003c= 0) { condition.await(); } permit--; } catch (InterruptedException ignored) { } finally { lock.unlock(); } } public void release() { lock.lock(); try { permit++; condition.signal(); } finally { lock.unlock(); } } } ","date":"2022-11-14 08:00:00","objectID":"/ooooo-notes/%E5%9C%A8-java-%E4%B8%AD%E4%BD%BF%E7%94%A8-lock-%E6%9D%A5%E5%AE%9E%E7%8E%B0-semaphore/:1:0","tags":["java"],"title":"在 java 中使用 Lock 来实现 Semaphore","uri":"/ooooo-notes/%E5%9C%A8-java-%E4%B8%AD%E4%BD%BF%E7%94%A8-lock-%E6%9D%A5%E5%AE%9E%E7%8E%B0-semaphore/"},{"categories":["微信文章"],"content":"2. 代码实现位置 github 地址 ","date":"2022-11-14 08:00:00","objectID":"/ooooo-notes/%E5%9C%A8-java-%E4%B8%AD%E4%BD%BF%E7%94%A8-lock-%E6%9D%A5%E5%AE%9E%E7%8E%B0-semaphore/:2:0","tags":["java"],"title":"在 java 中使用 Lock 来实现 Semaphore","uri":"/ooooo-notes/%E5%9C%A8-java-%E4%B8%AD%E4%BD%BF%E7%94%A8-lock-%E6%9D%A5%E5%AE%9E%E7%8E%B0-semaphore/"},{"categories":["微信文章"],"content":"1. 使用数组来实现栈 代码： 用数组来实现 用 CTL 来控制 测试类，参考 ConcurrentStackUsingArrayTest public class ConcurrentStackUsingArray\u003cE\u003e { private final AtomicInteger CTL = new AtomicInteger(0); private final AtomicReference\u003cE[]\u003e arr = new AtomicReference\u003c\u003e((E[]) new Object[10]); private final AtomicInteger index = new AtomicInteger(0); public void push(E e) { while (!CTL.compareAndSet(0, 1)) { Thread.yield(); } while (index.get() \u003e= arr.get().length) { E[] oldArr = arr.get(); E[] newArr = (E[]) new Object[oldArr.length * 2]; System.arraycopy(oldArr, 0, newArr, 0, oldArr.length); if (arr.compareAndSet(oldArr, newArr)) { break; } } arr.get()[index.getAndIncrement()] = e; CTL.lazySet(0); } public E pop() { while (!CTL.compareAndSet(0, 1)) { Thread.yield(); } E e = arr.get()[index.decrementAndGet()]; CTL.lazySet(0); return e; } } ","date":"2022-11-13 08:00:00","objectID":"/ooooo-notes/%E5%9C%A8-java-%E4%B8%AD%E4%BD%BF%E7%94%A8-cas-%E6%9D%A5%E5%AE%9E%E7%8E%B0%E6%A0%882/:1:0","tags":["java"],"title":"在 java 中使用 CAS 来实现栈2","uri":"/ooooo-notes/%E5%9C%A8-java-%E4%B8%AD%E4%BD%BF%E7%94%A8-cas-%E6%9D%A5%E5%AE%9E%E7%8E%B0%E6%A0%882/"},{"categories":["微信文章"],"content":"2. 代码实现位置 github 地址 ","date":"2022-11-13 08:00:00","objectID":"/ooooo-notes/%E5%9C%A8-java-%E4%B8%AD%E4%BD%BF%E7%94%A8-cas-%E6%9D%A5%E5%AE%9E%E7%8E%B0%E6%A0%882/:2:0","tags":["java"],"title":"在 java 中使用 CAS 来实现栈2","uri":"/ooooo-notes/%E5%9C%A8-java-%E4%B8%AD%E4%BD%BF%E7%94%A8-cas-%E6%9D%A5%E5%AE%9E%E7%8E%B0%E6%A0%882/"},{"categories":["微信文章"],"content":"1. java 多线程测试 在任何语言中，多线程测试都是比较困难的，在这里我介绍下 java 的多线程测试 jcstress. jcstress 是 OpenJDK 提供的一个测试多线程的框架 主要由多个 Actor 来构成，每个 Actor 就是一个线程。 通过匹配 Outcome 的结果来报告测试 运行之后的结果为 html 文件，需要你自己查看。 示例代码: 测试自旋锁，其实也告诉你该怎么编写 CAS 执行命令 gradle jcstress，会生成目录 build/reports/jcstress @JCStressTest @Outcome(id = {\"1, 2\", \"2, 1\"}, expect = Expect.ACCEPTABLE, desc = \"Mutex works\") @Outcome(id = \"1, 1\", expect = Expect.FORBIDDEN, desc = \"Mutex failure\") @State public class Mutex_03_SpinLock { private final AtomicBoolean taken = new AtomicBoolean(false); private int v; @Actor public void actor1(II_Result r) { while (taken.get() || !taken.compareAndSet(false, true)) ; // wait { // critical section r.r1 = ++v; } taken.set(false); } @Actor public void actor2(II_Result r) { while (taken.get() || !taken.compareAndSet(false, true)) ; // wait { // critical section r.r2 = ++v; } taken.set(false); } } ","date":"2022-11-12 09:00:00","objectID":"/ooooo-notes/%E5%9C%A8-java-%E4%B8%AD%E5%A6%82%E4%BD%95%E8%BF%9B%E8%A1%8C%E5%A4%9A%E7%BA%BF%E7%A8%8B%E6%B5%8B%E8%AF%95/:1:0","tags":["java"],"title":"在 java 中如何进行多线程测试","uri":"/ooooo-notes/%E5%9C%A8-java-%E4%B8%AD%E5%A6%82%E4%BD%95%E8%BF%9B%E8%A1%8C%E5%A4%9A%E7%BA%BF%E7%A8%8B%E6%B5%8B%E8%AF%95/"},{"categories":["微信文章"],"content":"2. 代码实现位置 github 地址 ","date":"2022-11-12 09:00:00","objectID":"/ooooo-notes/%E5%9C%A8-java-%E4%B8%AD%E5%A6%82%E4%BD%95%E8%BF%9B%E8%A1%8C%E5%A4%9A%E7%BA%BF%E7%A8%8B%E6%B5%8B%E8%AF%95/:2:0","tags":["java"],"title":"在 java 中如何进行多线程测试","uri":"/ooooo-notes/%E5%9C%A8-java-%E4%B8%AD%E5%A6%82%E4%BD%95%E8%BF%9B%E8%A1%8C%E5%A4%9A%E7%BA%BF%E7%A8%8B%E6%B5%8B%E8%AF%95/"},{"categories":["微信文章"],"content":"3. 参考 强烈建议大家看官方代码, 地址: https://github.com/openjdk/jcstress ","date":"2022-11-12 09:00:00","objectID":"/ooooo-notes/%E5%9C%A8-java-%E4%B8%AD%E5%A6%82%E4%BD%95%E8%BF%9B%E8%A1%8C%E5%A4%9A%E7%BA%BF%E7%A8%8B%E6%B5%8B%E8%AF%95/:3:0","tags":["java"],"title":"在 java 中如何进行多线程测试","uri":"/ooooo-notes/%E5%9C%A8-java-%E4%B8%AD%E5%A6%82%E4%BD%95%E8%BF%9B%E8%A1%8C%E5%A4%9A%E7%BA%BF%E7%A8%8B%E6%B5%8B%E8%AF%95/"},{"categories":["微信文章"],"content":"1. 实现简单的 CAS 例子 CAS 相信大家都听过，就是 compareAndSet(V expectedValue, V newValue), 真正会用的人很少，这里的难点主要是无阻塞算法。 先实现一个简单 CAS 例子，只具有学习的意义。 getValue: 获取值 compareAndSet: 比较旧值，设置新值 public class SimulatedCAS { private int value; public SimulatedCAS(int value) { this.value = value; } public synchronized int getValue() { return value; } public synchronized boolean compareAndSet(int expectedValue, int newValue) { if (expectedValue == value) { this.value = newValue; return true; } return false; } } 重点：真正用 CAS 的时候，都是 while 循环 ","date":"2022-11-09 08:00:00","objectID":"/ooooo-notes/%E5%9C%A8-java-%E4%B8%AD%E4%BD%BF%E7%94%A8-cas-%E6%9D%A5%E5%AE%9E%E7%8E%B0%E6%A0%88/:1:0","tags":["java"],"title":"在 java 中使用 CAS 来实现栈","uri":"/ooooo-notes/%E5%9C%A8-java-%E4%B8%AD%E4%BD%BF%E7%94%A8-cas-%E6%9D%A5%E5%AE%9E%E7%8E%B0%E6%A0%88/"},{"categories":["微信文章"],"content":"2. 用 CAS 来实现一个栈 代码： 用链表来实现，当然用数组实现也可以，比较麻烦一点，后面我再写一个示例 每次操作都是先 get 来获取 top 对象，然后再 compareAndSet top public class ConcurrentStack\u003cE\u003e { private final AtomicReference\u003cNode\u003cE\u003e\u003e top = new AtomicReference\u003c\u003e(); public void push(E e) { Node\u003cE\u003e newHead = new Node\u003c\u003e(e); Node\u003cE\u003e oldHead; do { oldHead = top.get(); newHead.next = oldHead; } while (!top.compareAndSet(oldHead, newHead)); } public E pop() { Node\u003cE\u003e oldHead; Node\u003cE\u003e newHead; do { oldHead = top.get(); if (oldHead == null) { return null; } newHead = oldHead.next; } while (!top.compareAndSet(oldHead, newHead)); oldHead.next = null; return oldHead.e; } private static class Node\u003cE\u003e { private final E e; private Node\u003cE\u003e next; public Node(E e) { this.e = e; } } } ","date":"2022-11-09 08:00:00","objectID":"/ooooo-notes/%E5%9C%A8-java-%E4%B8%AD%E4%BD%BF%E7%94%A8-cas-%E6%9D%A5%E5%AE%9E%E7%8E%B0%E6%A0%88/:2:0","tags":["java"],"title":"在 java 中使用 CAS 来实现栈","uri":"/ooooo-notes/%E5%9C%A8-java-%E4%B8%AD%E4%BD%BF%E7%94%A8-cas-%E6%9D%A5%E5%AE%9E%E7%8E%B0%E6%A0%88/"},{"categories":["微信文章"],"content":"3. 代码实现位置 github 地址 ","date":"2022-11-09 08:00:00","objectID":"/ooooo-notes/%E5%9C%A8-java-%E4%B8%AD%E4%BD%BF%E7%94%A8-cas-%E6%9D%A5%E5%AE%9E%E7%8E%B0%E6%A0%88/:3:0","tags":["java"],"title":"在 java 中使用 CAS 来实现栈","uri":"/ooooo-notes/%E5%9C%A8-java-%E4%B8%AD%E4%BD%BF%E7%94%A8-cas-%E6%9D%A5%E5%AE%9E%E7%8E%B0%E6%A0%88/"},{"categories":["english"],"content":"lesson 1, 2022.11.1 Last Week I Went to the theatre, I had a very good seat, the play was very interesting.I did not enjoy it. The young man and young woman was sitting behind me.They were talking loudly. I got very angry. I could not hear actors. I turned round. I looked at the young man and young woman angrily. They did not pay any attention. In the end. I could not bear it. I turned round again. “I can’t hear a word” I said angrily. “it’s none of your business.” the young man said rudely. “This is a private conversation”. ","date":"2022-11-01 08:00:00","objectID":"/ooooo-notes/english/02/:1:0","tags":["english"],"title":"新概念英语二","uri":"/ooooo-notes/english/02/"},{"categories":["english"],"content":"lesson 2, 2022.11.5 It was Sunday, I never get up early on the Sundays. I sometimes stay in bed until lunchtime. Last Sunday. I got up very late. I looked out of the window, It was dark outside. “What a day”, I thought. “It’s raining again.” Just then, the telephone rang. It was my Aunt Lucy. “I’ve just arrived by train”, she said. “I’m coming to see you”. “But I’m still having breakfast” I said. “What are you doing?” she asked. “I’m having breakfast”. I repeated. “Dear me” she said. “Do you always get up so late”, “It’s one o’clock.”. ","date":"2022-11-01 08:00:00","objectID":"/ooooo-notes/english/02/:2:0","tags":["english"],"title":"新概念英语二","uri":"/ooooo-notes/english/02/"},{"categories":["english"],"content":"lesson 3, 2028.11.7 Postcards always spoil my holidays. Last Summer, I went to go Italy. I visited the museums and sat in public gardens. The friendly waiter taught me a few words of Italian, then he lent me a book. I read a few lines, but I did not understand a word. Every day I thought about postcards. My holidays passed quickly. I did not send cards to my friends. On the last day I made a big decision. I got up early and bought thirty-seven cards. I spent the whole day in my room. but I did not write a single card. ","date":"2022-11-01 08:00:00","objectID":"/ooooo-notes/english/02/:3:0","tags":["english"],"title":"新概念英语二","uri":"/ooooo-notes/english/02/"},{"categories":["english"],"content":"lesson 4, 2022.11.7 I’ve just received a letter from my brother. Tim. He is in Australia.He has been there for six months. He is an engineer. He is working for a big firm and has already visited a great number of different places in Australia. he has just bought an Australian car and has gone to Alice Springs, a small town in the centre of Australia. He will soon visit Darwin, From there, He will fly to Perth, My brother has never been aboard before. so he is finding this trip very exciting. ","date":"2022-11-01 08:00:00","objectID":"/ooooo-notes/english/02/:4:0","tags":["english"],"title":"新概念英语二","uri":"/ooooo-notes/english/02/"},{"categories":["english"],"content":"lesson 5, 2022.11.8 Mr. James Scott has a garage in Silbury and now he has just bought another garage in Pinhurst, Pinhurst is only five miles from Silbury, but Mr. Scott cannot get a telephone for his new garage. so he has just bought twelve pigeons. Yesterday, A pigeon carried the first message from Silbury to Pinhurst, the bird covered the distance in three minutes. Up to now, Mr. scott has sent a great many requests for spare parts and other urgent messages from one garage to the other, In this way, he has begun his own private ’telephone’ service. ","date":"2022-11-01 08:00:00","objectID":"/ooooo-notes/english/02/:5:0","tags":["english"],"title":"新概念英语二","uri":"/ooooo-notes/english/02/"},{"categories":["english"],"content":"lesson 6, 2022.11.9 I have just moved to a house in Bridge street. Yesterday, a beggar knocked at my door, he asked me for a meal and a glass of beer. In return for this, the beggar stood on his head and sang songs. I gave him a meal, he ate the food and drank the beer. Then he put a piece of cheese in his pocket and went away. Later a neighbour told me about him, Everybody knows him, his name is Percy Buttons. he calls at every house in the street once a month and always asks for a meal and a glass of beer. ","date":"2022-11-01 08:00:00","objectID":"/ooooo-notes/english/02/:6:0","tags":["english"],"title":"新概念英语二","uri":"/ooooo-notes/english/02/"},{"categories":["english"],"content":"lesson 7, 2022.11.10 The plane was late and detectives were waiting at the airport all morning. They were expecting a valuable parcel of diamonds from South Africa. A few hours earlier, someone had told the police that thieves would try to steal the diamonds. When the plane arrived, some of the detectives were waiting inside the main building while others were waiting on the airfield. Two men took the parcel off plane and carried it into Customs House. When two detectives were keeping guard at the door, two others opened the parcel. to their surprise, the precious parcel was full of stones and sand. ","date":"2022-11-01 08:00:00","objectID":"/ooooo-notes/english/02/:7:0","tags":["english"],"title":"新概念英语二","uri":"/ooooo-notes/english/02/"},{"categories":["english"],"content":"lesson 8, 2022.11.12 Joe has the most beautiful garden in our town. Nearly everybody enters for “the nicest garden competition” each year, but Joe wins every time. Bill’s garden is larger than Joe’s. Bill works harder than Joe and grows more flowers and vegetables, but Joe’s garden is more interesting. He has made neat paths and has built a wooden bridge over a pool. I like gardens too, but I do not like hard work. Every year I enter for the garden competition too, and I always win a little prize for the worst garden in the town. ","date":"2022-11-01 08:00:00","objectID":"/ooooo-notes/english/02/:8:0","tags":["english"],"title":"新概念英语二","uri":"/ooooo-notes/english/02/"},{"categories":["english"],"content":"lesson 9, 2022.11.14 On Wednesday evening, we went to the Town Hall. It was the last day of the year and a large crowd of people had gathered under the Town Hall clock. It would strike twelve in twenty minutes’ time. Fifteen minutes passed and then, at five to twelve, the clock stopped. The big minute hand did not move. We waited and waited, but nothing happened. Suddenly someone shouted, “It’s two minutes past twelve, the clock has stopped.” I looked at my watch, it was true. The big clock refused to welcome the New Year. At that moment, everyone began to laugh and sing. ","date":"2022-11-01 08:00:00","objectID":"/ooooo-notes/english/02/:9:0","tags":["english"],"title":"新概念英语二","uri":"/ooooo-notes/english/02/"},{"categories":["english"],"content":"lesson 10, 2022.11.16 We have an old musical instrument, it is called a clavichord. It was made in Germany in 1681. Our clavichord is kept in the living room. It has belonged to our family for a long time. The instrument was bought by my grandfather many years ago. Recently it was damaged by a visitor. She tried to play jazz on it. She struck the keys too hard and two of the strings were broken. My father was shocked. Now we are not allowed to touch it. It is being repaired by a friend of my father’s. ","date":"2022-11-01 08:00:00","objectID":"/ooooo-notes/english/02/:10:0","tags":["english"],"title":"新概念英语二","uri":"/ooooo-notes/english/02/"},{"categories":["english"],"content":"lesson 11, 2022.11.18 I was having dinner in a restaurant when Tony came in. Tony worked in a lawyer’s office years ago, but he is now working at a bank. He gets a good salary, but he always borrows money from his friends and never pays it back. Tony saw me and came and sat at the same table. He has never borrowed money from me. While he was eating, I asked him to lend me twenty pounds. To my surprise, he gave me the money immediately. “I have never borrowed any money from you”, Tony said, “so now you can pay for my dinner.” ","date":"2022-11-01 08:00:00","objectID":"/ooooo-notes/english/02/:11:0","tags":["english"],"title":"新概念英语二","uri":"/ooooo-notes/english/02/"},{"categories":["english"],"content":"lesson 12, 2022.11.20 Our neighbour, Captain Charles Alison, will sail from Portsmouth tomorrow. We’ll meet him at the harbour early in the morning. He will be in his small boat, Topsail. Topsail is a famous little boat. It has sailed across the Atlantic many times. Captain Alison will set out at eight o’clock, so we’ll have plenty of time. We’ll see his boat and then we’ll say goodbye to him. He will be away for two months. We are very proud of him. He will take part in an important race across the Atlantic. ","date":"2022-11-01 08:00:00","objectID":"/ooooo-notes/english/02/:12:0","tags":["english"],"title":"新概念英语二","uri":"/ooooo-notes/english/02/"},{"categories":["english"],"content":"lesson 13, 2022.11.21 The Greenwood boys are a group of pop singers. At present, they are visiting all parts of the country. They will be arriving here tomorrow. They will be coming by train and most of the young people in the town will be meeting them at the station. Tomorrow evening they will be singing at the workers’ club. The Greenwood boys will be staying for five days. During this time, they will give five performances. As usual, the police will have a difficult time. They will be trying to keep order. It is always same on these occasions. ","date":"2022-11-01 08:00:00","objectID":"/ooooo-notes/english/02/:13:0","tags":["english"],"title":"新概念英语二","uri":"/ooooo-notes/english/02/"},{"categories":["english"],"content":"lesson 14, 2022.11.22 I had an amusing experience last year. After I had left a small village in the south of French, I drove on to the next town. On the way, the young man waved to me. I stopped and he asked me for a lift. As soon as he had got into the car, I said good morning to him in French and he replied in the same language. Apart from a few words, I do not know any French at all. Neither of us spoke during the journey. I had nearly reached the town, when the young man suddenly said, very slowly, ‘Do you speck English ?’. As I soon learnt, he was English himself. ","date":"2022-11-01 08:00:00","objectID":"/ooooo-notes/english/02/:14:0","tags":["english"],"title":"新概念英语二","uri":"/ooooo-notes/english/02/"},{"categories":["english"],"content":"lesson 15, 2022.11.23 The secretary told me that Mr.xxx would see me. I felt very nervous when I went info his office. He did not look up from his desk when I entered. After I had sat down, he said that business was very bad. He told me that the firm could not afford to pay such large salaries. Twenty people had already left. I knew that my turn had come. ‘Mr. xxx’ I said in a weak voice. ‘Don’t interrupt’, he said. Then he smiled and he told me I would receive an extra thousand pounds a year. ","date":"2022-11-01 08:00:00","objectID":"/ooooo-notes/english/02/:15:0","tags":["english"],"title":"新概念英语二","uri":"/ooooo-notes/english/02/"},{"categories":["english"],"content":"lesson 16, 2022.11.27 If you park your car in the wrong place, the traffic policeman will soon find it. You will be very lucky if he lets you go without a ticket. However, this does not always happen. Traffic police are sometimes very polite. During a holidays in Sweden. I found this note on my car, ‘Sir, We welcome you to our city, This is a ‘No Parking’ area. You will enjoy your stay here if you pay attention to our street signs. This note is only a reminder’. If you receive a request like this, you cannot fail to obey it. ","date":"2022-11-01 08:00:00","objectID":"/ooooo-notes/english/02/:16:0","tags":["english"],"title":"新概念英语二","uri":"/ooooo-notes/english/02/"},{"categories":["english"],"content":"lesson 17, 2022.11.28 My aunt Jennifer is an actress. She must be at least thirty-five years old. In spite of this, she often appears on the stage as a young girl. Jennifer will have to take part in a new play soon. This time, she will be a girl of seventeen. In the play, she must appear in a bright red dress and long black stockings. Last year in another play, she had to wear short socks, and a bright orange-colored dress. If anyone ever asks her how old she is, she always answers, ‘Darling, it must be terrible to be grown up’. ","date":"2022-11-01 08:00:00","objectID":"/ooooo-notes/english/02/:17:0","tags":["english"],"title":"新概念英语二","uri":"/ooooo-notes/english/02/"},{"categories":["english"],"content":"lesson 18, 2022.11.30 After I had had lunch in a village pub, I looked for my bag. I had left it on a chair beside the door and now it wasn’t there. As I was looking for it, the landlord came in. ‘Did you have a good meal?’ he asked. ‘Yes, thank you’ I answered, ‘but I can’t pay the bill, I haven’t got my bag’. The landlord smiled and immediately went out. In a few minutes he returned with my bag and gave it back to me. ‘I’m very sorry’ he said, ‘My dog had taken it into garden’, He often does this. ","date":"2022-11-01 08:00:00","objectID":"/ooooo-notes/english/02/:18:0","tags":["english"],"title":"新概念英语二","uri":"/ooooo-notes/english/02/"},{"categories":["english"],"content":"lesson 19, 2022.12.3 ‘The play may begin at any moment’ I said. ‘It may have begun already’ Susan answered. I hurried to the ticket office, ‘May I have two tickets please?’ I asked. ‘I’m sorry, We’ve sold out’ the girl said. ‘What a pity!’ Susan exclaimed. Just then, a man hurried to the ticket office, ‘Can I returned these two ticket?’ he asked. ‘Certainly.’ the girl said. I went back to the ticket office at once. ‘Could I have those two tickets please?’ ‘Certainly, but they’re for next Wednesday’s performance. Do you still want them?’. ‘I might as well have them’ I said sadly. ","date":"2022-11-01 08:00:00","objectID":"/ooooo-notes/english/02/:19:0","tags":["english"],"title":"新概念英语二","uri":"/ooooo-notes/english/02/"},{"categories":["english"],"content":"lesson 20, 2022.12.5 Fishing is my favorite sport. I often fish for hours without catching anything. But this does not worry me. Some fishermen are unlucky. Instead of catching, they catch old boots and rubbish. I am even less unlucky. I never catch anything, not even old boots. After having spent whole mornings on the river, I always go home with an empty bag. ‘You must give up fishing.’ My friends say. ‘It’s a waste of time.’ But they don’t realize one important thing. I am not really interested in fishing. I am only interested in sitting in a boat and doing nothing at all. ","date":"2022-11-01 08:00:00","objectID":"/ooooo-notes/english/02/:20:0","tags":["english"],"title":"新概念英语二","uri":"/ooooo-notes/english/02/"},{"categories":["english"],"content":"lesson 21, 2022.12.8 Aeroplanes are slowly driving me mad. I live near an airport and passing planes can be heard night and day. The airport was built years ago, but for some seasons it could not be used then. Last year, however, it came into use. Over a hundred people must have been driven away from their homes by the noise. I am one of the few people left. Sometimes I think this house will be knocked down by a passing plane. I have been offered a large sum of money to go away. but I am determined to stay here. Everyone says I must be mad and they are probably right. ","date":"2022-11-01 08:00:00","objectID":"/ooooo-notes/english/02/:21:0","tags":["english"],"title":"新概念英语二","uri":"/ooooo-notes/english/02/"},{"categories":["english"],"content":"lesson 22, 2022.12.12 My daughter, Jane, never dreamed of receiving a letter from a girl of her own age in Holland. Last year, we were traveling across the Channel and Jane put a piece of paper with her name and address on it into a bottle. She threw the bottle into the sea. Jane never thought of it again, but ten months later, she received a letter from a girl in Holland. Both girls write to each other regularly now. However, they have decided to use the post office. Letters will cost a little more, but they will certainly travel faster. ","date":"2022-11-01 08:00:00","objectID":"/ooooo-notes/english/02/:22:0","tags":["english"],"title":"新概念英语二","uri":"/ooooo-notes/english/02/"},{"categories":["english"],"content":"lesson 23, 2022.12.13 I had a letter from my sister yesterday. She lives in Nigeria. In her letter, she said me that she would come to England next year. If she comes, she will get a surprise. We are now living in a beautiful new house in the country. Work on it had begun before my sister left. The house was completed five months ago. In my letter, I told her that she could stay with us. The house has many larger rooms and there is a lovely garden. It is a very modern house, so it looks strange to some people. It must be the only modern house in the district. ","date":"2022-11-01 08:00:00","objectID":"/ooooo-notes/english/02/:23:0","tags":["english"],"title":"新概念英语二","uri":"/ooooo-notes/english/02/"},{"categories":["english"],"content":"lesson 24, 2022.12.15 I entered a hotel manager’s office and sat down. I had just lost 50 and I felt very upset. “I left the money in my room”. I said, “and it’s not there now”. The manager was sympathetic, but he could do nothing. “Everyone’s losing money these days” he said. He started to complain about this wicked world, but was interrupted by a knock at the door. A girl came in and put an envelope on his desk. It contained $50. “I found this outside this gentleman’s room.” she said. “Well”, I said to the manager. “there is still some honesty in this world”. ","date":"2022-11-01 08:00:00","objectID":"/ooooo-notes/english/02/:24:0","tags":["english"],"title":"新概念英语二","uri":"/ooooo-notes/english/02/"},{"categories":["english"],"content":"lesson 25, 2022.12.16 I arrived in London at last. The railway station was big, black and dark. I didn’t know the way to my hotel, so I asked a porter. I not only spoke English very carefully, but very clearly as well. The porter, however, could not understand me. I repeated my question several times and at last he understood. He answered me, but he spoke neither slowly nor clearly. “I am a foreigner.” I said. Then he spoke slowly, but I could not understand him. My teacher never spoke English like that. The porter and I looked at each other and smiled. Then he said something and I understood it. “You’ll soon learn english” he said. I wonder. In England, each person speaks a different language. The English understand each other, but I don’t understand them. Do they speak English? ","date":"2022-11-01 08:00:00","objectID":"/ooooo-notes/english/02/:25:0","tags":["english"],"title":"新概念英语二","uri":"/ooooo-notes/english/02/"},{"categories":["english"],"content":"lesson 26, 2022.12.20 I am an art student and paint a lot of pictures. Many people pretend that they understand modern art. They always tell you what a picture is about. Of course, Many pictures are not about anything. They are just pretty patterns. We like them in the same way that we like pretty curtain material. I think that young children often appreciate modern pictures better than anyone else. The notice more. My sister is only seven，but she always tells me whether my pictures are good or not. She came into my room yesterday. “What are you doing?” she asked. “I’m hanging this picture on the wall” I answered. “It’s a new no, Do you like it?”. She looked at it critically for a moment. “It’s all right.” she said. “But isn’t it upside down?” I looked at it again. She was right！It was. ","date":"2022-11-01 08:00:00","objectID":"/ooooo-notes/english/02/:26:0","tags":["english"],"title":"新概念英语二","uri":"/ooooo-notes/english/02/"},{"categories":["english"],"content":"lesson 27, 2022.12.24 Late in the afternoon. The boys put up their tent in the middle of a field. As soon as this was done, they cooked a meal over an open fire. They were all hungry and the food smelled good. After a wonderful meal, they told stories and sang songs by the campfire. But some time later it began to rain. The boys felt tired, so they put out the fire and crept into their tent. Their sleeping bags were warm and comfortable, so they all slept soundly. In the middle of the night. two boys woke up and began shouting. The tent was full of water. They leapt out of their sleeping bags and hurried outside. It was raining heavily and they found that a stream had formed in the field. The stream wound its way across the field and then flowed right under their tent. ","date":"2022-11-01 08:00:00","objectID":"/ooooo-notes/english/02/:27:0","tags":["english"],"title":"新概念英语二","uri":"/ooooo-notes/english/02/"},{"categories":["english"],"content":"lesson 28, 2022.12.24 Jasper White is one of those rare people who believes in ancient myths. he has just bought a new house in the city. But ever since he moved in, he has had trouble with cars and their owners. When he returns home at night, he always finds that someone has parked a car outside his gate. Because of this, he has not been able to get his own car into his garage even once. Jasper has put up “No parking” signs outside his gate, but these have not had any effect. Now he has put an ugly stone head over the gate. It is one of the ugliest faces I have ever seen. I asked him what it was, he told me that it was Medusa, the Gorgon, Jasper hopes that she will turn cars and their owners to stone. But none of them has been turned to stone yet. ","date":"2022-11-01 08:00:00","objectID":"/ooooo-notes/english/02/:28:0","tags":["english"],"title":"新概念英语二","uri":"/ooooo-notes/english/02/"},{"categories":["english"],"content":"lesson 29, 2023.1.4 Captain Ben Fawcett has bought an unusual taxi and has begun a new service. The “taxi” is a small Swiss aeroplane called a “Pilatus Porter”. This wonderful plane can carry seven passengers. The most surprising thing about it, however, is that it can land anywhere, on snow, water or even on a ploughed field. Captain Fawcett’s first passenger was a doctor who flew from Birmingham to a lonely village in the Welsh mountains. Since then, Captain Fawcett has flown passengers to many unusual places. Once he landed on the roof of a block of flats and on another occasion he landed in a deserted car park. Captain Fawcett has just refused a strange request from a businessman. The man wanted to fly Rockall, a lonely island in the Atlantic Ocean, but Captain Fawcett did not take him, because the trip was too dangerous. ","date":"2022-11-01 08:00:00","objectID":"/ooooo-notes/english/02/:29:0","tags":["english"],"title":"新概念英语二","uri":"/ooooo-notes/english/02/"},{"categories":["english"],"content":"lesson 30. 2023.1.4 The Wayle is a small river that cuts across the park near my home. I like sitting by the Wayle on fine afternoons. It was warm last Sunday, so I went and sat on the river bank as usual. Some children were playing games on the bank and there were some people rowing on the river. Suddenly, one of the children kicked a ball very hard and it went towards a passing boat. Some people on the bank called out to the man in the boat, but he did not hear him. The ball struck him so hard that he nearly fell into the water. I turned to look at the children, but there weren’t any in sight. They had all run away. The man laughed when he realized what had happened, he called out to the children and threw the ball back to the bank. ","date":"2022-11-01 08:00:00","objectID":"/ooooo-notes/english/02/:30:0","tags":["english"],"title":"新概念英语二","uri":"/ooooo-notes/english/02/"},{"categories":["english"],"content":"lesson 31, 2023.1.6 Yesterday afternoon Frank was telling me about his experiences as a young man. Before he retired, he was the head of a large business company, but as a boy he used to work in a small shop. It is his job to repair bicycles and at that time he used to work fourteen hours a day. He saved money for years and in 1958 he bought a small workshop of his own. In his twenties Frank used to make spare parts for aeroplanes. At that time he had two helpers. In a few years the small workshop had become a large factory which employed seven hundred and twenty-eight people. Frank smiled when he remembered his hard early years and a long road to success. Frank was still smiling when the door opened and his wife came in. She wanted him to repair their grandson’s bicycle. ","date":"2022-11-01 08:00:00","objectID":"/ooooo-notes/english/02/:31:0","tags":["english"],"title":"新概念英语二","uri":"/ooooo-notes/english/02/"},{"categories":["english"],"content":"lesson 32, 2023.1.6 People are not so honest as they once were. The temptation to steal is greater than ever before, especially in large shops. The detective recently watched a well-dressed woman who always went into a large store on Monday mornings. One Monday, there were fewer people in the shop than usual when the woman came in, so it was easier for the detective to watch him. The woman first bought a few small articles. After a little time, she chose one of the most expensive dresses in the shop and handed it to an assistant who wrapped it up for her as quick as possible. Then the woman simply took the parcel and walked out of the shop without paying. When she was arrested, the detective found out that the shop assistant was her daughter. The girl “gave” her mother a free dress once a week. ","date":"2022-11-01 08:00:00","objectID":"/ooooo-notes/english/02/:32:0","tags":["english"],"title":"新概念英语二","uri":"/ooooo-notes/english/02/"},{"categories":["english"],"content":"lesson 33, 2023.1.10 Nearly a week passed before the girl was able to explain what had happened to her. One afternoon she set out from the coast in a small boat and was caught in a storm. Towards evening, the boat struck a rock and the girl jumped into the sea. Then she swam to the shore after spending the whole night in the water. During that time she covered a distance of eight miles. Early next morning, she saw a light ahead. She knew she was near the shore because the light was high up on the cliffs. On arriving at the shore, the girl struggled up the cliff towards the light she had seen. That was all she remembered. When she woke up a day later, she found herself in hospital. ","date":"2022-11-01 08:00:00","objectID":"/ooooo-notes/english/02/:33:0","tags":["english"],"title":"新概念英语二","uri":"/ooooo-notes/english/02/"},{"categories":["english"],"content":"lesson 34, 2023.1.10 Dan Robinson has been worried all week. Last Tuesday he received a letter from the local police. In the letter, he was asked to call at the station. Dan wondered why he was wanted by the police. but he went into the station yesterday and now he is not worried anymore. At the station, he was told by a smiling policeman that his bicycle had been found. Five days ago, the policeman told him, the bicycle was picked up in a small village four hundred miles away. It is now being sent to his home by train. Dan was most surprised when he heard the news. He was amused too, because he never expected the bicycle to be found. It was stolen twenty years ago when Dan was a boy of fifteen. ","date":"2022-11-01 08:00:00","objectID":"/ooooo-notes/english/02/:34:0","tags":["english"],"title":"新概念英语二","uri":"/ooooo-notes/english/02/"},{"categories":["english"],"content":"lesson 35, 2023.1.12 Roy used to drive a taxi. A short while ago, however, he became to a bus driver and he has not regretted it. He is finding his new work far more exciting. When he was driving along Carford street recently, he saw two thieves rush out of a shop and run towards a waiting car. One of them was carrying a bag full of money. Roy acted quickly and drove his bus straight at two thieves. The one with the money got such a fright that he dropped the bag. As two thieves were trying to get away in their car, Roy drove his bus into the back of it. While the battered car was moving away, Roy stopped his bus and telephoned the police. The thieves’ car was badly damaged and easy to recognize. Shortly afterwards, the police stopped the car and both men were arrested. ","date":"2022-11-01 08:00:00","objectID":"/ooooo-notes/english/02/:35:0","tags":["english"],"title":"新概念英语二","uri":"/ooooo-notes/english/02/"},{"categories":["english"],"content":"lesson 36, 2023.1.21 Debbie Hart is going to swim across the English Channel tomorrow. She is going to set out from French coast at five o’clock in the morning. Debbie is only eleven years old and she hopes to set up a new world record. She is a strong swimmer and many people feel that she is sure to succeed. Debbie’s father will set out with her in a small boat. Mr. Hart has trained his daughter for years. Tomorrow he will be watching her anxiously as she swims the long distance to England. Debbie intends to take short rests every two hours. She will have something to drink but she will not eat any solid food. Most of Debbie’s school friends will be waiting for her on the English coast. Among them will be Debbie’s mother, who swam the Channel herself when she was a girl. ","date":"2022-11-01 08:00:00","objectID":"/ooooo-notes/english/02/:36:0","tags":["english"],"title":"新概念英语二","uri":"/ooooo-notes/english/02/"},{"categories":["english"],"content":"lesson 37, 2023.1.28 The Olympic Games will be held in our country in four years’ time. As a great many people will be visiting the country, the government will be building new hotels, an immense stadium, a new Olympic-standard swimming pool. They will also be building new roads and a special railway line. The Games will be held just outside the capital and the whole area will be called ‘Olympic City’. Workers will have completed new roads by the end of this year. By the end of next year, they will have finished work on the new stadium. The fantastic modern buildings have been designed by Kurt Gunter. Everyone will be watching anxiously as the new building go up. We are all very excited and are looking forward to the Olympic Games, because they have never been held before in this country. ","date":"2022-11-01 08:00:00","objectID":"/ooooo-notes/english/02/:37:0","tags":["english"],"title":"新概念英语二","uri":"/ooooo-notes/english/02/"},{"categories":["english"],"content":"lesson 38, 2023.1.30 My old friend, Harrison had lived in the Mediterranean for many years before he returned to England. He had often dreamed of retiring in England and had planned to settle down in the country. He had no sooner returned than he bought a house and went to live there. Almost immediately he began to complain about the weather for even though it was still summer, it rained continually and it was often bitterly cold. After so many years of sunshine, Harrison got a shock. He acted as if he had never lived in England before. In the end, it was more than he could bear. He had hardly had time to settle down when he sold the house and left the country. The dream he had had for so many years ended there. Harrison had thought of everything except the weather. ","date":"2022-11-01 08:00:00","objectID":"/ooooo-notes/english/02/:38:0","tags":["english"],"title":"新概念英语二","uri":"/ooooo-notes/english/02/"},{"categories":["english"],"content":"lesson 39, 2023.1.31 When John Gibert was in hospital, he asked his doctor to tell him whether his operation had been successful, but the doctor refused to do so. The following day, the patient asked for a bedside telephone. When he was alone, he telephoned the hospital exchange and asked for Doctor Millington. When the doctor answered the phone, Mr. Gibert said he was inquiring about a certain patient, a Mr. John Gibert. He asked if Mr. Gibert’s operation had been successful and the doctor told him that it had been. He then asked when Mr. Gibert would be allowed to go home and the doctor told him that he would have to stay in hospital for another two weeks. The Dr. Millington asked the caller if he was a relative of the patient, “No” the patient answered. “I’m Mr. John Gibert.” ","date":"2022-11-01 08:00:00","objectID":"/ooooo-notes/english/02/:39:0","tags":["english"],"title":"新概念英语二","uri":"/ooooo-notes/english/02/"},{"categories":["english"],"content":"lesson 40, 2023.2.2 Last week at a dinner party, the hostess asked me to sit next to Mrs. Rumbold. Mrs. Rumbold is a large, unsmiling lady in a tight black dress. She did not even look up when I took my seat beside her. Her eyes were fixed on her plate and in a short time, she was busy eating. I tried to make conversation. “A new player is coming to ‘The Globe’ soon” I said, \" Will you be seeing it?\". “No.” she answered. “Will you be spending your holidays abroad this year? \" I asked. “No.” she answered. “Will you be staying in England?” I asked. “No.” she answered. In despair, I asked her whether she was enjoying her dinner. “Young man.” she answered, “If you ate more and talked less, we would both enjoy our dinner.” ","date":"2022-11-01 08:00:00","objectID":"/ooooo-notes/english/02/:40:0","tags":["english"],"title":"新概念英语二","uri":"/ooooo-notes/english/02/"},{"categories":["english"],"content":"lesson 41, 2023.2.3 “Do you call that a hat?” I said to my wife. “You needn’t be so rude about it.” my wife answered as she looked at herself in the mirror. I sat down on one of those modern chairs with holes in it and waited. We had been in the hat shop for half an hour and my wife was still in front of the mirror. “We mustn’t bug things we don’t need”, I remarked suddenly. I regretted saying it almost at once. “You needn’t have said that”. my wife answered. “I needn’t remind you of that terrible hat you bought yesterday.” “I find it beautiful.” I said. “A man can never have too many ties.”. “And A woman can’t have too many hats.” she answered. Ten minutes later we walked out of the shop together. My wife was wearing a hat that looked like a lighthouse. ","date":"2022-11-01 08:00:00","objectID":"/ooooo-notes/english/02/:41:0","tags":["english"],"title":"新概念英语二","uri":"/ooooo-notes/english/02/"},{"categories":["english"],"content":"lesson 42, 2023.2.4 As we had had a long walk through one of markets of old Delhi, so we stopped at a square to have a rest. After a time, we noticed a snake charmer with two large baskets at the other side of the square, so we went to have a look at him. As soon as he saw us, he picked up a long pipe which was covered with coins and opened one of the baskets. When he began to play a tune, we had our first glimpse of the snake. It rose out of the basket and began to follow the movements of the pipe. We were very much surprised when the snake charmer suddenly began to play jazz and modern pop songs. The snake, however, continued to “dance” slowly. It obviously could not tell the difference between Indian music and jazz. ","date":"2022-11-01 08:00:00","objectID":"/ooooo-notes/english/02/:42:0","tags":["english"],"title":"新概念英语二","uri":"/ooooo-notes/english/02/"},{"categories":["english"],"content":"lesson 43, 2023.2.14 In 1929, three years after his fight over the North Pole, the American explorer, R. E. Byrd, successfully flew over the South Pole for the first time. Though, at first. Byrd and his men were able to take a great many of photographs of the mountains that lay below, they soon ran into serious trouble. At one point, it seemed certain that their plane would crush. It could only get over the mountains if it rose 10,000 feet. Byrd at once ordered his men to throw out two heavy food sacks. The plan was then able to rise and it cleared the mountains by 400 feet. Byrd now knew that he would be able to reach the South Pole which was 300 miles for there were no more mountains in sight. The aircraft was able to fly over the endless white plains without difficulty. ","date":"2022-11-01 08:00:00","objectID":"/ooooo-notes/english/02/:43:0","tags":["english"],"title":"新概念英语二","uri":"/ooooo-notes/english/02/"},{"categories":["微信文章"],"content":"grpc 使用方式 grpc 作为一个通信方式，现在可以说是非常流行。如果不会 grpc，你可能跟不上时代了, 这里我只是做一个很简单的例子，并说下如何进一步学习 grpc。 grpc 接口需要编写 .proto 文件，如下面的例子： 有一个接口类：Greeter. 有两个方法 SayHello, SayHi. syntax = \"proto3\"; option java_multiple_files = true; option java_package = \"com.ooooo.grpc.helloworld\"; option java_outer_classname = \"HelloWorldProto\"; package helloworld; // The greeting service definition. service Greeter { // Sends a greeting rpc SayHello (HelloRequest) returns (HelloReply) {} rpc SayHi (HelloRequest) returns (HelloReply) {} } // The request message containing the user's name. message HelloRequest { string name = 1; } // The response message containing the greetings message HelloReply { string message = 1; } 在这里，我很推荐大家看下，protobuf 是怎么编码的。 proto3, 官方地址： https://developers.google.com/protocol-buffers/docs/proto3 proto3 encoding, 官方地址： https://developers.google.com/protocol-buffers/docs/encoding 编写完 .proto 文件，执行 gradle 的 generateProto 任务, 就会生成相应的 java 代码。 然后编写入口程序 GrpcClient 和 GrpcServer。 ","date":"2022-10-22 09:00:00","objectID":"/ooooo-notes/%E5%9C%A8-java-%E4%B8%AD%E5%A6%82%E4%BD%95%E4%BD%BF%E7%94%A8-grpc/:1:0","tags":["java","grpc"],"title":"在 java 中如何使用 grpc","uri":"/ooooo-notes/%E5%9C%A8-java-%E4%B8%AD%E5%A6%82%E4%BD%95%E4%BD%BF%E7%94%A8-grpc/"},{"categories":["微信文章"],"content":"如何进一步学习 grpc 学习 grpc 如何使用，如何扩展，可以看 https://github.com/grpc/grpc-java/tree/master/examples. 在 spring-boot 中如何使用，有开源的 starter. grpc 是基于 http2 协议的，你必须熟悉 http2 协议。 更深入的学习，也就是学习源码，有时间给大家说下 grpc 的源码，也是比较简单的。 ","date":"2022-10-22 09:00:00","objectID":"/ooooo-notes/%E5%9C%A8-java-%E4%B8%AD%E5%A6%82%E4%BD%95%E4%BD%BF%E7%94%A8-grpc/:2:0","tags":["java","grpc"],"title":"在 java 中如何使用 grpc","uri":"/ooooo-notes/%E5%9C%A8-java-%E4%B8%AD%E5%A6%82%E4%BD%95%E4%BD%BF%E7%94%A8-grpc/"},{"categories":["微信文章"],"content":"2. 代码实现位置 github 地址 ","date":"2022-10-22 09:00:00","objectID":"/ooooo-notes/%E5%9C%A8-java-%E4%B8%AD%E5%A6%82%E4%BD%95%E4%BD%BF%E7%94%A8-grpc/:3:0","tags":["java","grpc"],"title":"在 java 中如何使用 grpc","uri":"/ooooo-notes/%E5%9C%A8-java-%E4%B8%AD%E5%A6%82%E4%BD%95%E4%BD%BF%E7%94%A8-grpc/"},{"categories":["微信文章"],"content":"1. 添加打印 SQL 的方式 打印 SQL 的方式有很多，比如有 idea 插件，有 mybatis 拦截器，有代理 datasource, 有代理 driver. 我比较认可的方式就是代理 driver. 这种无任何侵入性。 下面来介绍如何使用 p6spy Driver。 示例代码： 使用 BeanPostProcessor 来动态扩展 bean。 判断 bean 是否为 DataSource 类型，并判断开发配置 dev.sql-log.enabled。 根据现有的 driver 配置来创建新的 datasource，并设置 url。 实际开启，还需要 spy.properties 配置文件 @Slf4j @Configuration public class DevDataSourceConfiguration implements BeanPostProcessor, EnvironmentAware { @Setter private Environment environment; @Override public Object postProcessAfterInitialization(Object bean, String beanName) throws BeansException { if (bean instanceof DataSource \u0026\u0026 parseBoolean(environment.resolvePlaceholders(\"${dev.sql-log.enabled:false}\"))) { log.info(\"已开启日志打印，将使用[P6SpyDriver]\"); if (environment.acceptsProfiles(Profiles.of(\"run\"))) { log.warn(\"在生产环境一定要关闭配置[dev.sql-log.enabled]\"); } return proxyDataSource((DataSource) bean); } return bean; } public DataSource proxyDataSource(DataSource dataSource) { if (dataSource instanceof AbstractRoutingDataSource) { AbstractRoutingDataSource abstractRoutingDataSource = (AbstractRoutingDataSource) dataSource; // resolvedDataSources Field resolvedDataSourcesField = findField(AbstractRoutingDataSource.class, \"resolvedDataSources\"); makeAccessible(resolvedDataSourcesField); @SuppressWarnings(\"unchecked\") Map\u003cObject, DataSource\u003e resolvedDataSources = (Map\u003cObject, DataSource\u003e) getField(resolvedDataSourcesField, abstractRoutingDataSource); if (resolvedDataSources != null) { resolvedDataSources.forEach((k, v) -\u003e resolvedDataSources.put(k, convertToProxyDataSource(v))); } // resolvedDefaultDataSource Field resolvedDefaultDataSourceField = findField(AbstractRoutingDataSource.class, \"resolvedDefaultDataSource\"); makeAccessible(resolvedDefaultDataSourceField); DataSource resolvedDefaultDataSource = (DataSource) getField(resolvedDefaultDataSourceField, abstractRoutingDataSource); if (resolvedDefaultDataSource != null) { ReflectionUtils.setField(resolvedDefaultDataSourceField, abstractRoutingDataSource, convertToProxyDataSource(resolvedDefaultDataSource)); } return abstractRoutingDataSource; } return convertToProxyDataSource(dataSource); } public DataSource convertToProxyDataSource(DataSource dataSource) { if (dataSource instanceof HikariDataSource) { HikariConfig oldConfig = (HikariDataSource) dataSource; // jdbc:h2:mem:test to jdbc:p6spy:h2:mem:test String jdbcUrl = oldConfig.getJdbcUrl(); if (!jdbcUrl.contains(\"p6spy\")) { jdbcUrl = \"jdbc:p6spy\" + jdbcUrl.substring(4); } HikariConfig newConfig = new HikariConfig(); newConfig.setPoolName(\"proxy-P6SpyDriver\"); newConfig.setDriverClassName(\"com.p6spy.engine.spy.P6SpyDriver\"); newConfig.setJdbcUrl(jdbcUrl); newConfig.setUsername(oldConfig.getUsername()); newConfig.setPassword(oldConfig.getPassword()); return new HikariDataSource(newConfig); } return dataSource; } } ","date":"2022-10-22 09:00:00","objectID":"/ooooo-notes/%E5%9C%A8-java-%E4%B8%AD%E5%A6%82%E4%BD%95%E6%B7%BB%E5%8A%A0-sql-%E6%97%A5%E5%BF%97/:1:0","tags":["java"],"title":"在 java 中如何添加 SQL 日志","uri":"/ooooo-notes/%E5%9C%A8-java-%E4%B8%AD%E5%A6%82%E4%BD%95%E6%B7%BB%E5%8A%A0-sql-%E6%97%A5%E5%BF%97/"},{"categories":["微信文章"],"content":"2. 代码实现位置 github 地址 ","date":"2022-10-22 09:00:00","objectID":"/ooooo-notes/%E5%9C%A8-java-%E4%B8%AD%E5%A6%82%E4%BD%95%E6%B7%BB%E5%8A%A0-sql-%E6%97%A5%E5%BF%97/:2:0","tags":["java"],"title":"在 java 中如何添加 SQL 日志","uri":"/ooooo-notes/%E5%9C%A8-java-%E4%B8%AD%E5%A6%82%E4%BD%95%E6%B7%BB%E5%8A%A0-sql-%E6%97%A5%E5%BF%97/"},{"categories":["微信文章"],"content":"1. 自定义 classloader 有时候，我们在项目开发的时候，会遇到比较恶心的问题，存在两个不同 jar 包，但是类的全限定名是一样的，而这两个包都不能删除，这时候调用可能就会出问题。 如何解决上面的问题？ 我的答案就是自定义类加载器。 场景模拟 module-a: 表示 a.jar module-b: 表示 b.jar module-main: 表示 程序入口 由于在 module-main 项目中同时引入了 module-a 和 module-b 这两个 jar 包，但是存在冲突类 HelloService, 最终导致程序运行错误。 如何自定义 classloader，来解决问题？ 重新定义 loadClass 方法，打破父类委托机制 使用 getResources 方法来获取所有的 class 文件，然后判断 @SneakyThrows public String test1(String message) { // 查找类 ClassLoader classLoader = new ModuleAClassLoader(); Class\u003c?\u003e clazz = classLoader.loadClass(\"com.ooooo.HelloService\"); // 执行 Method test1 = ReflectionUtils.findMethod(clazz, \"test1\", String.class); Object result = test1.invoke(null, message); return (String) result; } @SneakyThrows public String test2(String message) { // 查找类 ClassLoader classLoader = new ModuleBClassLoader(); Class\u003c?\u003e clazz = classLoader.loadClass(\"com.ooooo.HelloService\"); // 执行 Method test2 = ReflectionUtils.findMethod(clazz, \"test2\", String.class); Object result = test2.invoke(null, message); return (String) result; } private static class ModuleAClassLoader extends ClassLoader { public ModuleAClassLoader() { super(ModuleAClassLoader.class.getClassLoader()); } @SneakyThrows @Override protected Class\u003c?\u003e loadClass(String name, boolean resolve) throws ClassNotFoundException { Class\u003c?\u003e c = findLoadedClass(name); if (c == null) { // 当前路径下去找 if (name.contains(\"com.ooooo\")) { String path = name.replace(\".\", \"/\") + \".class\"; Enumeration\u003cURL\u003e resources = getResources(path); URL targetUrl = null; while (resources.hasMoreElements()) { targetUrl = resources.nextElement(); if (targetUrl.toString().contains(\"module-a\")) { break; } } // 读取 class 文件 InputStream in = targetUrl.openStream(); byte[] bytes = StreamUtils.copyToByteArray(in); in.close(); c = defineClass(name, bytes, 0, bytes.length); } } if (c == null) { c = getParent().loadClass(name); } if (resolve) { resolveClass(c); } return c; } } private static class ModuleBClassLoader extends ClassLoader { public ModuleBClassLoader() { super(ModuleAClassLoader.class.getClassLoader()); } @SneakyThrows @Override protected Class\u003c?\u003e loadClass(String name, boolean resolve) throws ClassNotFoundException { Class\u003c?\u003e c = findLoadedClass(name); if (c == null) { // 当前路径下去找 if (name.contains(\"com.ooooo\")) { String path = name.replace(\".\", \"/\") + \".class\"; Enumeration\u003cURL\u003e resources = getResources(path); URL targetUrl = null; while (resources.hasMoreElements()) { targetUrl = resources.nextElement(); if (targetUrl.toString().contains(\"module-b\")) { break; } } // 读取 class 文件 InputStream in = targetUrl.openStream(); byte[] bytes = StreamUtils.copyToByteArray(in); in.close(); c = defineClass(name, bytes, 0, bytes.length); } } if (c == null) { c = getParent().loadClass(name); } if (resolve) { resolveClass(c); } return c; } } } ","date":"2022-10-18 09:00:00","objectID":"/ooooo-notes/%E5%9C%A8-java-%E4%B8%AD%E5%A6%82%E4%BD%95%E8%A7%A3%E5%86%B3%E7%B1%BB%E5%86%B2%E7%AA%81%E9%97%AE%E9%A2%98/:1:0","tags":["java"],"title":"在 java 中如何解决类冲突问题","uri":"/ooooo-notes/%E5%9C%A8-java-%E4%B8%AD%E5%A6%82%E4%BD%95%E8%A7%A3%E5%86%B3%E7%B1%BB%E5%86%B2%E7%AA%81%E9%97%AE%E9%A2%98/"},{"categories":["微信文章"],"content":"2. 代码实现位置 github 地址 ","date":"2022-10-18 09:00:00","objectID":"/ooooo-notes/%E5%9C%A8-java-%E4%B8%AD%E5%A6%82%E4%BD%95%E8%A7%A3%E5%86%B3%E7%B1%BB%E5%86%B2%E7%AA%81%E9%97%AE%E9%A2%98/:2:0","tags":["java"],"title":"在 java 中如何解决类冲突问题","uri":"/ooooo-notes/%E5%9C%A8-java-%E4%B8%AD%E5%A6%82%E4%BD%95%E8%A7%A3%E5%86%B3%E7%B1%BB%E5%86%B2%E7%AA%81%E9%97%AE%E9%A2%98/"},{"categories":["微信文章"],"content":"1. methodHandle 调用 在过去，我们调用一个类的方法，除了直接调用，再就是使用反射来调用了。而今天我要说说 jdk 新引入的方式来调用。 比如我们有一个很简单的 UserService 类。 public class UserService { public String getUsername(String id) { return \"username\" + id; } } 直接调用和反射调用的方式比较简单，我就不说明了。 下面来演示 invoke 包的使用。 使用 MethodHandles.lookup() 来查找对应的方法 使用 methodHandle 来调用，分为几种不同的方式 public class UserServiceTest { private MethodType methodType; private Lookup lookup; private MethodHandle methodHandle; @SneakyThrows @BeforeEach public void beforeEach() { methodType = MethodType.methodType(String.class, String.class); lookup = MethodHandles.lookup(); methodHandle = lookup.findVirtual(UserService.class, \"getUsername\", methodType); } @SneakyThrows @Test public void invokeWithArguments() { UserService userService = new UserService(); Object obj = methodHandle.bindTo(userService).invokeWithArguments(\"1\"); assertEquals(\"username1\", obj); } @SneakyThrows @Test public void invoke() { UserService userService = new UserService(); Object obj = methodHandle.invoke(userService, \"1\"); assertEquals(\"username1\", obj); } /** * 要求类型全匹配, 包括返回值类型 */ @SneakyThrows @Test public void invokeExact() { UserService userService = new UserService(); String s = (String) methodHandle.invokeExact(userService, \"1\"); assertEquals(\"username1\", s); } } ","date":"2022-10-17 09:00:00","objectID":"/ooooo-notes/%E5%A6%82%E4%BD%95%E4%BD%BF%E7%94%A8-java-%E4%B8%AD-invoke-%E5%8C%85/:1:0","tags":["java"],"title":"如何使用 java 中 invoke 包?","uri":"/ooooo-notes/%E5%A6%82%E4%BD%95%E4%BD%BF%E7%94%A8-java-%E4%B8%AD-invoke-%E5%8C%85/"},{"categories":["微信文章"],"content":"2. 代码实现位置 github 地址 ","date":"2022-10-17 09:00:00","objectID":"/ooooo-notes/%E5%A6%82%E4%BD%95%E4%BD%BF%E7%94%A8-java-%E4%B8%AD-invoke-%E5%8C%85/:2:0","tags":["java"],"title":"如何使用 java 中 invoke 包?","uri":"/ooooo-notes/%E5%A6%82%E4%BD%95%E4%BD%BF%E7%94%A8-java-%E4%B8%AD-invoke-%E5%8C%85/"},{"categories":["微信文章"],"content":"1. jmh 微基准测试 实际上，在 java 中进行微基椎测试并不容易，主要原因在于解释执行，编译执行，而编译执行又分为 C1编译, C2编译。即使是对同一个代码来说，不同的 jvm 参数也会导致测试不一样。 那是否应该了解微基准测试？ 我的答案，是必须掌握的。 jmh 就是我们应该学习的微基准测试的框架。 下面我以一个示例来说明如何快速上手测试，测试 StringBuilder， StringBuffer， String 连接字符的性能。 注意点： Fork 参数可以测试不同的 Jvm 参数。 输出结果的时间单位最好是纳秒。 测试模式可以自由选择，如果测试性能，最好选择平均时间。 如果是在 IDE 中，建议安装 jmh 插件来执行。 代码示例 // 预热的参数 @Warmup(time = 1) // 测试的参数 @Measurement(time = 1) // 可以添加 JVM 参数来测试 @Fork(value = 1) @State(Scope.Thread) @OutputTimeUnit(TimeUnit.NANOSECONDS) @BenchmarkMode(Mode.AverageTime) public class TestStringBenchmark { @Benchmark public String stringBuilder() { StringBuilder sb = new StringBuilder(); sb.append(\"hello\"); sb.append(\"world\"); return sb.toString(); } @Benchmark public String stringBuffer() { StringBuffer sb = new StringBuffer(); sb.append(\"hello\"); sb.append(\"world\"); return sb.toString(); } @Benchmark public String stringConcat() { return \"hello\" + \"world\"; } } ","date":"2022-10-16 09:00:00","objectID":"/ooooo-notes/%E5%9C%A8-java-%E4%B8%AD%E5%A6%82%E4%BD%95%E8%BF%9B%E8%A1%8C%E5%BE%AE%E5%9F%BA%E5%87%86%E6%B5%8B%E8%AF%95/:1:0","tags":["java"],"title":"在 java 中如何进行微基准测试 ?","uri":"/ooooo-notes/%E5%9C%A8-java-%E4%B8%AD%E5%A6%82%E4%BD%95%E8%BF%9B%E8%A1%8C%E5%BE%AE%E5%9F%BA%E5%87%86%E6%B5%8B%E8%AF%95/"},{"categories":["微信文章"],"content":"2. 代码实现位置 github 地址 ","date":"2022-10-16 09:00:00","objectID":"/ooooo-notes/%E5%9C%A8-java-%E4%B8%AD%E5%A6%82%E4%BD%95%E8%BF%9B%E8%A1%8C%E5%BE%AE%E5%9F%BA%E5%87%86%E6%B5%8B%E8%AF%95/:2:0","tags":["java"],"title":"在 java 中如何进行微基准测试 ?","uri":"/ooooo-notes/%E5%9C%A8-java-%E4%B8%AD%E5%A6%82%E4%BD%95%E8%BF%9B%E8%A1%8C%E5%BE%AE%E5%9F%BA%E5%87%86%E6%B5%8B%E8%AF%95/"},{"categories":["微信文章"],"content":"3. 参考 强烈建议大家看官方代码, 地址: https://github.com/openjdk/jmh ","date":"2022-10-16 09:00:00","objectID":"/ooooo-notes/%E5%9C%A8-java-%E4%B8%AD%E5%A6%82%E4%BD%95%E8%BF%9B%E8%A1%8C%E5%BE%AE%E5%9F%BA%E5%87%86%E6%B5%8B%E8%AF%95/:3:0","tags":["java"],"title":"在 java 中如何进行微基准测试 ?","uri":"/ooooo-notes/%E5%9C%A8-java-%E4%B8%AD%E5%A6%82%E4%BD%95%E8%BF%9B%E8%A1%8C%E5%BE%AE%E5%9F%BA%E5%87%86%E6%B5%8B%E8%AF%95/"},{"categories":["微信文章"],"content":"我们在开发过程中，经常会使用 lambda 函数式编程，这样会更加简单。 ","date":"2022-10-11 22:25:00","objectID":"/ooooo-notes/%E5%9C%A8-java-%E4%B8%AD%E5%A6%82%E4%BD%95%E8%8E%B7%E5%8F%96-lambda-%E7%9A%84%E6%96%B9%E6%B3%95%E5%90%8D%E7%A7%B0/:0:0","tags":["IDE"],"title":"在 java 中如何获取 lambda 的方法名称?","uri":"/ooooo-notes/%E5%9C%A8-java-%E4%B8%AD%E5%A6%82%E4%BD%95%E8%8E%B7%E5%8F%96-lambda-%E7%9A%84%E6%96%B9%E6%B3%95%E5%90%8D%E7%A7%B0/"},{"categories":["微信文章"],"content":"1. 使用方式 比如下面有一个很简单的 User 类。其中有一个属性 username @Data public class User { private String username; public String getPassword(String password) { return password; } } // 使用的方式 // user -\u003e user.getUsername() 等价于 User::getUsername Function\u003cUser, String\u003e getUsername1 = User::getUsername 比如我现在要使用 user -\u003e user.getUsername()， 这样的 lambda 表达式来获取一个 User 对象的 username 属性值。 我现在可以这样来获取 username 这个方法名称。 public class LambdaUtils { // 正常情况，要做缓存 public static \u003cT\u003e String resolveMethod(SFunction\u003cT, ?\u003e func) { SerializedLambda serializedLambda = resovle(func); String methodName = serializedLambda.getImplMethodName(); return methodName; } @SneakyThrows public static SerializedLambda resovle(SFunction\u003c?, ?\u003e func) { Method method = func.getClass().getDeclaredMethod(\"writeReplace\"); method.setAccessible(true); return (SerializedLambda) method.invoke(func); } } ","date":"2022-10-11 22:25:00","objectID":"/ooooo-notes/%E5%9C%A8-java-%E4%B8%AD%E5%A6%82%E4%BD%95%E8%8E%B7%E5%8F%96-lambda-%E7%9A%84%E6%96%B9%E6%B3%95%E5%90%8D%E7%A7%B0/:1:0","tags":["IDE"],"title":"在 java 中如何获取 lambda 的方法名称?","uri":"/ooooo-notes/%E5%9C%A8-java-%E4%B8%AD%E5%A6%82%E4%BD%95%E8%8E%B7%E5%8F%96-lambda-%E7%9A%84%E6%96%B9%E6%B3%95%E5%90%8D%E7%A7%B0/"},{"categories":["微信文章"],"content":"2. 代码实现位置 本节的内容，我叙述的不是很好，可能看的一脸懵, 使用过 mybatis-plus 的人，可能会有点印象 LambdaQueryWrapper， 推荐看下这个测试类 LambdaUtilsTests github 地址 ","date":"2022-10-11 22:25:00","objectID":"/ooooo-notes/%E5%9C%A8-java-%E4%B8%AD%E5%A6%82%E4%BD%95%E8%8E%B7%E5%8F%96-lambda-%E7%9A%84%E6%96%B9%E6%B3%95%E5%90%8D%E7%A7%B0/:2:0","tags":["IDE"],"title":"在 java 中如何获取 lambda 的方法名称?","uri":"/ooooo-notes/%E5%9C%A8-java-%E4%B8%AD%E5%A6%82%E4%BD%95%E8%8E%B7%E5%8F%96-lambda-%E7%9A%84%E6%96%B9%E6%B3%95%E5%90%8D%E7%A7%B0/"},{"categories":["微信文章"],"content":"如何设计一个连接池 ? ","date":"2022-09-13 10:20:00","objectID":"/ooooo-notes/%E5%A6%82%E4%BD%95%E8%AE%BE%E8%AE%A1%E4%B8%80%E4%B8%AA%E8%BF%9E%E6%8E%A5%E6%B1%A0/:0:0","tags":["java"],"title":"如何设计一个连接池","uri":"/ooooo-notes/%E5%A6%82%E4%BD%95%E8%AE%BE%E8%AE%A1%E4%B8%80%E4%B8%AA%E8%BF%9E%E6%8E%A5%E6%B1%A0/"},{"categories":["微信文章"],"content":"1. 需求 ","date":"2022-09-13 10:20:00","objectID":"/ooooo-notes/%E5%A6%82%E4%BD%95%E8%AE%BE%E8%AE%A1%E4%B8%80%E4%B8%AA%E8%BF%9E%E6%8E%A5%E6%B1%A0/:1:0","tags":["java"],"title":"如何设计一个连接池","uri":"/ooooo-notes/%E5%A6%82%E4%BD%95%E8%AE%BE%E8%AE%A1%E4%B8%80%E4%B8%AA%E8%BF%9E%E6%8E%A5%E6%B1%A0/"},{"categories":["微信文章"],"content":"2. 实现的关键点 ","date":"2022-09-13 10:20:00","objectID":"/ooooo-notes/%E5%A6%82%E4%BD%95%E8%AE%BE%E8%AE%A1%E4%B8%80%E4%B8%AA%E8%BF%9E%E6%8E%A5%E6%B1%A0/:2:0","tags":["java"],"title":"如何设计一个连接池","uri":"/ooooo-notes/%E5%A6%82%E4%BD%95%E8%AE%BE%E8%AE%A1%E4%B8%80%E4%B8%AA%E8%BF%9E%E6%8E%A5%E6%B1%A0/"},{"categories":["微信文章"],"content":"3. druid datasource ","date":"2022-09-13 10:20:00","objectID":"/ooooo-notes/%E5%A6%82%E4%BD%95%E8%AE%BE%E8%AE%A1%E4%B8%80%E4%B8%AA%E8%BF%9E%E6%8E%A5%E6%B1%A0/:3:0","tags":["java"],"title":"如何设计一个连接池","uri":"/ooooo-notes/%E5%A6%82%E4%BD%95%E8%AE%BE%E8%AE%A1%E4%B8%80%E4%B8%AA%E8%BF%9E%E6%8E%A5%E6%B1%A0/"},{"categories":["微信文章"],"content":"4. common-pool2 ","date":"2022-09-13 10:20:00","objectID":"/ooooo-notes/%E5%A6%82%E4%BD%95%E8%AE%BE%E8%AE%A1%E4%B8%80%E4%B8%AA%E8%BF%9E%E6%8E%A5%E6%B1%A0/:4:0","tags":["java"],"title":"如何设计一个连接池","uri":"/ooooo-notes/%E5%A6%82%E4%BD%95%E8%AE%BE%E8%AE%A1%E4%B8%80%E4%B8%AA%E8%BF%9E%E6%8E%A5%E6%B1%A0/"},{"categories":["微信文章"],"content":"如何设计一个对象池 ? ","date":"2022-09-12 10:20:00","objectID":"/ooooo-notes/%E5%A6%82%E4%BD%95%E8%AE%BE%E8%AE%A1%E4%B8%80%E4%B8%AA%E5%AF%B9%E8%B1%A1%E6%B1%A0/:0:0","tags":["java"],"title":"如何设计一个对象池 ?","uri":"/ooooo-notes/%E5%A6%82%E4%BD%95%E8%AE%BE%E8%AE%A1%E4%B8%80%E4%B8%AA%E5%AF%B9%E8%B1%A1%E6%B1%A0/"},{"categories":["微信文章"],"content":"1. 需求 ","date":"2022-09-12 10:20:00","objectID":"/ooooo-notes/%E5%A6%82%E4%BD%95%E8%AE%BE%E8%AE%A1%E4%B8%80%E4%B8%AA%E5%AF%B9%E8%B1%A1%E6%B1%A0/:1:0","tags":["java"],"title":"如何设计一个对象池 ?","uri":"/ooooo-notes/%E5%A6%82%E4%BD%95%E8%AE%BE%E8%AE%A1%E4%B8%80%E4%B8%AA%E5%AF%B9%E8%B1%A1%E6%B1%A0/"},{"categories":["微信文章"],"content":"2. 简单的实现 ","date":"2022-09-12 10:20:00","objectID":"/ooooo-notes/%E5%A6%82%E4%BD%95%E8%AE%BE%E8%AE%A1%E4%B8%80%E4%B8%AA%E5%AF%B9%E8%B1%A1%E6%B1%A0/:2:0","tags":["java"],"title":"如何设计一个对象池 ?","uri":"/ooooo-notes/%E5%A6%82%E4%BD%95%E8%AE%BE%E8%AE%A1%E4%B8%80%E4%B8%AA%E5%AF%B9%E8%B1%A1%E6%B1%A0/"},{"categories":["微信文章"],"content":"3. 实现垃圾回收 软引用，弱引用 ","date":"2022-09-12 10:20:00","objectID":"/ooooo-notes/%E5%A6%82%E4%BD%95%E8%AE%BE%E8%AE%A1%E4%B8%80%E4%B8%AA%E5%AF%B9%E8%B1%A1%E6%B1%A0/:3:0","tags":["java"],"title":"如何设计一个对象池 ?","uri":"/ooooo-notes/%E5%A6%82%E4%BD%95%E8%AE%BE%E8%AE%A1%E4%B8%80%E4%B8%AA%E5%AF%B9%E8%B1%A1%E6%B1%A0/"},{"categories":["微信文章"],"content":"在 idea 中实际有一个非常有用的功能，那就是远端构建和远端运行。 在我们实际开发项目中，自己的本地环境和服务器环境不太一样，例如 go 开发中的 build-tags, 还有 c/c++ 开发中的API 调用不一样，无法模拟相同的开发环境。 ","date":"2022-09-12 08:14:00","objectID":"/ooooo-notes/%E5%9C%A8-idea-%E4%B8%AD%E4%BD%BF%E7%94%A8%E8%BF%9C%E7%AB%AF-build-%E5%92%8C-run/:0:0","tags":["IDE"],"title":"在 idea 中使用远端 build 和 run","uri":"/ooooo-notes/%E5%9C%A8-idea-%E4%B8%AD%E4%BD%BF%E7%94%A8%E8%BF%9C%E7%AB%AF-build-%E5%92%8C-run/"},{"categories":["微信文章"],"content":"1. 问题说明 这里拿 运行 kubernetes 来说明这个问题。 我们都知道在 windows 系统 和 mac 系统中，你是很难在本地运行 kubernetes 的，因为需要涉及到很多的组件，很多功能也只有在 linux 系统中才会开启。 一般来说，任何程序都应该是有远程调试这个功能的，但是这个功能在本地学习源码时会有点不方便，主要体现在自己在本地改了源码，需要同步到你远端的机器上，然后 build 和 run， 整套过程不是自动的。 ","date":"2022-09-12 08:14:00","objectID":"/ooooo-notes/%E5%9C%A8-idea-%E4%B8%AD%E4%BD%BF%E7%94%A8%E8%BF%9C%E7%AB%AF-build-%E5%92%8C-run/:1:0","tags":["IDE"],"title":"在 idea 中使用远端 build 和 run","uri":"/ooooo-notes/%E5%9C%A8-idea-%E4%B8%AD%E4%BD%BF%E7%94%A8%E8%BF%9C%E7%AB%AF-build-%E5%92%8C-run/"},{"categories":["微信文章"],"content":"2. 使用教程 选择在远端机器上运行。 选择在远端机器上构建和编译完成后运行。 指定程序参数。 如下图： 在远端构建和运行 新建一个target配置。 新建或选择一个ssh配置, 对于 rsync 可以不用开启。 设置远端go的执行路径，这个必须要指定，如果 GOPATH 不指定，则使用远端默认配置*。 建议指定一个具体的路径，否则，每次都会生成新的目录，构建比较缓慢。 如下图： 在远端构建和运行 ","date":"2022-09-12 08:14:00","objectID":"/ooooo-notes/%E5%9C%A8-idea-%E4%B8%AD%E4%BD%BF%E7%94%A8%E8%BF%9C%E7%AB%AF-build-%E5%92%8C-run/:2:0","tags":["IDE"],"title":"在 idea 中使用远端 build 和 run","uri":"/ooooo-notes/%E5%9C%A8-idea-%E4%B8%AD%E4%BD%BF%E7%94%A8%E8%BF%9C%E7%AB%AF-build-%E5%92%8C-run/"},{"categories":["微信文章"],"content":"在 spring 中，我们常常会基于现有的代码来扩展之前的功能，或者换一个实现的方式。 在上一篇中，我使用 BeanPostProcessor 来进行扩展。 而在这一篇中，我使用 BeanDefinitionRegistryPostProcessor 来进行扩展。 由于已经实现过一次，我这里就不多说了。 ","date":"2022-09-05 21:09:01","objectID":"/ooooo-notes/%E5%9C%A8-spring-%E4%B8%AD%E5%A6%82%E4%BD%95%E6%89%A9%E5%B1%95%E7%8E%B0%E6%9C%89%E7%B1%BB%E7%9A%84%E5%8A%9F%E8%83%BD2/:0:0","tags":["java","spring","spring-extension"],"title":"在 spring 中如何扩展现有类的功能2 ?","uri":"/ooooo-notes/%E5%9C%A8-spring-%E4%B8%AD%E5%A6%82%E4%BD%95%E6%89%A9%E5%B1%95%E7%8E%B0%E6%9C%89%E7%B1%BB%E7%9A%84%E5%8A%9F%E8%83%BD2/"},{"categories":["微信文章"],"content":"1. 实现思路 判断 beanName 删除原有的 beanDefinition 注册新的 beanDefinition 注意点： 新实现的类，必须要是 CompositePropertySources 的子类，否则注入会有问题 所有方法都必须重新实现一遍，无法复用父类的方法 @Component public class CompositePropertySourcesBeanDefinitionRegistry implements BeanDefinitionRegistryPostProcessor { public static final String COMPOSITE_PROPERTY_SOURCES_BEAN_NAME = \"compositePropertySources\"; @Override public void postProcessBeanDefinitionRegistry(BeanDefinitionRegistry registry) throws BeansException { if (registry.containsBeanDefinition(COMPOSITE_PROPERTY_SOURCES_BEAN_NAME)) { registry.removeBeanDefinition(COMPOSITE_PROPERTY_SOURCES_BEAN_NAME); RootBeanDefinition definition = new RootBeanDefinition(ProxyCompositePropertySources.class); definition.setAutowireMode(AbstractBeanDefinition.AUTOWIRE_CONSTRUCTOR); registry.registerBeanDefinition(COMPOSITE_PROPERTY_SOURCES_BEAN_NAME, definition); } } @Override public void postProcessBeanFactory(ConfigurableListableBeanFactory beanFactory) throws BeansException { } } ","date":"2022-09-05 21:09:01","objectID":"/ooooo-notes/%E5%9C%A8-spring-%E4%B8%AD%E5%A6%82%E4%BD%95%E6%89%A9%E5%B1%95%E7%8E%B0%E6%9C%89%E7%B1%BB%E7%9A%84%E5%8A%9F%E8%83%BD2/:1:0","tags":["java","spring","spring-extension"],"title":"在 spring 中如何扩展现有类的功能2 ?","uri":"/ooooo-notes/%E5%9C%A8-spring-%E4%B8%AD%E5%A6%82%E4%BD%95%E6%89%A9%E5%B1%95%E7%8E%B0%E6%9C%89%E7%B1%BB%E7%9A%84%E5%8A%9F%E8%83%BD2/"},{"categories":["微信文章"],"content":"2. 如何选择 如果是改进之前的功能，就使用第一种方式, BeanPostProcessor 如果是重写之前的功能，就使用第二种方式, BeanDefinitionRegistryPostProcessor 如果不好选择，就啥用第二种方式，也是最强大的。 ","date":"2022-09-05 21:09:01","objectID":"/ooooo-notes/%E5%9C%A8-spring-%E4%B8%AD%E5%A6%82%E4%BD%95%E6%89%A9%E5%B1%95%E7%8E%B0%E6%9C%89%E7%B1%BB%E7%9A%84%E5%8A%9F%E8%83%BD2/:2:0","tags":["java","spring","spring-extension"],"title":"在 spring 中如何扩展现有类的功能2 ?","uri":"/ooooo-notes/%E5%9C%A8-spring-%E4%B8%AD%E5%A6%82%E4%BD%95%E6%89%A9%E5%B1%95%E7%8E%B0%E6%9C%89%E7%B1%BB%E7%9A%84%E5%8A%9F%E8%83%BD2/"},{"categories":["微信文章"],"content":"2. 完整代码实现 github 地址 ","date":"2022-09-05 21:09:01","objectID":"/ooooo-notes/%E5%9C%A8-spring-%E4%B8%AD%E5%A6%82%E4%BD%95%E6%89%A9%E5%B1%95%E7%8E%B0%E6%9C%89%E7%B1%BB%E7%9A%84%E5%8A%9F%E8%83%BD2/:3:0","tags":["java","spring","spring-extension"],"title":"在 spring 中如何扩展现有类的功能2 ?","uri":"/ooooo-notes/%E5%9C%A8-spring-%E4%B8%AD%E5%A6%82%E4%BD%95%E6%89%A9%E5%B1%95%E7%8E%B0%E6%9C%89%E7%B1%BB%E7%9A%84%E5%8A%9F%E8%83%BD2/"},{"categories":["微信文章"],"content":"在 spring 中，我们常常会基于现有的代码来扩展之前的功能，或者换一个实现的方式。 ","date":"2022-09-05 21:09:00","objectID":"/ooooo-notes/%E5%9C%A8-spring-%E4%B8%AD%E5%A6%82%E4%BD%95%E6%89%A9%E5%B1%95%E7%8E%B0%E6%9C%89%E7%B1%BB%E7%9A%84%E5%8A%9F%E8%83%BD/:0:0","tags":["java","spring","spring-extension"],"title":"在 spring 中如何扩展现有类的功能 ?","uri":"/ooooo-notes/%E5%9C%A8-spring-%E4%B8%AD%E5%A6%82%E4%BD%95%E6%89%A9%E5%B1%95%E7%8E%B0%E6%9C%89%E7%B1%BB%E7%9A%84%E5%8A%9F%E8%83%BD/"},{"categories":["微信文章"],"content":"1. 原有的功能 在这里基于之前的功能获取属性来继续深入。 大致代码如下 @Slf4j @Order public class CompositePropertySources implements PropertySources { private final MutablePropertySources mutablePropertySources = new MutablePropertySources(); public CompositePropertySources(List\u003cAbstractSimplePropertySource\u003e sources) { if (sources == null) return; AnnotationAwareOrderComparator.sort(sources); for (AbstractSimplePropertySource source : sources) { mutablePropertySources.addLast(source); } } public boolean containsProperty(String name) { return stream().anyMatch(p -\u003e p.containsProperty(name)); } public String getProperty(String name) { return isBlank(name) ? name : getProperty(name, null); } public String getProperty(String propertyName, String defaultValue) { String value = null; for (PropertySource\u003c?\u003e ps : mutablePropertySources) { value = (String) ps.getProperty(propertyName); if (value != null) { return value; } } return defaultValue; } public Map\u003cString, String\u003e getProperties(String... propertyNames) { if (propertyNames != null) { Map\u003cString, String\u003e map = new HashMap\u003c\u003e(); for (String key : propertyNames) { map.put(key, getProperty(key, null)); } return map; } return null; } } ","date":"2022-09-05 21:09:00","objectID":"/ooooo-notes/%E5%9C%A8-spring-%E4%B8%AD%E5%A6%82%E4%BD%95%E6%89%A9%E5%B1%95%E7%8E%B0%E6%9C%89%E7%B1%BB%E7%9A%84%E5%8A%9F%E8%83%BD/:1:0","tags":["java","spring","spring-extension"],"title":"在 spring 中如何扩展现有类的功能 ?","uri":"/ooooo-notes/%E5%9C%A8-spring-%E4%B8%AD%E5%A6%82%E4%BD%95%E6%89%A9%E5%B1%95%E7%8E%B0%E6%9C%89%E7%B1%BB%E7%9A%84%E5%8A%9F%E8%83%BD/"},{"categories":["微信文章"],"content":"2. 新的需求 由于之前的功能是根据 key 来获取 value的，而现在需要根据业务编号和 key 来获取 value。 先根据 businType.key 来获取 value 如果结果不是 null，则返回 如果结果是 null， 再根据 key 来获取 value 根据上面的描述，也就是优先取业务类型的配置 因为我们的功能实际上在上一篇就已经完成了，所以在这一节中，只需要扩展原有的功能就行了。 这里我使用 BeanPostProcessor 来进行扩展，选择这个类的原因是原有的 bean 已经生成了，无需更改 bean 定义. 实现如下： 判断 bean 是否为 CompositePropertySources 的实例 使用 ProxyCompositePropertySources 对象来代替原有的类 使用 propertyNamesFunction 来分隔 propertyName @Component public class CompositePropertySourcesBeanPostProcessor implements BeanPostProcessor { @Override public Object postProcessAfterInitialization(Object bean, String beanName) throws BeansException { if (bean instanceof CompositePropertySources) { return new ProxyCompositePropertySources((CompositePropertySources) bean); } return bean; } protected static class ProxyCompositePropertySources extends CompositePropertySources { private final CompositePropertySources compositePropertySources; public ProxyCompositePropertySources(CompositePropertySources compositePropertySources) { super(null); this.compositePropertySources = compositePropertySources; } @Override public String getProperty(String propertyName, String defaultValue) { String[] propertyNames = propertyNamesFunction.apply(propertyName); for (String p : propertyNames) { String v = compositePropertySources.getProperty(p); if (v != null) { return v; } } return defaultValue; } @Override public boolean containsProperty(String propertyName) { String[] propertyNames = propertyNamesFunction.apply(propertyName); for (String p : propertyNames) { boolean contains = compositePropertySources.containsProperty(p); if (contains) { return true; } } return false; } } private static final Function\u003cString, String[]\u003e propertyNamesFunction = (propertyName) -\u003e { if (propertyName == null) { return new String[0]; } propertyName = propertyName.trim(); if (propertyName.contains(\".\")) { return new String[]{propertyName, propertyName.substring(propertyName.lastIndexOf(\".\") + 1)}; } return new String[]{propertyName}; }; } ","date":"2022-09-05 21:09:00","objectID":"/ooooo-notes/%E5%9C%A8-spring-%E4%B8%AD%E5%A6%82%E4%BD%95%E6%89%A9%E5%B1%95%E7%8E%B0%E6%9C%89%E7%B1%BB%E7%9A%84%E5%8A%9F%E8%83%BD/:2:0","tags":["java","spring","spring-extension"],"title":"在 spring 中如何扩展现有类的功能 ?","uri":"/ooooo-notes/%E5%9C%A8-spring-%E4%B8%AD%E5%A6%82%E4%BD%95%E6%89%A9%E5%B1%95%E7%8E%B0%E6%9C%89%E7%B1%BB%E7%9A%84%E5%8A%9F%E8%83%BD/"},{"categories":["微信文章"],"content":"3. 完整代码实现 github 地址 ","date":"2022-09-05 21:09:00","objectID":"/ooooo-notes/%E5%9C%A8-spring-%E4%B8%AD%E5%A6%82%E4%BD%95%E6%89%A9%E5%B1%95%E7%8E%B0%E6%9C%89%E7%B1%BB%E7%9A%84%E5%8A%9F%E8%83%BD/:3:0","tags":["java","spring","spring-extension"],"title":"在 spring 中如何扩展现有类的功能 ?","uri":"/ooooo-notes/%E5%9C%A8-spring-%E4%B8%AD%E5%A6%82%E4%BD%95%E6%89%A9%E5%B1%95%E7%8E%B0%E6%9C%89%E7%B1%BB%E7%9A%84%E5%8A%9F%E8%83%BD/"},{"categories":["微信文章"],"content":"1. 需求 希望根据 propertyName 来获取相应的 propertyValue, 这个接口需要支持多种数据来源。 ","date":"2022-09-05 21:09:00","objectID":"/ooooo-notes/%E5%9C%A8-spring-%E4%B8%AD%E5%A6%82%E4%BD%95%E8%AE%BE%E8%AE%A1%E4%B8%80%E4%B8%AA%E8%8E%B7%E5%8F%96%E9%85%8D%E7%BD%AE%E7%9A%84%E6%8E%A5%E5%8F%A3/:1:0","tags":["java","spring","spring-extension"],"title":"在 spring 中如何设计一个获取配置的接口?","uri":"/ooooo-notes/%E5%9C%A8-spring-%E4%B8%AD%E5%A6%82%E4%BD%95%E8%AE%BE%E8%AE%A1%E4%B8%80%E4%B8%AA%E8%8E%B7%E5%8F%96%E9%85%8D%E7%BD%AE%E7%9A%84%E6%8E%A5%E5%8F%A3/"},{"categories":["微信文章"],"content":"2. 设计接口 很明显这个接口应该设计为这样, 有一个方法为 Object getProperty(String name) 来获取属性。 因为是在 spring 中，所以我就直接复用了 org.springframework.core.env.PropertySource, 但这个类需要泛型，所以我就随便实现了一个 Map\u003cString, String\u003e 的泛型，也不会用到这个。 抽象类的设计如下： public abstract class AbstractSimplePropertySource extends PropertySource\u003cMap\u003cString, String\u003e\u003e implements EnvironmentAware { protected Environment environment; public AbstractSimplePropertySource(String name) { super(name, Collections.emptyMap()); } public void setEnvironment(@NonNull Environment environment) { this.environment = environment; } public Environment getEnvironment() { return environment; } } ","date":"2022-09-05 21:09:00","objectID":"/ooooo-notes/%E5%9C%A8-spring-%E4%B8%AD%E5%A6%82%E4%BD%95%E8%AE%BE%E8%AE%A1%E4%B8%80%E4%B8%AA%E8%8E%B7%E5%8F%96%E9%85%8D%E7%BD%AE%E7%9A%84%E6%8E%A5%E5%8F%A3/:2:0","tags":["java","spring","spring-extension"],"title":"在 spring 中如何设计一个获取配置的接口?","uri":"/ooooo-notes/%E5%9C%A8-spring-%E4%B8%AD%E5%A6%82%E4%BD%95%E8%AE%BE%E8%AE%A1%E4%B8%80%E4%B8%AA%E8%8E%B7%E5%8F%96%E9%85%8D%E7%BD%AE%E7%9A%84%E6%8E%A5%E5%8F%A3/"},{"categories":["微信文章"],"content":"3. 具体类的实现 ","date":"2022-09-05 21:09:00","objectID":"/ooooo-notes/%E5%9C%A8-spring-%E4%B8%AD%E5%A6%82%E4%BD%95%E8%AE%BE%E8%AE%A1%E4%B8%80%E4%B8%AA%E8%8E%B7%E5%8F%96%E9%85%8D%E7%BD%AE%E7%9A%84%E6%8E%A5%E5%8F%A3/:3:0","tags":["java","spring","spring-extension"],"title":"在 spring 中如何设计一个获取配置的接口?","uri":"/ooooo-notes/%E5%9C%A8-spring-%E4%B8%AD%E5%A6%82%E4%BD%95%E8%AE%BE%E8%AE%A1%E4%B8%80%E4%B8%AA%E8%8E%B7%E5%8F%96%E9%85%8D%E7%BD%AE%E7%9A%84%E6%8E%A5%E5%8F%A3/"},{"categories":["微信文章"],"content":"3.1 根据请求参数来获取配置 先从请求参数中获取 再从请求头中获取 代码逻辑是比较简单的，我就不解释了。 这种形式，解决了前端可以传入相应的配置，来改变后端的执行逻辑。 @Order(0) public class RequestParamsPropertySource extends AbstractSimplePropertySource { public RequestParamsPropertySource() { super(ENV_PREFIX + \"request_params\"); } @Override public Object getProperty(String name) { String propertyKey = ENV_PREFIX + name.replace(\".\", \"_\"); String propertyValue = null; try { // 先请求参数中获取 ServletRequestAttributes attr = (ServletRequestAttributes) RequestContextHolder.currentRequestAttributes(); HttpServletRequest request = attr.getRequest(); propertyValue = request.getParameter(propertyKey); if (StringUtils.isBlank(propertyValue)) { // 再从请求头中获取 propertyValue = request.getHeader(propertyKey); } } catch (Throwable ignored) { } // 空字符也当做null propertyValue = defaultIfBlank(propertyValue, null); return propertyValue; } } ","date":"2022-09-05 21:09:00","objectID":"/ooooo-notes/%E5%9C%A8-spring-%E4%B8%AD%E5%A6%82%E4%BD%95%E8%AE%BE%E8%AE%A1%E4%B8%80%E4%B8%AA%E8%8E%B7%E5%8F%96%E9%85%8D%E7%BD%AE%E7%9A%84%E6%8E%A5%E5%8F%A3/:3:1","tags":["java","spring","spring-extension"],"title":"在 spring 中如何设计一个获取配置的接口?","uri":"/ooooo-notes/%E5%9C%A8-spring-%E4%B8%AD%E5%A6%82%E4%BD%95%E8%AE%BE%E8%AE%A1%E4%B8%80%E4%B8%AA%E8%8E%B7%E5%8F%96%E9%85%8D%E7%BD%AE%E7%9A%84%E6%8E%A5%E5%8F%A3/"},{"categories":["微信文章"],"content":"3.2 根据环境变量来获取配置 代码逻辑是比较简单的，我就不解释了。 @Order(2) public class EnvironmentPropertySource extends AbstractSimplePropertySource { public EnvironmentPropertySource() { super(ENV_PREFIX + \"environment\"); } @Override public Object getProperty(String name) { String env_property_key = ENV_PREFIX + name.replace(\".\", \"_\"); return System.getenv(env_property_key.toUpperCase()); } } ","date":"2022-09-05 21:09:00","objectID":"/ooooo-notes/%E5%9C%A8-spring-%E4%B8%AD%E5%A6%82%E4%BD%95%E8%AE%BE%E8%AE%A1%E4%B8%80%E4%B8%AA%E8%8E%B7%E5%8F%96%E9%85%8D%E7%BD%AE%E7%9A%84%E6%8E%A5%E5%8F%A3/:3:2","tags":["java","spring","spring-extension"],"title":"在 spring 中如何设计一个获取配置的接口?","uri":"/ooooo-notes/%E5%9C%A8-spring-%E4%B8%AD%E5%A6%82%E4%BD%95%E8%AE%BE%E8%AE%A1%E4%B8%80%E4%B8%AA%E8%8E%B7%E5%8F%96%E9%85%8D%E7%BD%AE%E7%9A%84%E6%8E%A5%E5%8F%A3/"},{"categories":["微信文章"],"content":"3.3 根据本地配置文件来获取参数 在实际开发过程，大家可能是共用一套数据库环境，在这个情况下，如果某一个人改了数据库配置，这样会对其他人造成影响，所以必须要设计出一个用于开发的配置类。 使用 apache 的 configuration 包，来实现配置文件的动态刷新 从 builder.getConfiguration() 对象中获取配置 大致代码如下： @Order(1) public class LocalPropertiesPropertySource extends AbstractSimplePropertySource implements InitializingBean { @Autowired private ApplicationEventPublisher publisher; private ReloadingFileBasedConfigurationBuilder\u003cPropertiesConfiguration\u003e builder; private static final String DEFAULT_LOCAL_PROPERTIES_PATH = \"sysoptions.properties\"; public LocalPropertiesPropertySource() { super(ENV_PREFIX + \"local_properties\"); log.debug(\"开发环境，启用[{}]配置\", getClass()); } @Override public void afterPropertiesSet() { Integer sysOptionsLoadInterval = getEnvironment().getProperty(\"dev.localPropertiesLoadInterval\", Integer.class, 1); String sysOptionsPath = getEnvironment().getProperty(\"dev.localPropertiesPath\", DEFAULT_LOCAL_PROPERTIES_PATH); File propertiesFile = new File(sysOptionsPath); if (!propertiesFile.exists()) { return; } // server boot will publish event publisher.publishEvent(new LocalProperitesReReloadingEvent(new Object(), propertiesFile.getAbsolutePath())); builder = new ReloadingFileBasedConfigurationBuilder\u003c\u003e(PropertiesConfiguration.class).configure(new Parameters().fileBased().setFile(propertiesFile)); ReloadingController reloadingController = builder.getReloadingController(); reloadingController.addEventListener(ReloadingEvent.ANY, e -\u003e publisher.publishEvent(new LocalProperitesReReloadingEvent(e, propertiesFile.getAbsolutePath()))); PeriodicReloadingTrigger trigger = new PeriodicReloadingTrigger(reloadingController, null, sysOptionsLoadInterval, SECONDS); trigger.start(); } @Override public Object getProperty(@NonNull String name) { if (builder == null) return null; Configuration configuration = null; try { configuration = builder.getConfiguration(); String config_value = configuration.getString(name); if (config_value != null) { return config_value; } } catch (ConfigurationException e) { log.error(e.getMessage(), e); } return null; } } ","date":"2022-09-05 21:09:00","objectID":"/ooooo-notes/%E5%9C%A8-spring-%E4%B8%AD%E5%A6%82%E4%BD%95%E8%AE%BE%E8%AE%A1%E4%B8%80%E4%B8%AA%E8%8E%B7%E5%8F%96%E9%85%8D%E7%BD%AE%E7%9A%84%E6%8E%A5%E5%8F%A3/:3:3","tags":["java","spring","spring-extension"],"title":"在 spring 中如何设计一个获取配置的接口?","uri":"/ooooo-notes/%E5%9C%A8-spring-%E4%B8%AD%E5%A6%82%E4%BD%95%E8%AE%BE%E8%AE%A1%E4%B8%80%E4%B8%AA%E8%8E%B7%E5%8F%96%E9%85%8D%E7%BD%AE%E7%9A%84%E6%8E%A5%E5%8F%A3/"},{"categories":["微信文章"],"content":"3.4 其他的扩展 到这里，你就应该很清楚的知道怎么去扩展其他的类了，比如数据库的实现， redis 的实现 ","date":"2022-09-05 21:09:00","objectID":"/ooooo-notes/%E5%9C%A8-spring-%E4%B8%AD%E5%A6%82%E4%BD%95%E8%AE%BE%E8%AE%A1%E4%B8%80%E4%B8%AA%E8%8E%B7%E5%8F%96%E9%85%8D%E7%BD%AE%E7%9A%84%E6%8E%A5%E5%8F%A3/:3:4","tags":["java","spring","spring-extension"],"title":"在 spring 中如何设计一个获取配置的接口?","uri":"/ooooo-notes/%E5%9C%A8-spring-%E4%B8%AD%E5%A6%82%E4%BD%95%E8%AE%BE%E8%AE%A1%E4%B8%80%E4%B8%AA%E8%8E%B7%E5%8F%96%E9%85%8D%E7%BD%AE%E7%9A%84%E6%8E%A5%E5%8F%A3/"},{"categories":["微信文章"],"content":"4. 使用的入口 上面只是定义了一个接口和几个实现类，统一的入口类，实际上还没有。 使用构造函数的方式来注入所有的 AbstractSimplePropertySource 。 对于每个获取配置的类，肯定有优先级，所以要排序。 使用 MutablePropertySources 这个类做辅助。 大致代码如下： @Slf4j @Order public class CompositePropertySources implements PropertySources { private final MutablePropertySources mutablePropertySources = new MutablePropertySources(); public CompositePropertySources(List\u003cAbstractSimplePropertySource\u003e sources) { if (sources == null) return; AnnotationAwareOrderComparator.sort(sources); for (AbstractSimplePropertySource source : sources) { mutablePropertySources.addLast(source); } } public boolean containsProperty(String name) { return stream().anyMatch(p -\u003e p.containsProperty(name)); } public String getProperty(String name) { return isBlank(name) ? name : getProperty(name, null); } public String getProperty(String propertyName, String defaultValue) { String value = null; for (PropertySource\u003c?\u003e ps : mutablePropertySources) { value = (String) ps.getProperty(propertyName); if (value != null) { return value; } } return defaultValue; } public Map\u003cString, String\u003e getProperties(String... propertyNames) { if (propertyNames != null) { Map\u003cString, String\u003e map = new HashMap\u003c\u003e(); for (String key : propertyNames) { map.put(key, getProperty(key, null)); } return map; } return null; } } ","date":"2022-09-05 21:09:00","objectID":"/ooooo-notes/%E5%9C%A8-spring-%E4%B8%AD%E5%A6%82%E4%BD%95%E8%AE%BE%E8%AE%A1%E4%B8%80%E4%B8%AA%E8%8E%B7%E5%8F%96%E9%85%8D%E7%BD%AE%E7%9A%84%E6%8E%A5%E5%8F%A3/:4:0","tags":["java","spring","spring-extension"],"title":"在 spring 中如何设计一个获取配置的接口?","uri":"/ooooo-notes/%E5%9C%A8-spring-%E4%B8%AD%E5%A6%82%E4%BD%95%E8%AE%BE%E8%AE%A1%E4%B8%80%E4%B8%AA%E8%8E%B7%E5%8F%96%E9%85%8D%E7%BD%AE%E7%9A%84%E6%8E%A5%E5%8F%A3/"},{"categories":["微信文章"],"content":"5. 完整代码实现 github 地址 ","date":"2022-09-05 21:09:00","objectID":"/ooooo-notes/%E5%9C%A8-spring-%E4%B8%AD%E5%A6%82%E4%BD%95%E8%AE%BE%E8%AE%A1%E4%B8%80%E4%B8%AA%E8%8E%B7%E5%8F%96%E9%85%8D%E7%BD%AE%E7%9A%84%E6%8E%A5%E5%8F%A3/:5:0","tags":["java","spring","spring-extension"],"title":"在 spring 中如何设计一个获取配置的接口?","uri":"/ooooo-notes/%E5%9C%A8-spring-%E4%B8%AD%E5%A6%82%E4%BD%95%E8%AE%BE%E8%AE%A1%E4%B8%80%E4%B8%AA%E8%8E%B7%E5%8F%96%E9%85%8D%E7%BD%AE%E7%9A%84%E6%8E%A5%E5%8F%A3/"},{"categories":["微信文章"],"content":"作为一个 Java 开发，Spring 的技术可以说是必须要掌握的，不仅仅是会使用，而且要掌握原理，学会扩展。 今天我就说说，哪些核心类和扩展类是必须要掌握的，同时我也说明这些扩展可以干什么，后面 Spring 文章，我会用到这些扩展类，让你学懂这些类。 ","date":"2022-09-05 18:00:00","objectID":"/ooooo-notes/%E5%9C%A8-spring-%E4%B8%AD%E6%9C%89%E5%93%AA%E4%BA%9B%E6%A0%B8%E5%BF%83%E7%B1%BB%E5%92%8C%E6%89%A9%E5%B1%95%E7%B1%BB/:0:0","tags":["java","spring"],"title":"在 spring 中有哪些核心类和扩展类?","uri":"/ooooo-notes/%E5%9C%A8-spring-%E4%B8%AD%E6%9C%89%E5%93%AA%E4%BA%9B%E6%A0%B8%E5%BF%83%E7%B1%BB%E5%92%8C%E6%89%A9%E5%B1%95%E7%B1%BB/"},{"categories":["微信文章"],"content":"核心类： IOC容器: org.springframework.context.ApplicationContext 配置类: org.springframework.core.env.Environment Bean工厂：org.springframework.beans.factory.BeanFactory 事件发布器： org.springframework.context.ApplicationEventPublisher 资源加载器： org.springframework.core.io.ResourceLoader 上面这几个类，是我们经常会用到的。它们都有相应的 Aware 接口, 如 org.springframework.context.ApplicationContextAware, 可以设置 applicationContext 对象到我们自己定义的 bean 对象中. 注意这样 setApplicationContext 的方式比 @Autowired 注解注入 applicationContext 的方式的时机要早很多，所以一般推荐用 setApplicationContext 的方式。 相应的源码 ApplicationContextAwareProcessor ","date":"2022-09-05 18:00:00","objectID":"/ooooo-notes/%E5%9C%A8-spring-%E4%B8%AD%E6%9C%89%E5%93%AA%E4%BA%9B%E6%A0%B8%E5%BF%83%E7%B1%BB%E5%92%8C%E6%89%A9%E5%B1%95%E7%B1%BB/:1:0","tags":["java","spring"],"title":"在 spring 中有哪些核心类和扩展类?","uri":"/ooooo-notes/%E5%9C%A8-spring-%E4%B8%AD%E6%9C%89%E5%93%AA%E4%BA%9B%E6%A0%B8%E5%BF%83%E7%B1%BB%E5%92%8C%E6%89%A9%E5%B1%95%E7%B1%BB/"},{"categories":["微信文章"],"content":"扩展类 beanFactory的后置处理器： org.springframework.beans.factory.config.BeanFactoryPostProcessor bean的后置处理器： org.springframework.beans.factory.config.BeanPostProcessor 上面的两个类非常重要，如果你现在还不会熟练使用它们，说明 spring 掌握的很一般。 ","date":"2022-09-05 18:00:00","objectID":"/ooooo-notes/%E5%9C%A8-spring-%E4%B8%AD%E6%9C%89%E5%93%AA%E4%BA%9B%E6%A0%B8%E5%BF%83%E7%B1%BB%E5%92%8C%E6%89%A9%E5%B1%95%E7%B1%BB/:2:0","tags":["java","spring"],"title":"在 spring 中有哪些核心类和扩展类?","uri":"/ooooo-notes/%E5%9C%A8-spring-%E4%B8%AD%E6%9C%89%E5%93%AA%E4%BA%9B%E6%A0%B8%E5%BF%83%E7%B1%BB%E5%92%8C%E6%89%A9%E5%B1%95%E7%B1%BB/"},{"categories":["微信文章"],"content":"非常有用的类 代理工厂： org.springframework.aop.framework.ProxyFactory bean工厂： org.springframework.beans.factory.ObjectFactory 属性绑定： org.springframework.boot.context.properties.bind.Binder 选择性导入bean: org.springframework.context.annotation.ImportSelector 上面这几个类，一般在扩展功能时，都会用到。 ","date":"2022-09-05 18:00:00","objectID":"/ooooo-notes/%E5%9C%A8-spring-%E4%B8%AD%E6%9C%89%E5%93%AA%E4%BA%9B%E6%A0%B8%E5%BF%83%E7%B1%BB%E5%92%8C%E6%89%A9%E5%B1%95%E7%B1%BB/:3:0","tags":["java","spring"],"title":"在 spring 中有哪些核心类和扩展类?","uri":"/ooooo-notes/%E5%9C%A8-spring-%E4%B8%AD%E6%9C%89%E5%93%AA%E4%BA%9B%E6%A0%B8%E5%BF%83%E7%B1%BB%E5%92%8C%E6%89%A9%E5%B1%95%E7%B1%BB/"},{"categories":["随笔"],"content":"1. 下载代码 git clone git@github.com:apache/tomcat.git ","date":"2022-08-10 08:00:00","objectID":"/ooooo-notes/%E6%90%AD%E5%BB%BA-tomcat-%E6%BA%90%E7%A0%81%E8%B0%83%E8%AF%95%E7%8E%AF%E5%A2%83/:1:0","tags":["tomcat","source code"],"title":"搭建 tomcat 源码调试环境","uri":"/ooooo-notes/%E6%90%AD%E5%BB%BA-tomcat-%E6%BA%90%E7%A0%81%E8%B0%83%E8%AF%95%E7%8E%AF%E5%A2%83/"},{"categories":["随笔"],"content":"2. 安装ant 我本地安装的是 1.10.12 版本, ant 下载地址 配置环境变量 ANT_HOME, 加入到 PATH 环境变量中 执行命令验证 ant -version ","date":"2022-08-10 08:00:00","objectID":"/ooooo-notes/%E6%90%AD%E5%BB%BA-tomcat-%E6%BA%90%E7%A0%81%E8%B0%83%E8%AF%95%E7%8E%AF%E5%A2%83/:2:0","tags":["tomcat","source code"],"title":"搭建 tomcat 源码调试环境","uri":"/ooooo-notes/%E6%90%AD%E5%BB%BA-tomcat-%E6%BA%90%E7%A0%81%E8%B0%83%E8%AF%95%E7%8E%AF%E5%A2%83/"},{"categories":["随笔"],"content":"3. 导入到 idea 中 # 进入 tomcat 根目前 cd tomcat # 复制配置文件 build.properties cp build.properties.default build.properties # 更改 build.properties 中的配置 base.path=第三方jar的下载目录 # 设置 idea ant ide-intellij # 执行编译命令, 会生成 output 目录 ant deploy 然后用 idea 打开项目，idea 会弹出让你配置下面的变量 ANT_HOME = ${ant.home} TOMCAT_BUILD_LIBS = ${base.path} ","date":"2022-08-10 08:00:00","objectID":"/ooooo-notes/%E6%90%AD%E5%BB%BA-tomcat-%E6%BA%90%E7%A0%81%E8%B0%83%E8%AF%95%E7%8E%AF%E5%A2%83/:3:0","tags":["tomcat","source code"],"title":"搭建 tomcat 源码调试环境","uri":"/ooooo-notes/%E6%90%AD%E5%BB%BA-tomcat-%E6%BA%90%E7%A0%81%E8%B0%83%E8%AF%95%E7%8E%AF%E5%A2%83/"},{"categories":["随笔"],"content":"4. idea 中 配置 检查你的项目依赖有没有问题 项目依赖配置 上面的三个依赖，其实就是 ServletContainerInitializer 的实现, 比如 res/META-INF/jasper.jar/services/jakarta.servlet.ServletContainerInitializer. 更改配置文件 conf/server.xml # 改为编译输出目录 appBase=\"output/build/webapps\" 运行程序 org.apache.catalina.startup.Bootstrap#main ","date":"2022-08-10 08:00:00","objectID":"/ooooo-notes/%E6%90%AD%E5%BB%BA-tomcat-%E6%BA%90%E7%A0%81%E8%B0%83%E8%AF%95%E7%8E%AF%E5%A2%83/:4:0","tags":["tomcat","source code"],"title":"搭建 tomcat 源码调试环境","uri":"/ooooo-notes/%E6%90%AD%E5%BB%BA-tomcat-%E6%BA%90%E7%A0%81%E8%B0%83%E8%AF%95%E7%8E%AF%E5%A2%83/"},{"categories":["随笔"],"content":" deployment 资源是我们经常需要使用的，也是我们最应该熟悉的源码. 对于调试源码，我使用是 deployment_controller_test.go 测试类， TestSyncDeploymentCreatesReplicaSet 方法. ","date":"2022-07-15 08:00:00","objectID":"/ooooo-notes/%E8%B0%83%E8%AF%95-deployment-controller-%E7%9A%84%E6%BA%90%E7%A0%81/:0:0","tags":["k8s","cloud native","source code"],"title":"调试 deployment-controller 的源码","uri":"/ooooo-notes/%E8%B0%83%E8%AF%95-deployment-controller-%E7%9A%84%E6%BA%90%E7%A0%81/"},{"categories":["随笔"],"content":"TestSyncDeploymentCreatesReplicaSet 测试方法的结构 源码路径：kubernetes\\pkg\\controller\\deployment\\deployment_controller_test.go 测试配置对象 f := newFixture(t) 创建一个 fixture 对象， 里面有 objects 属性，这个用来模拟 clientSet, 也就是请求 etcd 的接口，后面将会详细描述。 创建一个 Deployment 对象， 标签为 “foo”: “bar” d := newDeployment(\"foo\", 1, nil, nil, nil, map[string]string{\"foo\": \"bar\"}) 添加缓存对象，用于后续的List接口 f.dLister = append(f.dLister, d) f.objects = append(f.objects, d) 创建一个 ReplicaSet 对象 rs := newReplicaSet(d, \"deploymentrs-4186632231\", 1) 希望的测试结果 f.expectCreateRSAction(rs) f.expectUpdateDeploymentStatusAction(d) f.expectUpdateDeploymentStatusAction(d) 从上面的语句就可以发现，kubernetes 的测试类，意图非常明确。 也就是说 我创建一个 Deployment ** 对象，肯定会产生一个 ReplicaSet 对象，并且 DeploymentStatus 会被更新两次, 接下来，我们来看看是kubernetes如何做到的。 ","date":"2022-07-15 08:00:00","objectID":"/ooooo-notes/%E8%B0%83%E8%AF%95-deployment-controller-%E7%9A%84%E6%BA%90%E7%A0%81/:1:0","tags":["k8s","cloud native","source code"],"title":"调试 deployment-controller 的源码","uri":"/ooooo-notes/%E8%B0%83%E8%AF%95-deployment-controller-%E7%9A%84%E6%BA%90%E7%A0%81/"},{"categories":["随笔"],"content":"测试方法的执行 f.run(testutil.GetKey(d, t)) 获取 Deployment 的 key 属性 代码 testutil.GetKey(d, t) func GetKey(obj interface{}, t *testing.T) string { // 每个删除的对象都是这个类型, 这里取出了真实的对象 tombstone, ok := obj.(cache.DeletedFinalStateUnknown) if ok { // if tombstone , try getting the value from tombstone.Obj obj = tombstone.Obj } // 取出指针类型中 value，获取 Name 属性 val := reflect.ValueOf(obj).Elem() name := val.FieldByName(\"Name\").String() if len(name) == 0 { t.Errorf(\"Unexpected object %v\", obj) } // 获取key, 结果就是 {namespace}/{name} key, err := keyFunc(obj) if err != nil { t.Errorf(\"Unexpected error getting key for %T %v: %v\", val.Interface(), name, err) return \"\" } return key } 调用 fixture.run_() 方法 这个函数有三个入参：run_(deploymentName string, startInformers bool, expectError bool). 当前的测试方法 TestSyncDeploymentCreatesReplicaSet 的 startInformers 参数为 false, 表示不启动 Informer, 后续会用另外一个测试类来说明 Informer 的启动过程. 接下来详细看看 fixture.run_() 方法都干了啥。 创建一个 controller c, informers, err := f.newController() func (f *fixture) newController() (*DeploymentController, informers.SharedInformerFactory, error) { // 这个也就是之前说的，objects 会用来构建 模拟的 clientSet f.client = fake.NewSimpleClientset(f.objects...) // 创建了 informer 和 deploymentController informers := informers.NewSharedInformerFactory(f.client, controller.NoResyncPeriodFunc()) c, err := NewDeploymentController(informers.Apps().V1().Deployments(), informers.Apps().V1().ReplicaSets(), informers.Core().V1().Pods(), f.client) if err != nil { return nil, nil, err } // 模拟一个 recorder c.eventRecorder = \u0026record.FakeRecorder{} // 所有状态默认为 synced c.dListerSynced = alwaysReady c.rsListerSynced = alwaysReady c.podListerSynced = alwaysReady // 下面这个代码很关键 // 先前在 fixture 对象中加入了相应的 Lister，在这里遍历这些 Lister, 就是为了模拟 Informer 的本地缓存 // kube_controller_manager 程序启动之后，会请求 kube_apiserver 来获取相应的资源，从而更新到自己的缓存中 for _, d := range f.dLister { informers.Apps().V1().Deployments().Informer().GetIndexer().Add(d) } for _, rs := range f.rsLister { informers.Apps().V1().ReplicaSets().Informer().GetIndexer().Add(rs) } for _, pod := range f.podLister { informers.Core().V1().Pods().Informer().GetIndexer().Add(pod) } return c, informers, nil } 详细分析是怎么模拟 clientSet 的？ f.client = fake.NewSimpleClientset(f.objects...) func NewSimpleClientset(objects ...runtime.Object) *Clientset { // 这个是模拟的最终实现对象，所有操作都是依赖它来完成的 o := testing.NewObjectTracker(scheme, codecs.UniversalDecoder()) // 遍历对象，依次添加 for _, obj := range objects { if err := o.Add(obj); err != nil { panic(err) } } // 创建一个 clientSet cs := \u0026Clientset{tracker: o} // 下面三个都是依赖 tracker 来实现的， 通过不同的 Action, 比如 ListActionImpl、GetActionImpl 等 cs.discovery = \u0026fakediscovery.FakeDiscovery{Fake: \u0026cs.Fake} cs.AddReactor(\"*\", \"*\", testing.ObjectReaction(o)) cs.AddWatchReactor(\"*\", func(action testing.Action) (handled bool, ret watch.Interface, err error) { gvr := action.GetResource() ns := action.GetNamespace() watch, err := o.Watch(gvr, ns) if err != nil { return false, nil, err } return true, watch, nil }) return cs } ObjectTracker 如果添加对象的？ testing.NewObjectTracker(scheme, codecs.UniversalDecoder()) func (t *tracker) Add(obj runtime.Object) error { // 添加 List if meta.IsListType(obj) { return t.addList(obj, false) } // 用来获取 namespace objMeta, err := meta.Accessor(obj) // 获取 gvk gvks, _, err := t.scheme.ObjectKinds(obj) for _, gvk := range gvks { // NOTE: UnsafeGuessKindToResource is a heuristic and default match. The // actual registration in apiserver can specify arbitrary route for a // gvk. If a test uses such objects, it cannot preset the tracker with // objects via Add(). Instead, it should trigger the Create() function // of the tracker, where an arbitrary gvr can be specified. gvr, _ := meta.UnsafeGuessKindToResource(gvk) // Resource doesn't have the concept of \"__internal\" version, just set it to \"\". if gvr.Version == runtime.APIVersionInternal { gvr.Version = \"\" } // 添加这个 err := t.add(gvr, obj, objMeta.GetNamespace(), false) if err != nil { return err } } return nil } func (t *tracker) add(gvr schema.GroupVersionResource, obj runtime.Object, ns st","date":"2022-07-15 08:00:00","objectID":"/ooooo-notes/%E8%B0%83%E8%AF%95-deployment-controller-%E7%9A%84%E6%BA%90%E7%A0%81/:2:0","tags":["k8s","cloud native","source code"],"title":"调试 deployment-controller 的源码","uri":"/ooooo-notes/%E8%B0%83%E8%AF%95-deployment-controller-%E7%9A%84%E6%BA%90%E7%A0%81/"},{"categories":["随笔"],"content":"1. GVK 定义 GVK(group version kind): 资源组、资源版本、资源类型 表示 apps 组下 v1 版本 Deployment 类型的资源。 apiVersion: apps/v1 kind: Deployment 表示 core 组下 v1 版本 Pod 类型的资源。(没有组信息表示核心组) apiVersion: v1 kind: Pod ","date":"2022-07-02 08:00:00","objectID":"/ooooo-notes/%E5%AD%A6%E4%B9%A0-k8s-%E6%BA%90%E7%A0%81%E7%9A%84%E5%89%8D%E7%BD%AE%E7%9F%A5%E8%AF%86/:1:0","tags":["k8s","cloud native","source code"],"title":"学习 k8s 源码的前置知识","uri":"/ooooo-notes/%E5%AD%A6%E4%B9%A0-k8s-%E6%BA%90%E7%A0%81%E7%9A%84%E5%89%8D%E7%BD%AE%E7%9F%A5%E8%AF%86/"},{"categories":["随笔"],"content":"2. kubernetes 对象结构 每个对象都可以分为四个部分。 例如 Deployment 资源： TypeMeta: GVK 信息 ObjectMeta: 对象元数据，比如有属性 name、namespace DeploymentSpec: 对象定义规范，比如有属性 replicas(控制副本数量)、template(定义 Pod 的模板)、selector(标签选择器，与 Pod 标签一样)、strategy(Pod 升级策略) DeploymentStatus: 对象运行时状态， 比如有属性 replicas(总的副本数) 代码路径：kubernetes\\vendor\\k8s.io\\api\\apps\\v1\\types.go type Deployment struct { metav1.TypeMeta `json:\",inline\"` // Standard object's metadata. // More info: https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata // +optional metav1.ObjectMeta `json:\"metadata,omitempty\" protobuf:\"bytes,1,opt,name=metadata\"` // Specification of the desired behavior of the Deployment. // +optional Spec DeploymentSpec `json:\"spec,omitempty\" protobuf:\"bytes,2,opt,name=spec\"` // Most recently observed status of the Deployment. // +optional Status DeploymentStatus `json:\"status,omitempty\" protobuf:\"bytes,3,opt,name=status\"` } ","date":"2022-07-02 08:00:00","objectID":"/ooooo-notes/%E5%AD%A6%E4%B9%A0-k8s-%E6%BA%90%E7%A0%81%E7%9A%84%E5%89%8D%E7%BD%AE%E7%9F%A5%E8%AF%86/:2:0","tags":["k8s","cloud native","source code"],"title":"学习 k8s 源码的前置知识","uri":"/ooooo-notes/%E5%AD%A6%E4%B9%A0-k8s-%E6%BA%90%E7%A0%81%E7%9A%84%E5%89%8D%E7%BD%AE%E7%9F%A5%E8%AF%86/"},{"categories":["随笔"],"content":"3. kubernetes 源码目录结构 cmd: 可执行程序包， 例如 kubelet 的入口为 kubernetes\\cmd\\kubelet\\kubelet.go pkg: kubernetes 包路径, 有些子目录与 cmd 目录一样，就是入口文件依赖的包 vendor: 第三方包，其中也有 kubernetes 的包 plugin: 准入插件和认证插件 hack: 脚本路径，非常有用 api: OpenAPI 定义 上述,只是简单的描述了，目前只需要知道 cmd, pkg, vendor 是非常重要的，也是我们经常看的。 ","date":"2022-07-02 08:00:00","objectID":"/ooooo-notes/%E5%AD%A6%E4%B9%A0-k8s-%E6%BA%90%E7%A0%81%E7%9A%84%E5%89%8D%E7%BD%AE%E7%9F%A5%E8%AF%86/:3:0","tags":["k8s","cloud native","source code"],"title":"学习 k8s 源码的前置知识","uri":"/ooooo-notes/%E5%AD%A6%E4%B9%A0-k8s-%E6%BA%90%E7%A0%81%E7%9A%84%E5%89%8D%E7%BD%AE%E7%9F%A5%E8%AF%86/"},{"categories":["随笔"],"content":"4. 如何去阅读源码，真的需要把整个项目都运行起来吗？ 我个人认为是完全不需要。 一般来说看源码，只需要了解主线代码，知道哪些类是怎么配合的，一起完成了什么样的功能，即使你把整个程序都运行起来了，有些分支条件的代码，需要特殊的输入数据，在你不熟悉代码的情况下，你也很难去模拟，这时候我们只能看代码的测试类 ，来了解代码是怎样处理这个特殊数据的。 特意说明一下： 以后的阅读代码的部分，我基本以测试类来带领大家阅读。 ","date":"2022-07-02 08:00:00","objectID":"/ooooo-notes/%E5%AD%A6%E4%B9%A0-k8s-%E6%BA%90%E7%A0%81%E7%9A%84%E5%89%8D%E7%BD%AE%E7%9F%A5%E8%AF%86/:4:0","tags":["k8s","cloud native","source code"],"title":"学习 k8s 源码的前置知识","uri":"/ooooo-notes/%E5%AD%A6%E4%B9%A0-k8s-%E6%BA%90%E7%A0%81%E7%9A%84%E5%89%8D%E7%BD%AE%E7%9F%A5%E8%AF%86/"},{"categories":["随笔"],"content":"1. 方法1，在本机的 IDE 来调试源码 如果你是 linux 系统，可以在 linux 中搭建一个 kubernetes 单机的集群，在此系统中安装 IDE(Goland) 来调试. 具体步骤如下： ","date":"2022-06-28 08:00:00","objectID":"/ooooo-notes/%E6%90%AD%E5%BB%BA-k8s-1.24-%E7%89%88%E6%9C%AC%E7%9A%84%E6%BA%90%E7%A0%81%E8%B0%83%E8%AF%95%E7%8E%AF%E5%A2%83/:1:0","tags":["k8s","cloud native","source code"],"title":"搭建 k8s 1.24 版本的源码调试环境","uri":"/ooooo-notes/%E6%90%AD%E5%BB%BA-k8s-1.24-%E7%89%88%E6%9C%AC%E7%9A%84%E6%BA%90%E7%A0%81%E8%B0%83%E8%AF%95%E7%8E%AF%E5%A2%83/"},{"categories":["随笔"],"content":"1. 下载源码 (go 的版本要求 1.18.x) git clone git@github.com:kubernetes/kubernetes.git cd kubernetes git checkout -b origin/release-1.24 go mod download ","date":"2022-06-28 08:00:00","objectID":"/ooooo-notes/%E6%90%AD%E5%BB%BA-k8s-1.24-%E7%89%88%E6%9C%AC%E7%9A%84%E6%BA%90%E7%A0%81%E8%B0%83%E8%AF%95%E7%8E%AF%E5%A2%83/:1:1","tags":["k8s","cloud native","source code"],"title":"搭建 k8s 1.24 版本的源码调试环境","uri":"/ooooo-notes/%E6%90%AD%E5%BB%BA-k8s-1.24-%E7%89%88%E6%9C%AC%E7%9A%84%E6%BA%90%E7%A0%81%E8%B0%83%E8%AF%95%E7%8E%AF%E5%A2%83/"},{"categories":["随笔"],"content":"2. 用 IDE 打开 kubernetes 源码 ","date":"2022-06-28 08:00:00","objectID":"/ooooo-notes/%E6%90%AD%E5%BB%BA-k8s-1.24-%E7%89%88%E6%9C%AC%E7%9A%84%E6%BA%90%E7%A0%81%E8%B0%83%E8%AF%95%E7%8E%AF%E5%A2%83/:1:2","tags":["k8s","cloud native","source code"],"title":"搭建 k8s 1.24 版本的源码调试环境","uri":"/ooooo-notes/%E6%90%AD%E5%BB%BA-k8s-1.24-%E7%89%88%E6%9C%AC%E7%9A%84%E6%BA%90%E7%A0%81%E8%B0%83%E8%AF%95%E7%8E%AF%E5%A2%83/"},{"categories":["随笔"],"content":"3. 找到服务的启动参数（比如 kube-controller-manager） # 执行命令 ps aux | grep kube-controller-manager | grep -v grep # 命令执行之后，输出如下, kube-controller-manager 后面的就是程序的参数 root 1584 4.0 0.8 820020 110072 ? Ssl 23:28 0:02 kube-controller-manager --allocate-node-cidrs=true --authentication-kubeconfig=/etc/kubernetes/controller-manager.conf --authorization-kubeconfig=/etc/kubernetes/controller-manager.conf --bind-address=127.0.0.1 --client-ca-file=/etc/kubernetes/pki/ca.crt --cluster-cidr=10.244.0.0/16 --cluster-name=kubernetes --cluster-signing-cert-file=/etc/kubernetes/pki/ca.crt --cluster-signing-key-file=/etc/kubernetes/pki/ca.key --controllers=*,bootstrapsigner,tokencleaner --kubeconfig=/etc/kubernetes/controller-manager.conf --leader-elect=true --requestheader-client-ca-file=/etc/kubernetes/pki/front-proxy-ca.crt --root-ca-file=/etc/kubernetes/pki/ca.crt --service-account-private-key-file=/etc/kubernetes/pki/sa.key --service-cluster-ip-range=10.96.0.0/12 --use-service-account-credentials=true ","date":"2022-06-28 08:00:00","objectID":"/ooooo-notes/%E6%90%AD%E5%BB%BA-k8s-1.24-%E7%89%88%E6%9C%AC%E7%9A%84%E6%BA%90%E7%A0%81%E8%B0%83%E8%AF%95%E7%8E%AF%E5%A2%83/:1:3","tags":["k8s","cloud native","source code"],"title":"搭建 k8s 1.24 版本的源码调试环境","uri":"/ooooo-notes/%E6%90%AD%E5%BB%BA-k8s-1.24-%E7%89%88%E6%9C%AC%E7%9A%84%E6%BA%90%E7%A0%81%E8%B0%83%E8%AF%95%E7%8E%AF%E5%A2%83/"},{"categories":["随笔"],"content":"4. 移动 kubernetes 的静态 pod （比如 kube-controller-manager） cd /etc/kubernetes mv manifests/kube-controller-manager.yaml ./ ","date":"2022-06-28 08:00:00","objectID":"/ooooo-notes/%E6%90%AD%E5%BB%BA-k8s-1.24-%E7%89%88%E6%9C%AC%E7%9A%84%E6%BA%90%E7%A0%81%E8%B0%83%E8%AF%95%E7%8E%AF%E5%A2%83/:1:4","tags":["k8s","cloud native","source code"],"title":"搭建 k8s 1.24 版本的源码调试环境","uri":"/ooooo-notes/%E6%90%AD%E5%BB%BA-k8s-1.24-%E7%89%88%E6%9C%AC%E7%9A%84%E6%BA%90%E7%A0%81%E8%B0%83%E8%AF%95%E7%8E%AF%E5%A2%83/"},{"categories":["随笔"],"content":"5. 用 IDE 启动服务（比如 kube-controller-manager） 程序的入口： cmd/kube-controller-manager/controller-manager.go (其他的服务也是类似的路径) 点击，配置启动参数，如下图 配置启动参数 现在基本就配置好了 ","date":"2022-06-28 08:00:00","objectID":"/ooooo-notes/%E6%90%AD%E5%BB%BA-k8s-1.24-%E7%89%88%E6%9C%AC%E7%9A%84%E6%BA%90%E7%A0%81%E8%B0%83%E8%AF%95%E7%8E%AF%E5%A2%83/:1:5","tags":["k8s","cloud native","source code"],"title":"搭建 k8s 1.24 版本的源码调试环境","uri":"/ooooo-notes/%E6%90%AD%E5%BB%BA-k8s-1.24-%E7%89%88%E6%9C%AC%E7%9A%84%E6%BA%90%E7%A0%81%E8%B0%83%E8%AF%95%E7%8E%AF%E5%A2%83/"},{"categories":["随笔"],"content":"6. 检查服务是否正常启动 （比如 kube-controller-manager） # 执行命令看没有 kube-controller-manager kubectl get pods -A ","date":"2022-06-28 08:00:00","objectID":"/ooooo-notes/%E6%90%AD%E5%BB%BA-k8s-1.24-%E7%89%88%E6%9C%AC%E7%9A%84%E6%BA%90%E7%A0%81%E8%B0%83%E8%AF%95%E7%8E%AF%E5%A2%83/:1:6","tags":["k8s","cloud native","source code"],"title":"搭建 k8s 1.24 版本的源码调试环境","uri":"/ooooo-notes/%E6%90%AD%E5%BB%BA-k8s-1.24-%E7%89%88%E6%9C%AC%E7%9A%84%E6%BA%90%E7%A0%81%E8%B0%83%E8%AF%95%E7%8E%AF%E5%A2%83/"},{"categories":["随笔"],"content":"2. 方法2. 借助 dlv 来调试源码 如果你是 mac/window 系统，可以借助 dlv 来调试源码。 具体步骤如下： ","date":"2022-06-28 08:00:00","objectID":"/ooooo-notes/%E6%90%AD%E5%BB%BA-k8s-1.24-%E7%89%88%E6%9C%AC%E7%9A%84%E6%BA%90%E7%A0%81%E8%B0%83%E8%AF%95%E7%8E%AF%E5%A2%83/:2:0","tags":["k8s","cloud native","source code"],"title":"搭建 k8s 1.24 版本的源码调试环境","uri":"/ooooo-notes/%E6%90%AD%E5%BB%BA-k8s-1.24-%E7%89%88%E6%9C%AC%E7%9A%84%E6%BA%90%E7%A0%81%E8%B0%83%E8%AF%95%E7%8E%AF%E5%A2%83/"},{"categories":["随笔"],"content":"1. 下载源码 (go 的版本要求 1.18.x) git clone git@github.com:kubernetes/kubernetes.git cd kubernetes git checkout -b origin/release-1.24 go mod download 上述下载源码，需要在 本地window 和 k8s节点 上都下载。 注意：由于是远端调试，所以需要在 k8s master 节点上，重新编译源码，去掉 -N -l. ","date":"2022-06-28 08:00:00","objectID":"/ooooo-notes/%E6%90%AD%E5%BB%BA-k8s-1.24-%E7%89%88%E6%9C%AC%E7%9A%84%E6%BA%90%E7%A0%81%E8%B0%83%E8%AF%95%E7%8E%AF%E5%A2%83/:2:1","tags":["k8s","cloud native","source code"],"title":"搭建 k8s 1.24 版本的源码调试环境","uri":"/ooooo-notes/%E6%90%AD%E5%BB%BA-k8s-1.24-%E7%89%88%E6%9C%AC%E7%9A%84%E6%BA%90%E7%A0%81%E8%B0%83%E8%AF%95%E7%8E%AF%E5%A2%83/"},{"categories":["随笔"],"content":"2. 在 k8s master 节点上，重新编译源码 cd kubernetes make DBG=1 # 在 hack/lib/golang.sh 中 ","date":"2022-06-28 08:00:00","objectID":"/ooooo-notes/%E6%90%AD%E5%BB%BA-k8s-1.24-%E7%89%88%E6%9C%AC%E7%9A%84%E6%BA%90%E7%A0%81%E8%B0%83%E8%AF%95%E7%8E%AF%E5%A2%83/:2:2","tags":["k8s","cloud native","source code"],"title":"搭建 k8s 1.24 版本的源码调试环境","uri":"/ooooo-notes/%E6%90%AD%E5%BB%BA-k8s-1.24-%E7%89%88%E6%9C%AC%E7%9A%84%E6%BA%90%E7%A0%81%E8%B0%83%E8%AF%95%E7%8E%AF%E5%A2%83/"},{"categories":["随笔"],"content":"3. 下载可能用到的工具，如 dlv (你可能需要提前设置 GOPATH 环境变量) # 可能非常慢，需要设置代理 go get -u github.com/cloudflare/cfssl/cmd/cfssl go get -u github.com/cloudflare/cfssl/cmd/cfssljson go get -u github.com/go-delve/delve/cmd/dlv # 配置环境变量 PATH=$PATH:$GOPATH/bin ","date":"2022-06-28 08:00:00","objectID":"/ooooo-notes/%E6%90%AD%E5%BB%BA-k8s-1.24-%E7%89%88%E6%9C%AC%E7%9A%84%E6%BA%90%E7%A0%81%E8%B0%83%E8%AF%95%E7%8E%AF%E5%A2%83/:2:3","tags":["k8s","cloud native","source code"],"title":"搭建 k8s 1.24 版本的源码调试环境","uri":"/ooooo-notes/%E6%90%AD%E5%BB%BA-k8s-1.24-%E7%89%88%E6%9C%AC%E7%9A%84%E6%BA%90%E7%A0%81%E8%B0%83%E8%AF%95%E7%8E%AF%E5%A2%83/"},{"categories":["随笔"],"content":"4. 找到服务的启动参数（比如 kube-controller-manager） # 执行命令 ps aux | grep kube-controller-manager | grep -v grep # 命令执行之后，输出如下, kube-controller-manager 后面的就是程序的参数 root 1584 4.0 0.8 820020 110072 ? Ssl 23:28 0:02 kube-controller-manager --allocate-node-cidrs=true --authentication-kubeconfig=/etc/kubernetes/controller-manager.conf --authorization-kubeconfig=/etc/kubernetes/controller-manager.conf --bind-address=127.0.0.1 --client-ca-file=/etc/kubernetes/pki/ca.crt --cluster-cidr=10.244.0.0/16 --cluster-name=kubernetes --cluster-signing-cert-file=/etc/kubernetes/pki/ca.crt --cluster-signing-key-file=/etc/kubernetes/pki/ca.key --controllers=*,bootstrapsigner,tokencleaner --kubeconfig=/etc/kubernetes/controller-manager.conf --leader-elect=true --requestheader-client-ca-file=/etc/kubernetes/pki/front-proxy-ca.crt --root-ca-file=/etc/kubernetes/pki/ca.crt --service-account-private-key-file=/etc/kubernetes/pki/sa.key --service-cluster-ip-range=10.96.0.0/12 --use-service-account-credentials=true ","date":"2022-06-28 08:00:00","objectID":"/ooooo-notes/%E6%90%AD%E5%BB%BA-k8s-1.24-%E7%89%88%E6%9C%AC%E7%9A%84%E6%BA%90%E7%A0%81%E8%B0%83%E8%AF%95%E7%8E%AF%E5%A2%83/:2:4","tags":["k8s","cloud native","source code"],"title":"搭建 k8s 1.24 版本的源码调试环境","uri":"/ooooo-notes/%E6%90%AD%E5%BB%BA-k8s-1.24-%E7%89%88%E6%9C%AC%E7%9A%84%E6%BA%90%E7%A0%81%E8%B0%83%E8%AF%95%E7%8E%AF%E5%A2%83/"},{"categories":["随笔"],"content":"5. 移动 kubernetes 的静态 pod （比如 kube-controller-manager） cd /etc/kubernetes mv manifests/kube-controller-manager.yaml ./ ","date":"2022-06-28 08:00:00","objectID":"/ooooo-notes/%E6%90%AD%E5%BB%BA-k8s-1.24-%E7%89%88%E6%9C%AC%E7%9A%84%E6%BA%90%E7%A0%81%E8%B0%83%E8%AF%95%E7%8E%AF%E5%A2%83/:2:5","tags":["k8s","cloud native","source code"],"title":"搭建 k8s 1.24 版本的源码调试环境","uri":"/ooooo-notes/%E6%90%AD%E5%BB%BA-k8s-1.24-%E7%89%88%E6%9C%AC%E7%9A%84%E6%BA%90%E7%A0%81%E8%B0%83%E8%AF%95%E7%8E%AF%E5%A2%83/"},{"categories":["随笔"],"content":"6. 用 dlv 启动服务 注意: 启动参数和程序路径，配置成你自己的，监听的端口是 2346 dlv 在配置程序参数时，有 --， 如果后面参数有特殊符号，用 --key=\"value\" 形式 dlv 启动之后，必须要触发(IDE go remote)，才能启动, 否则会一直等着。 dlv --listen=:2346 --headless=true --api-version=2 --accept-multiclient exec /root/kubernetes/_output/bin/kube-controller-manager -- --allocate-node-cidrs=true --authentication-kubeconfig=/etc/kubernetes/controller-manager.conf --authorization-kubeconfig=/etc/kubernetes/controller-manager.conf --bind-address=127.0.0.1 --client-ca-file=/etc/kubernetes/pki/ca.crt --cluster-cidr=10.244.0.0/16 --cluster-name=kubernetes --cluster-signing-cert-file=/etc/kubernetes/pki/ca.crt --cluster-signing-key-file=/etc/kubernetes/pki/ca.key --controllers=*,bootstrapsigner,tokencleaner --kubeconfig=/etc/kubernetes/controller-manager.conf --leader-elect=true --requestheader-client-ca-file=/etc/kubernetes/pki/front-proxy-ca.crt --root-ca-file=/etc/kubernetes/pki/ca.crt --service-account-private-key-file=/etc/kubernetes/pki/sa.key --service-cluster-ip-range=10.96.0.0/12 --use-service-account-credentials=true ","date":"2022-06-28 08:00:00","objectID":"/ooooo-notes/%E6%90%AD%E5%BB%BA-k8s-1.24-%E7%89%88%E6%9C%AC%E7%9A%84%E6%BA%90%E7%A0%81%E8%B0%83%E8%AF%95%E7%8E%AF%E5%A2%83/:2:6","tags":["k8s","cloud native","source code"],"title":"搭建 k8s 1.24 版本的源码调试环境","uri":"/ooooo-notes/%E6%90%AD%E5%BB%BA-k8s-1.24-%E7%89%88%E6%9C%AC%E7%9A%84%E6%BA%90%E7%A0%81%E8%B0%83%E8%AF%95%E7%8E%AF%E5%A2%83/"},{"categories":["随笔"],"content":"7. 用 IDE 连接 dlv 服务（比如 kube-controller-manager） 程序的入口： cmd/kube-controller-manager/controller-manager.go (其他的服务也是类似的路径) 添加 Go Remote， 配置 host 和 port。 点击 ok，然后启动服务。 连接dlv 现在基本就配置好了 ","date":"2022-06-28 08:00:00","objectID":"/ooooo-notes/%E6%90%AD%E5%BB%BA-k8s-1.24-%E7%89%88%E6%9C%AC%E7%9A%84%E6%BA%90%E7%A0%81%E8%B0%83%E8%AF%95%E7%8E%AF%E5%A2%83/:2:7","tags":["k8s","cloud native","source code"],"title":"搭建 k8s 1.24 版本的源码调试环境","uri":"/ooooo-notes/%E6%90%AD%E5%BB%BA-k8s-1.24-%E7%89%88%E6%9C%AC%E7%9A%84%E6%BA%90%E7%A0%81%E8%B0%83%E8%AF%95%E7%8E%AF%E5%A2%83/"},{"categories":["随笔"],"content":"8. 提供一个调试的脚本 （可选） 你现在会发现，如果想要调试，就必须要把 manifests/kube-controller-manager.yaml 移出去，等不需要调试了，再把这个文件移回来，这样非常麻烦。所以使用一个脚本来实现。 脚本内容如下(注意更改你的路径)： cleanup() { mv /etc/kubernetes/kube-controller-manager.yaml /etc/kubernetes/manifests } trap cleanup EXIT mv /etc/kubernetes/manifests/kube-controller-manager.yaml /etc/kubernetes dlv --listen=:2346 --headless=true --api-version=2 --accept-multiclient exec /root/kubernetes/_output/bin/kube-controller-manager -- --allocate-node-cidrs=true --authentication-kubeconfig=/etc/kubernetes/controller-manager.conf --authorization-kubeconfig=/etc/kubernetes/controller-manager.conf --bind-address=127.0.0.1 --client-ca-file=/etc/kubernetes/pki/ca.crt --cluster-cidr=10.244.0.0/16 --cluster-name=kubernetes --cluster-signing-cert-file=/etc/kubernetes/pki/ca.crt --cluster-signing-key-file=/etc/kubernetes/pki/ca.key --controllers=*,bootstrapsigner,tokencleaner --kubeconfig=/etc/kubernetes/controller-manager.conf --leader-elect=true --requestheader-client-ca-file=/etc/kubernetes/pki/front-proxy-ca.crt --root-ca-file=/etc/kubernetes/pki/ca.crt --service-account-private-key-file=/etc/kubernetes/pki/sa.key --service-cluster-ip-range=10.96.0.0/12 --use-service-account-credentials=true ","date":"2022-06-28 08:00:00","objectID":"/ooooo-notes/%E6%90%AD%E5%BB%BA-k8s-1.24-%E7%89%88%E6%9C%AC%E7%9A%84%E6%BA%90%E7%A0%81%E8%B0%83%E8%AF%95%E7%8E%AF%E5%A2%83/:2:8","tags":["k8s","cloud native","source code"],"title":"搭建 k8s 1.24 版本的源码调试环境","uri":"/ooooo-notes/%E6%90%AD%E5%BB%BA-k8s-1.24-%E7%89%88%E6%9C%AC%E7%9A%84%E6%BA%90%E7%A0%81%E8%B0%83%E8%AF%95%E7%8E%AF%E5%A2%83/"},{"categories":["随笔"],"content":"1. install logstash refer to logstash document ","date":"2022-06-01 08:00:00","objectID":"/ooooo-notes/logstash-%E7%9A%84%E7%AE%80%E5%8D%95%E4%BD%BF%E7%94%A8/:1:0","tags":["logstash"],"title":"logstash 的简单使用","uri":"/ooooo-notes/logstash-%E7%9A%84%E7%AE%80%E5%8D%95%E4%BD%BF%E7%94%A8/"},{"categories":["随笔"],"content":"2. logstash example config file input { tcp { port =\u003e 12345 codec =\u003e \"json_lines\" } } filter{ grok { match =\u003e [\"message\", \"%{TIMESTAMP_ISO8601:logdate}\"] } date { match =\u003e [\"logdate\", \"yyyy-MM-dd HH:mm:ss.SSS\"] target =\u003e \"@timestamp\" } mutate { remove_field =\u003e [\"logdate\"] } ruby { code =\u003e \"event.set('timestamp', event.get('@timestamp').time.localtime + 8*60*60)\" } ruby { code =\u003e \"event.set('@timestamp',event.get('timestamp'))\" } mutate { remove_field =\u003e [\"timestamp\"] } } output { stdout { codec =\u003e rubydebug { metadata =\u003e true } } file { path =\u003e \"./logs/%{+YYYY-MM-dd-HH}.log\" codec =\u003e line { format =\u003e \"%{message}\"} } } notes: it will serve TCP connection on localhost:12345. uses codec named json_lines, json data format such as { \"message\" : \"xxxx\" }. matche date pattern in the log, then use it as its time. reset the field @timestamp, output to the local file. ","date":"2022-06-01 08:00:00","objectID":"/ooooo-notes/logstash-%E7%9A%84%E7%AE%80%E5%8D%95%E4%BD%BF%E7%94%A8/:1:1","tags":["logstash"],"title":"logstash 的简单使用","uri":"/ooooo-notes/logstash-%E7%9A%84%E7%AE%80%E5%8D%95%E4%BD%BF%E7%94%A8/"},{"categories":["随笔"],"content":"1. install wsl open Microsoft Store, then search ubuntu and click to install it. open terminal # list all linux subsystem wsl --list # set default linux subsystem, then you can input 'wsl' to inter system wsl --set-default ubuntu # enter default linux subsystem wsl # install cmake, g++, gcc，gdb cd /usr/local wget https://cmake.org/files/v3.22/cmake-3.22.0-linux-x86_64.tar.gz tar xf cmake-3.22.0-linux-x86_64.tar.gz ln -s /usr/local/cmake-3.22.0-linux-x86_64/bin/cmake /usr/bin sudo apt install build-essential ","date":"2022-06-01 08:00:00","objectID":"/ooooo-notes/%E5%9C%A8-windows-%E4%B8%8A%E8%B0%83%E8%AF%95-redis/:1:0","tags":["redis"],"title":"在 windows 上调试 redis","uri":"/ooooo-notes/%E5%9C%A8-windows-%E4%B8%8A%E8%B0%83%E8%AF%95-redis/"},{"categories":["随笔"],"content":"2. setting clion open File | Settings | Build, Execution, Deployment | Toolchains menu. add new toolchains and select wsl. 在 clion 中创建工具链 setting wsl configuration, you maybe install cmake, gcc, g++, gdb. you must execute command git config core.autocrlf input in your terminal, because windows is CRLF, then git clone . select wsl in File | Settings | Build, Execution, Deployment | Makefile, because building redis by using makefile. login wsl and enter redis directory, for example: cd /mnt/c/Users/ooooo/Development/code/Demo/redis execute command make, you maybe need to execute cd src \u0026\u0026 ls | grep .sh | xargs chmod a+x 选择可执行文件 ","date":"2022-06-01 08:00:00","objectID":"/ooooo-notes/%E5%9C%A8-windows-%E4%B8%8A%E8%B0%83%E8%AF%95-redis/:2:0","tags":["redis"],"title":"在 windows 上调试 redis","uri":"/ooooo-notes/%E5%9C%A8-windows-%E4%B8%8A%E8%B0%83%E8%AF%95-redis/"},{"categories":["随笔"],"content":"1. 机器初始化设置 ","date":"2022-04-01 08:00:00","objectID":"/ooooo-notes/%E4%BD%BF%E7%94%A8-ubuntu-%E6%9D%A5%E5%AE%89%E8%A3%85-kubernetes-1.24-%E7%89%88%E6%9C%AC/:1:0","tags":["k8s","cloud native"],"title":"使用 ubuntu 来安装 kubernetes 1.24 版本","uri":"/ooooo-notes/%E4%BD%BF%E7%94%A8-ubuntu-%E6%9D%A5%E5%AE%89%E8%A3%85-kubernetes-1.24-%E7%89%88%E6%9C%AC/"},{"categories":["随笔"],"content":"hostname 设置 hostnamectl ## 查看当前的hostname hostnamectl set-hostname node1 ## 设置主机名为node1 ","date":"2022-04-01 08:00:00","objectID":"/ooooo-notes/%E4%BD%BF%E7%94%A8-ubuntu-%E6%9D%A5%E5%AE%89%E8%A3%85-kubernetes-1.24-%E7%89%88%E6%9C%AC/:1:1","tags":["k8s","cloud native"],"title":"使用 ubuntu 来安装 kubernetes 1.24 版本","uri":"/ooooo-notes/%E4%BD%BF%E7%94%A8-ubuntu-%E6%9D%A5%E5%AE%89%E8%A3%85-kubernetes-1.24-%E7%89%88%E6%9C%AC/"},{"categories":["随笔"],"content":"/etc/hosts 文件 和 DNS 配置 # k8s master 192.168.130.131 node1 # 更改dns配置 vim /etc/systemd/resolved.conf # 更改下面内容 [Resolve] DNS=8.8.8.8 8.8.4.4 # 重启dns systemctl restart systemd-resolved.service refer: ubuntu dns resolver ","date":"2022-04-01 08:00:00","objectID":"/ooooo-notes/%E4%BD%BF%E7%94%A8-ubuntu-%E6%9D%A5%E5%AE%89%E8%A3%85-kubernetes-1.24-%E7%89%88%E6%9C%AC/:1:2","tags":["k8s","cloud native"],"title":"使用 ubuntu 来安装 kubernetes 1.24 版本","uri":"/ooooo-notes/%E4%BD%BF%E7%94%A8-ubuntu-%E6%9D%A5%E5%AE%89%E8%A3%85-kubernetes-1.24-%E7%89%88%E6%9C%AC/"},{"categories":["随笔"],"content":"创建非 root 用户(可选) # 添加用户 useradd ooooo -g ooooo # 修改用户密码 passwd ooooo ","date":"2022-04-01 08:00:00","objectID":"/ooooo-notes/%E4%BD%BF%E7%94%A8-ubuntu-%E6%9D%A5%E5%AE%89%E8%A3%85-kubernetes-1.24-%E7%89%88%E6%9C%AC/:1:3","tags":["k8s","cloud native"],"title":"使用 ubuntu 来安装 kubernetes 1.24 版本","uri":"/ooooo-notes/%E4%BD%BF%E7%94%A8-ubuntu-%E6%9D%A5%E5%AE%89%E8%A3%85-kubernetes-1.24-%E7%89%88%E6%9C%AC/"},{"categories":["随笔"],"content":"安装 containerd 和 runc 安装 containerd wget https://github.com/containerd/containerd/releases/download/v1.6.6/containerd-1.6.6-linux-amd64.tar.gz tar Cxzvf /usr/local containerd-1.6.6-linux-amd64.tar.gz mkdir -p /usr/local/lib/systemd/system/ 通过 systemd 来启动 containerd 将下面的内容写入 /usr/local/lib/systemd/system/containerd.service # Copyright The containerd Authors. # # Licensed under the Apache License, Version 2.0 (the \"License\"); # you may not use this file except in compliance with the License. # You may obtain a copy of the License at # # http://www.apache.org/licenses/LICENSE-2.0 # # Unless required by applicable law or agreed to in writing, software # distributed under the License is distributed on an \"AS IS\" BASIS, # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. # See the License for the specific language governing permissions and # limitations under the License. [Unit] Description=containerd container runtime Documentation=https://containerd.io After=network.target local-fs.target [Service] ExecStartPre=-/sbin/modprobe overlay ExecStart=/usr/local/bin/containerd Type=notify Delegate=yes KillMode=process Restart=always RestartSec=5 # Having non-zero Limit*s causes performance problems due to accounting overhead # in the kernel. We recommend using cgroups to do container-local accounting. LimitNPROC=infinity LimitCORE=infinity LimitNOFILE=infinity # Comment TasksMax if your systemd version does not supports it. # Only systemd 226 and above support this version. TasksMax=infinity OOMScoreAdjust=-999 [Install] WantedBy=multi-user.target 启动 containerd systemctl daemon-reload systemctl enable --now containerd 配置 containerd mkdir -p /etc/containerd # 生成默认配置文件 containerd config default | tee /etc/containerd/config.toml # 修改 /etc/containerd/config.toml 配置 # image 使用阿里云的地址， SystemdCgroup 更改为 true sandbox_image = \"registry.aliyuncs.com/google_containers/pause:3.6\" ... [plugins.\"io.containerd.grpc.v1.cri\".containerd.runtimes.runc.options] ... SystemdCgroup = true # 修改完成后 systemctl restart containerd 安装 runc wget https://github.com/opencontainers/runc/releases/download/v1.1.3/runc.amd64 install -m 755 runc.amd64 /usr/local/sbin/runc 安装 cni 插件 wget https://github.com/containernetworking/plugins/releases/download/v1.1.1/cni-plugins-linux-amd64-v1.1.1.tgz mkdir -p /opt/cni/bin tar Cxzvf /opt/cni/bin cni-plugins-linux-amd64-v1.1.1.tgz ","date":"2022-04-01 08:00:00","objectID":"/ooooo-notes/%E4%BD%BF%E7%94%A8-ubuntu-%E6%9D%A5%E5%AE%89%E8%A3%85-kubernetes-1.24-%E7%89%88%E6%9C%AC/:1:4","tags":["k8s","cloud native"],"title":"使用 ubuntu 来安装 kubernetes 1.24 版本","uri":"/ooooo-notes/%E4%BD%BF%E7%94%A8-ubuntu-%E6%9D%A5%E5%AE%89%E8%A3%85-kubernetes-1.24-%E7%89%88%E6%9C%AC/"},{"categories":["随笔"],"content":"2. k8s 安装 官方 k8s 安装文档 # 参考文档检查服务器的状态是否可以安装 k8s 服务 # 临时关闭 swap 分区 swapoff -a # 查看 swap 分区是否关闭，显示 0 表示已关闭 free -h # 永久关闭 swap 分区 编辑 /etc/fstab 文件, 注释最后一行 # 检查 br_netfilter 是否被加载，没有任何输出，表示没有加载 lsmod | grep br_netfilter # 加载 br_netfilter 模块 sudo modprobe br_netfilter ## 配置网络 cat \u003c\u003cEOF | sudo tee /etc/modules-load.d/k8s.conf br_netfilter EOF cat \u003c\u003cEOF | sudo tee /etc/sysctl.d/k8s.conf net.bridge.bridge-nf-call-ip6tables = 1 net.bridge.bridge-nf-call-iptables = 1 net.ipv4.ip_forward = 1 EOF sudo sysctl --system # 安装软件 sudo apt-get update sudo apt-get install -y apt-transport-https ca-certificates curl sudo curl -fsSLo /usr/share/keyrings/kubernetes-archive-keyring.gpg https://mirrors.aliyun.com/kubernetes/apt/doc/apt-key.gpg echo \"deb [signed-by=/usr/share/keyrings/kubernetes-archive-keyring.gpg] https://mirrors.aliyun.com/kubernetes/apt/ kubernetes-xenial main\" | sudo tee /etc/apt/sources.list.d/kubernetes.list sudo apt-get update # 默认安装最新版本 sudo apt-get install -y kubelet kubeadm kubectl # 不自动更新 sudo apt-mark hold kubelet kubeadm kubectl # 查看镜像列表， 报错需要添加配置, crictl 是官方提供的 crictl images # vim /etc/crictl.yaml 添加以下内容 runtime-endpoint: unix:///run/containerd/containerd.sock image-endpoint: unix:///run/containerd/containerd.sock timeout: 10 debug: false # 设置 kubelet 开机启动，并且现在启动 # 启动之后可能会报错，如果原因是 没有读取到 kubelet 的配置文件，这里可以不用管，稍后会重启这个服务 sudo systemctl enable --now kubelet # 查看 kubelet 的状态 sudo systemctl status kubelet # 查看 kubelet 的日志 journalctl -xeu kubelet ","date":"2022-04-01 08:00:00","objectID":"/ooooo-notes/%E4%BD%BF%E7%94%A8-ubuntu-%E6%9D%A5%E5%AE%89%E8%A3%85-kubernetes-1.24-%E7%89%88%E6%9C%AC/:2:0","tags":["k8s","cloud native"],"title":"使用 ubuntu 来安装 kubernetes 1.24 版本","uri":"/ooooo-notes/%E4%BD%BF%E7%94%A8-ubuntu-%E6%9D%A5%E5%AE%89%E8%A3%85-kubernetes-1.24-%E7%89%88%E6%9C%AC/"},{"categories":["随笔"],"content":"3. 创建 k8s 集群 创建 k8s 集群官方文档 k8s pod network 插件文档 # 执行 kubeadm init 命令， 在 k8s master 机器上执行，默认情况下， k8s 创建 pod 不会在 master 机器上 # 重点注意: --pod-network-cidr=10.244.0.0/16 这个参数必须要有，没有的话安装 cni 会报错 # 注意 preflight 的前置检查输出，如果有问题，百度自行解决 # 替换为你自己的 ip 和 hostname sudo kubeadm init --image-repository registry.aliyuncs.com/google_containers --apiserver-advertise-address=192.168.130.128 --pod-network-cidr=10.244.0.0/16 --control-plane-endpoint=node1 # 执行命令之后，会有 kubeadm join 输出行 # （分为 master-token 和 worker-token）， 类似于下面的命令，可以在另一个节点上执行 worker-join-token 的命令 sudo kubeadm join 192.168.130.128:6443 --token 8auvt0.zfw0ayr45d80q8pb \\ --discovery-token-ca-cert-hash sha256:efe854739efef5fbaf3f6e28c899481c8d7797c1997fc8315b921a9ede400ca8 # 去掉污点，让单个节点也可以运行, (我这里只有一个节点) kubectl taint nodes --all node-role.kubernetes.io/control-plane- node-role.kubernetes.io/master- ## 在机器上执行 kubeadm join 或者 kubeadm init 命令之后，重启 kubelet 服务 sudo systemctl restart kubelet sudo systemctl status kubelet # 设置 kubectl 的配置文件， 为 $HOME/.kube/config mkdir -p $HOME/.kube sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config sudo chown $(id -u):$(id -g) $HOME/.kube/config # 安装 pod network 插件, 这里使用 calico 插件 curl -o calico-operator.yaml https://projectcalico.docs.tigera.io/manifests/tigera-operator.yaml curl -o calico-custom-resources.yaml https://projectcalico.docs.tigera.io/manifests/custom-resources.yaml # 重点 # 更改 calico-custom-resources.yaml 的 cidr 配置, 值为 --pod-network-cidr （在 kubeadm init 指定了） # 多个网卡，也可以更改，否则可能会报错，搜索 interface kubectl create -f calico-operator.yaml kubectl create -f calico-custom-resources.yaml # 查看 calico 是否已经启动完成, cni 也启动成功 kubectl get pods -A # 成功之后会有下面的服务， 都是 running 状态 calico-apiserver calico-apiserver-78c5f69667-gbxbv 1/1 Running 0 88s calico-apiserver calico-apiserver-78c5f69667-h64wk 1/1 Running 0 88s calico-system calico-kube-controllers-68884f975d-q4l8s 1/1 Running 0 40m calico-system calico-node-4d7hs 1/1 Running 0 40m calico-system calico-typha-854c6b9b4b-s8ls7 1/1 Running 0 40m kube-system coredns-74586cf9b6-4pkxf 1/1 Running 0 76m kube-system coredns-74586cf9b6-9hxwl 1/1 Running 0 76m kube-system etcd-node1 1/1 Running 0 76m kube-system kube-apiserver-node1 1/1 Running 0 76m kube-system kube-controller-manager-node1 1/1 Running 0 76m kube-system kube-proxy-mn6fr 1/1 Running 0 76m kube-system kube-scheduler-node1 1/1 Running 0 76m tigera-operator tigera-operator-5fb55776df-gjs7s 1/1 Running 0 64m ","date":"2022-04-01 08:00:00","objectID":"/ooooo-notes/%E4%BD%BF%E7%94%A8-ubuntu-%E6%9D%A5%E5%AE%89%E8%A3%85-kubernetes-1.24-%E7%89%88%E6%9C%AC/:3:0","tags":["k8s","cloud native"],"title":"使用 ubuntu 来安装 kubernetes 1.24 版本","uri":"/ooooo-notes/%E4%BD%BF%E7%94%A8-ubuntu-%E6%9D%A5%E5%AE%89%E8%A3%85-kubernetes-1.24-%E7%89%88%E6%9C%AC/"},{"categories":["随笔"],"content":"install nfs in docker ","date":"2022-03-01 08:00:00","objectID":"/ooooo-notes/%E5%9C%A8-docker-%E4%B8%8A%E5%AE%89%E8%A3%85-nfs/:1:0","tags":["docker","nfs"],"title":"在 docker 上安装 nfs","uri":"/ooooo-notes/%E5%9C%A8-docker-%E4%B8%8A%E5%AE%89%E8%A3%85-nfs/"},{"categories":["随笔"],"content":"1. create share directory used by nfs mkdir -p /home/ooooo/shared/nfs ","date":"2022-03-01 08:00:00","objectID":"/ooooo-notes/%E5%9C%A8-docker-%E4%B8%8A%E5%AE%89%E8%A3%85-nfs/:1:1","tags":["docker","nfs"],"title":"在 docker 上安装 nfs","uri":"/ooooo-notes/%E5%9C%A8-docker-%E4%B8%8A%E5%AE%89%E8%A3%85-nfs/"},{"categories":["随笔"],"content":"2. create exports.txt used by nfs This the exports.txt mainly used to mount dir (path in the container ) and permission. for example: It indicates read only for all ip. vim /home/ooooo/exports.txt /home/ooooo/shared/nfs *(ro,no_subtree_check) ","date":"2022-03-01 08:00:00","objectID":"/ooooo-notes/%E5%9C%A8-docker-%E4%B8%8A%E5%AE%89%E8%A3%85-nfs/:1:2","tags":["docker","nfs"],"title":"在 docker 上安装 nfs","uri":"/ooooo-notes/%E5%9C%A8-docker-%E4%B8%8A%E5%AE%89%E8%A3%85-nfs/"},{"categories":["随笔"],"content":"3. execute docker command docker run -d \\ -v /home/ooooo/shared/nfs:/home/ooooo/shared/nfs \\ -v /home/ooooo/exports.txt:/etc/exports:ro \\ --cap-add SYS_ADMIN \\ -p 2049:2049 \\ erichough/nfs-server # check nfs server netstat -nla | grep 2049 # mount nfs dir (check mount.nfs whether is exist ) # 172.17.0.2 is container ip mount 172.17.0.2:/home/ooooo/shared/nfs /home/ooooo/nfs-mount ","date":"2022-03-01 08:00:00","objectID":"/ooooo-notes/%E5%9C%A8-docker-%E4%B8%8A%E5%AE%89%E8%A3%85-nfs/:1:3","tags":["docker","nfs"],"title":"在 docker 上安装 nfs","uri":"/ooooo-notes/%E5%9C%A8-docker-%E4%B8%8A%E5%AE%89%E8%A3%85-nfs/"},{"categories":["随笔"],"content":"5. 参考 docker images ","date":"2022-03-01 08:00:00","objectID":"/ooooo-notes/%E5%9C%A8-docker-%E4%B8%8A%E5%AE%89%E8%A3%85-nfs/:1:4","tags":["docker","nfs"],"title":"在 docker 上安装 nfs","uri":"/ooooo-notes/%E5%9C%A8-docker-%E4%B8%8A%E5%AE%89%E8%A3%85-nfs/"},{"categories":["随笔"],"content":" cygwin 的环境变量要放在第一个，这样 rsync 和 ssh 都是 cygwin 的. window 是执行命令 where ssh， 看看 ssh 有几个实现 （比如 openssh 和 cygwin 的 ssh ） refer: window install cygwin ","date":"2022-01-03 08:00:00","objectID":"/ooooo-notes/%E5%9C%A8-windows-%E4%B8%AD-rsync-%E9%97%AE%E9%A2%98/:0:0","tags":["resolution"],"title":"在 windows 中 rsync 问题","uri":"/ooooo-notes/%E5%9C%A8-windows-%E4%B8%AD-rsync-%E9%97%AE%E9%A2%98/"},{"categories":["随笔"],"content":" win10安装wireshark经常报“KB2999226 和 KB3118401” install wireshark open installation directory, manually install vc_redist.x64.exe by double click reinstall wireshark ","date":"2022-01-02 08:00:00","objectID":"/ooooo-notes/%E5%9C%A8-windows-%E4%B8%AD%E5%AE%89%E8%A3%85-wireshark-%E6%8A%A5%E9%94%99/:0:0","tags":["resolution","wireshark"],"title":"在 windows 中安装 wireshark 报错","uri":"/ooooo-notes/%E5%9C%A8-windows-%E4%B8%AD%E5%AE%89%E8%A3%85-wireshark-%E6%8A%A5%E9%94%99/"},{"categories":["计划"],"content":"0、持续学习者 Talk is cheap. Show me the code. 英语比编程简单。 学习和实践要平衡。 学会和时间做朋友。 学会投资，学会理财。 学会先做减法，再做加法。 学英语很重要，学英语很重要，学英语很重要。 说明： ⭕ 进行中 ✅ 已完成 ❌ 已废弃 ❓ 有必要 ❗ 重要性 📝 记笔记 🖊 写代码 ","date":"2022-01-01 09:00:00","objectID":"/ooooo-notes/2022%E5%B9%B4%E8%AE%A1%E5%88%92/:1:0","tags":["learning"],"title":"2022年学习计划","uri":"/ooooo-notes/2022%E5%B9%B4%E8%AE%A1%E5%88%92/"},{"categories":["计划"],"content":"1、关于英语 《新概念英语》 ⭕ 《每日英语听力 ~ EnglishPod》 ","date":"2022-01-01 09:00:00","objectID":"/ooooo-notes/2022%E5%B9%B4%E8%AE%A1%E5%88%92/:2:0","tags":["learning"],"title":"2022年学习计划","uri":"/ooooo-notes/2022%E5%B9%B4%E8%AE%A1%E5%88%92/"},{"categories":["计划"],"content":"2、关于技术 计划 🎉： 只记录自己认为有用的笔记。 ","date":"2022-01-01 09:00:00","objectID":"/ooooo-notes/2022%E5%B9%B4%E8%AE%A1%E5%88%92/:3:0","tags":["learning"],"title":"2022年学习计划","uri":"/ooooo-notes/2022%E5%B9%B4%E8%AE%A1%E5%88%92/"},{"categories":["计划"],"content":"1、书籍 0️⃣1️⃣. 《Java 性能优化权威指南》 ✅ 0️⃣2️⃣. 《深入理解 Java 虚拟机（第3版）》 ✅ 0️⃣3️⃣. 《Spring Cloud 微服务：入门、实战与进阶》 ❌ 0️⃣4️⃣. 《arthas》 0️⃣5️⃣. 《Java 并发编程实战》 ✅ 0️⃣6️⃣. 《深入理解 Kafka：核心设计与实践原理》 0️⃣7️⃣. 《从零开始学架构》 ✅ 0️⃣8️⃣. 《高可用可伸缩微服务架构》 ❌ 0️⃣9️⃣. 《分布式一致性算法开发实战》 1️⃣0️⃣. 《Go Web 编程》 ⭕ 1️⃣2️⃣. 《Effective C++》 1️⃣3️⃣. 《More Effective C++》 1️⃣4️⃣. 《深度探索C++对象模型》 1️⃣5️⃣. 《Go语言设计与实现》 1️⃣6️⃣. 《wireshark网络分析的艺术》 ✅ 1️⃣7️⃣. 《Vim实用技巧（第2版）》 ⭕ 1️⃣8️⃣. 《RocketMQ技术内幕 第二版》 1️⃣9️⃣. 《Kubernetes in Action中文版》 ✅ 2️⃣1️⃣. 《rocketmq 源码》 ⭕ 2️⃣2️⃣. 《kubernetes 源码》 ⭕ 2️⃣3️⃣. 《istio 源码》 2️⃣4️⃣. 《etcd 源码》 2️⃣5️⃣. 《dubbo 源码》 ⭕ 2️⃣6️⃣. 《pulsar 源码》 ❌ 2️⃣7️⃣. 《nsq 源码》 2️⃣8️⃣. 《eventing 源码》 2️⃣9️⃣. 《serving 源码》 3️⃣0️⃣. 《深入浅出Istio：Service Mesh快速入门与实践》 ✅ 3️⃣1️⃣. 《Istio服务网格技术解析与实践》 ✅ 3️⃣2️⃣. 《云原生服务网格Istio：原理、实践、架构与源码解析》 3️⃣3️⃣. 《gRPC与云原生应用开发》 ✅ 3️⃣4️⃣. 《Quarkus 实战》 ✅ 3️⃣4️⃣. 《gin 源码》 ✅ 3️⃣4️⃣. 《grpc-go 源码》 ⭕ 3️⃣5️⃣. 《Go语言精进之路 I》 ✅ 3️⃣6️⃣. 《Go语言精进之路 II》 ✅ 3️⃣7️⃣. 《Wireshark网络分析就这么简单》 ✅ 3️⃣8️⃣. 《MySQL技术内幕》 3️⃣9️⃣. 《深入解析Java虚拟机HotSpot》 4️⃣0️⃣. 《Rust权威指南》 ⭕ 4️⃣1️⃣. 《深入剖析Kubernetes》 4️⃣2️⃣. 《Kubernetes编程》 ⭕ 4️⃣3️⃣. 《Kubernetes源码剖析》 ✅ 4️⃣4️⃣. 《Kubernetes设计模式》 4️⃣5️⃣. 《Helm学习指南：Kubernetes上的应用程序管理》 ✅ 4️⃣6️⃣. 《Knative实战:基于Kubernetes的无服务器架构实践》 ✅ 4️⃣7️⃣. 《深入剖析Java虚拟机》 4️⃣8️⃣. 《Groovy程序设计》 ✅ ","date":"2022-01-01 09:00:00","objectID":"/ooooo-notes/2022%E5%B9%B4%E8%AE%A1%E5%88%92/:3:1","tags":["learning"],"title":"2022年学习计划","uri":"/ooooo-notes/2022%E5%B9%B4%E8%AE%A1%E5%88%92/"},{"categories":["计划"],"content":"2、文档 0️⃣1️⃣. 《深入拆解 Java 虚拟机》 0️⃣2️⃣. 《How to write Go code》 0️⃣3️⃣. 《Effective Go》 0️⃣4️⃣. 从 0 开始带你成为JVM实战高手 ⭕ 0️⃣4️⃣. Go 语言项目开发实战 ⭕ 0️⃣5️⃣. Redis 核心技术与实战 ✅ 0️⃣6️⃣. Redis 源码剖析与实战 ⭕ 0️⃣7️⃣. 深入拆解 Tomcat \u0026 Jetty ✅ 0️⃣8️⃣. 深入 C 语言和程序运行原理 0️⃣9️⃣. 罗剑锋的 C++ 实战笔记 ✅ ","date":"2022-01-01 09:00:00","objectID":"/ooooo-notes/2022%E5%B9%B4%E8%AE%A1%E5%88%92/:3:2","tags":["learning"],"title":"2022年学习计划","uri":"/ooooo-notes/2022%E5%B9%B4%E8%AE%A1%E5%88%92/"},{"categories":["计划"],"content":"3、视频 0️⃣1️⃣. 《玩转算法系列–图论精讲》 0️⃣2️⃣. 《玩转算法面试》 ⭕ 0️⃣3️⃣. 《看的见的算法》 0️⃣4️⃣. 《极客时间– 算法进阶训练营》 ✅ 0️⃣5️⃣. 《Dubbo 3 深度剖析 - 透过源码认识你》 ✅ 0️⃣5️⃣. 《Go 微服务实战 38 讲》 ✅ 0️⃣6️⃣. 《Netty 源码剖析与实战》 ✅ ","date":"2022-01-01 09:00:00","objectID":"/ooooo-notes/2022%E5%B9%B4%E8%AE%A1%E5%88%92/:3:3","tags":["learning"],"title":"2022年学习计划","uri":"/ooooo-notes/2022%E5%B9%B4%E8%AE%A1%E5%88%92/"},{"categories":["计划"],"content":"4、算法 0️⃣1️⃣. 每周至少 5 道 Leetcode。 ⭕ 0️⃣2️⃣. leetcode 全站排名1000以内。 ⭕ 0️⃣3️⃣. leetcode 周赛全国排名2000以内。 ⭕ 0️⃣4️⃣. leetcode 周赛全球排名10000以内。 ⭕ ","date":"2022-01-01 09:00:00","objectID":"/ooooo-notes/2022%E5%B9%B4%E8%AE%A1%E5%88%92/:3:4","tags":["learning"],"title":"2022年学习计划","uri":"/ooooo-notes/2022%E5%B9%B4%E8%AE%A1%E5%88%92/"},{"categories":["计划"],"content":"4、关于其他 ","date":"2022-01-01 09:00:00","objectID":"/ooooo-notes/2022%E5%B9%B4%E8%AE%A1%E5%88%92/:4:0","tags":["learning"],"title":"2022年学习计划","uri":"/ooooo-notes/2022%E5%B9%B4%E8%AE%A1%E5%88%92/"},{"categories":["计划"],"content":"1、书籍 🎉 0️⃣1️⃣. 《聪明的投资者》 0️⃣2️⃣. 《一万小时天才理论》 ❌ 0️⃣3️⃣. 《番茄工作法图解》 ❌ 0️⃣4️⃣. 《三体》 ✅ 0️⃣5️⃣. 《三体Ⅱ》 0️⃣6️⃣. 《三体Ⅲ》 0️⃣7️⃣. 《非暴力沟通》 ⭕ 0️⃣8️⃣. 《管理你的每一天》 ❌ 0️⃣9️⃣. 《原则》 1️⃣0️⃣. 《思考，快与慢》 1️⃣1️⃣. 《关键对话》 1️⃣2️⃣. 《当下的启蒙》 1️⃣3️⃣. 《把时间当作朋友》 ❌ 1️⃣4️⃣. 《白夜行》 ❌ 1️⃣5️⃣. 《亲密关系：通往灵魂的桥梁》 ⭕ 1️⃣6️⃣. 《蛤蟆先生去看心理医生》 ✅ 1️⃣7️⃣. 《刻意练习》 ⭕ 1️⃣7️⃣. 《卓有成效的工程师》 ⭕ ","date":"2022-01-01 09:00:00","objectID":"/ooooo-notes/2022%E5%B9%B4%E8%AE%A1%E5%88%92/:4:1","tags":["learning"],"title":"2022年学习计划","uri":"/ooooo-notes/2022%E5%B9%B4%E8%AE%A1%E5%88%92/"},{"categories":["计划"],"content":"2、选读 🎉 0️⃣1️⃣. 《人性的弱点》 0️⃣2️⃣. 《算法 4》 0️⃣3️⃣. 《数据密集型应用系统设计》 0️⃣4️⃣. 《当我谈跑步时，我谈些什么》 0️⃣5️⃣. 《Kafka 官网》 ❓ 0️⃣6️⃣. 《MIT 高级数据课程》 ❓ ","date":"2022-01-01 09:00:00","objectID":"/ooooo-notes/2022%E5%B9%B4%E8%AE%A1%E5%88%92/:4:2","tags":["learning"],"title":"2022年学习计划","uri":"/ooooo-notes/2022%E5%B9%B4%E8%AE%A1%E5%88%92/"},{"categories":["计划"],"content":"3、尝试 🎉 0️⃣1️⃣. 学会使用尤克里里弹奏 ","date":"2022-01-01 09:00:00","objectID":"/ooooo-notes/2022%E5%B9%B4%E8%AE%A1%E5%88%92/:4:3","tags":["learning"],"title":"2022年学习计划","uri":"/ooooo-notes/2022%E5%B9%B4%E8%AE%A1%E5%88%92/"},{"categories":["计划"],"content":"4、了解 🎉 0️⃣1️⃣. 暂无 ","date":"2022-01-01 09:00:00","objectID":"/ooooo-notes/2022%E5%B9%B4%E8%AE%A1%E5%88%92/:4:4","tags":["learning"],"title":"2022年学习计划","uri":"/ooooo-notes/2022%E5%B9%B4%E8%AE%A1%E5%88%92/"},{"categories":["计划"],"content":"5、娱乐 0️⃣1️⃣. 《荒岛余生》 ✅ 0️⃣2️⃣. 《星际穿越》 ✅ 0️⃣3️⃣. 《这个杀手不太冷》 ✅ 0️⃣4️⃣. 《美丽人生》 ✅ 0️⃣5️⃣. 《阿甘正传》 ✅ 0️⃣6️⃣. 《奇遇人生 第一季》 0️⃣7️⃣. 《一本好书 1》 0️⃣8️⃣. 《一本好书 2》 0️⃣9️⃣. 《楚门的世界》 ✅ 1️⃣0️⃣. 《穿越时空的少女》 ✅ 1️⃣1️⃣. 《五等分的新娘 剧场版》 ✅ 1️⃣2️⃣. 《你的名字》 ✅ 1️⃣3️⃣. 《工作细胞》 ✅ ","date":"2022-01-01 09:00:00","objectID":"/ooooo-notes/2022%E5%B9%B4%E8%AE%A1%E5%88%92/:4:5","tags":["learning"],"title":"2022年学习计划","uri":"/ooooo-notes/2022%E5%B9%B4%E8%AE%A1%E5%88%92/"},{"categories":["随笔"],"content":" idea gradle project show duplicated file tree（project view and packages view） you must select idea as run test and building idea gradle 项目结构显示错误 ","date":"2022-01-01 08:00:00","objectID":"/ooooo-notes/idea-%E4%B8%AD-gradle-%E9%A1%B9%E7%9B%AE%E7%BB%93%E6%9E%84%E6%98%BE%E7%A4%BA%E9%94%99%E8%AF%AF/:0:0","tags":["resolution"],"title":"idea 中 gradle 项目结构显示错误","uri":"/ooooo-notes/idea-%E4%B8%AD-gradle-%E9%A1%B9%E7%9B%AE%E7%BB%93%E6%9E%84%E6%98%BE%E7%A4%BA%E9%94%99%E8%AF%AF/"},{"categories":["随笔"],"content":"1. 两台机器初始化设置 ","date":"2021-12-01 08:00:00","objectID":"/ooooo-notes/%E5%9C%A8-centos-%E7%B3%BB%E7%BB%9F%E4%B8%AD%E5%AE%89%E8%A3%85-k8s/:1:0","tags":["k8s","cloud native"],"title":"在 centos 上安装 k8s","uri":"/ooooo-notes/%E5%9C%A8-centos-%E7%B3%BB%E7%BB%9F%E4%B8%AD%E5%AE%89%E8%A3%85-k8s/"},{"categories":["随笔"],"content":"1.1 hostname 设置 hostnamectl ## 查看当前的hostname hostnamectl set-hostname centos1 ## 设置主机名为centos1, 在 192.168.130.131 上执行 hostnamectl set-hostname centos2 ## 设置主机名为centos1, 在 192.168.130.132 上执行 ","date":"2021-12-01 08:00:00","objectID":"/ooooo-notes/%E5%9C%A8-centos-%E7%B3%BB%E7%BB%9F%E4%B8%AD%E5%AE%89%E8%A3%85-k8s/:1:1","tags":["k8s","cloud native"],"title":"在 centos 上安装 k8s","uri":"/ooooo-notes/%E5%9C%A8-centos-%E7%B3%BB%E7%BB%9F%E4%B8%AD%E5%AE%89%E8%A3%85-k8s/"},{"categories":["随笔"],"content":"1.2 /etc/hosts 文件 (两个机器都需要) 192.168.1.8 ooooo 192.168.130.131 centos1 ## k8s master 192.168.130.132 centos2 ## k8s worker ","date":"2021-12-01 08:00:00","objectID":"/ooooo-notes/%E5%9C%A8-centos-%E7%B3%BB%E7%BB%9F%E4%B8%AD%E5%AE%89%E8%A3%85-k8s/:1:2","tags":["k8s","cloud native"],"title":"在 centos 上安装 k8s","uri":"/ooooo-notes/%E5%9C%A8-centos-%E7%B3%BB%E7%BB%9F%E4%B8%AD%E5%AE%89%E8%A3%85-k8s/"},{"categories":["随笔"],"content":"1.3 创建非 root 用户 (两个机器都需要) useradd ooooo -g ooooo ## 添加用户，两个机器都执行 passwd ooooo ## 修改用户密码，两个机器都执行 ","date":"2021-12-01 08:00:00","objectID":"/ooooo-notes/%E5%9C%A8-centos-%E7%B3%BB%E7%BB%9F%E4%B8%AD%E5%AE%89%E8%A3%85-k8s/:1:3","tags":["k8s","cloud native"],"title":"在 centos 上安装 k8s","uri":"/ooooo-notes/%E5%9C%A8-centos-%E7%B3%BB%E7%BB%9F%E4%B8%AD%E5%AE%89%E8%A3%85-k8s/"},{"categories":["随笔"],"content":"1.4 添加 yum 代理 (两个机器都需要) sudo vim /etc/yum.conf ## 编辑 yum 配置文件 proxy=http://ooooo:10800 ## 在文件中添加一行 ","date":"2021-12-01 08:00:00","objectID":"/ooooo-notes/%E5%9C%A8-centos-%E7%B3%BB%E7%BB%9F%E4%B8%AD%E5%AE%89%E8%A3%85-k8s/:1:4","tags":["k8s","cloud native"],"title":"在 centos 上安装 k8s","uri":"/ooooo-notes/%E5%9C%A8-centos-%E7%B3%BB%E7%BB%9F%E4%B8%AD%E5%AE%89%E8%A3%85-k8s/"},{"categories":["随笔"],"content":"1.5 安装 docker 服务 (两个机器都需要) 官方 docker 安装文档 参考文档安装 docker sudo vim /etc/docker/daemon.json ## 编辑 docker 配置文件， 添加下面 json 配置，这是因为 k8s 默认使用的 cgroup driver 是 systemd { \"exec-opts\": [\"native.cgroupdriver=systemd\"] } sudo systemctl enable --now docker.service ## 设置 docker 服务开机启动，并且现在启动 sudo systemctl status docker ## 查看 docker 服务的状态， 失败了，使用下一条命令查看日志 journalctl -xeu docker ## 查看 docker 日志服务 ","date":"2021-12-01 08:00:00","objectID":"/ooooo-notes/%E5%9C%A8-centos-%E7%B3%BB%E7%BB%9F%E4%B8%AD%E5%AE%89%E8%A3%85-k8s/:1:5","tags":["k8s","cloud native"],"title":"在 centos 上安装 k8s","uri":"/ooooo-notes/%E5%9C%A8-centos-%E7%B3%BB%E7%BB%9F%E4%B8%AD%E5%AE%89%E8%A3%85-k8s/"},{"categories":["随笔"],"content":"2. k8s 的 kubeadm 安装 (两台都需要) 官方 k8s 安装文档 参考文档检查服务器的状态是否可以安装 k8s 服务 ## 关闭 swap 分区 swapoff -a sudo echo vm.swappiness=0 \u003e\u003e /etc/sysctl.con ## 永久关闭 swap 分区， k8s 不能运行在有 swap 分区的机器上 free -h ## 查看 swap 分区是否关闭，显示 0 表示已关闭 ## 检查 br_netfilter 是否被加载，没有任何输出，表示没有加载 lsmod | grep br_netfilter sudo modprobe br_netfilter ## 加载 br_netfilter 模块 ## 配置网络 cat \u003c\u003cEOF | sudo tee /etc/modules-load.d/k8s.conf br_netfilter EOF cat \u003c\u003cEOF | sudo tee /etc/sysctl.d/k8s.conf net.bridge.bridge-nf-call-ip6tables = 1 net.bridge.bridge-nf-call-iptables = 1 EOF sudo sysctl --system 安装容器运行时(runtime),k8s 高版本采用自动检查方式,不用做任何处理 ## 添加 k8s 镜像仓库，在前面中，设置了 yum 代理 ## 在官方文档中多了 exclude=kubelet kubeadm kubectl ，这里去掉, 直接安装最新版本的 cat \u003c\u003cEOF | sudo tee /etc/yum.repos.d/kubernetes.repo [kubernetes] name=Kubernetes baseurl=http://mirrors.aliyun.com/kubernetes/yum/repos/kubernetes-el7-\\$basearch enabled=1 gpgcheck=1 gpgkey=http://mirrors.aliyun.com/kubernetes/yum/doc/rpm-package-key.gpg EOF ## 关闭 selinux sudo setenforce 0 sudo sed -i 's/^SELINUX=enforcing$/SELINUX=permissive/' /etc/selinux/config ## 安装 k8s 服务, --disableexcludes=kubernetes 表示排除 kubernetes 之外的镜像源 sudo yum install -y kubelet kubeadm kubectl --disableexcludes=kubernetes ## 设置 kubelet 开机启动，并且现在启动 ## 启动之后可能会报错，如果原因是 没有读取到 kubelet 的配置文件，这里可以不用管，稍后会重启这个服务 sudo systemctl enable --now kubelet sudo systemctl status kubelet ## 查看 kubelet 的状态 journalctl -xeu kubelet ## 查看 kubelet 的日志 ","date":"2021-12-01 08:00:00","objectID":"/ooooo-notes/%E5%9C%A8-centos-%E7%B3%BB%E7%BB%9F%E4%B8%AD%E5%AE%89%E8%A3%85-k8s/:2:0","tags":["k8s","cloud native"],"title":"在 centos 上安装 k8s","uri":"/ooooo-notes/%E5%9C%A8-centos-%E7%B3%BB%E7%BB%9F%E4%B8%AD%E5%AE%89%E8%A3%85-k8s/"},{"categories":["随笔"],"content":"3. 创建 k8s 集群 (两台都需要) 创建 k8s 集群官方文档 k8s pod network 插件文档 ## 执行 kubeadm init 命令， 在 k8s master 机器上执行，默认情况下， k8s 创建 pod 不会在 master 机器上 ## 重点注意: --pod-network-cidr=10.244.0.0/16 这个参数必须要有，没有的话安装 cni 会报错 ## 注意 preflight 的前置检查输出，可能需要添加 docker group，这个会输出有提示的命令 sudo kubeadm init --image-repository registry.aliyuncs.com/google_containers --apiserver-advertise-address=192.168.130.131 --pod-network-cidr=10.244.0.0/16 ## 执行命令之后，会有 kubeadm join 输出行 ## （分为 master-token 和 worker-token）， 类似于下面的命令，在 centos2 上执行 worker-join-token 的命令 sudo kubeadm join 192.168.130.131:6443 --token 8auvt0.zfw0ayr45d80q8pb \\ --discovery-token-ca-cert-hash sha256:efe854739efef5fbaf3f6e28c899481c8d7797c1997fc8315b921a9ede400ca8 ## 在机器上执行 kubeadm join 或者 kubeadm init 命令之后，重启 kubelet 服务 sudo systemctl restart kubelet sudo systemctl status kubelet ## 设置 kubectl 的配置文件， 为 $HOME/.kube/config mkdir -p $HOME/.kube sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config sudo chown $(id -u):$(id -g) $HOME/.kube/config ## 安装 pod network 插件, 这里使用 flannel 插件 kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml ## 查看 flannel 是否已经启动完成, cni 也启动成功 kubectl get pods -A ## 成功之后会有下面的服务， 都是 running 状态 kube-system coredns-7f6cbbb7b8-5hqt5 1/1 Running 15 (76m ago) 26h kube-system coredns-7f6cbbb7b8-lwdrv 1/1 Running 15 (76m ago) 26h kube-system etcd-centos1 1/1 Running 18 (76m ago) 26h kube-system kube-apiserver-centos1 1/1 Running 25 (76m ago) 26h kube-system kube-controller-manager-centos1 1/1 Running 12 (76m ago) 26h kube-system kube-flannel-ds-6lx7s 1/1 Running 6 (76m ago) 21h kube-system kube-flannel-ds-n5tfn 1/1 Running 6 (76m ago) 21h kube-system kube-proxy-78jrm 1/1 Running 8 (76m ago) 26h kube-system kube-proxy-wl5jg 1/1 Running 8 (76m ago) 26h kube-system kube-scheduler-centos1 1/1 Running 16 (76m ago) 26h ","date":"2021-12-01 08:00:00","objectID":"/ooooo-notes/%E5%9C%A8-centos-%E7%B3%BB%E7%BB%9F%E4%B8%AD%E5%AE%89%E8%A3%85-k8s/:3:0","tags":["k8s","cloud native"],"title":"在 centos 上安装 k8s","uri":"/ooooo-notes/%E5%9C%A8-centos-%E7%B3%BB%E7%BB%9F%E4%B8%AD%E5%AE%89%E8%A3%85-k8s/"},{"categories":["随笔"],"content":"在 ~\\.gradle 目录下新建文件 init.gradle, 内容如下 allprojects { repositories { mavenLocal() maven { name \"Alibaba\" ; url \"https://maven.aliyun.com/repository/public\" } maven { name \"Bstek\" ; url \"http://nexus.bsdn.org/content/groups/public/\" } } buildscript { repositories { maven { name \"Alibaba\" ; url 'https://maven.aliyun.com/repository/public' } maven { name \"Bstek\" ; url 'https://nexus.bsdn.org/content/groups/public/' } maven { name \"M2\" ; url 'https://plugins.gradle.org/m2/' } } } } ","date":"2021-01-02 08:00:00","objectID":"/ooooo-notes/gradle-%E5%85%A8%E5%B1%80%E8%AE%BE%E7%BD%AE%E4%BB%93%E5%BA%93%E9%95%9C%E5%83%8F/:0:0","tags":["resolution"],"title":"gradle 全局设置仓库镜像","uri":"/ooooo-notes/gradle-%E5%85%A8%E5%B1%80%E8%AE%BE%E7%BD%AE%E4%BB%93%E5%BA%93%E9%95%9C%E5%83%8F/"},{"categories":["计划"],"content":"0、持续学习者 Talk is cheap. Show me the code. 英语比编程简单。 学习和实践要平衡。 学会和时间做朋友。 学会投资，学会理财。 学会先做减法，再做加法。 学英语很重要，学英语很重要，学英语很重要。 说明： ⭕ 进行中 ✅ 已完成 ❌ 已废弃 ❓ 有必要 ❗ 重要性 📝 记笔记 🖊 写代码 ","date":"2021-01-01 09:00:00","objectID":"/ooooo-notes/2021%E5%B9%B4%E8%AE%A1%E5%88%92/:1:0","tags":["learning"],"title":"2021年学习计划","uri":"/ooooo-notes/2021%E5%B9%B4%E8%AE%A1%E5%88%92/"},{"categories":["计划"],"content":"1、关于英语 听说读写，目前的学习重点是日常沟通，所以放弃背单词。 计划 🎉： 目前我已经背单词 518 多天，我将会继续背单词（墨墨背单词）。 ❌ 学习《新概念英语一》 ✅ 看 Spring Framework。 学习《赖世雄美语音标》 学习《新概念英语二》 目前状态: 《新概念英语二》。 ","date":"2021-01-01 09:00:00","objectID":"/ooooo-notes/2021%E5%B9%B4%E8%AE%A1%E5%88%92/:2:0","tags":["learning"],"title":"2021年学习计划","uri":"/ooooo-notes/2021%E5%B9%B4%E8%AE%A1%E5%88%92/"},{"categories":["计划"],"content":"2、关于技术 计划 🎉： 只记录自己认为有用的笔记。 ","date":"2021-01-01 09:00:00","objectID":"/ooooo-notes/2021%E5%B9%B4%E8%AE%A1%E5%88%92/:3:0","tags":["learning"],"title":"2021年学习计划","uri":"/ooooo-notes/2021%E5%B9%B4%E8%AE%A1%E5%88%92/"},{"categories":["计划"],"content":"1、书籍 0️⃣1️⃣. 《Java 性能优化权威指南》 0️⃣2️⃣. 《Netty 实战》 ✅ 0️⃣3️⃣. 《程序员面试金典（第6版）》 ⭕ 0️⃣4️⃣. 《图解TCP/IP》 ✅ 0️⃣5️⃣. 《Spring 源码深度解析》 ✅ 0️⃣6️⃣. 《深入理解 Java 虚拟机（第3版）》 ⭕ 0️⃣7️⃣. 《Spring Cloud 微服务：入门、实战与进阶》 0️⃣8️⃣. 《Spring Cloud Alibaba 微服务原理与实战》 ✅ 0️⃣9️⃣. 《深入理解 Apache Dubbo 与实战》 ✅ 1️⃣0️⃣. 《arthas》 1️⃣1️⃣. 《Java 并发编程实战》 1️⃣2️⃣. 《深入理解 Kafka：核心设计与实践原理》 1️⃣3️⃣. 《Spring 5核心原理与30个类手写实战》 ❌ 1️⃣4️⃣. 《Netty 4核心原理与手写RPC框架实战》 ❌ 1️⃣5️⃣. 《从零开始学架构》 1️⃣6️⃣. 《高可用可伸缩微服务架构》 1️⃣7️⃣. 《实战Java虚拟机》 ❓ 1️⃣9️⃣. 《分布式一致性算法开发实战》 2️⃣0️⃣. 《Go Web 编程》 2️⃣1️⃣. 《consul》 ❌ 2️⃣2️⃣. 《Java 异步编程实战》 ❓ 2️⃣3️⃣. 《Effective C++》 2️⃣4️⃣. More Effective C++ 2️⃣5️⃣. 深度探索C++对象模型 2️⃣6️⃣. 《深入浅出 Docker》 ✅ 2️⃣7️⃣. 《码出高效：Java开发手册》 2️⃣8️⃣. 《Go 专家编程》 2️⃣9️⃣. 《流畅的 Python》 3️⃣0️⃣. 《wireshark网络分析的艺术》 3️⃣2️⃣. 《RocketMQ技术内幕》 ✅ 3️⃣3️⃣. 《RocketMQ分布式消息中间件：核心原理与最佳实践》 ✅ 3️⃣4️⃣. 《RocketMQ实战与原理解析》 ✅ 3️⃣5️⃣. 《Vim实用技巧（第2版）》 ⭕ 3️⃣6️⃣. 《RocketMQ技术内幕 第二版》 3️⃣7️⃣. 《Kubernetes in Action中文版》 ⭕ 3️⃣8️⃣. 《rocketmq》 ⭕ 3️⃣9️⃣. 《spring cloud stream》 ✅ 3️⃣9️⃣. 《dubbo》 ⭕ 4️⃣0️⃣. 《pulsar》 ⭕ 4️⃣0️⃣. 《nsq》 4️⃣0️⃣. 《eventing》 4️⃣0️⃣. 《serving》 4️⃣1️⃣. 《Activiti》 ✅ ","date":"2021-01-01 09:00:00","objectID":"/ooooo-notes/2021%E5%B9%B4%E8%AE%A1%E5%88%92/:3:1","tags":["learning"],"title":"2021年学习计划","uri":"/ooooo-notes/2021%E5%B9%B4%E8%AE%A1%E5%88%92/"},{"categories":["计划"],"content":"2、文档 0️⃣1️⃣. 《深入拆解 Java 虚拟机》 0️⃣2️⃣. 通读 Spring 官网，+实践+代码+笔记。 0️⃣1️⃣ spring-cloud-netflix-eureka-clients 0️⃣2️⃣ spring-cloud-netflix-eureka-server 0️⃣3️⃣ spring-cloud-task ✅ 0️⃣3️⃣. 学习 Go 语言，通读 Go 官网，+实践+代码+笔记。 0️⃣1️⃣. 《A Tour of Go》 ✅ 0️⃣2️⃣. 《Tutorial: Create a module》 ✅ 0️⃣3️⃣. 《Writing Web Applications》 ✅ 0️⃣4️⃣. 《How to write Go code》 0️⃣5️⃣. 《Effective Go》 0️⃣4️⃣. Protobuf javaTutorial ✅ 0️⃣5️⃣. Go语言核心36讲 ✅ 0️⃣6️⃣. 消息队列高手课 ✅ 0️⃣7️⃣. 从 0 开始带你成为消息中间件实战高手 ✅ 0️⃣8️⃣. 从 0 开始带你成为JVM实战高手 ⭕ ","date":"2021-01-01 09:00:00","objectID":"/ooooo-notes/2021%E5%B9%B4%E8%AE%A1%E5%88%92/:3:2","tags":["learning"],"title":"2021年学习计划","uri":"/ooooo-notes/2021%E5%B9%B4%E8%AE%A1%E5%88%92/"},{"categories":["计划"],"content":"3、视频 0️⃣1️⃣. 《玩转算法系列–图论精讲》 0️⃣2️⃣. 《玩转算法面试》 ⭕ 0️⃣3️⃣. 《利用Go优越的性能设计与实现高性能企业级微服务网关》 ⭕ 0️⃣3️⃣. 《看的见的算法》 0️⃣4️⃣. 《极客时间– Java 进阶训练营》 ❌ 0️⃣5️⃣. 《极客时间– go 进阶训练营》 ❌ 0️⃣5️⃣. 《极客时间– 算法进阶训练营》 ⭕ ","date":"2021-01-01 09:00:00","objectID":"/ooooo-notes/2021%E5%B9%B4%E8%AE%A1%E5%88%92/:3:3","tags":["learning"],"title":"2021年学习计划","uri":"/ooooo-notes/2021%E5%B9%B4%E8%AE%A1%E5%88%92/"},{"categories":["计划"],"content":"4、算法 0️⃣1️⃣. 每周至少 5 道 Leetcode。 ⭕ 0️⃣2️⃣. leetcode 全站排名1000以内。 ⭕ 0️⃣3️⃣. leetcode 周赛全国排名2000以内。 ⭕ 0️⃣4️⃣. leetcode 周赛全球排名10000以内。 ⭕ ","date":"2021-01-01 09:00:00","objectID":"/ooooo-notes/2021%E5%B9%B4%E8%AE%A1%E5%88%92/:3:4","tags":["learning"],"title":"2021年学习计划","uri":"/ooooo-notes/2021%E5%B9%B4%E8%AE%A1%E5%88%92/"},{"categories":["计划"],"content":"4、关于其他 ","date":"2021-01-01 09:00:00","objectID":"/ooooo-notes/2021%E5%B9%B4%E8%AE%A1%E5%88%92/:4:0","tags":["learning"],"title":"2021年学习计划","uri":"/ooooo-notes/2021%E5%B9%B4%E8%AE%A1%E5%88%92/"},{"categories":["计划"],"content":"1、书籍 🎉 0️⃣1️⃣. 《聪明的投资者》 0️⃣2️⃣. 《一万小时天才理论》 ⭕ 0️⃣3️⃣. 《番茄工作法图解》 ⭕ 0️⃣4️⃣. 《三体》 ✅ 0️⃣5️⃣. 《三体Ⅱ》 0️⃣6️⃣. 《三体Ⅲ》 0️⃣7️⃣. 《非暴力沟通》 ⭕ 0️⃣8️⃣. 《管理你的每一天》 ⭕ 0️⃣9️⃣. 《原则》 1️⃣0️⃣. 《思考，快与慢》 1️⃣1️⃣. 《关键对话》 1️⃣2️⃣. 《当下的启蒙》 1️⃣3️⃣. 《把时间当作朋友》 ⭕ 1️⃣3️⃣. 《活着》 ✅ 1️⃣4️⃣. 《白夜行》 ⭕ 1️⃣5️⃣. 《亲密关系：通往灵魂的桥梁》 ⭕ ","date":"2021-01-01 09:00:00","objectID":"/ooooo-notes/2021%E5%B9%B4%E8%AE%A1%E5%88%92/:4:1","tags":["learning"],"title":"2021年学习计划","uri":"/ooooo-notes/2021%E5%B9%B4%E8%AE%A1%E5%88%92/"},{"categories":["计划"],"content":"2、选读 🎉 0️⃣1️⃣. 《C++ Prime（第5版）》 ❌ 0️⃣2️⃣. 《算法 4》 0️⃣3️⃣. 《JMC 工具(Java Mission Control)》 0️⃣4️⃣. 《ZooKeeper》 0️⃣5️⃣. 《MIT 高级数据课程》 0️⃣6️⃣. 《Resilience4j》 ❓ 0️⃣7️⃣. 《Google Guava》 ❓ 0️⃣8️⃣. 《Kafka 官网》 0️⃣9️⃣. 《Spring Security 实战》 ❌ 1️⃣0️⃣. 《Jvisualvm》 1️⃣1️⃣. 《深入理解 Nginx（第 2 版）》 ❓ 1️⃣2️⃣. 《分布式服务框架：原理与实践》 1️⃣3️⃣. 《chrome-devtools》 ❌ 1️⃣4️⃣. 《人性的弱点》 1️⃣5️⃣. 《深入剖析 Tomcat》 1️⃣6️⃣. 《Java 编程方法论：响应式Spring Reactor 3设计与实现》 1️⃣7️⃣. 《数据密集型应用系统设计》 1️⃣8️⃣. 《Go 程序设计语言》 ❌ 1️⃣9️⃣. 《机器学习实战：基于Scikit-Learn、Keras和TensorFlow》 2️⃣0️⃣. 《Python深度学习》 2️⃣1️⃣. 《当我谈跑步时，我谈些什么》 ","date":"2021-01-01 09:00:00","objectID":"/ooooo-notes/2021%E5%B9%B4%E8%AE%A1%E5%88%92/:4:2","tags":["learning"],"title":"2021年学习计划","uri":"/ooooo-notes/2021%E5%B9%B4%E8%AE%A1%E5%88%92/"},{"categories":["计划"],"content":"3、尝试 🎉 0️⃣1️⃣. 学会使用尤克里里弹奏 ","date":"2021-01-01 09:00:00","objectID":"/ooooo-notes/2021%E5%B9%B4%E8%AE%A1%E5%88%92/:4:3","tags":["learning"],"title":"2021年学习计划","uri":"/ooooo-notes/2021%E5%B9%B4%E8%AE%A1%E5%88%92/"},{"categories":["计划"],"content":"4、了解 🎉 0️⃣1️⃣. 《hugo》 ✅ ","date":"2021-01-01 09:00:00","objectID":"/ooooo-notes/2021%E5%B9%B4%E8%AE%A1%E5%88%92/:4:4","tags":["learning"],"title":"2021年学习计划","uri":"/ooooo-notes/2021%E5%B9%B4%E8%AE%A1%E5%88%92/"},{"categories":["计划"],"content":"5、娱乐 0️⃣1️⃣. 《傲慢与偏见》 ✅ 0️⃣2️⃣. 《肖申克的救赎》 ✅ 0️⃣3️⃣. 《志明与春娇》 ✅ 0️⃣4️⃣. 《春娇与志明》 ✅ 0️⃣4️⃣. 《春娇救志明》 ✅ ","date":"2021-01-01 09:00:00","objectID":"/ooooo-notes/2021%E5%B9%B4%E8%AE%A1%E5%88%92/:4:5","tags":["learning"],"title":"2021年学习计划","uri":"/ooooo-notes/2021%E5%B9%B4%E8%AE%A1%E5%88%92/"},{"categories":["计划"],"content":"6. 总结 今年大部分的时间都放在阅读源码上，导致很多的书籍没有看完，也放弃了英语学习。 年初定的计划在实际执行过程中，两次改变了学习重点， 1. mq 源码 2. k8s 源码。 认真思考指定 2022 的计划 ","date":"2021-01-01 09:00:00","objectID":"/ooooo-notes/2021%E5%B9%B4%E8%AE%A1%E5%88%92/:4:6","tags":["learning"],"title":"2021年学习计划","uri":"/ooooo-notes/2021%E5%B9%B4%E8%AE%A1%E5%88%92/"},{"categories":["随笔"],"content":"在 conf/server.xml 中的 Host标签添加 \u003cHost name=\"localhost\" appBase=\"webapps\" unpackWARs=\"true\" autoDeploy=\"true\"\u003e \u003cValve className=\"org.apache.catalina.valves.AccessLogValve\" directory=\"logs\" prefix=\"localhost_access_log\" suffix=\".txt\" pattern=\"%h %l %u %t \u0026quot;%r\u0026quot; %s %b\" /\u003e // 这是新加的 \u003cValve className=\"org.apache.catalina.valves.ErrorReportValve\" errorCode.400=\"webapps/ROOT/error.jsp\" errorCode.0=\"webapps/ROOT/error.jsp\" showReport=\"false\" showServerInfo=\"false\" /\u003e // 这是新加的 \u003c/Host\u003e 上面的 error.jsp 放在 webapps/ROOT/ 参考 https://stackoverflow.com/questions/52814582/tomcat-is-not-redirecting-to-400-bad-request-custom-error-page 参考 https://tomcat.apache.org/tomcat-9.0-doc/config/valve.html#Error_Report_Valve ","date":"2021-01-01 08:00:00","objectID":"/ooooo-notes/tomcat-%E8%87%AA%E5%AE%9A%E4%B9%89%E9%94%99%E8%AF%AF%E9%A1%B5/:0:0","tags":["resolution"],"title":"tomcat 自定义错误页","uri":"/ooooo-notes/tomcat-%E8%87%AA%E5%AE%9A%E4%B9%89%E9%94%99%E8%AF%AF%E9%A1%B5/"},{"categories":["随笔"],"content":" 重写方法XMLHttpRequest.prototype.send XMLHttpRequest.prototype._send = XMLHttpRequest.prototype.send XMLHttpRequest.prototype.send = function (params) { var attached_params = mdcUtil.MDC_DEVICE_ID + \"=\" + mdcUtil.getMdcDeviceId(); if (params) { params += \"\u0026\" + attached_params; } else { params = attached_params; } return this._send(params) } ","date":"2020-01-05 08:00:00","objectID":"/ooooo-notes/%E5%9C%A8-js-%E4%B8%AD%E7%BB%9F%E4%B8%80%E8%AE%BE%E7%BD%AE%E8%AF%B7%E6%B1%82%E5%8F%82%E6%95%B0%E7%9A%84%E5%8F%A6%E4%B8%80%E7%A7%8D%E6%96%B9%E6%B3%95/:0:0","tags":["resolution"],"title":"在 js 中统一设置请求参数的另一种方法","uri":"/ooooo-notes/%E5%9C%A8-js-%E4%B8%AD%E7%BB%9F%E4%B8%80%E8%AE%BE%E7%BD%AE%E8%AF%B7%E6%B1%82%E5%8F%82%E6%95%B0%E7%9A%84%E5%8F%A6%E4%B8%80%E7%A7%8D%E6%96%B9%E6%B3%95/"},{"categories":["随笔"],"content":" 编辑运行配置，设置环境变量中的工作目录为当前模块目录。 ","date":"2020-01-04 08:00:00","objectID":"/ooooo-notes/idea-%E5%A4%9A%E6%A8%A1%E5%9D%97%E9%A1%B9%E7%9B%AE%E5%90%AF%E5%8A%A8webapp-%E7%9B%AE%E5%BD%95%E4%B8%8B%E6%96%87%E4%BB%B6%E8%AE%BF%E9%97%AE%E4%B8%8D%E5%88%B0/:0:0","tags":["resolution"],"title":"idea 多模块项目启动，webapp 目录下文件访问不到","uri":"/ooooo-notes/idea-%E5%A4%9A%E6%A8%A1%E5%9D%97%E9%A1%B9%E7%9B%AE%E5%90%AF%E5%8A%A8webapp-%E7%9B%AE%E5%BD%95%E4%B8%8B%E6%96%87%E4%BB%B6%E8%AE%BF%E9%97%AE%E4%B8%8D%E5%88%B0/"},{"categories":["随笔"],"content":" 去掉属性 required，添加 rules 规则 { required: true, message: '请输入姓名', trigger: 'blur' } ","date":"2020-01-03 08:00:00","objectID":"/ooooo-notes/element-ui-%E4%B8%AD-el-form-item-%E6%A0%A1%E9%AA%8C%E5%87%BA%E7%8E%B0%E8%8B%B1%E6%96%87/:0:0","tags":["resolution"],"title":"element-ui 中 el-form-item 校验出现英文","uri":"/ooooo-notes/element-ui-%E4%B8%AD-el-form-item-%E6%A0%A1%E9%AA%8C%E5%87%BA%E7%8E%B0%E8%8B%B1%E6%96%87/"},{"categories":["随笔"],"content":"问题 If you see that the storm process is getting crashed even though you have enough memory (swap/free) available then you should also check the “/proc/sys/vm/overcommit_memory” This switch knows 3 different settings: =\u003e 0: The Linux kernel is free to over commit memory(this is the default), a heuristic algorithm is applied to figure out if enough memory is available. =\u003e 1: The Linux kernel will always over commit memory, and never check if enough memory is available. This increases the risk of out-of-memory situations, but also improves memory-intensive workloads. =\u003e 2: The Linux kernel will not over commit memory, and only allocate as much memory as defined in over commit_ratio. As sometimes OS kills /crashes a process due to a system OS setting, the system OS memory overcommit setting was 2 (when it should have been set to 0) - ","date":"2020-01-02 08:00:00","objectID":"/ooooo-notes/java-%E5%87%BA%E7%8E%B0%E5%86%85%E5%AD%98%E5%88%86%E9%85%8D%E5%A4%B1%E8%B4%A5/:0:0","tags":["resolution"],"title":"java 出现内存分配失败","uri":"/ooooo-notes/java-%E5%87%BA%E7%8E%B0%E5%86%85%E5%AD%98%E5%88%86%E9%85%8D%E5%A4%B1%E8%B4%A5/"},{"categories":["计划"],"content":"0、持续学习者 Talk is cheap. Show me the code. 英语比编程简单。 学习和实践要平衡。 学会和时间做朋友。 学会投资，学会理财。 学会先做减法，再做加法。 学英语很重要，学英语很重要，学英语很重要。 说明： ⭕ 进行中 ✅ 已完成 ❌ 已废弃 ❓ 有必要 ❗ 重要 📝 记笔记* 🖊 写代码 ","date":"2020-01-01 09:00:00","objectID":"/ooooo-notes/2020%E5%B9%B4%E8%AE%A1%E5%88%92/:1:0","tags":["learning"],"title":"2020年学习计划","uri":"/ooooo-notes/2020%E5%B9%B4%E8%AE%A1%E5%88%92/"},{"categories":["计划"],"content":"1、关于英语 听说读写，其中最容易的应该是读，然后再是写，我有很大的信心能在两三年之内（2022）正常读写。剩余就是听和说了，目前对我真的太难了。 计划 🎉： 目前我已经背单词 430 多天，我将会继续背单词（墨墨背单词）😄 。 看 YouTube - English with Lucy。 ❓ 看 Spring。 看 Friends。 ❓ 目前状态: 扇贝阅读。 ❗ ","date":"2020-01-01 09:00:00","objectID":"/ooooo-notes/2020%E5%B9%B4%E8%AE%A1%E5%88%92/:2:0","tags":["learning"],"title":"2020年学习计划","uri":"/ooooo-notes/2020%E5%B9%B4%E8%AE%A1%E5%88%92/"},{"categories":["计划"],"content":"2、关于技术 只记录自己认为有用的笔记。 计划 🎉： ","date":"2020-01-01 09:00:00","objectID":"/ooooo-notes/2020%E5%B9%B4%E8%AE%A1%E5%88%92/:3:0","tags":["learning"],"title":"2020年学习计划","uri":"/ooooo-notes/2020%E5%B9%B4%E8%AE%A1%E5%88%92/"},{"categories":["计划"],"content":"1. 书籍 0️⃣1️⃣. 《Java 并发编程的艺术》 ✅ 0️⃣2️⃣. 《Redis 开发与运维》 ✅ 0️⃣3️⃣. 《剑指 Offer》 ✅ 0️⃣4️⃣. 《Effective Java中文版（第3版）》 ✅ 0️⃣5️⃣. 《Java 8 函数式编程 》 ✅ 0️⃣6️⃣. 《算法图解》 ✅ 0️⃣7️⃣. 《设计模式》 ✅ 0️⃣8️⃣. 《图解 HTTP 》 ✅ 0️⃣9️⃣. 《Spring Boot编程思想（核心篇）》 ✅ ","date":"2020-01-01 09:00:00","objectID":"/ooooo-notes/2020%E5%B9%B4%E8%AE%A1%E5%88%92/:3:1","tags":["learning"],"title":"2020年学习计划","uri":"/ooooo-notes/2020%E5%B9%B4%E8%AE%A1%E5%88%92/"},{"categories":["计划"],"content":"2. 文档 0️⃣1️⃣. 《MySQL 实战 45 讲》 ✅ 0️⃣2️⃣. 《Kafka 核心技术与实战》 ✅ 0️⃣3️⃣. 《Java 核心技术 36 讲》 ✅ 0️⃣4️⃣. 《Java 并发编程实战》 ✅ ","date":"2020-01-01 09:00:00","objectID":"/ooooo-notes/2020%E5%B9%B4%E8%AE%A1%E5%88%92/:3:2","tags":["learning"],"title":"2020年学习计划","uri":"/ooooo-notes/2020%E5%B9%B4%E8%AE%A1%E5%88%92/"},{"categories":["计划"],"content":"3. 视频 0️⃣1️⃣. 《算法面试通关 40 讲》 ✅ 0️⃣2️⃣. 《玩转算法系列–玩转数据结构 Java 版》 ✅ 0️⃣3️⃣. 《算法与数据结构-综合提升 C++ 版》 ✅ 0️⃣4️⃣. 《玩转 Spring 全家桶》 ✅ 0️⃣5️⃣. 《Go 语言从入门到实战》 ✅ ","date":"2020-01-01 09:00:00","objectID":"/ooooo-notes/2020%E5%B9%B4%E8%AE%A1%E5%88%92/:3:3","tags":["learning"],"title":"2020年学习计划","uri":"/ooooo-notes/2020%E5%B9%B4%E8%AE%A1%E5%88%92/"},{"categories":["计划"],"content":"4. 通读 Spring 官网 0️⃣1️⃣ spring-cloud-stream ✅ 0️⃣2️⃣ spring-cloud-netflix-zuul ✅ ","date":"2020-01-01 09:00:00","objectID":"/ooooo-notes/2020%E5%B9%B4%E8%AE%A1%E5%88%92/:3:4","tags":["learning"],"title":"2020年学习计划","uri":"/ooooo-notes/2020%E5%B9%B4%E8%AE%A1%E5%88%92/"},{"categories":["计划"],"content":"4、关于其他 阅读，+笔记 0️⃣1️⃣. 《指数基金投资指南》 ✅ 0️⃣2️⃣. 《富爸爸穷爸爸》 ✅ 0️⃣3️⃣. 《解读基金》 ✅ 0️⃣4️⃣. 《富爸爸财务自由之路》 ✅ 0️⃣5️⃣. 《克莱因壶》 ✅ ","date":"2020-01-01 09:00:00","objectID":"/ooooo-notes/2020%E5%B9%B4%E8%AE%A1%E5%88%92/:4:0","tags":["learning"],"title":"2020年学习计划","uri":"/ooooo-notes/2020%E5%B9%B4%E8%AE%A1%E5%88%92/"},{"categories":["计划"],"content":"5、选读 🎉 0️⃣1️⃣. 《阿里巴巴 Java 开发手册》 ✅ 0️⃣2️⃣. 《Spring 实战（第5版 ）》 ✅ 0️⃣3️⃣. 《Cloud Native Java》 ✅ 0️⃣4️⃣. 《MyBatis 技术内幕》 ✅ 0️⃣5️⃣. 《看透 Spring MVC》 ✅ 0️⃣6️⃣. 《Redis 深度历险：核心原理与应用实践》 ✅ 0️⃣7️⃣. 《Offer来了：Java面试核心知识点精讲（原理篇）》 ✅ 0️⃣8️⃣. 《Offer来了：Java面试核心知识点精讲（框架篇）》 ✅ ","date":"2020-01-01 09:00:00","objectID":"/ooooo-notes/2020%E5%B9%B4%E8%AE%A1%E5%88%92/:5:0","tags":["learning"],"title":"2020年学习计划","uri":"/ooooo-notes/2020%E5%B9%B4%E8%AE%A1%E5%88%92/"},{"categories":["随笔"],"content":" 执行命令 rm -rf ~/.zcompdump* ","date":"2020-01-01 08:00:00","objectID":"/ooooo-notes/zsh-%E6%B7%BB%E5%8A%A0%E6%8F%92%E4%BB%B6%E5%90%8E%E4%B8%8D%E7%94%9F%E6%95%88/:0:0","tags":["resolution"],"title":"zsh 添加插件后，不生效","uri":"/ooooo-notes/zsh-%E6%B7%BB%E5%8A%A0%E6%8F%92%E4%BB%B6%E5%90%8E%E4%B8%8D%E7%94%9F%E6%95%88/"},{"categories":null,"content":"1、Redis 特性 ","date":"0001-01-01 00:00:00","objectID":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/01/:1:0","tags":null,"title":"","uri":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/01/"},{"categories":null,"content":"1、速度快 Redis 的所有数据都是放在内存中的。 Redis 是 C 语言实现的。 Redis 使用单线程架构，避免多线程环境上下文切换。 ","date":"0001-01-01 00:00:00","objectID":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/01/:1:1","tags":null,"title":"","uri":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/01/"},{"categories":null,"content":"2、基于键值对的数据结构服务器 Redis 主要提供五种基本数据结构：string（字符串）、hash（哈希）、list（列表）、set（集合）、zset（有序集合），还提供 Bitmaps（位图）、HyperLogLog（基数统计算法）、GEO（地理位置）高级数据结构。 ","date":"0001-01-01 00:00:00","objectID":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/01/:1:2","tags":null,"title":"","uri":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/01/"},{"categories":null,"content":"3、丰富的功能 提供了键过期功能，可以用来实现缓存。 提供了发布订阅功能，可以用来实现消息系统。 支持 Lua 脚本功能，可以利用 Lua 创造新的 Redis 命令。 提供了简单事务功能。 提供了 Pipeline（流水线）功能。 ","date":"0001-01-01 00:00:00","objectID":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/01/:1:3","tags":null,"title":"","uri":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/01/"},{"categories":null,"content":"4、简单稳定 早期的 Redis 源码只有两万行，3.0 版本后添加了集群特性，代码增到 5 万行左右，Redis 自己实现了事件处理功能。 ","date":"0001-01-01 00:00:00","objectID":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/01/:1:4","tags":null,"title":"","uri":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/01/"},{"categories":null,"content":"5、客户端语言多 Redis 提供了简单的 TCP 通信协议，主流的编程语言都有其客户端实现。 ","date":"0001-01-01 00:00:00","objectID":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/01/:1:5","tags":null,"title":"","uri":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/01/"},{"categories":null,"content":"6、持久化 Redis 提供了 AOF 、 RDB 两种持久化方式。 ","date":"0001-01-01 00:00:00","objectID":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/01/:1:6","tags":null,"title":"","uri":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/01/"},{"categories":null,"content":"7、主从复制、高可用和分布式 Redis Sentinel 、Redis Cluster ","date":"0001-01-01 00:00:00","objectID":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/01/:1:7","tags":null,"title":"","uri":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/01/"},{"categories":null,"content":"2、Redis 使用场景 ","date":"0001-01-01 00:00:00","objectID":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/01/:2:0","tags":null,"title":"","uri":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/01/"},{"categories":null,"content":"Redis 可以做什么 缓存。（ Redis 提供了键过期时间设置） 排行榜系统。（ Redis 提供了列表 list 和有序集合 set 数据结构） 计数器应用。（ Redis 提供了计数功能 incr、decr ） 社交网络。（赞/踩、粉丝、共同好友/喜好） 消息队列。（ Redis 提供了列表 list 的 rpush, blpop 命令 ） ","date":"0001-01-01 00:00:00","objectID":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/01/:2:1","tags":null,"title":"","uri":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/01/"},{"categories":null,"content":"Redis 不可以做什么 Redis 基于内存，不能做存储。 避免用 Redis 来缓存冷数据。 ","date":"0001-01-01 00:00:00","objectID":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/01/:2:2","tags":null,"title":"","uri":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/01/"},{"categories":null,"content":"1、预备 ","date":"0001-01-01 00:00:00","objectID":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/02/:1:0","tags":null,"title":"","uri":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/02/"},{"categories":null,"content":"1、全局命令 help HELP command 查看所有的key KEYS * 键总数 DBSIZE 检查键是否存在 EXISTS key 删除键 DEL key 键过期 EXPIRE key sencond ttl 返回过期时间 TTL key \u003e0: 剩余过期时间 -1: 没有设置过期时间 -2: 键不存在 key的数据结构类型 TYPE key ","date":"0001-01-01 00:00:00","objectID":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/02/:1:1","tags":null,"title":"","uri":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/02/"},{"categories":null,"content":"2、数据结构与内部编码 每种数据结构都有自己底层的内部编码实现，通过命令 OBJECT ENCODING key 来查看。 string 内部编码： raw、int、embstr hash 内部编码： ziplist、hashtable list 内部编码： ziplist、linkedlist、quicklist set 内部编码： intset、hashtable zset 内部编码： ziplist、skiplist ","date":"0001-01-01 00:00:00","objectID":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/02/:1:2","tags":null,"title":"","uri":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/02/"},{"categories":null,"content":"2、string ","date":"0001-01-01 00:00:00","objectID":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/02/:2:0","tags":null,"title":"","uri":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/02/"},{"categories":null,"content":"1、常用命令 设置值、获取值 SET key value [expiration EX seconds|PX milliseconds] [NX|XX] GET key 说明： NX：不存在 key，才设置成功。同命令 SETNX key value XX：存在 key，才设置成功。 批量设置、批量获取 MSET key value [key value ...] MGET key [key ...] 说明： 批量操作可以减少网络时间。 计数 INCR key INCRBY key increment INCRBYFLOAT key increment DECR key DECRBY key decrement ","date":"0001-01-01 00:00:00","objectID":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/02/:2:1","tags":null,"title":"","uri":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/02/"},{"categories":null,"content":"2、不常用的命令 追加值 APPEND key value 字符串长度 STRLEN key 设置并返回原值 GETSET key value 设置指定位置的字符 SETRANGE key offset value 获取部分字符串 GETRANGE key start end ","date":"0001-01-01 00:00:00","objectID":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/02/:2:2","tags":null,"title":"","uri":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/02/"},{"categories":null,"content":"3、内部编码 字符串内部编码有三种： int：8 个字节的长整型 embstr：小于等于 39 个字节的字符串 raw：大于 39 个字节的字符串 ","date":"0001-01-01 00:00:00","objectID":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/02/:2:3","tags":null,"title":"","uri":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/02/"},{"categories":null,"content":"4、使用场景 缓存 （网站请求数据缓存） 计数 （网站的浏览数和播放数） 共享 Session （用户登录信息） 限速 （验证码接口） ","date":"0001-01-01 00:00:00","objectID":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/02/:2:4","tags":null,"title":"","uri":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/02/"},{"categories":null,"content":"3、hash ","date":"0001-01-01 00:00:00","objectID":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/02/:3:0","tags":null,"title":"","uri":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/02/"},{"categories":null,"content":"1、命令 设置值、获取值 HSET key field value HGET key field 删除 field HDEL key field [field ...] 计算 field 的个数 HLEN key 批量设置、批量获取 HMSET key field value [field value ...] HMGET key field [field ...] 是否存在 field HEXISTS key field 获取所有的 field HKEYS key 获取所有的 value HVALS key 获取所有的 field-value HGETALL key 说明： field 个数比较多时，会阻塞 redis。 计数 HINCRBY key field increment HINCRBYFLOAT key field increment 获取 value 长度 HSTRLEN key field ","date":"0001-01-01 00:00:00","objectID":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/02/:3:1","tags":null,"title":"","uri":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/02/"},{"categories":null,"content":"2、内部编码 ziplist（压缩表）：元素个数小于 hash-max-ziplist-entries = 512 ，同时 value 小于 hash-max-ziplist-value = 64，就使用 ziplist， 配置参数在 redis.conf 中。 hashtable（哈希表）：无法满足 ziplist 的条件，会使用 hashtable。 ","date":"0001-01-01 00:00:00","objectID":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/02/:3:2","tags":null,"title":"","uri":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/02/"},{"categories":null,"content":"4、list ","date":"0001-01-01 00:00:00","objectID":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/02/:4:0","tags":null,"title":"","uri":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/02/"},{"categories":null,"content":"1、命令 添加 RPUSH key value [value ...] # 右边添加 LPUSH key value [value ...] # 左边添加 LINSERT key BEFORE|AFTER pivot value # 指定位置插入 查询 LRANGE key start stop # 范围为[start, stop], 查询所有是 start = 0, stop = -1 LLEN key # 列表长度 删除 LPOP key # 左边弹出 RPOP key # 右边弹出 LREM key count value ## 删除 count 个 value 值, count \u003e 0,从左边删除；count \u003c 0,从右边删除；count = 0, 删除所有 LTRIM key start stop # 只保留[start, stop]的元素 修改 LSET key index value # 设置指定索引的值 阻塞 BLPOP key [key ...] timeout # 从左边弹出元素，如果为空，则阻塞 BRPOP key [key ...] timeout # 从右边弹出元素，如果为空，则阻塞 # timeout：阻塞时间。多个 key, 从左扫描。 ","date":"0001-01-01 00:00:00","objectID":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/02/:4:1","tags":null,"title":"","uri":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/02/"},{"categories":null,"content":"2、内部编码 ziplist（压缩表）：元素个数小于 list-max-ziplist-entries = 512 ，同时 value 小于 list-max-ziplist-value = 64，就使用 ziplist， 配置参数在 redis.conf 中。 linkedlist（链表）：无法满足 ziplist 的条件，会使用 linkedlist。 ","date":"0001-01-01 00:00:00","objectID":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/02/:4:2","tags":null,"title":"","uri":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/02/"},{"categories":null,"content":"5、set ","date":"0001-01-01 00:00:00","objectID":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/02/:5:0","tags":null,"title":"","uri":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/02/"},{"categories":null,"content":"1、集合内操作 添加 SADD key member [member ...] 删除 SREM key member [member ...] 计算元素个数 SCARD key 是否在集合中 SISMEMBER key member # 随机返回 count 个元素，不会删除元素 SRANDMEMBER key [count] 随机弹出 count 个元素，会删除元素 SPOP key [count] 获取所有元素 SMEMBERS key ","date":"0001-01-01 00:00:00","objectID":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/02/:5:1","tags":null,"title":"","uri":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/02/"},{"categories":null,"content":"2、集合间操作 多个集合的交集 SINTER key [key ...] 多个集合的并集 SUNION key [key ...] 多个集合的差集 SDIFF key [key ...] 将交集、并集、差集的结果保存 SINTERSTORE destination key [key ...] SUNIONSTORE destination key [key ...] SDIFFSTORE destination key [key ...] 说明： destination 表示目标 key。 key 表示需要操作的 key。 ","date":"0001-01-01 00:00:00","objectID":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/02/:5:2","tags":null,"title":"","uri":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/02/"},{"categories":null,"content":"3、内部编码 intset（整数集合）：value 值为整型，个数小于 set-max-intset-entries = 512 时，使用 intset。配置参数在 redis.conf 中。 hashtable（哈希表）：不满足 intset 条件时，使用 hashtable。 ","date":"0001-01-01 00:00:00","objectID":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/02/:5:3","tags":null,"title":"","uri":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/02/"},{"categories":null,"content":"4、使用场景 标签系统：计算不同人相同喜好的标签 (SINTER命令)。 SADD user1:tags tag1 tag2 SADD user2:tags tag2 tag3 SINTER user1:tags user2:tags ","date":"0001-01-01 00:00:00","objectID":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/02/:5:4","tags":null,"title":"","uri":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/02/"},{"categories":null,"content":"6、zset ","date":"0001-01-01 00:00:00","objectID":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/02/:6:0","tags":null,"title":"","uri":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/02/"},{"categories":null,"content":"1、集合内操作 添加 ZADD key [NX|XX] [CH] [INCR] score member [score member ...] # NX: 不存在 key，才设置成功 # XX: 存在 可以，才设置成功 计算成员个数 ZCARD key 获取某个成员的分数 ZSCORE key member 获取某个成员的排名 ZRANK key member # 从低到高 ZREVRANK key member # 从高到低 删除 ZREM key member [member ...] # 删除成员 ZREMRANGEBYRANK key start stop # 删除指定排名范围的成员 ZREMRANGEBYSCORE key min max # 删除指定分数范围的成员 增加成员的分数 ZINCRBY key increment member 获取指定排名范围的成员 ZRANGE key start stop [WITHSCORES] # 从低到高 ZREVRANGE key start stop [WITHSCORES] # 从高到底 获取指定分数范围的成员 ZRANGEBYSCORE key min max [WITHSCORES] [LIMIT offset count] # WITHSCORES：结果返回分数 # LIMIT offset count：限制返回个数 获取指定分数范围的成员个数 ZCOUNT key min max ","date":"0001-01-01 00:00:00","objectID":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/02/:6:1","tags":null,"title":"","uri":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/02/"},{"categories":null,"content":"2、集合间操作 交集 ZINTERSTORE destination numkeys key [key ...] [WEIGHTS weight] [AGGREGATE SUM|MIN|MAX] # destination：计算结果保存的键 # numkeys：参与的键，也就是 key 的总数 # weight：每一个 key 参与的权重，默认为 1 # AGGREGATE：聚合操作，默认为 sum 并集 ZUNIONSTORE destination numkeys key [key ...] [WEIGHTS weight] [AGGREGATE SUM|MIN|MAX] # 参数同 ZINTERSTORE ","date":"0001-01-01 00:00:00","objectID":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/02/:6:2","tags":null,"title":"","uri":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/02/"},{"categories":null,"content":"3、内部编码 ziplist（压缩列表）：个数小于 zset-max-ziplist-entries = 128 时，value 小于 zet-max-ziplist-value = 64 使用 ziplist。配置参数在 redis.conf 中。 skiplist（跳跃表）：不满足 ziplist 条件时，使用 skiplist。 ","date":"0001-01-01 00:00:00","objectID":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/02/:6:3","tags":null,"title":"","uri":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/02/"},{"categories":null,"content":"6、键管理 ","date":"0001-01-01 00:00:00","objectID":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/02/:7:0","tags":null,"title":"","uri":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/02/"},{"categories":null,"content":"1、单个键管理 键重命名 RENAME key newkey # 存在 key，会覆盖 RENAMENX key newkey # 不存在 key，才重命名成功 随机返回一个键 RANDOMKEY 键过期 EXPIRE key seconds # \u003e0: 剩余过期时间 # -1: 没有设置过期时间； # -2: 键不存在 Redis 不支持二级数据结构（哈希表、列表）过期 setex 原子命令设置 value 和 expire 迁移键 MOVE key db # 迁移到另一个db, 不建议使用，因为集群环境只能使用一个数据库 DUMP key; RESTORE key ttl serialized-value [REPLACE] # 操作麻烦，不建议使用 MIGRATE host port key| destination-db timeout [COPY] [REPLACE] [KEYS key] # 可以使用 # COPY: 迁移后不会删除源键 # REPLACE: 迁移后会覆盖目标库的键 示例： 迁移到 localhost:6380 的 db0 库上，timeout为 1000ms，命令为 MIGRATE localhost 6380 hello 0 1000。 迁移多个键 k1, k2, k3，命令为 MIGRATE localhost 6380 \"\" 0 1000 KEYS k1 k2 k3。 ","date":"0001-01-01 00:00:00","objectID":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/02/:7:1","tags":null,"title":"","uri":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/02/"},{"categories":null,"content":"2、遍历键 全量遍历键 KEYS pattern # 键很多时，会阻塞 Redis 渐进式遍历建 SCAN cursor [MATCH pattern] [COUNT count] # count: 每次查询 key 的个数。 # pattern: 同命令 scan。 说明： 第一次查询设置 cursor 为 0，结果会返回 cursor，如果 cursor 为 0，表示遍历结束，否则设置 cursor 为当前返回值，再次查询。 ","date":"0001-01-01 00:00:00","objectID":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/02/:7:2","tags":null,"title":"","uri":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/02/"},{"categories":null,"content":"3、数据库管理 无，因为集群模式下，只能使用一个数据库，生产环境也是如此。 ","date":"0001-01-01 00:00:00","objectID":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/02/:7:3","tags":null,"title":"","uri":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/02/"},{"categories":null,"content":"1、慢查询 ","date":"0001-01-01 00:00:00","objectID":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/03/:1:0","tags":null,"title":"","uri":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/03/"},{"categories":null,"content":"1、慢查询配置 slowlog-log-slower-than: 10000 (默认值，单位微秒)，超过 10 毫秒的语句就会被记录下来。 slowlog-max-len: 128（默认值），Redis 内部使用列表来保存慢查询日志。 lowlog-log-slower-than = 0, 会记录所有命令。 lowlog-log-slower-than \u003c 0, 不会记录任何命令。 配置方式： 修改配置文件 redis.conf。 动态修改 config set lowlog-log-slower-than 20000 # 设置慢查询时间 config set slowlog-max-len 1000 # 设置慢查询日志大小 config rewrite # 持久化到配置文件 慢查询命令 SLOWLOG GET 10 # 获取最近 10 条日志 SLOWLOG LEN # 获取日志条数 SLOWLOG RESET # 慢查询日志重置 ","date":"0001-01-01 00:00:00","objectID":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/03/:1:1","tags":null,"title":"","uri":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/03/"},{"categories":null,"content":"2、最佳实践 参数 slowlog-max-len，建议调大日志列表，比如 1000以上；参数 slowlog-log-slower-than，默认超过 10ms 就判断为慢查询，如果每条命令执行时间在 1ms 以上，则 1s 的并发量不足 1000，所以对于高 OPS 场景设置为 1ms。 慢查询只记录命令执行时间，不包括命令排队和网络传输时间。 慢查询日志只是一个先进先出的队列，如果查询较多，可能会丢失日志数据，可以利用 SLOWLOG GET 命令将日志存入 mysql 中，也可以利用开源工具 CacheCloud。 ","date":"0001-01-01 00:00:00","objectID":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/03/:1:2","tags":null,"title":"","uri":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/03/"},{"categories":null,"content":"2、Redis shell ","date":"0001-01-01 00:00:00","objectID":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/03/:2:0","tags":null,"title":"","uri":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/03/"},{"categories":null,"content":"1、redis-cli 命令 -x 参数 echo \"world\" | redis-cli -x set hello # 设置key为 hello， value为 world -c 参数 集群参数，防止 moved 和 asked 异常。 –rdb 参数 请求 Redis 实例生成 RDB 文件，保存在本地。 –bigkeys 参数 选出大 key，这些 key 可能是系统瓶颈。 –eval 参数 运行 lua 脚本。 latency 参数 –latency: 客户端与主机延迟 。 –latency-history: 分时段展示延迟，用 -i 参数来指定，默认为 15s。 –latency-dist: 统计图表形式展示延迟。 –stat 参数 实时获取 Redis 重要统计信息，信息比 info 命令少。 ","date":"0001-01-01 00:00:00","objectID":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/03/:2:1","tags":null,"title":"","uri":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/03/"},{"categories":null,"content":"2、redis-server 命令 –test-memory 参数 redis-server --test-memory 1024 # 检测是否可以给 Redis 分配 1G 内存 ","date":"0001-01-01 00:00:00","objectID":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/03/:2:2","tags":null,"title":"","uri":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/03/"},{"categories":null,"content":"3、redis-benchmark 命令 用来做基准性能测试。 redis-benchmark -c 100 -n 20000 -q -r 10000 -t get,set --csv # -c 客户端并发数 # -n 客户端请求总数 # -q 仅仅显示 requests per second 信息 # -r 随机键的范围（0-9999），不是个数 # -t 指定命令 # --csv 结果按照 csv 格式输出 ","date":"0001-01-01 00:00:00","objectID":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/03/:2:3","tags":null,"title":"","uri":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/03/"},{"categories":null,"content":"3、Pipeline ","date":"0001-01-01 00:00:00","objectID":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/03/:3:0","tags":null,"title":"","uri":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/03/"},{"categories":null,"content":"1、pipeline 概念 redis 执行一条命令可以分为四个过程： 发送命令 命令排队 执行命令 返回结果 其中 1. 和 4. 称为 RTT (往返时间)。 pipeline 可以将一组 redis 命令通过一次 RTT 发给 Redis，再按照执行结果返回给客户端。 redis-cli 脚本的 –pipe 选项就是使用 pipeline 机制。 ","date":"0001-01-01 00:00:00","objectID":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/03/:3:1","tags":null,"title":"","uri":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/03/"},{"categories":null,"content":"2、性能测试 pipeline 执行速度一般比逐条执行快，客户端与服务端网路延时越大，效果越明显。 ","date":"0001-01-01 00:00:00","objectID":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/03/:3:2","tags":null,"title":"","uri":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/03/"},{"categories":null,"content":"3、原生批量和 pipeline 原生批量命令是原子的，pipeline 不是原子的（中间可以执行其他命令）。 原生批量命令是一个命令对应多个 key, pipeline 支持多个命令。 原生批零命令是 Redis 服务端实现的，pipeline 是客户端和服务端共同实现的。 ","date":"0001-01-01 00:00:00","objectID":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/03/:3:3","tags":null,"title":"","uri":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/03/"},{"categories":null,"content":"4、最佳实践 pipeline 封装的数据不能过多，即大数据可以拆分为批量的小 pipeline 命令。 pipeline 只能操作一个 Redis 实例。 ","date":"0001-01-01 00:00:00","objectID":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/03/:3:4","tags":null,"title":"","uri":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/03/"},{"categories":null,"content":"4、事务与Lua 为了保证多个命令组合的原子性，Redis 提供了简单事务功能和 lua 脚本。 ","date":"0001-01-01 00:00:00","objectID":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/03/:4:0","tags":null,"title":"","uri":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/03/"},{"categories":null,"content":"1、事务 MULTI # 开启事务 set a 1 # 执行命令，实际上把命令放到队列中 set b 1 # 执行命令，实际上把命令放到队列中 EXEC # 真正的执行命令 命令错误，会导致事务执行失败，比如 set a 1 写成了 sett a 1。 运行时错误，redis 不支持回滚，比如 sadd a 1 写成了 zadd a 1 b，假设 a 这个 key 已经存在，就会抛出错误。 事务简单主要原因就是，redis 不支持回滚。 ","date":"0001-01-01 00:00:00","objectID":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/03/:4:1","tags":null,"title":"","uri":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/03/"},{"categories":null,"content":"2、Lua 在 Redis 中使用 Lua，有两种方式 eval 和 evalsha。 eval EVAL script numkeys key [key ...] arg [arg ...] eval 'return \"hello \" .. KEYS[1] .. ARGV[1]' 1 world redis # 例子 # 输出 \"hello worldredis\" 如果 Lua 脚本较长，可以使用 redis-cli –eval 选项来执行。 evalsha 使用 eval 命令，每次都需要将脚本发送到服务端，使用 evalsha 命令就避免了开销。 redis-cli script load hello.lua # 加载 lua 脚本到服务端，会返回 sha1 值。 EVALSHA sha1 numkeys key [key ...] arg [arg ...] # 执行 lua 脚本，参数 sha1 就是返回的 sha1 值，其他参数同 eval 命令。 lua 中使用 redis API redis.call(\"set\", \"a\" , 1) redis.call(\"get\", \"a\" ) 也可以使用 redis.pcall 命令，两者差别在于 pcall 命令会忽略错误继续执行，call 遇到错误停止。 lua 脚本执行是原子性的，中间不会插入别的命令。 管理 lua 脚本命令 SCRIPT LOAD [script] # 加载 lua 脚本，返回 sha1 值 SCRIPT EXISTS sha1 [sha1] # 是否存在 sha1 的脚本 SCRIPT FLUSH # 清空 lua 脚本 SCRIPT KILL # 杀掉 lua 脚本 ","date":"0001-01-01 00:00:00","objectID":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/03/:4:2","tags":null,"title":"","uri":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/03/"},{"categories":null,"content":"5、Bitmaps ","date":"0001-01-01 00:00:00","objectID":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/03/:5:0","tags":null,"title":"","uri":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/03/"},{"categories":null,"content":"1、数据结构模型 Bitmaps 不是一种数据结构，实际上它是字符串，但它可以对字符串的位进行操作，你可以想象一个以位为单位的数组，每个单元只能存储 0 和1。 ","date":"0001-01-01 00:00:00","objectID":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/03/:5:1","tags":null,"title":"","uri":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/03/"},{"categories":null,"content":"2、命令 设置值 SETBIT key offset value # offset 从 0 开始 获取值 GETBIT key offset # 结果只有 0 或者 1 BITCOUNT key [start end] # 对[start, end]范围获取值为 1 的个数 Bitmaps 间的运算 BITOP operation destkey key [key ...] # operation 可以是 and(交集)、or(并集)、not(非)、xor(异或) 获取第一个为 bit 的 offset 值 BITPOS key bit [start] [end] # [start,end]范围中第一个出现 bit 的 offset ","date":"0001-01-01 00:00:00","objectID":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/03/:5:2","tags":null,"title":"","uri":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/03/"},{"categories":null,"content":"3、分析 利用 Bitmaps 来统计网站访问用户： SETBIT users:2020-03-22 1 1 # 2020-03-22 这一天 1 号访问了。 SETBIT users:2020-03-23 2 1 # 2020-03-23 这一天 2 号访问了。 BITCOUNT users:2020-03-23 # 2020-03-23 这一天 访问用户量 BITOP and users:2020-03-22_23 users:2020-03-23 users:2020-03-22 # 两天都访问的用户量 set 和 bitmaps 对比： 数据类型 每个用户 id 占用空间 需要存储用户量 全部内存量 set 64 位 5 千万 64 位 * 5 千万 = 400 MB bitmaps 1 位 1 亿 1 位 * 1 亿 = 12.5 MB 从表格可以看出 bitmaps 节省内存。 但如果每天的活跃用户很少，set 可能比 bitmaps 好，因为 set 需要内存 64 位 * 10 万 = 800 KB，而 bitmap 还是需要 12.5 MB 内存。 ","date":"0001-01-01 00:00:00","objectID":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/03/:5:3","tags":null,"title":"","uri":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/03/"},{"categories":null,"content":"6、HyperLogLog HyperLoglog 不是一种新的数据结构，而是一种基数算法，可以利用极小的内存空间完成独立总数统计，数据集可以 ID、Email、IP。 命令 PFADD key element [element ...] # 添加元素 PFCOUNT key [key ...] # 计数 PFMERGE destkey sourcekey [sourcekey ...] # merge 注意： 只是计算独立总数，不需要获取单条数据 HyperLogLog 有误差 ","date":"0001-01-01 00:00:00","objectID":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/03/:6:0","tags":null,"title":"","uri":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/03/"},{"categories":null,"content":"7、发布订阅 Redis 提供发布/订阅模式的消息机制。 命令 PUBLISH channel message # 向指定的 channel 发布消息 SUBSCRIBE channel [channel ...] # 向指定的 channel 订阅消息 PSUBSCRIBE pattern [pattern ...] # 模式订阅消息 UNSUBSCRIBE [channel [channel ...]] # 取消订阅 PUNSUBSCRIBE [pattern [pattern ...]] # 模式取消订阅 PUBSUB subcommand [argument [argument ...]] # 查看订阅 # PUBSUB channels [pattern] # 频道 # PUBSUB numsub [channel ...] # channel 订阅数 # PUBSUB numpat # 模式订阅数 注意： Redis 提供的消息机制，无法实现消息堆积、回溯 消息队列的优点：异步、解耦、削峰，缺点：复杂度提高。 ","date":"0001-01-01 00:00:00","objectID":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/03/:7:0","tags":null,"title":"","uri":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/03/"},{"categories":null,"content":"8、GEO Redis 提供了 GEO（地址位置）功能，支持存储地理位置信息。 添加位置信息 GEOADD key longitude latitude member [longitude latitude member ...] 获取位置信息 GEOPOS key member [member ...] 获取两个地理位置的距离 GEODIST key member1 member2 [unit] # unit: m(米)；（km）公里；（mi）英里；（fl）尺 获取指定范围内的地理位置集合 GEORADIUS key longitude latitude radius m|km|ft|mi [WITHCOORD] [WITHDIST] [WITHHASH] [COUNT count] [ASC|DESC] [STORE key] [STOREDIST key] # 根据具体的经纬度来获取 GEORADIUSBYMEMBER key member radius m|km|ft|mi [WITHCOORD] [WITHDIST] [WITHHASH] [COUNT count] [ASC|DESC] [STORE key] [STOREDIST key] # 根据某一个成员来获取 获取 geohash GEOHASH key member [member ...] # Redis 将二维的经纬度转化为一维字符串 删除地理位置 ZREM key member [member ...] # Redis 没有提供专门的删除命令，可以借助 ZREM 命令来删除 # GEO 的数据类型为 zset ","date":"0001-01-01 00:00:00","objectID":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/03/:8:0","tags":null,"title":"","uri":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/03/"},{"categories":null,"content":"1、客户端通信协议 Redis 制定了 RESP（redis序列化协议）实现客户端和服务端的正常交互。 ","date":"0001-01-01 00:00:00","objectID":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/04/:1:0","tags":null,"title":"","uri":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/04/"},{"categories":null,"content":"1、发送命令格式 CRLF 为 ‘\\r\\n’ *\u003c参数数量\u003e CRLF $\u003c参数 1 的字节数量\u003e CRLF \u003c参数 1\u003e CRLF ... $\u003c参数 N 的字节数量\u003e CRLF \u003c参数 N\u003e CRLF 以 set hello world 命令为例： *3 $3 set $5 hello $5 world ","date":"0001-01-01 00:00:00","objectID":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/04/:1:1","tags":null,"title":"","uri":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/04/"},{"categories":null,"content":"2、返回结果格式 状态回复，第一个字节为 “+\"。如 set 错误回复，第一个字节为 “-\"。如 错误命令 整数回复，第一个字节为 “:\"。如 incr 字符串回复，第一个字节为 “$\"。如 get 多条字符串回复，第一个字节为 “*\"。如 mget ","date":"0001-01-01 00:00:00","objectID":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/04/:1:2","tags":null,"title":"","uri":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/04/"},{"categories":null,"content":"2、Java 客户端 Jedis jedis 用的很少了，请参考 lettuce、redisson ","date":"0001-01-01 00:00:00","objectID":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/04/:2:0","tags":null,"title":"","uri":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/04/"},{"categories":null,"content":"3、Python 客户端 redis-py 略 ","date":"0001-01-01 00:00:00","objectID":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/04/:3:0","tags":null,"title":"","uri":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/04/"},{"categories":null,"content":"4、客户端管理 ","date":"0001-01-01 00:00:00","objectID":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/04/:4:0","tags":null,"title":"","uri":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/04/"},{"categories":null,"content":"1、客户端 API 1、client list 与 Redis 服务端相连的所有客户端连接信息 说明： id: 客户端连接唯一标识，递增，但 Redis 重启后重置为 0。 addr: 客户端 IP 和 PORT。 fd: socket 文件描述符，与 lsof 命令中 fd 是同一个。 name: 客户端名字，与 client setName 和 client getName 有关 Redis 为每个客户端分配了输入缓冲区，它的作用将客户端发送的命令临时保存，Redis 会从输入缓冲区中拉取命令并执行。不受 maxmemory 参数影响。 qbuf: 客户端的输入缓冲区总容量 qbuf-free: 客户端的输入缓冲区剩余容量 输入缓冲区过大的原因： Redis 处理速度跟不上输入缓冲区的输入速度，可能存在 bigKey。 Redis 发生了阻塞。 Redis 为每个客户端分配了输出缓冲区，它的作用是保存命令执行的结果返回给客户端。通过配置文件中的 client-output-buffer-limit \u003cclass\u003e \u003chard limit\u003e \u003csoft limit\u003e \u003csoft seconds\u003e 来配置。不受 maxmemory 参数影响。 obl: 固定输出缓冲区大小 oll: 动态输出缓冲区大小，当固定缓冲区满了，就会使用动态缓冲区 omem: 输出缓冲区总计的字节数 其他信息： age: 已连接的时间 idle: 最近一次空闲时间 flag: S 表示 slave 客户端，N 表示普通客户端，O 表示执行 monitor 命令的客户端 db: 数据库索引下标 sub/psub: 当前客户端订阅的频道 multi: 当前事务已执行命令个数 客户端限制 maxclients (默认为 1000) 和 timeout，通过 config set maxclients 10000 命令和 config set timeout 30 命令来设置。 监控缓冲区方法： 定期执行 client list 命令，收集 qbuf 和 qbuf-free。 执行 info clients 命令，找到最大的输入缓冲区 client_recent_max_input_buffer 2、client getName / setName 给当前客户端设置名字 3、client kill 杀掉指定 ip 和 port 的客户端 client kill ip:port 4、client pause 阻塞客户端 timeout 毫秒 client pause timeout(毫秒) 5、monitor 监控 Redis 正在执行的命令，如果并发量过大，会造成输出缓冲区暴涨。 monitor ","date":"0001-01-01 00:00:00","objectID":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/04/:4:1","tags":null,"title":"","uri":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/04/"},{"categories":null,"content":"2、客户端相关配置 客户端的配置如下： timeout： 空闲连接超时时间 tcp-keepalive： 检查死的连接 tcp-backlog: TCP 连接过后，会将接受的连接放入队列中，tcp-backlog 就是这个队列的大小。 ","date":"0001-01-01 00:00:00","objectID":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/04/:4:2","tags":null,"title":"","uri":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/04/"},{"categories":null,"content":"3、客户端统计信息 1、info clients 运行命令如下： 2、info stats 运行命令如下： 客户端相关的指标 total_connections_received： 总共接受的连接数 rejected_connections： 拒绝的连接数 ","date":"0001-01-01 00:00:00","objectID":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/04/:4:3","tags":null,"title":"","uri":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/04/"},{"categories":null,"content":"5、客户端常见异常 ","date":"0001-01-01 00:00:00","objectID":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/04/:5:0","tags":null,"title":"","uri":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/04/"},{"categories":null,"content":"1、无法从连接池中获取连接 可能的原因： 连接池设置过小。 没有正确使用连接池，用过后没有释放。 具体还是要看选用的客户端，没有连接了是怎么处理的？（是等待还是直接拒接抛出异常） ","date":"0001-01-01 00:00:00","objectID":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/04/:5:1","tags":null,"title":"","uri":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/04/"},{"categories":null,"content":"2、客户端读写超时 可能的原因： 读写超时时间设置短。 命令本身就很慢。 网络不正常。 Redis 阻塞。 ","date":"0001-01-01 00:00:00","objectID":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/04/:5:2","tags":null,"title":"","uri":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/04/"},{"categories":null,"content":"3、客户端连接超时 可能的原因： 连接超时时间设置短。 网络不正常。 Redis 发生阻塞，导致 tcp-backlog 已满。 ","date":"0001-01-01 00:00:00","objectID":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/04/:5:3","tags":null,"title":"","uri":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/04/"},{"categories":null,"content":"4、客户端缓冲区异常 可能的原因： 输出缓冲区满，比如用 get 命令来获取一个bigKey。 长时间空闲连接被服务端主动断开。 不正常的并发读写，Redis 对象被多个线程并发操作。 ","date":"0001-01-01 00:00:00","objectID":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/04/:5:4","tags":null,"title":"","uri":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/04/"},{"categories":null,"content":"5、Lua 脚本执行 可能的原因： lua 脚本执行时间超过参数 lua-time-limit。 ","date":"0001-01-01 00:00:00","objectID":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/04/:5:5","tags":null,"title":"","uri":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/04/"},{"categories":null,"content":"6、客户端连接数过大 客户端连接数超过 maxclients，新的连接就会被拒绝。 从两个方面来解决： 客户端：通过下线部分应用节点，使 Redis 的连接数降下来，从而继续找其根本原因，或者调整 maxclients 参数。 服务端：如果 Redis 是高可用模式，可以把当前的节点故障转移。 ","date":"0001-01-01 00:00:00","objectID":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/04/:5:6","tags":null,"title":"","uri":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/04/"},{"categories":null,"content":"6、客户端案例分析 ","date":"0001-01-01 00:00:00","objectID":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/04/:6:0","tags":null,"title":"","uri":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/04/"},{"categories":null,"content":"1、Redis 内存陡增 现象： 服务端：Redis 主节点内存陡增，从节点内存无变化。 客户端：产生 OOM 异常。 可能的原因： 确实有大量的写入，通过执行命令 dbsize 来获取主从节点的键个数。 排查是否由客户端缓冲区应引发的问题，通过执行命令 info clients来查看。 处理方法： 通过命令 redis-cli info list | grep -v \"omemo=0\"， 找到非零的客户端连接，然后 kill 掉。 可能就是运行命令 monitor 造成的，一般都建议在生厂环境中禁用 monitor。 ","date":"0001-01-01 00:00:00","objectID":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/04/:6:1","tags":null,"title":"","uri":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/04/"},{"categories":null,"content":"2、客户端周期性超时 现象： 客户端：客户端周期性超时 服务端：无明显现象，只是一些慢查询。 可能的原因： 网络不正常。 执行命令造成慢查询导致的周期性超时。 处理方法： 运维层面，监控慢查询，一旦超多阈值，就发出报警。 避免不正确使用命令，如 KEYS *、HGETALL key 等。 ","date":"0001-01-01 00:00:00","objectID":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/04/:6:2","tags":null,"title":"","uri":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/04/"},{"categories":null,"content":"1、RDB ","date":"0001-01-01 00:00:00","objectID":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/05/:1:0","tags":null,"title":"","uri":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/05/"},{"categories":null,"content":"1、触发机制 手动触发，分别有 save 和 bgsave 两个命令。 save： 会阻塞当前 Redis 服务器，直到 RDB 过程完成为止，不建议使用。 bgsave： Redis 进程会 fork 出子进程，子进程进行 RDB 持久化，阻塞只会发生在 fork 阶段。 自动触发的场景： save m n 配置，表示在 m 秒中存在 n 次数据改变，才会触发 bgsave。 从节点全量复制过程中，主节点会执行 bgsave 生成 RDB 文件，发送子节点。 默认关闭情况下，如果没有开启 AOF，也会执行 bgsave。 ","date":"0001-01-01 00:00:00","objectID":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/05/:1:1","tags":null,"title":"","uri":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/05/"},{"categories":null,"content":"2、触发流程 bgsave 命令的运行流程如下图： 说明； 执行 bgsave 命令， 判断是否有 AOF/RDB 进程。 执行 info stats 命令，选项 latest_fork_usec 表示最后一次 fork 使用的秒数。 bgsave 命令执行完成后，会出现 Background saving started 提示。 子进程创建 RDB 文件成功后，对原有的文件进行原子替换, 执行 lastsave 命令获取最后一次生成 RDB 文件的时间，对应 info Persistence 命令中的选项 rdb_last_save_time。 ","date":"0001-01-01 00:00:00","objectID":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/05/:1:2","tags":null,"title":"","uri":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/05/"},{"categories":null,"content":"3、RDB 文件的处理 RDB 文件通过配置文件参数 dbfilename 和 dir来配置，也可以通过命令 config set dir {dir} 和 config set dbfilename {dbfilename} 来动态配置。 RDB 文件默认采用 LZF 压缩，通常建议开启，因为主从复制时，需要发送 RDB 文件到从节点，这样可以节省带宽。 RDB 默认也开启校验，可以通过脚本 redis-check-rdb 来校验生成相应的错误报告。 ","date":"0001-01-01 00:00:00","objectID":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/05/:1:3","tags":null,"title":"","uri":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/05/"},{"categories":null,"content":"4、RDB 的优缺点 优点： RDB 非常适合备份、全量复制等场景，比如每 6 小时定时执行 bgsave，可用于灾难恢复。 RDB 的恢复数据远远快于 AOF 方式。 缺点： RDB 无法做到秒级持久化，fork 创建子进程也属于重量级操作。 RDB 用特定的二进制格式保存，可能有版本不兼容问题。 ","date":"0001-01-01 00:00:00","objectID":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/05/:1:4","tags":null,"title":"","uri":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/05/"},{"categories":null,"content":"2、AOF 以独立的日志记录每次写命令，重启时再重新执行 AOF 文件中的命令达到恢复数据的目的。 AOF 解决了数据持久化的实时性。 ","date":"0001-01-01 00:00:00","objectID":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/05/:2:0","tags":null,"title":"","uri":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/05/"},{"categories":null,"content":"1、AOF 工作流程 开启 AOF 需要设置参数 appendonly yes。 通过参数 appendfilename 来设置文件名。 工作流程如下图： 说明： 所有的写入命令会追加到 aof_buf (缓冲区)中。 AOF 缓冲区会根据同步策略（参数默认设置 appendfsync everysec）来做同步操作。 会定期对 AOF 文件进行 rewrite，达到压缩的目的，因为可能有些 key 过期了。 机器重启时，如果开启了 AOF，则使用 AOF 加载数据。 ","date":"0001-01-01 00:00:00","objectID":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/05/:2:1","tags":null,"title":"","uri":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/05/"},{"categories":null,"content":"2、命令写入 AOF 采用文本协议格式，也就是说 AOF 文件中存储就是写入的命令，这样具有阅读性、便于修改。 AOF 把命令先写入 aof_buf 中，根据不同的同步策略可以在性能和安全上做出平衡，没有特殊要求，就设置为 everysec。 三种策略； no: don’t fsync, just let the OS flush the data when it wants. Faster. always: fsync after every write to the append only log. Slow, Safest. everysec: fsync only one time every second. Compromise. ","date":"0001-01-01 00:00:00","objectID":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/05/:2:2","tags":null,"title":"","uri":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/05/"},{"categories":null,"content":"3、重写机制 AOF 文件可以变小的原因： 超时的数据，可以不用再写入文件中。 key 过期了，可能含有无效命令，如 del key1。 多个命令可以合并成一个，如 lpush list a 和 lpush list b 可以合并为 lpush list a b。 触发 AOF 重写方式： 手动执行命令 bgrewriteaof 自动触发，根据配置参数 auto-aof-rewrite-percentage 100 和 auto-aof-rewrite-min-size 64mb。 参数说明： This is how it works: Redis remembers the size of the AOF file after the latest rewrite (if no rewrite has happened since the restart, the size of the AOF at startup is used). This base size is compared to the current size. If the current size is bigger than the specified percentage, the rewrite is triggered. Also you need to specify a minimal size for the AOF file to be rewritten, this is useful to avoid rewriting the AOF file even if the percentage increase is reached but it is still pretty small 自动触发时机: aof_current_size \u003e auto-aof-rewrite-min-size \u0026\u0026 (aof_current_size - aof_base_size) / aof_base_size \u003e auto-aof-rewrite-percentage AOF 重写流程图如下： 说明： 执行 AOF 重写请求，如果有子进程在执行 bgsave 则等待完成之后再操作。 fork 子进程进行重写，父进程接受请求，修改命令写入 aof_buf 中根据策略同步到磁盘。 fork 操作运用写时复制技术，所以子进程只能共享操作 fork 时的内存，这时父进程可能还在响应请求，所以把重写后的新命令放入 aof_rewrite_buf 缓冲区中。 把 aof_rewrite_buf 中数据写入新的 AOF 文件中，根据开启参数 aof-rewrite-incremental-fsync yes，每 32MB 同步到磁盘。 ","date":"0001-01-01 00:00:00","objectID":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/05/:2:3","tags":null,"title":"","uri":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/05/"},{"categories":null,"content":"4、重启加载 重启加载图： ","date":"0001-01-01 00:00:00","objectID":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/05/:2:4","tags":null,"title":"","uri":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/05/"},{"categories":null,"content":"5、文件校验 加载损坏的 AOF 文件会拒绝启动，可以先备份文件，然后再执行命令 redis-check-aof [--fix] \u003cfile.aof\u003e 来进行修复。 ","date":"0001-01-01 00:00:00","objectID":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/05/:2:5","tags":null,"title":"","uri":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/05/"},{"categories":null,"content":"3、问题定位与优化 ","date":"0001-01-01 00:00:00","objectID":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/05/:3:0","tags":null,"title":"","uri":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/05/"},{"categories":null,"content":"1、fork 操作 Redis 做 RDB 或者 AOF 重写时，必不可少的操作就是 fork。fork 用的写时复制技术，会复制父进程的内存页表。 改善 fork操作的耗时： 优先使用物理机或者高效支持 fork 操作的虚拟化技术。 fork 耗时和内存量成正比，单个 Redis 实例建议不超过 10G。 linux 内存分配策略，避免物理内存不足导致 fork 失败。 降低 fork 操作频率，比如避免不必要的全量复制，适当放宽 AOF 自动触发时机。 ","date":"0001-01-01 00:00:00","objectID":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/05/:3:1","tags":null,"title":"","uri":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/05/"},{"categories":null,"content":"2、子进程开销监控和优化 CPU，子进程负责把内存中的数据写入文件中，属于 IO 密集型操作，不要和其他 IO 密集型服务部署在一起。 内存，写时复制技术，避免在大量写入时做子进程重写操作，导致父进程维护大量页副本，造成内存消耗，可以关闭 THP。 磁盘，AOF 重写会消耗大量磁盘 IO，可以关闭，参数设置为 no-appendfsync-on-rewrite yes，默认是关闭的，但是开启后，可能丢失数据。 ","date":"0001-01-01 00:00:00","objectID":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/05/:3:2","tags":null,"title":"","uri":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/05/"},{"categories":null,"content":"3、AOF 追加阻塞 AOF 持久化，常用的同步策略是 everysec，用于平衡性能和安全性，对于这种方式，Redis 使用另一个线程每秒执行 fsync 同步磁盘，当系统磁盘繁忙时，可能造成 Redis 主进程阻塞。 ","date":"0001-01-01 00:00:00","objectID":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/05/:3:3","tags":null,"title":"","uri":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/05/"},{"categories":null,"content":"4、多实例部署 略 ","date":"0001-01-01 00:00:00","objectID":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/05/:4:0","tags":null,"title":"","uri":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/05/"},{"categories":null,"content":"1、配置 ","date":"0001-01-01 00:00:00","objectID":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/06/:1:0","tags":null,"title":"","uri":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/06/"},{"categories":null,"content":"1、建立复制 建立复制前会删除全部数据。 配置复制的方式有三种； 配置文件中加入 slaveof {masterHost} {masterPort}。 redis 启动命令后加入 slaveof {masterHost} {masterPort}。 直接执行命令 slaveof {masterHost} {masterPort}。 ","date":"0001-01-01 00:00:00","objectID":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/06/:1:1","tags":null,"title":"","uri":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/06/"},{"categories":null,"content":"2、断开复制 在从节点执行命令 slaveof no one 来断开复制。所谓切主操作，就是先断开复制，然后再建立复制。 ","date":"0001-01-01 00:00:00","objectID":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/06/:1:2","tags":null,"title":"","uri":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/06/"},{"categories":null,"content":"3、安全性 为了安全性，一般都会在主节点上设置 requirepass 123456，所有客户端访问必须使用 auth 123456 验证。因此从节点开启复制时，也要设置 masterauth 123456。 ","date":"0001-01-01 00:00:00","objectID":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/06/:1:3","tags":null,"title":"","uri":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/06/"},{"categories":null,"content":"4、只读 默认情况下，从节点使用 slave-read-only=yes 配置为只读模式。由于已经开启了复制，建议从节点保持只读模式。 ","date":"0001-01-01 00:00:00","objectID":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/06/:1:4","tags":null,"title":"","uri":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/06/"},{"categories":null,"content":"5、传输延迟 主从节点之间复制数据，肯定会有延迟。 redis 提供了 repl-disable-tcp-nodelay 参数用于关闭 TCP_NODELAY，默认关闭。 当关闭时，主节点的数据无论大小都会发送到从节点，这样就降低了延迟，但增加了带宽。 当开启时，主节点会合并较小的 TCP 数据包，从而节省带宽。 ","date":"0001-01-01 00:00:00","objectID":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/06/:1:5","tags":null,"title":"","uri":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/06/"},{"categories":null,"content":"2、拓扑 主要有三种；一主一从，一主多从，树状主从。 一主一从： 最简单的结构，一般只在 从节点上开启 AOF 操作。 一主多从： 用于读多写少、读写分离的场景 树状主从： 从节点不但可以复制主节点的数据，还可以作为其他的节点的主节点。 ","date":"0001-01-01 00:00:00","objectID":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/06/:2:0","tags":null,"title":"","uri":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/06/"},{"categories":null,"content":"3、原理 ","date":"0001-01-01 00:00:00","objectID":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/06/:3:0","tags":null,"title":"","uri":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/06/"},{"categories":null,"content":"1、复制过程 执行 slaveof {masterHost} {masterPort} 命令后，保存主节点信息。 从节点每秒运行定时任务维护复制逻辑，当发现新的主节点后，建立连接。 发送 ping 命令，主要是检查网络是否可用和是否可以处理命令（可能主节点阻塞了）。 权限验证，requirepass 和 masterauth 是否匹配。 同步数据集，分为全量同步和部分同步。 命令持续复制，新的命令持续发给从节点。 ","date":"0001-01-01 00:00:00","objectID":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/06/:3:1","tags":null,"title":"","uri":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/06/"},{"categories":null,"content":"2、数据同步 同步过程分为全量复制和部分复制。 参与主从复制的节点都会维护自身复制偏移量。命令 info 中 master_repl_offset 和 slave_repl_offset 。 复制积压缓冲区，主节点把命令发送从节点，还会把命令写入复制积压缓冲区，这个用于部分复制和命令丢失的场景。命令 info 中 repl_backlog_*。 主节点运行 ID，用来唯一识别 Redis 节点，当运行 ID 变化了，从节点将做全量复制了。节点重启了，运行 ID 也会变化。可以执行命令 debug reload 来重新加载并保持运行 ID 不变，但是会阻塞当前 Redis。 命令 info 中 run_id。 ","date":"0001-01-01 00:00:00","objectID":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/06/:3:2","tags":null,"title":"","uri":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/06/"},{"categories":null,"content":"3、全量复制 ","date":"0001-01-01 00:00:00","objectID":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/06/:3:3","tags":null,"title":"","uri":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/06/"},{"categories":null,"content":"4、部分复制 ","date":"0001-01-01 00:00:00","objectID":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/06/:3:4","tags":null,"title":"","uri":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/06/"},{"categories":null,"content":"5、心跳 ","date":"0001-01-01 00:00:00","objectID":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/06/:3:5","tags":null,"title":"","uri":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/06/"},{"categories":null,"content":"6、异步复制 ","date":"0001-01-01 00:00:00","objectID":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/06/:3:6","tags":null,"title":"","uri":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/06/"},{"categories":null,"content":"4、开发与运维中的问题 ","date":"0001-01-01 00:00:00","objectID":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/06/:4:0","tags":null,"title":"","uri":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/06/"},{"categories":null,"content":"Redis 开发与运维 第一章 初识 Redis 第二章 API 的理解和使用 第三章 小功能大用处 第四章 客户端 第五章 持久化 第六章 复制 ","date":"0001-01-01 00:00:00","objectID":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/readme/:1:0","tags":null,"title":"","uri":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/readme/"},{"categories":null,"content":"1、上下文切换 CPU 通过时间片分配算法来循环执行任务，当前任务执行完一个时间片后会切换到下一个任务。但是在切换前会保存上一个任务的状态，以便下次切换回这个任务时，再次加载该任务状态。这就是上下文切换。 创建过多的线程，会使上下文切换频繁，执行效率也可能不如单线程。 上图的 cs (context switch) 表示上下文切换次数。 减少上下文切换的方法： 无锁并发编程，多线程处理数据时，可以用一个方法来避免锁。如将数据的 ID 按照 Hash 算法取余分段，不同的线程处理不同段的数据。 CAS 算法，Java 的 Atomic 包。 使用最少线程，避免创建不需要的线程，比如任务很少，创建的线程较多。 协程，单线程实现多任务的调度，并维持多任务状态切换。 减少上下文切换示例 jstack 命令来 dump 线程 jstack 31177 \u003e /home/xxx/dump-31177 统计线程都处于什么状态 grep java.lang.Thread.State dump-31177 | awk '{print $2$3$4$5}' | sort | uniq -c 查看这些 waiting 的线程，根据需要合理配置线程数。 ","date":"0001-01-01 00:00:00","objectID":"/ooooo-notes/old-notes/books/the-art-of-java-concurrent-programming/01/:1:0","tags":null,"title":"","uri":"/ooooo-notes/old-notes/books/the-art-of-java-concurrent-programming/01/"},{"categories":null,"content":"2、死锁 死锁示例： public static void main(String[] args) { Object lockA = new Object(); Object lockB = new Object(); Thread t1 = new Thread(() -\u003e { synchronized (lockA) { System.out.println(\"get lockA\"); timeSleep(2); synchronized (lockB) { System.out.println(\"get lockB\"); } } }); Thread t2 = new Thread(() -\u003e { synchronized (lockB) { System.out.println(\"get lockB\"); synchronized (lockA) { System.out.println(\"get lockA\"); } } }); t1.start(); t2.start(); } 避免死锁的方法： 避免一个线程同时获取多个锁，也就是同时申请所有的资源。 尝试使用定时锁，如 lock.tryLock(timeout) 来替换内部锁。 对于数据库锁，加锁和加锁必须在一个数据库连接里。 ","date":"0001-01-01 00:00:00","objectID":"/ooooo-notes/old-notes/books/the-art-of-java-concurrent-programming/01/:2:0","tags":null,"title":"","uri":"/ooooo-notes/old-notes/books/the-art-of-java-concurrent-programming/01/"},{"categories":null,"content":"3、资源限制的挑战 带宽，比如带宽只有 20M, 一个线程最多只能使用 10M，也就是说线程数最大只能是 2，多余的线程没有资源可以使用。 磁盘 ","date":"0001-01-01 00:00:00","objectID":"/ooooo-notes/old-notes/books/the-art-of-java-concurrent-programming/01/:3:0","tags":null,"title":"","uri":"/ooooo-notes/old-notes/books/the-art-of-java-concurrent-programming/01/"},{"categories":null,"content":"4、总结 强烈建议使用 JDK 并发包提供的并发容器和工具类来解决并发问题。 ","date":"0001-01-01 00:00:00","objectID":"/ooooo-notes/old-notes/books/the-art-of-java-concurrent-programming/01/:4:0","tags":null,"title":"","uri":"/ooooo-notes/old-notes/books/the-art-of-java-concurrent-programming/01/"},{"categories":null,"content":"Java 并发编程的艺术 第一章 并发编程的挑战 第二章 Java 并发机制的底层实现原理 第三章 Java 内存模型 ","date":"0001-01-01 00:00:00","objectID":"/ooooo-notes/old-notes/books/the-art-of-java-concurrent-programming/readme/:1:0","tags":null,"title":"","uri":"/ooooo-notes/old-notes/books/the-art-of-java-concurrent-programming/readme/"},{"categories":null,"content":"Java 并发编程实战 ","date":"0001-01-01 00:00:00","objectID":"/ooooo-notes/old-notes/geektime/java-con-practice/readme/:1:0","tags":null,"title":"","uri":"/ooooo-notes/old-notes/geektime/java-con-practice/readme/"},{"categories":null,"content":"1、Java本身有两个显著的特性 JRE 就是 Java 运行环境， JDK 就是 Java 开发工具包 跨平台运行（一次编写，到处运行） 垃圾回收器（程序员不用手动回收内存，但仍然可能存在内存泄漏） ","date":"0001-01-01 00:00:00","objectID":"/ooooo-notes/old-notes/geektime/java-core-36/01/:1:0","tags":null,"title":"","uri":"/ooooo-notes/old-notes/geektime/java-core-36/01/"},{"categories":null,"content":"2、Java是解析执行？（不太正确） 我们开发的 java 源代码，经过 javac 编译成为字节码，在运行时，通过 JVM 内置的解析器将字节码装换为机器码。 常见的 JVM， 比如 Oracle 的 Hotspot JVM，提供了 JIT（Just-In-Time）动态编译器。 在主流的 Java 版本中，Java 8 采用混合模式-Xmixed进行。 Oracle Hotspot JVM 提供了两种不同的 JIT 编译器，C1 对应 client 模式，适用于启动敏感的应用，C2 对应 server 模式，适用于长时间运行的服务器。默认采用的是分层编译。 JVM 启动时，可以通过指定不同的参数对运行模式选择。 -Xint JVM 只进行解释执行。 -Xcomp JVM 只进行编译执行。 除了上面的编译方式，还有一种新的编译方式（AOT），就是直接把字节码编译为机器码。 利用下面的命令把某个类或者某个模块编译成为AOT库 jaotc --output libHelloWorld.so HelloWorld.class jaotc --output libjava.base.so --module java.base 然后在启动时直接指定 java -XX:AOTLibrary=./libHelloWorld.so,./libjava.base.so HelloWorld ","date":"0001-01-01 00:00:00","objectID":"/ooooo-notes/old-notes/geektime/java-core-36/01/:2:0","tags":null,"title":"","uri":"/ooooo-notes/old-notes/geektime/java-core-36/01/"},{"categories":null,"content":"1、Exception 和 Error Exception 和 Error 都继承 Throwable 类，只有 Throwable 类的实例才可以抛出。 Exception 是可以预料的意外情况，可以被捕获进行相应的处理。而 Error 是不太可能出现的情况，可能会造成程序终止，如 OutOfMemoryError（内存溢出）。 Exception 分为可检查（checked）异常和不检查（unchecked）异常，可检查异常必须显式捕获处理，不检查异常就是运行时异常。如 NullPointerException 。 常见的 Exception NullPointerException （空指针异常） ArrayIndexOutOfBoundsException （数组越界异常） NoSuchFileException （文件没有找到异常） InterruptedException （线程被打断异常） ClassCastException （类型转换异常） 常见的 Error NoClassDefFoundError （类没有被找到错误） OutOfMemoryError （堆内存溢出错误） StackOverflowError （栈内存溢出错误） ","date":"0001-01-01 00:00:00","objectID":"/ooooo-notes/old-notes/geektime/java-core-36/02/:1:0","tags":null,"title":"","uri":"/ooooo-notes/old-notes/geektime/java-core-36/02/"},{"categories":null,"content":"2、try-catch-finally try (BuferedReader br = new BuferedReader(...); BuferedWriter writer = new BuferedWriter(...)) { // do something catch ( IOException | XEception e) { // Multiple catch // Handle it } finally { // do something } 注意 尽量不要捕获 Exception 类型的异常，具体异常具体处理。 不要生吞（swallow）异常，避免错误后出现难以诊断的情况，可以输出到日志中。 Java 的异常处理机制会有额外的开销 try-catch 的代码段会影响 JVM 的优化，尽量只捕获有必要的代码段。 Java 每实例化一个 Exception，就会对当前栈进行快照。 ","date":"0001-01-01 00:00:00","objectID":"/ooooo-notes/old-notes/geektime/java-core-36/02/:2:0","tags":null,"title":"","uri":"/ooooo-notes/old-notes/geektime/java-core-36/02/"},{"categories":null,"content":"1、final final 修饰的类，不能被继承。 final 修饰的变量，不能被修改。 final 修饰的方法，不能被重写。 final 不是 immutable，对象的属性还是可以改变的。 ","date":"0001-01-01 00:00:00","objectID":"/ooooo-notes/old-notes/geektime/java-core-36/03/:1:0","tags":null,"title":"","uri":"/ooooo-notes/old-notes/geektime/java-core-36/03/"},{"categories":null,"content":"2、finally finally 是 Java 保证代码一定会被执行的机制，可以使用 try-catch-finally、try-finally 来关闭数据库连接，unlock()等。 如果是利用 finally 机制来关闭资源，最好是用 try-with-resources。 特例 try { // do something Sysem.exit(1); } finally{ Sysem.out.println(“Print from fnally”); } 上面的 finally 语句不会被执行。 ","date":"0001-01-01 00:00:00","objectID":"/ooooo-notes/old-notes/geektime/java-core-36/03/:2:0","tags":null,"title":"","uri":"/ooooo-notes/old-notes/geektime/java-core-36/03/"},{"categories":null,"content":"3、finalize finalize 方法是 Object 中一个方法，它的设计目的是保证对象在垃圾收集前完成资源的回收，现在已经不推荐使用，在 Java 9 中已被标记为 @Deprecated。 使用 finalize 可能会使程序性能降低，因为 JVM 会做额外处理。 Java 目前使用 Cleaner 来替换 finalize，Cleaner的实现利用了幻象引用（虚引用）和引用队列，比如 mysql-connector-java 就是利用幻象引用来回收资源。 ","date":"0001-01-01 00:00:00","objectID":"/ooooo-notes/old-notes/geektime/java-core-36/03/:3:0","tags":null,"title":"","uri":"/ooooo-notes/old-notes/geektime/java-core-36/03/"},{"categories":null,"content":"1、kafka概念 ​ Apache Kafka是一款开源的消息引擎系统，也是一个分布式流处理平台；消息引擎系统是一组规范，企业利用这组规范在不同系统之间传递语义准确的消息，实现松耦合的异步式数据传输。 ","date":"0001-01-01 00:00:00","objectID":"/ooooo-notes/old-notes/geektime/java-core-36/04/:1:0","tags":null,"title":"","uri":"/ooooo-notes/old-notes/geektime/java-core-36/04/"},{"categories":null,"content":"2、kafka特点 使用纯二进制的字节序列 同时支持两种消息引擎模型（点对点、发布/订阅） ","date":"0001-01-01 00:00:00","objectID":"/ooooo-notes/old-notes/geektime/java-core-36/04/:2:0","tags":null,"title":"","uri":"/ooooo-notes/old-notes/geektime/java-core-36/04/"},{"categories":null,"content":"3、kafka的架构 Topic：主题，承载消息的逻辑容器，用来区分业务； Producer：生产者，向主题发布新消息的应用程序； Consumer：消费者，向主题订阅新消息的应用程序； Partition：分区，每个Topic可以设置多个分区； Replica：副本，同一个消息以提供数据冗余可以有多个副本，分为领导者副本（可对外提供服务）和追随者副本（不可以对外提供服务）；对于分区实现高可用； Consumer Group：消费者组，多个消费者可组成一个消费者组，同时消费多个分区实现高吞吐；同一个消费者组内的消费者不可重复消费同一分区的消息； Rebalance：重平衡，消费者组内的某个消费者挂掉后，会重新分配订阅主题分区； Offset：位移，有分区位移和消费者位移两个概念；分区位移是消息的位置标记（从0开始），是固定的；消费者位移是消费者在订阅消息时的消费进度，是动态的； 图解partition和replication的概念 ","date":"0001-01-01 00:00:00","objectID":"/ooooo-notes/old-notes/geektime/java-core-36/04/:3:0","tags":null,"title":"","uri":"/ooooo-notes/old-notes/geektime/java-core-36/04/"},{"categories":null,"content":"4、多种Kafka对比 Apache Kafka：社区版kafka；迭代速度快，社区响应快，但是仅提供核心功能，缺少高级特性； Confluent Kafka：Confluent公司提供的Kafka；集成了很多高级特性，分免费版和收费版，但是相关资料不全，普及率低； CDH/HDP Kafka：大数据平台内嵌的Apache Kafka，操作简单，节省运维成本，但是把控度低，演进速度慢； ","date":"0001-01-01 00:00:00","objectID":"/ooooo-notes/old-notes/geektime/java-core-36/04/:4:0","tags":null,"title":"","uri":"/ooooo-notes/old-notes/geektime/java-core-36/04/"},{"categories":null,"content":"5、Kafka版本演变 ​ kafka版本命名规范：例kafka_2.11-2.1.1.tgz（2.11表示scala版本，2.1.1表示kafka版本） kafka版本号规范：大版本号 - 小版本号 - Patch 号（修订号） 0.7版本：只提供最基础的消息队列功能； 0.8版本：引入了副本机制， 成为了一个真正意义上完备的分布式高可靠消息队列解决方案；（但是生产和消费的客户端还是老版本的，应指定zk地址而不是broker地址）； 0.8.2.0版本：引入了新版本Producer API（但是bug还有点多，不建议使用）； 0.8.2.2版本：老版本的Consumer API比较稳定了； 0.9版本：增加了基础的安全认证/权限功能，同时使用java重写了Consumer API，还引入了Kafka Connect组件用于实现高性能的数据抽取；另外新版本的Producer API比较稳定了，不建议使用新版本的Consumer API； 0.10版本：引入了Kafka Streams，正式升级为分布式流处理平台； 0.10.2.2版本：新版本的Consumer API比较稳定了，该版本也修复了一个可能导致Producer性能降低的bug； 0.11版本：提供了幂等性Producer API（幂等性就是消息去重，默认不开启）和事务API（实现流处理结果正确性的基石），还对Kafka消息格式做了重构；（该版本也是主流版本）； 0.11.0.3版本：消息引擎功能非常完善了； 1.1版本：实现故障转移（即Failover）； 1.0版本和2.0版本：只要是是对Kafka Streams的改进； ","date":"0001-01-01 00:00:00","objectID":"/ooooo-notes/old-notes/geektime/java-core-36/04/:5:0","tags":null,"title":"","uri":"/ooooo-notes/old-notes/geektime/java-core-36/04/"},{"categories":null,"content":"6、Kafka的核心参数 ","date":"0001-01-01 00:00:00","objectID":"/ooooo-notes/old-notes/geektime/java-core-36/04/:6:0","tags":null,"title":"","uri":"/ooooo-notes/old-notes/geektime/java-core-36/04/"},{"categories":null,"content":"1、配置文件参数 log.dirs：指定broker需要使用的若干个文件目录路径，无默认值；生产环境必须配置，CSV格式（例如：/home/kafka1,/home/kafka2） zookeeper.connect：指定zk的地址和端口（例hadoop01:2181,hadoop02:2181,hadoop03:2181），zk保存了topic、分区的信息等等，如果多个kafka集群共有一个zk集群，加上chroot即可，chroot是别名，则指定格式为hadoop01:2181,hadoop02:2181,hadoop03:2181/kafka1或hadoop01:2181,hadoop02:2181,hadoop03:2181/kafka2; listeners：监听器，指定协议、主机名、端口； advertised.listeners：指该监听器是broker对外发布的； host.name/port：过期参数，可以不指定； auto.create.topics.enable：是否允许自动创建topic； unclean.leader.election.enable：是否允许unclean leader选举，原本数据多的分区才有资格选举leader，该参数设置为true后，数据少的也可以参与选举，会造成数据丢失，建议设置为false； auto.leader.rebalance.enable：是否允许定期选举leader，不建议开启； log.retention.{hour|minutes|ms}：控制一条消息被保存多长时间，ms优先级最高； log.retention.bytes：指定broker为消息保存的总磁盘容量大小，默认值为-1，表示没有上限； message.max.bytes：控制broker能够接收的最大消息大小，默认值为1000012，太小，建议重设置； ","date":"0001-01-01 00:00:00","objectID":"/ooooo-notes/old-notes/geektime/java-core-36/04/:6:1","tags":null,"title":"","uri":"/ooooo-notes/old-notes/geektime/java-core-36/04/"},{"categories":null,"content":"2、Topic级别参数 retention.ms：规定了该topic消息被保存的时长； retention.bytes：规定了要为该topic预留多大的磁盘空间（默认-1，表示没有上限）； max.message.bytes：Broker能够接收的该topic的最大消息大小； 以上参数可以通过两种方式设置 创建topic时进行设置：例bin/kafka-topics.sh –zookeeper hadoop01:2181,hadoop02:2181,hadoop03:2181 –create –topic my-topic –partitions 1 –replication-factor 1 –config max.message.bytes=64000 –config flush.messages=1 修改topic时设置：例bin/kafka-topics.sh –zookeeper hadoop01:2181,hadoop02:2181,hadoop03:2181 –alter –topic my-topic –config max.message.bytes=128000 ","date":"0001-01-01 00:00:00","objectID":"/ooooo-notes/old-notes/geektime/java-core-36/04/:6:2","tags":null,"title":"","uri":"/ooooo-notes/old-notes/geektime/java-core-36/04/"},{"categories":null,"content":"3、JVM参数 KAFKA_HEAP_OPTS：指定堆大小； KAFKA_JVM_PERFORMANCE_OPTS：指定GC参数； 在启动kafka前设置这两个环境变量。 ","date":"0001-01-01 00:00:00","objectID":"/ooooo-notes/old-notes/geektime/java-core-36/04/:6:3","tags":null,"title":"","uri":"/ooooo-notes/old-notes/geektime/java-core-36/04/"},{"categories":null,"content":"4、操作系统参数 ulimit -n：打开文件描述符最大值（例ulimit -n 100000）； 文件系统类型：建议选择XFS； swappniess：swap空间大小，建议设置略大于0的值； 提交时间：即flush落盘时间，kafka的数据会先写到操作系统的页缓存上，然后会根据LRU算法定期将页缓存的数据落盘到磁盘，默认为5秒，可适当增大时间间隔； ","date":"0001-01-01 00:00:00","objectID":"/ooooo-notes/old-notes/geektime/java-core-36/04/:6:4","tags":null,"title":"","uri":"/ooooo-notes/old-notes/geektime/java-core-36/04/"},{"categories":null,"content":"7、分区策略 ","date":"0001-01-01 00:00:00","objectID":"/ooooo-notes/old-notes/geektime/java-core-36/04/:7:0","tags":null,"title":"","uri":"/ooooo-notes/old-notes/geektime/java-core-36/04/"},{"categories":null,"content":"1、生产者分区策略 轮询策略（Round-robin）：kafka生产者API默认的分区策略，最大限度负载均衡； 随机策略（Randomness）：可自定义实现该策略； 按消息键保存策略（Key-ordering）：key可以是业务上的字段信息，按业务场景自定义分区； ","date":"0001-01-01 00:00:00","objectID":"/ooooo-notes/old-notes/geektime/java-core-36/04/:7:1","tags":null,"title":"","uri":"/ooooo-notes/old-notes/geektime/java-core-36/04/"},{"categories":null,"content":"2、消费者分区策略 按范围分配：指定分区消费； 轮询分配：按顺序分配给消费者； 自定义：设置partition.assignment.strategy为自定义的类； ","date":"0001-01-01 00:00:00","objectID":"/ooooo-notes/old-notes/geektime/java-core-36/04/:7:2","tags":null,"title":"","uri":"/ooooo-notes/old-notes/geektime/java-core-36/04/"},{"categories":null,"content":"8、压缩 ​ kafka的消息层次分为消息集合和消息。一个消息集合包含若干条日志项，日志项就是装消息的地方；kafka底层的消息日志由一系列消息集合日志项组成，kafka是在消息集合层面上进行写入操作； ","date":"0001-01-01 00:00:00","objectID":"/ooooo-notes/old-notes/geektime/java-core-36/04/:8:0","tags":null,"title":"","uri":"/ooooo-notes/old-notes/geektime/java-core-36/04/"},{"categories":null,"content":"1、kafka消息格式 ​ kafka有两大消息格式：V1和V2（0.11.0.0版本后引入的）； ​ V1中每条消息需要执行CRC校验，但是在某些情况下CRC值是会变化的，会浪费空间和耽误CPU时间；在保存压缩消息上，是把多条消息进行压缩然后保存到外层消息的消息体字段中。 ​ V2对V1改进了很多，CRC校验工作移到了消息集合这一层，而且在保存压缩消息上，是对整个消息集合进行压缩。 ","date":"0001-01-01 00:00:00","objectID":"/ooooo-notes/old-notes/geektime/java-core-36/04/:8:1","tags":null,"title":"","uri":"/ooooo-notes/old-notes/geektime/java-core-36/04/"},{"categories":null,"content":"2、何时压缩和解压缩 生产者端：生产消息时指定压缩方法。 broker端：默认的压缩方式是producer，如果指定了跟producer不同的压缩方式时，会先解压缩再按新指定的方式压缩；或者broker端发生了消息格式转换也会重压缩。 consumer端：解压缩。 ","date":"0001-01-01 00:00:00","objectID":"/ooooo-notes/old-notes/geektime/java-core-36/04/:8:2","tags":null,"title":"","uri":"/ooooo-notes/old-notes/geektime/java-core-36/04/"},{"categories":null,"content":"3、压缩算法 （kafka2.1.0版本前支持的算法：GZIP、Snappy、LZ4；该版本开始后支持Zstandard算法） 压缩算法的优劣有两个指标：压缩比和压缩/解压缩吞吐量； 压缩比：zstd \u003e LZ4 \u003e GZIP \u003e Snappy 吞吐量：LZ4 \u003e Snappy \u003e zstd / GZIP ","date":"0001-01-01 00:00:00","objectID":"/ooooo-notes/old-notes/geektime/java-core-36/04/:8:3","tags":null,"title":"","uri":"/ooooo-notes/old-notes/geektime/java-core-36/04/"},{"categories":null,"content":"9、无消息丢失配置 kafka只对已提交的消息做有限度的持久化保证。 不要使用producer.send(msg),而要使用producer.send(msg,callback); 设置acks = all；表明所有副本broker都要接收到消息，保证消息”已提交“； 设置retries为一个较大的值； 设置unclean.leader.election.enable = false；表示禁止落后的broker被选为leader。 设置replication.factor \u003e= 3； 设置min.insync.replicas \u003e 1；控制消息至少被写入多少个副本才算“已提交”； 确保replication.factor \u003e min.insync.replicas；推荐replication.factor = min.insync.replicas + 1； 设置enable.auto.commit = false；确保消息消费完成再提交； ","date":"0001-01-01 00:00:00","objectID":"/ooooo-notes/old-notes/geektime/java-core-36/04/:9:0","tags":null,"title":"","uri":"/ooooo-notes/old-notes/geektime/java-core-36/04/"},{"categories":null,"content":"10、幂等性和事务性 ​ 幂等性：指某些操作执行一次或多次的结果是一样的，在kafka中是对重复消息去重。 ​ 引入事务的作用：1、 生产者多次发送消息可以封装成一个原子操作，要么都成功，要么失败；2、 consumer-transform-producer模式下，因为消费者提交偏移量出现问题，导致在重复消费消息时，生产者重复生产消息。需要将这个模式下消费者提交偏移量操作和生产者一系列生成消息的操作封装成一个原子操作。 ​ kafka事务一般为两种：1、 只有Producer生产消息 ；2、生产消费并存（consumer-transform-producer）；3、只有Consumer消费消息。 幂等性Producer：只能保证单分区、单会话上的消息幂等性（设置幂等性：props.put(“enable.idempotence”, true)或props.put(ProducerConfig.ENABLE_IDEMPOTENCE_CONFIG, true)） 事务提供的ACID特性：原子性（Atomicity）、一致性（Consistency）、隔离性（Isolation）、持久性（Durability） 隔离性：表示并发执行的事务彼此相互隔离，互不影响。 事务型Producer：能够保证跨分区、跨会话间的幂等性(设置事务型Producer：开启enable.idempotence = true，然后设置Producer端参数transctional.id,还要调用一些事务API，如下列代码；表示record1和record2要么全部写入成功要么失败。在consumer端要设置isolation.level，read_uncommitted是默认值，表示可以读取到kafka任何消息，不管事务型Producer是提交事务还是终止事务；建议使用read_committed，表示只读取事务型成功提交的消息以及非事务型Producer写入的消息。) producer.initTransactions(); try{ producer.beginTransaction(); producer.send(record1); producer.send(record2); producer.commitTransaction(); }catch(KafkaException e){ producer.abortTransaction(); } ","date":"0001-01-01 00:00:00","objectID":"/ooooo-notes/old-notes/geektime/java-core-36/04/:10:0","tags":null,"title":"","uri":"/ooooo-notes/old-notes/geektime/java-core-36/04/"},{"categories":null,"content":"11、Consumer Group ","date":"0001-01-01 00:00:00","objectID":"/ooooo-notes/old-notes/geektime/java-core-36/04/:11:0","tags":null,"title":"","uri":"/ooooo-notes/old-notes/geektime/java-core-36/04/"},{"categories":null,"content":"1、特性 一个Consumer Group可以有一个或多个Consumer； Droup ID是一个字符串，标识这一个唯一的Consumer Group； 同一个Group中的Consumer不能重复订阅一个分区； （老版本的Consumer Group把消费者位移保存在zk中，由于频繁读写会导致zk集群性能降低，新版本把消费者位移保存在kafka的_consumer_offsets的topic中。） ","date":"0001-01-01 00:00:00","objectID":"/ooooo-notes/old-notes/geektime/java-core-36/04/:11:1","tags":null,"title":"","uri":"/ooooo-notes/old-notes/geektime/java-core-36/04/"},{"categories":null,"content":"2、rebalance的触发条件 Consumer Group中的成员数变更； 订阅的topic数变更（比如订阅了用正则匹配的topic，新增了一个符合的topic）； 订阅的topic的分区数变更； ","date":"0001-01-01 00:00:00","objectID":"/ooooo-notes/old-notes/geektime/java-core-36/04/:11:2","tags":null,"title":"","uri":"/ooooo-notes/old-notes/geektime/java-core-36/04/"},{"categories":null,"content":"3、rebalance的弊端 rebalance影响Consumer端TPS；（rebalance期间，consumer会停止工作） rebalance过程很慢； rebalance效率不高； ","date":"0001-01-01 00:00:00","objectID":"/ooooo-notes/old-notes/geektime/java-core-36/04/:11:3","tags":null,"title":"","uri":"/ooooo-notes/old-notes/geektime/java-core-36/04/"},{"categories":null,"content":"4、非必要rebalance consumer未能及时给coordinator发送心跳，导致consumer被踢出Consumer Group；需设置合理的session.timeout.ms（默认值是10s，表示coordinator在10s内没收到consumer的心跳消息，该consumer被判定为dead）和heartbeat.interval.ms（表示consumer发送心跳请求的频率）的值 （推荐session.timeout.ms=2s，heartbeat.interval.ms=6s） consumer消费时间过长；需设置max.poll.interval.ms的值，表示下游处理数据的时间； ","date":"0001-01-01 00:00:00","objectID":"/ooooo-notes/old-notes/geektime/java-core-36/04/:11:4","tags":null,"title":"","uri":"/ooooo-notes/old-notes/geektime/java-core-36/04/"},{"categories":null,"content":"12、位移主题（_consumer_offsets） ​ _consumer_offsets的主要作用就是保存Kafka消费者的位移消息。消息格式是KV对，Key保存的是\u003cGroup ID,主题名，分区号\u003e；另外还有两种消息格式：1.用于保存Consumer Group信息的消息；2.用于删除Group过期位移甚至是删除Group的消息。 ​ 当有第一个Consumer消费数据时，Kafka就会自动创建_consumer_offsets这个主题，默认有50个分区，3个副本。 ","date":"0001-01-01 00:00:00","objectID":"/ooooo-notes/old-notes/geektime/java-core-36/04/:12:0","tags":null,"title":"","uri":"/ooooo-notes/old-notes/geektime/java-core-36/04/"},{"categories":null,"content":"13、提交位移 ​ 从用户角度，分为自动提交和手动提交；从Consumer角度分为同步提交和异步提交。 org.apache.kafka.common.serialization.StringSerializer 自动提交 设置为自动提交后，调用poll方法时，会提交上次poll返回的所有消息，poll方法的逻辑是先提交上一批消息的位移，再处理下一批消息，可以保证不出现消费丢失的情况，缺点是可能出现重复消费。 Properties props = new Properties(); props.put(\"bootstrap.servers\",\"localhost:9092\"); props.put(\"group.id\",\"test\"); props.put(\"enable.auto.commit\",\"true\"); props.put(\"auto.commit.interval.ms\",\"2000\"); props.put(\"key.deserializer\",\"org.apache.kafka.common.serialization.String*Serializer\"); props.put(\"value.deserializer\",\"org.apache.kafka.common.serialization.String*Serializer\"); KafkaConsumer\u003cString,String\u003e consumer = new KafkaConsumer\u003c\u003e(props); consumer.subscribe(Arrays.asList(\"foo\",\"bar\")); while(true){ ConsumerRecords\u003cString,String\u003e records = consumer.poll(100); for(ConsumerRecord\u003cString,String\u003e record : records){ System.out.printf(\"offset = %d, key = %s, value = %s%n\", record.offset(), record.key(), resord.value()); } } 同步提交commitSync() 手动提交在调用commitSync()时，Consumer会处于阻塞状态，直到Broker端返回结果，影响整个程序的TPS。 while(true){ ConsumerRecords\u003cString,String\u003e records = consumer.poll(Duration.ofSeconds(1)); process(records); //处理消息 try{ consumer.commitSync(); }catch (CommitFailedException e){ handle(e); } } 异步提交commitAsync() 在调用commitAsync()时，会立即返回结果，不会阻塞；缺点是出现问题时不能自动重试。 while(true){ ConsumerRecords\u003cString,String\u003e records = consumer.poll(Duration.ofSeconds(1)); process(records); //处理消息 consumer.commitAsync((offsets,exception) -\u003e { if(exception != null) handle(exception); }); } 同步+异步提交 手动提交中，commitSync()和commitAsync()结合使用会有很好的效果，利用commitSync()的自动重试避免瞬时错误，利用commitAsync()不会阻塞。 try { while (true) { ConsumerRecords\u003cString, String\u003e records = consumer.poll(Duration.ofSeconds(1)); process(records); // 处理消息 commitAysnc(); // 使用异步提交规避阻塞 } } catch (Exception e) { handle(e); // 处理异常 } finally { try { consumer.commitSync(); // 最后一次提交使用同步阻塞式提交 } finally { consumer.close(); } } 同步/异步的细粒度提交 通常poll的数据全部处理完后再提交位移，如果poll的总数很大，而处理过程中出现差错了，下一次会重复消费，就需要设置细粒度的提交位移。 private Map\u003cTopicPartition, OffsetAndMetadata\u003e offsets = new HashMap\u003c\u003e(); int count = 0; …… while (true) { ConsumerRecords\u003cString, String\u003e records = consumer.poll(Duration.ofSeconds(1)); for (ConsumerRecord\u003cString, String\u003e record: records) { process(record); // 处理消息 offsets.put(new TopicPartition(record.topic(), record.partition()), new OffsetAndMetadata(record.offset() + 1)); if（count % 100 == 0） consumer.commitAsync(offsets, null); // 回调处理逻辑是 count++; } } ","date":"0001-01-01 00:00:00","objectID":"/ooooo-notes/old-notes/geektime/java-core-36/04/:13:0","tags":null,"title":"","uri":"/ooooo-notes/old-notes/geektime/java-core-36/04/"},{"categories":null,"content":"14、CommitFailedException ​ CommitFailedException是指Consumer客户端在提交位移时出现了错误或异常，而且不可恢复。 ","date":"0001-01-01 00:00:00","objectID":"/ooooo-notes/old-notes/geektime/java-core-36/04/:14:0","tags":null,"title":"","uri":"/ooooo-notes/old-notes/geektime/java-core-36/04/"},{"categories":null,"content":"1、场景一 ​ 当消息处理的总时间超过预设的 max.poll.interval.ms 参数值时，Kafka Consumer 端会抛出 CommitFailedException 异常。 解决方法如代码： … Properties props = new Properties(); … props.put(\"max.poll.interval.ms\", 5000); consumer.subscribe(Arrays.asList(\"test-topic\")); while (true) { ConsumerRecords\u003cString, String\u003e records = consumer.poll(Duration.ofSeconds(1)); // 使用 Thread.sleep 模拟真实的消息处理逻辑 Thread.sleep(6000L); consumer.commitSync(); } 防止出现该异常的办法： 缩短单条消息处理时间； 增加Comsumer端允许下游系统消费一批消息的最大时长（设置max.poll.interval.ms的值，在0.10.1.0版本后才有该参数）； 减少下游系统一次性消费的消息总数（设置max.poll.records的值）； 下游系统使用多线程加速消费； ","date":"0001-01-01 00:00:00","objectID":"/ooooo-notes/old-notes/geektime/java-core-36/04/:14:1","tags":null,"title":"","uri":"/ooooo-notes/old-notes/geektime/java-core-36/04/"},{"categories":null,"content":"2、场景二 ​ Standalone Consumer在消费时也需要指定groud.id，如果出现了一个相同group.id的Consumer Group，kafka也会抛出异常；这种情况很少见，目前没有解决办法，要尽量去避免。 ","date":"0001-01-01 00:00:00","objectID":"/ooooo-notes/old-notes/geektime/java-core-36/04/:14:2","tags":null,"title":"","uri":"/ooooo-notes/old-notes/geektime/java-core-36/04/"},{"categories":null,"content":"15、多线程 ​ kafka 0.10.1.0版本开始，KafkaConsumer就是双线程设计，即用户主线程和心跳线程；用户主线程就是启动Consumer应用程序main方法的那个程序，心跳线程只负责定期给对应的的Broker机器发送心跳请求，以标识消费者应用的存活性。所以在消费层面上，Consumer依然是单线程设计。 多线程方案： 方案一： ​ 消费者程序启动多个线程，每个线程维护专属的KafkaConsumer实例，负责完整的消息获取和消息处理流程； 优点： 实现起来简单； 线程之间没有交互，省去保障线程安全的开销； 由于每个线程使用专属的Consumer实例执行消息获取和消息处理，可以保证分区内的消费顺序； 缺点： 由于每个线程要维护自己的Consumer实例，会占用很多系统资源； 线程数受限于Consumer订阅主题的总分区数； 如有某个线程处理较慢，会造成rebalance； 实现代码如下： public class KafkaConsumerRunner implements Runnable { private final AtomicBoolean closed = new AtomicBoolean(false); private final KafkaConsumer consumer; public void run() { try { consumer.subscribe(Arrays.asList(\"topic\")); while (!closed.get()) { ConsumerRecords records = consumer.poll(Duration.ofMillis(10000)); // 执行消息处理逻辑 ... } } catch (WakeupException e) { // Ignore exception if closing if (!closed.get()) throw e; } finally { consumer.close(); } } // Shutdown hook which can be called from a separate thread public void shutdown() { closed.set(true); consumer.wakeup(); } } 方案二： ​ 消费者程序使用单或多线程获取消息，同时创建多个消费线程执行消息处理逻辑； 优点： 高伸缩性，自由调节消息获取和消息处理的线程数； 缺点： 实现难度大； 由于消息获取和消息处理解耦，会破坏消息在分区中的顺序； 会使得整个消息消费链路被拉长，位移提交可能会出错，导致重复消费； private final KafkaConsumer\u003cString, String\u003e consumer; private ExecutorService executors; ... private int workerNum = ...; executors = new ThreadPoolExecutor( workerNum, workerNum, 0L, TimeUnit.MILLISECONDS, new ArrayBlockingQueue\u003c\u003e(1000), new ThreadPoolExecutor.CallerRunsPolicy() ); ... while (true) { ConsumerRecords\u003cString, String\u003e records = consumer.poll(Duration.ofSeconds(1)); for (final ConsumerRecord record : records) { executors.submit(new Worker(record)); } } .. ","date":"0001-01-01 00:00:00","objectID":"/ooooo-notes/old-notes/geektime/java-core-36/04/:15:0","tags":null,"title":"","uri":"/ooooo-notes/old-notes/geektime/java-core-36/04/"},{"categories":null,"content":"Java 核心技术 36讲 01、谈谈你对 Java 的理解 02、Exception 和 Error 的区别 03、final、finally、finalize 的区别 ","date":"0001-01-01 00:00:00","objectID":"/ooooo-notes/old-notes/geektime/java-core-36/readme/:1:0","tags":null,"title":"","uri":"/ooooo-notes/old-notes/geektime/java-core-36/readme/"},{"categories":null,"content":"1、Kafka 是一款消息引擎系统 消息引擎系统是一组规范。企业利用这组规范在不同系统之间传递语义准确的消息，实现松耦合的异步式数据传递。 Kafka 的消息编码格式是 二进制的字节序列 Kafka 支持的两种消息模型 点对点模型，系统 A 发送的消息只能被 B 系统消费，其他系统不能读取 A 系统发送的消息，一对一的关系。 发布 / 订阅模型，有 Topic（主题）、Producer（生产者）、Consumer（消费者）的概念，可能会存在多个 Producer 向 Topic 发送消息，也可能存在多个 Consumer 来消费消息，多对多的关系。 消息队列的优点：解耦、异步、削峰 ","date":"0001-01-01 00:00:00","objectID":"/ooooo-notes/old-notes/geektime/kafka-core-tech/01/:1:0","tags":null,"title":"","uri":"/ooooo-notes/old-notes/geektime/kafka-core-tech/01/"},{"categories":null,"content":"1、Kafka 术语 ","date":"0001-01-01 00:00:00","objectID":"/ooooo-notes/old-notes/geektime/kafka-core-tech/02/:1:0","tags":null,"title":"","uri":"/ooooo-notes/old-notes/geektime/kafka-core-tech/02/"},{"categories":null,"content":"Record（消息） Kafka 处理的主要对象。 ","date":"0001-01-01 00:00:00","objectID":"/ooooo-notes/old-notes/geektime/kafka-core-tech/02/:1:1","tags":null,"title":"","uri":"/ooooo-notes/old-notes/geektime/kafka-core-tech/02/"},{"categories":null,"content":"Topic（主题） 主题是承载消息的逻辑容器，在实际使用中多用来区分具体的业务。 你可以为每个业务、每个应用甚至是每类数据都创建专属的主题。 ","date":"0001-01-01 00:00:00","objectID":"/ooooo-notes/old-notes/geektime/kafka-core-tech/02/:1:2","tags":null,"title":"","uri":"/ooooo-notes/old-notes/geektime/kafka-core-tech/02/"},{"categories":null,"content":"Partition（分区） 一个有序不变的消息序列。每个主题下可以有多个分区。 ","date":"0001-01-01 00:00:00","objectID":"/ooooo-notes/old-notes/geektime/kafka-core-tech/02/:1:3","tags":null,"title":"","uri":"/ooooo-notes/old-notes/geektime/kafka-core-tech/02/"},{"categories":null,"content":"Offset（消息位移） 分区中每条消息的位置信息，是一个单调递增且不变的值。 ","date":"0001-01-01 00:00:00","objectID":"/ooooo-notes/old-notes/geektime/kafka-core-tech/02/:1:4","tags":null,"title":"","uri":"/ooooo-notes/old-notes/geektime/kafka-core-tech/02/"},{"categories":null,"content":"Producer（生产者） 向主题发布新消息的应用程序。 ","date":"0001-01-01 00:00:00","objectID":"/ooooo-notes/old-notes/geektime/kafka-core-tech/02/:1:5","tags":null,"title":"","uri":"/ooooo-notes/old-notes/geektime/kafka-core-tech/02/"},{"categories":null,"content":"Consumer（消费者） 从主题订阅新消息的应用程序。 ","date":"0001-01-01 00:00:00","objectID":"/ooooo-notes/old-notes/geektime/kafka-core-tech/02/:1:6","tags":null,"title":"","uri":"/ooooo-notes/old-notes/geektime/kafka-core-tech/02/"},{"categories":null,"content":"Consumer Offset（消费者位移） 消费者消费进度，每个消费者都有自己的消费者位移。 ","date":"0001-01-01 00:00:00","objectID":"/ooooo-notes/old-notes/geektime/kafka-core-tech/02/:1:7","tags":null,"title":"","uri":"/ooooo-notes/old-notes/geektime/kafka-core-tech/02/"},{"categories":null,"content":"Consumer Group（消费者组） 多个消费者实例共同组成的一个组，同时消费多个分区以实现高吞吐。 ","date":"0001-01-01 00:00:00","objectID":"/ooooo-notes/old-notes/geektime/kafka-core-tech/02/:1:8","tags":null,"title":"","uri":"/ooooo-notes/old-notes/geektime/kafka-core-tech/02/"},{"categories":null,"content":"Rebalance （重平衡） 消费者组内某个消费者实例挂掉后，其他消费者实例自动重新分配订阅主题分区的过程。 Rebalance 是 Kafka 消费者端实现高可用的重要手段。 ","date":"0001-01-01 00:00:00","objectID":"/ooooo-notes/old-notes/geektime/kafka-core-tech/02/:1:9","tags":null,"title":"","uri":"/ooooo-notes/old-notes/geektime/kafka-core-tech/02/"},{"categories":null,"content":"2、Kafka 三层消息架构 第一层是主题层，每个主题可以配置 M 个分区，而每个分区又可以配置 N 个副本。 第二层是分区层，每个分区的 N 个副本中只能有一个充当领导者角色，对外提供服务；其他 N-1 个副本是追随者副本，只是提供数据冗余之用。 第三层是消息层，分区中包含若干条消息，每条消息的位移从 0 开始，依次递增。 最后，客户端程序只能与分区的领导者副本进行交互。 ","date":"0001-01-01 00:00:00","objectID":"/ooooo-notes/old-notes/geektime/kafka-core-tech/02/:2:0","tags":null,"title":"","uri":"/ooooo-notes/old-notes/geektime/kafka-core-tech/02/"},{"categories":null,"content":"3、Kafka 持久化数据 Kafka 使用消息日志（Log）来保存数据，消息日志只能追加写入，所以避免了随机 I/O 操作，改为性能较好的顺序 I/O 写操作。 在 Kafka 底层，一个日志会细分成多个日志段，消息被追加写到当前最新的日志段中，当写满了一个日志段后，Kafka 会自动切分出一个新的日志段，并将老的日志段封存起来。Kafka 在后台还有定时任务会定期地检查老的日志段是否能够被删除，从而实现回收磁盘空间的目的。 ","date":"0001-01-01 00:00:00","objectID":"/ooooo-notes/old-notes/geektime/kafka-core-tech/02/:3:0","tags":null,"title":"","uri":"/ooooo-notes/old-notes/geektime/kafka-core-tech/02/"},{"categories":null,"content":"1、Kafka 是分布式流处理平台 Apache Kafka 是消息引擎系统，也是一个分布式流处理平台。 Kafka 的特性： 提供一套 API 实现生产者和消费者。 降低网络传输和磁盘存储开销。 实现高伸缩性架构。 可以实现端到端的精确一次处理语义。 ","date":"0001-01-01 00:00:00","objectID":"/ooooo-notes/old-notes/geektime/kafka-core-tech/03/:1:0","tags":null,"title":"","uri":"/ooooo-notes/old-notes/geektime/kafka-core-tech/03/"},{"categories":null,"content":"1、Kafka 不同的\"发行版\" ","date":"0001-01-01 00:00:00","objectID":"/ooooo-notes/old-notes/geektime/kafka-core-tech/04/:1:0","tags":null,"title":"","uri":"/ooooo-notes/old-notes/geektime/kafka-core-tech/04/"},{"categories":null,"content":"1、Apache Kafka 只提供最基础的组件，没有任何监控框架或工具。 如果你仅仅需要一个消息引擎系统亦或是简单的流处理应用场景，同时需要对系统有较大把控度，那么我推荐你使用 Apache Kafka。 优势在于迭代速度快，社区响应度高，使用它可以让你有更高的把控度；缺陷在于仅提供基础核心组件，缺失一些高级的特性。 ","date":"0001-01-01 00:00:00","objectID":"/ooooo-notes/old-notes/geektime/kafka-core-tech/04/:1:1","tags":null,"title":"","uri":"/ooooo-notes/old-notes/geektime/kafka-core-tech/04/"},{"categories":null,"content":"2、Confluent Kafka 分为免费版和企业版。 免费版包含 Schema 注册中心和 REST proxy 两大功能。前者是帮助你集中管理 Kafka 消息格式以实现数据前向 / 后向兼容；后者用开放 HTTP 接口的方式允许你通过网络访问 Kafka 的各种功能，这两个都是 Apache Kafka 所没有的。 企业版包含跨数据中心备份和集群监控两大功能。 优势在于集成了很多高级特性且由 Kafka 原班人马打造，质量上有保证；缺陷在于相关文档资料不全，普及率较低，没有太多可供参考的范例。 ","date":"0001-01-01 00:00:00","objectID":"/ooooo-notes/old-notes/geektime/kafka-core-tech/04/:1:2","tags":null,"title":"","uri":"/ooooo-notes/old-notes/geektime/kafka-core-tech/04/"},{"categories":null,"content":"3、Cloudera/Hortonworks Kafka 大数据云公司发布的 Kafka（CDH/HDP Kafka）。这些大数据平台天然集成了 Apache Kafka，通过便捷化的界面操作将 Kafka 的安装、运维、管理、监控全部统一在控制台中。 如果你需要快速地搭建消息引擎系统，或者你需要搭建的是多框架构成的数据平台且 Kafka 只是其中一个组件，那么我推荐你使用这些大数据云公司提供的 Kafka。 优势在于操作简单，节省运维成本；缺陷在于把控度低，演进速度较慢。 ","date":"0001-01-01 00:00:00","objectID":"/ooooo-notes/old-notes/geektime/kafka-core-tech/04/:1:3","tags":null,"title":"","uri":"/ooooo-notes/old-notes/geektime/kafka-core-tech/04/"},{"categories":null,"content":"1、Kafka 的版本 [scala-version] - [kafka-version].tar.gz 0.10.0.0 引进 Kafka Stream 0.11.0.0 添加幂等性 Producer 和事务 API，并对 Kafka 消息格式进行重构 总结： 如果只使用 Kafka 的消息引擎功能， 最少 0.10.2.2， 可以使用新版的 Consumer API。 如果使用 Kafka Stream，最少 2.0.0。 ","date":"0001-01-01 00:00:00","objectID":"/ooooo-notes/old-notes/geektime/kafka-core-tech/05/:1:0","tags":null,"title":"","uri":"/ooooo-notes/old-notes/geektime/kafka-core-tech/05/"},{"categories":null,"content":"1、操作系统 三个方面选择： I/O 模型的使用 数据网络传输效率 社区支持度 五种 I/O 模型：阻塞式 I/O、非阻塞式 I/O、I/O 多路复用、信号驱动 I/O 和异步 I/O。 Linux 中的系统调用 select 函数就属于 I/O 多路复用模型；epoll 系统调用则介于第三种和第四种模型之间。 Linux 还可以使用 zero copy。 总结； 高性能只能选择 linux。 ","date":"0001-01-01 00:00:00","objectID":"/ooooo-notes/old-notes/geektime/kafka-core-tech/06/:1:0","tags":null,"title":"","uri":"/ooooo-notes/old-notes/geektime/kafka-core-tech/06/"},{"categories":null,"content":"2、磁盘 总结： 追求性价比的公司可以不搭建 RAID。 使用机械磁盘完全能够胜任 Kafka 线上环境。 ","date":"0001-01-01 00:00:00","objectID":"/ooooo-notes/old-notes/geektime/kafka-core-tech/06/:2:0","tags":null,"title":"","uri":"/ooooo-notes/old-notes/geektime/kafka-core-tech/06/"},{"categories":null,"content":"3、磁盘容量 规划磁盘容量的思考因素： 新增消息数 消息留存时间 平均消息大小 备份数 是否启用压缩 例如： 每一天 1 亿条 1KB 大小的消息，保存两份且存留两周的时间。 一天数据大小： 1 亿 * 1 KB * 2 份 / 1000 / 1000 = 200 GB 预留 10% 的磁盘空间： 200 GB * 1.1 = 220 GB 两周时间： 220 GB * 14 = 3 TB 开启了压缩比，比如 0.75： 3 TB * 0.75 = 2.25 TB ","date":"0001-01-01 00:00:00","objectID":"/ooooo-notes/old-notes/geektime/kafka-core-tech/06/:3:0","tags":null,"title":"","uri":"/ooooo-notes/old-notes/geektime/kafka-core-tech/06/"},{"categories":null,"content":"4、带宽 以 1Gbps 的千兆网络为例， 在 1 小时内处理 1TB 的业务数据，需要机器数？ 带宽是 1Gbps，即每秒处理 1Gb 的数据，通常假设 Kafka 只能用到 70% 的带宽资源，毕竟其他进程也需要一些资源，也就是Kafka 最大只能使用 700MB 的带宽资源，常规性使用要预留 2/3，所以单台 Kafka 使用的带宽为 700 Mb / 3 = 240 Mbps。 1 个小时处理 1TB 数据， 1024 * 1024 / 3600 / (240 / 8) = 9.7 (注意 240 Mbps 是带宽，变为 MB 单位时，要除以 8)。 如果消息的副本数为 3，大约就是 30 台机器。 ","date":"0001-01-01 00:00:00","objectID":"/ooooo-notes/old-notes/geektime/kafka-core-tech/06/:4:0","tags":null,"title":"","uri":"/ooooo-notes/old-notes/geektime/kafka-core-tech/06/"},{"categories":null,"content":"1、Broker 端参数 ","date":"0001-01-01 00:00:00","objectID":"/ooooo-notes/old-notes/geektime/kafka-core-tech/07/:1:0","tags":null,"title":"","uri":"/ooooo-notes/old-notes/geektime/kafka-core-tech/07/"},{"categories":null,"content":"1、broker 存储信息 log.dirs: 日志目录，例如 /home/kafka1,/home/kafka2,/home/kafka3。 log.dir: 只需要设置参数log.dirs，此参数不需要。 ","date":"0001-01-01 00:00:00","objectID":"/ooooo-notes/old-notes/geektime/kafka-core-tech/07/:1:1","tags":null,"title":"","uri":"/ooooo-notes/old-notes/geektime/kafka-core-tech/07/"},{"categories":null,"content":"2、zk 信息 zookeeper.connect: 连接 zk 的参数，例如 zk1:2181,zk2:2181,zk3:2181。 如果让多个 Kafka 集群使用同一套 zk 集群，利用 zk 的 chroot 设置，例如 zookeeper.connect 可以设置为 zk1:2181,zk2:2181,zk3:2181/kafka1 和 zk1:2181,zk2:2181,zk3:2181/kafka2。 ","date":"0001-01-01 00:00:00","objectID":"/ooooo-notes/old-notes/geektime/kafka-core-tech/07/:1:2","tags":null,"title":"","uri":"/ooooo-notes/old-notes/geektime/kafka-core-tech/07/"},{"categories":null,"content":"3、broker 连接信息 listeners: 监听器，也就是通过什么协议访问指定主机名和端口开放的 Kafka 服务。 advertised.listeners: Broker 用于对外发布的监听器。 监听器配置，由三元组 \u003c协议名称，主机名，端口号\u003e 构成，例如你自己定义的协议名字 CONTROLLER://localhost:9092。 一旦你自己定义了协议名称，你必须还要指定 listener.security.protocol.map 参数告诉这个协议底层使用了哪种安全协议，比如指定 listener.security.protocol.map=CONTROLLER:PLAINTEXT 表示 CONTROLLER 这个自定义协议底层使用明文不加密传输数据。 host.name/port: 过期。 ","date":"0001-01-01 00:00:00","objectID":"/ooooo-notes/old-notes/geektime/kafka-core-tech/07/:1:3","tags":null,"title":"","uri":"/ooooo-notes/old-notes/geektime/kafka-core-tech/07/"},{"categories":null,"content":"4、topic 管理 auto.create.topics.enable: 是否允许自动创建 Topic, 建议为 false。 unclean.leader.election.enable： 是否允许 Unclean Leader 选举， 建议为 false。 Unclean Leader 选举，指的是落后太多的副本参与选举，可能会使数据丢失。 auto.leader.rebalance.enable: 是否允许定期进行 Leader 选举，建议为 false。 ","date":"0001-01-01 00:00:00","objectID":"/ooooo-notes/old-notes/geektime/kafka-core-tech/07/:1:4","tags":null,"title":"","uri":"/ooooo-notes/old-notes/geektime/kafka-core-tech/07/"},{"categories":null,"content":"5、数据留存 log.retention.{hour|minutes|ms}: 日志保留时间。例如 log.retention.hour=168 表示默认保存 7 天的数据。 log.retention.bytes: 消息保存的总磁盘容量大小。默认为 -1，表示容量无限制，在云上的多租户才用到此参数。 message.max.bytes: 最大消息大小。实际上，1MB 的消息很常见。 注意：上述的参数都不能使用默认值。 ","date":"0001-01-01 00:00:00","objectID":"/ooooo-notes/old-notes/geektime/kafka-core-tech/07/:1:5","tags":null,"title":"","uri":"/ooooo-notes/old-notes/geektime/kafka-core-tech/07/"},{"categories":null,"content":"1、Topic 级别参数 如果同时设置了 Topic 级别参数和全局 Broker 参数，Topic 级别参数会覆盖全局 Broker 参数的值，而每个 Topic 都能设置自己的参数值，这就是所谓的 Topic 级别参数。 retention.ms: 该 Topic 消息被保存的时长，会覆盖掉 Broker 端的全局参数值。 retention.bytes: 该 Topic 预留多大的磁盘空间，当前默认值是 -1，表示可以无限使用磁盘空间，在多租户的 Kafka 集群中用到。 max.message.bytes: Topic 的最大消息大小。 Topic 设置方式（🎉Kafka官方文档🎉）： 创建 Topic 时设置 –config 后面指定了想要设置的 Topic 级别参数。 bin/kafka-topics.sh --bootstrap-server localhost:9092 --create --topic my-topic --partitions 1 --replication-factor 1 --config retention.ms=15552000000 --config max.message.bytes=5242880 修改 Topic 时设置 bin/kafka-configs.sh --zookeeper localhost:2181 --entity-type topics --entity-name my-topic --alter --add-config max.message.bytes=10485760 ","date":"0001-01-01 00:00:00","objectID":"/ooooo-notes/old-notes/geektime/kafka-core-tech/08/:1:0","tags":null,"title":"","uri":"/ooooo-notes/old-notes/geektime/kafka-core-tech/08/"},{"categories":null,"content":"2、JVM 参数 Java7 中，如果 Broker 所在机器的 CPU 资源非常充裕，则建议开启 CMS 垃圾回收器, -XX:+UseCurrentMarkSweepGC。否则，使用吞吐量收集器。开启方法是指定 -XX:+UseParallelGC。 Java8 中，建议使用 G1 垃圾回收器。 建议使用 Java8。 KAFKA_HEAP_OPTS: 堆大小, 建议为 6GB，这是比较公认的合理值。 KAFKA_JVM_PERFORMANCE_OPTS: 指定 GC 参数。 比如你可以这样启动 Kafka： export KAFKA_HEAP_OPTS=--Xms6g --Xmx6g export KAFKA_JVM_PERFORMANCE_OPTS= -server -XX:+UseG1GC -XX:MaxGCPauseMillis=20 -XX:InitiatingHeapOccupancyPercent=35 -XX:+ExplicitGCInvokesConcurrent -Djava.awt.headless=true bin/kafka-server-start.sh config/server.properties ","date":"0001-01-01 00:00:00","objectID":"/ooooo-notes/old-notes/geektime/kafka-core-tech/08/:2:0","tags":null,"title":"","uri":"/ooooo-notes/old-notes/geektime/kafka-core-tech/08/"},{"categories":null,"content":"3、操作系统参数 主要系统参数： 文件描述符限制 执行命令 ulimit -n 1000000 来设置。 文件系统类型 XFS 的性能要强于 ext4。 Swappiness 将 swap 交换内存配置成一个接近 0 但不为 0 的值，比如 1。 提交时间 向 Kafka 发送数据并不是真要等数据被写入磁盘才会认为成功，而是只要数据被写入到操作系统的页缓存（Page Cache）上就可以了，随后操作系统根据 LRU 算法会定期将页缓存上的\"脏\"数据落盘到物理磁盘上。这个定期就是由提交时间来确定的，默认是 5 秒。由于 Kafka 的多副本的冗余机制，可以稍微拉大提交间隔来提高性能。 ","date":"0001-01-01 00:00:00","objectID":"/ooooo-notes/old-notes/geektime/kafka-core-tech/08/:2:1","tags":null,"title":"","uri":"/ooooo-notes/old-notes/geektime/kafka-core-tech/08/"},{"categories":null,"content":"1、为什么分区 Kafka 的消息组织方式实际上是三级结构：主题 - 分区 - 消息，主题下的每条消息只会保存在某一个分区中。 Kafka 的三级结构： 分区是为了实现系统的高伸缩性（Scalability），可以通过添加新的节点来增加整体系统的吞吐量。 ","date":"0001-01-01 00:00:00","objectID":"/ooooo-notes/old-notes/geektime/kafka-core-tech/09/:1:0","tags":null,"title":"","uri":"/ooooo-notes/old-notes/geektime/kafka-core-tech/09/"},{"categories":null,"content":"2、分区策略 所谓分区策略是决定生产者将消息发送到哪个分区的算法。 ","date":"0001-01-01 00:00:00","objectID":"/ooooo-notes/old-notes/geektime/kafka-core-tech/09/:2:0","tags":null,"title":"","uri":"/ooooo-notes/old-notes/geektime/kafka-core-tech/09/"},{"categories":null,"content":"1、自定义分区策略 你需要显式地配置生产者端的参数 partitioner.class，在编写生产者程序时，你可以编写一个具体的类实现 org.apache.kafka.clients.producer.Partitioner 接口，实现 partition() 和 close() 接口。 方法签名： int partition(String topic, Object key, byte[] keyBytes, Object value, byte[] valueBytes, Cluster cluster); ","date":"0001-01-01 00:00:00","objectID":"/ooooo-notes/old-notes/geektime/kafka-core-tech/09/:2:1","tags":null,"title":"","uri":"/ooooo-notes/old-notes/geektime/kafka-core-tech/09/"},{"categories":null,"content":"2、轮询策略 Kafka 默认的分区策略就是轮询策略，轮询策略有非常优秀的负载均衡表现，它总是能保证消息最大限度地被平均分配到所有分区上。 ","date":"0001-01-01 00:00:00","objectID":"/ooooo-notes/old-notes/geektime/kafka-core-tech/09/:2:2","tags":null,"title":"","uri":"/ooooo-notes/old-notes/geektime/kafka-core-tech/09/"},{"categories":null,"content":"3、随机策略 要实现随机策略版的 partition 方法(自定义分区策略)，如下： List\u003cPartitionInfo\u003e partitions = cluster.partitionsForTopic(topic); return ThreadLocalRandom.current().nextInt(partitions.size()); ","date":"0001-01-01 00:00:00","objectID":"/ooooo-notes/old-notes/geektime/kafka-core-tech/09/:2:3","tags":null,"title":"","uri":"/ooooo-notes/old-notes/geektime/kafka-core-tech/09/"},{"categories":null,"content":"4、按消息键保序策略 实现这个策略的 partition 方法(自定义分区策略)，如下： List\u003cPartitionInfo\u003e partitions = cluster.partitionsForTopic(topic); return Math.abs(key.hashCode()) % partitions.size(); Kafka 默认分区策略实际上同时实现了两种策略：如果指定了 Key，那么默认实现按消息键保序策略；如果没有指定 Key，则使用轮询策略。 ","date":"0001-01-01 00:00:00","objectID":"/ooooo-notes/old-notes/geektime/kafka-core-tech/09/:2:4","tags":null,"title":"","uri":"/ooooo-notes/old-notes/geektime/kafka-core-tech/09/"},{"categories":null,"content":"5、其他分区策略 比较常见的一种就是基于地理位置的分区策略。 比如根据 Broker 所在的 IP 地址实现定制化的分区策略，如下： List\u003cPartitionInfo\u003e partitions = cluster.partitionsForTopic(topic); return partitions.stream().filter(p -\u003e isSouth(p.leader().host())).map(PartitionInfo::partition).findAny().get(); 我们可以从所有分区中找出那些 Leader 副本在南方的所有分区，然后随机挑选一个进行消息发送。 ","date":"0001-01-01 00:00:00","objectID":"/ooooo-notes/old-notes/geektime/kafka-core-tech/09/:2:5","tags":null,"title":"","uri":"/ooooo-notes/old-notes/geektime/kafka-core-tech/09/"},{"categories":null,"content":"1、消息格式 Kafka 的消息格式目前有两种：V1 和 V2。 不论是哪个版本，Kafka 的消息层次都分为两层：消息集合(message set) 和 消息(message)。一个消息集合包含若干条日志项(record item)，日志项中包含多条消息。 在 V1 版本中，每条消息都要执行 CRC 校验，但有些情况下消息的 CRC 值是会发生变化的。比如在 Broker 端可能会对消息时间戳字段进行更新，或者在执行消息格式转换（兼容老版本客户端程序）。对于这些情况，每条消息都执行 CRC 校验就有点没必要了。在 V2 版本中，消息的 CRC 校验工作就被移到了消息集合这一层。 在 V1 版本中，保存压缩消息的方法是把多条消息进行压缩然后保存到外层消息的消息体字段中；而在 V2 版本中，是对整个消息集合进行压缩。显然后者应该比前者有更好的压缩效果。 在相同条件下，不论是否启用压缩，V2 版本都比 V1 版本节省磁盘空间。当启用压缩时，这种节省空间的效果更加明显。 ","date":"0001-01-01 00:00:00","objectID":"/ooooo-notes/old-notes/geektime/kafka-core-tech/10/:1:0","tags":null,"title":"","uri":"/ooooo-notes/old-notes/geektime/kafka-core-tech/10/"},{"categories":null,"content":"2、何时压缩 在 Kafka 中，压缩可能发生在两个地方：生产者端和 Broker 端。 Properties props = new Properties(); props.put(\"bootstrap.servers\", \"localhost:9092\"); props.put(\"compression.type\", \"gzip\"); Producer\u003cString, String\u003e producer = new KafkaProducer\u003c\u003e(props); 上述代码，表明该 Producer 的压缩算法使用的是 GZIP。 但有两种例外情况让 Broker 重新压缩消息： Broker 端指定了和 Producer 端不同的压缩算法。 Broker 端发生了消息格式转换（兼容老的客户端程序）。 ","date":"0001-01-01 00:00:00","objectID":"/ooooo-notes/old-notes/geektime/kafka-core-tech/10/:2:0","tags":null,"title":"","uri":"/ooooo-notes/old-notes/geektime/kafka-core-tech/10/"},{"categories":null,"content":"3、何时解压缩 一句话：Producer 端压缩、Broker 端保持、Consumer 端解压缩。 除了在 Consumer 端解压缩，Broker 端也会进行解压缩，目的是为了对消息执行各种验证。 ","date":"0001-01-01 00:00:00","objectID":"/ooooo-notes/old-notes/geektime/kafka-core-tech/10/:3:0","tags":null,"title":"","uri":"/ooooo-notes/old-notes/geektime/kafka-core-tech/10/"},{"categories":null,"content":"4、压缩算法对比 在 Kafka 2.1.0 版本之前，Kafka 支持 3 种压缩算法：GZIP、Snappy 和 LZ4。从 2.1.0 开始，Kafka 正式支持 Zstandard 算法（zstd）。 压缩算法有两个重要指标：压缩比、压缩/解压缩吞吐量。 吞吐量方面: LZ4 \u003e Snappy \u003e zstd 和 GZIP。 压缩比方面：zstd \u003e LZ4 \u003e GZIP \u003e Snappy 总结： 机器的 CPU 资源不充足，不建议开启压缩，因为压缩需要消耗大量 CPU。 机器的 CPU 资源充足，强烈建议你开启 zstd 压缩，这样能极大地节省网络资源消耗。 ","date":"0001-01-01 00:00:00","objectID":"/ooooo-notes/old-notes/geektime/kafka-core-tech/10/:4:0","tags":null,"title":"","uri":"/ooooo-notes/old-notes/geektime/kafka-core-tech/10/"},{"categories":null,"content":"Kafka 核心技术与实战 01、消息引擎系统 02、Kafka 术语 ","date":"0001-01-01 00:00:00","objectID":"/ooooo-notes/old-notes/geektime/kafka-core-tech/readme/:1:0","tags":null,"title":"","uri":"/ooooo-notes/old-notes/geektime/kafka-core-tech/readme/"},{"categories":null,"content":"1、MySQL 的基本架构 主要分为Server层和存储引擎层。 Server层包括连接器、查询缓存、分析器、优化器、执行器等，涵盖MySQL的大多数核心服务功能，以及所有的内置函数（如日期、时间、数学和加密函数等），所有跨存储引擎的功能都在这一层实现，比如存储过程、触发器、视图等。 存储引擎层负责数据的存储和提取。其架构模式是插件式的，支持 InnoDB 、 MyISAM 、 Memory 等多个存储引擎。现在最常用的存储引擎是 InnoDB ，它从 MySQL 5.5.5版本开始成为了默认存储引擎。 ","date":"0001-01-01 00:00:00","objectID":"/ooooo-notes/old-notes/geektime/mysql-45/01/:1:0","tags":null,"title":"","uri":"/ooooo-notes/old-notes/geektime/mysql-45/01/"},{"categories":null,"content":"2、查询语句的执行流程 ","date":"0001-01-01 00:00:00","objectID":"/ooooo-notes/old-notes/geektime/mysql-45/01/:2:0","tags":null,"title":"","uri":"/ooooo-notes/old-notes/geektime/mysql-45/01/"},{"categories":null,"content":"1、连接器 连接器负责跟客户端建立连接、获取权限、维持和管理连接。 连接命令： mysql -h$ip -P$port -u$user -p 如果用户名密码认证通过，连接器会到权限表里面查出你拥有的权限。 连接完成后，如果你没有后续的动作，这个连接就处于空闲状态（Sleep），通过 show processlist 命令查看如下： 客户端长时间处于 Sleep 状态，连接器就会自动将它断开。由参数 wait_timeout 控制的，默认值是8小时。 数据库里面，长连接是指连接成功后，如果客户端持续有请求，则一直使用同一个连接。短连接则是指每次执行完很少的几次查询就断开连接，下次查询再重新建立一个。建立连接的过程通常是比较复杂的，所以尽量使用长连接。 但全部使用长连接后，有些时候 MySQL 占用内存涨得特别快，这是因为 MySQL 在执行过程中临时使用的内存是管理在连接对象里面的，这些资源会在连接断开的时候才释放。所以如果长连接累积下来，可能导致内存占用太大，被系统强行杀掉（OOM），从现象看就是 MySQL 异常重启了。 长连接解决方案： 定期断开长连接. 在 MySQL 5.7 版本后，通过执行 mysql_reset_connection来重新初始化连接资源， 无需重新连接和权限认证。 ","date":"0001-01-01 00:00:00","objectID":"/ooooo-notes/old-notes/geektime/mysql-45/01/:2:1","tags":null,"title":"","uri":"/ooooo-notes/old-notes/geektime/mysql-45/01/"},{"categories":null,"content":"2、查询缓存 只要对一个表更新，这个表上所有的查询缓存都会被清空。 MySQL 8.0 缓存功能已经被删除。 ","date":"0001-01-01 00:00:00","objectID":"/ooooo-notes/old-notes/geektime/mysql-45/01/:2:2","tags":null,"title":"","uri":"/ooooo-notes/old-notes/geektime/mysql-45/01/"},{"categories":null,"content":"3、分析器 对SQL语句做解析，词法分析（select、insert、delete、update）、语法分析（语法是否正确）。 ","date":"0001-01-01 00:00:00","objectID":"/ooooo-notes/old-notes/geektime/mysql-45/01/:2:3","tags":null,"title":"","uri":"/ooooo-notes/old-notes/geektime/mysql-45/01/"},{"categories":null,"content":"4、优化器 优化器选择索引。 例如，对于下面的语句，有两种查询逻辑： mysql\u003e select * from t1 join t2 using(ID) where t1.c=10 and t2.d=20; 先从 t1 里面取出 c = 10 的记录的ID值，再根据 ID 值关联到 t2，再判断 t2 里面 d 的值是否等于 20。 先从 t2 里面取出 d = 20 的记录的ID值，再根据 ID 值关联到 t1，再判断 t1 里面 c 的值是否等于 10。 ","date":"0001-01-01 00:00:00","objectID":"/ooooo-notes/old-notes/geektime/mysql-45/01/:2:4","tags":null,"title":"","uri":"/ooooo-notes/old-notes/geektime/mysql-45/01/"},{"categories":null,"content":"5、执行器 先判断查询权限，如果有权限，执行器就会调用存储引擎提供的接口。 ","date":"0001-01-01 00:00:00","objectID":"/ooooo-notes/old-notes/geektime/mysql-45/01/:2:5","tags":null,"title":"","uri":"/ooooo-notes/old-notes/geektime/mysql-45/01/"},{"categories":null,"content":"3、问题 如果你执行 select * from T where k=1，报不存在 K 这一列。是在哪个阶段报出来的？ 分析器阶段 ","date":"0001-01-01 00:00:00","objectID":"/ooooo-notes/old-notes/geektime/mysql-45/01/:3:0","tags":null,"title":"","uri":"/ooooo-notes/old-notes/geektime/mysql-45/01/"},{"categories":null,"content":"1、更新语句的执行流程 创建表 T: mysql\u003e create table T(ID int primary key, c int); 将 ID = 2 这一行的值加 1 的 SQL 语句: mysql\u003e update T set c=c+1 where ID=2; 查询语句的那一套流程，更新语句也是同样会走一遍。 经过连接器和查询缓存之后，分析器通过词法和语法分析知道这是一条更新语句，优化器决定要使用 ID 这个索引，执行器负责具体执行，找到这一行，然后更新。 与查询流程不一样的是，更新流程还涉及两个重要的日志模块： **redo log（重做日志）**和 binlog（归档日志）。 ","date":"0001-01-01 00:00:00","objectID":"/ooooo-notes/old-notes/geektime/mysql-45/02/:1:0","tags":null,"title":"","uri":"/ooooo-notes/old-notes/geektime/mysql-45/02/"},{"categories":null,"content":"2、redo log redo log 是 InnoDB 引擎特有的日志模块。 在 MySQL 中，如果每一次的更新操作都需要写进磁盘，然后磁盘也要找到对应的那条记录，然后再更新，整个过程 IO 成本、查找成本都很高。为了提高更新效率，MySQL 使用 WAL（Write-Ahead Logging）技术，它的关键点就是先写日志，再写磁盘。 当有记录需要更新时，InnoDB 引擎就会先把记录写到 redo log 里面，并更新内存，这时更新就算完成了。同时，InnoDB引擎会在适当的时候（系统比较空闲），将这个记录更新到磁盘里面。 InnoDB的 redo log 是固定大小的，比如可以配置为一组 4 个文件，每个文件的大小是 1GB，那么总共可以记录 4GB 的操作。从头开始写，写到末尾又回到开头循环写。 write pos 是当前记录的位置，一边写一边后移。 checkpoint 是当前要擦除的位置，擦除记录前要把记录更新到数据文件。 有了 redo log，InnoDB 就可以保证即使数据库发生异常重启，之前提交的记录都不会丢失，这个能力称为 crash-safe。 ","date":"0001-01-01 00:00:00","objectID":"/ooooo-notes/old-notes/geektime/mysql-45/02/:2:0","tags":null,"title":"","uri":"/ooooo-notes/old-notes/geektime/mysql-45/02/"},{"categories":null,"content":"3、binlog binlog 是 Sever 层的日志模块，只能用于归档，比如主从复制。 两种日志不同： redo log 是InnoDB引擎特有的；binlog是MySQL的Server层实现的，所有引擎都可以使用。 redo log 是物理日志，记录的是“在某个数据页上做了什么修改”；binlog 是逻辑日志，记录的是这个语句的原始逻辑，比如“给ID=2这一行的c字段加1”。 redo log 是循环写，写到末尾又回到开头写；binlog是追加写入，写到一定大小后会切换到下一个，并不会覆盖以前的日志。 ","date":"0001-01-01 00:00:00","objectID":"/ooooo-notes/old-notes/geektime/mysql-45/02/:3:0","tags":null,"title":"","uri":"/ooooo-notes/old-notes/geektime/mysql-45/02/"},{"categories":null,"content":"4、两阶段提交 上面简单的 updata 语句的执行流程。 执行器先找引擎取 ID = 2 这一行。ID 是主键，引擎直接用树搜索找到这一行。如果ID=2 这一行所在的数据页本来就在内存中，就直接返回给执行器；否则，需要先从磁盘读入内存，然后再返回。 执行器拿到引擎给的行数据，把这个值加上1，比如原来是N，现在就是N+1，得到新的一行数据，再调用引擎接口写入这行新数据。 引擎将这行新数据更新到内存中，同时将这个更新操作记录到 redo log 里面，此时 redo log 处于 prepare 状态。然后告知执行器执行完成了，随时可以提交事务。 执行器生成这个操作的 binlog，并把 binlog 写入磁盘。 执行器调用引擎的提交事务接口，引擎把刚刚写入的 redo log 改成 commit 状态，更新完成。 如果不使用两阶段提交，会有什么问题？ 先写 redo log 后写 binlog。假设在 redo log 写完，binlog 还没有写完的时候，MySQL进程异常重启，binlog 中没有更新的数据。 先写 binlog 后写 redo log。如果在 binlog 写完之后 crash，由于 redo log 没有写，崩溃恢复以后这个事务无效，所以这一行 c 的值是 0，但 binlog 中 c = 1 了。 ","date":"0001-01-01 00:00:00","objectID":"/ooooo-notes/old-notes/geektime/mysql-45/02/:4:0","tags":null,"title":"","uri":"/ooooo-notes/old-notes/geektime/mysql-45/02/"},{"categories":null,"content":"5、配置 双1设置： innodb_flush_log_at_trx_commit = 1 表示每次事务的 redo log 都直接持久化到磁盘。这样可以保证 MySQL 异常重启之后数据不丢失。 sync_binlog = 1 表示每次事务的 binlog 都持久化到磁盘。这样可以保证 MySQL 异常重启之后 binlog 不丢失。 ","date":"0001-01-01 00:00:00","objectID":"/ooooo-notes/old-notes/geektime/mysql-45/02/:5:0","tags":null,"title":"","uri":"/ooooo-notes/old-notes/geektime/mysql-45/02/"},{"categories":null,"content":"6、问题 在什么场景下，一天一备会比一周一备更有优势呢？ 好处是“最长恢复时间”更短。 在一天一备的模式里，最坏情况下需要应用一天的 binlog。比如，你每天 0 点做一次全量备份，而要恢复出一个到昨天晚上 23 点的备份。 一周一备最坏情况就要应用一周的 binlog 了。 ","date":"0001-01-01 00:00:00","objectID":"/ooooo-notes/old-notes/geektime/mysql-45/02/:6:0","tags":null,"title":"","uri":"/ooooo-notes/old-notes/geektime/mysql-45/02/"},{"categories":null,"content":"1、事务隔离级别 事务的 ACID 中的 I 指的就是隔离性（Isolation）。 ","date":"0001-01-01 00:00:00","objectID":"/ooooo-notes/old-notes/geektime/mysql-45/03/:1:0","tags":null,"title":"","uri":"/ooooo-notes/old-notes/geektime/mysql-45/03/"},{"categories":null,"content":"1、隔离级别 读未提交（read-uncommitted）：一个事务还没提交时，它做的变更就能被别的事务看到。 读提交（read-committed）：一个事务提交之后，它做的变更才会被其他事务看到。 可重复读（repeatable-read）：一个事务执行过程中看到的数据，总是跟这个事务在启动时看到的数据是一致的。 串行化（serializable）：当出现读写锁冲突的时候，后执行事务必须等前一个事务执行完成，才能继续执行。 MySQL 的隔离级别设置为\"读提交\"。 ","date":"0001-01-01 00:00:00","objectID":"/ooooo-notes/old-notes/geektime/mysql-45/03/:1:1","tags":null,"title":"","uri":"/ooooo-notes/old-notes/geektime/mysql-45/03/"},{"categories":null,"content":"2、事务隔离例子 mysql\u003e create table T(c int) engine=InnoDB; insert into T(c) values(1); 在不同的隔离级别下的结果： 隔离级别为读未提交，v1 = 2, v2 = 2, v3 = 2。 隔离级别为读提交，v1 = 1, v2 = 2, v3 = 2。 隔离级别为可重复读，v1 = 1, v2 = 1, v3 = 2。 隔离级别为串行化，v1 = 1, v2 = 1, v3 = 2。 在实现上，数据库里面会创建一个视图，访问的时候以视图的逻辑结果为准。 在可重复读隔离级别下，这个视图是在事务启动时创建的，整个事务存在期间都用这个视图。 在读提交隔离级别下，这个视图是在每个SQL语句开始执行的时候创建的。 在读未提交隔离级别下，直接返回记录上的最新值，没有视图概念。 在串行化隔离级别下，直接用加锁的方式来避免并行访问。 ","date":"0001-01-01 00:00:00","objectID":"/ooooo-notes/old-notes/geektime/mysql-45/03/:1:2","tags":null,"title":"","uri":"/ooooo-notes/old-notes/geektime/mysql-45/03/"},{"categories":null,"content":"3、事务配置方式 通过命令 show variables like 'transaction_isolation'; 来查看当前隔离级别。通过命令 set transaction_isolation = 'read-committed'; 来设置隔离级别。 ","date":"0001-01-01 00:00:00","objectID":"/ooooo-notes/old-notes/geektime/mysql-45/03/:1:3","tags":null,"title":"","uri":"/ooooo-notes/old-notes/geektime/mysql-45/03/"},{"categories":null,"content":"2、事务隔离的实现 在 MySQL 中，实际上每条记录在更新的时候都会同时记录一条回滚操作。记录上的最新值，通过回滚操作，都可以得到前一个状态的值。 假设一个值从 1 被按顺序改成了 2、3、4，在回滚日志里面就会有类似下面的记录。 长事务意味着系统里面会存在很老的事务视图。由于这些事务随时可能访问数据库里面的任何数据，所以这个事务提交之前，数据库里面它可能用到的回滚记录都必须保留，这就会导致大量占用存储空间。长事务还占用锁资源，也可能拖垮整个库。 ","date":"0001-01-01 00:00:00","objectID":"/ooooo-notes/old-notes/geektime/mysql-45/03/:2:0","tags":null,"title":"","uri":"/ooooo-notes/old-notes/geektime/mysql-45/03/"},{"categories":null,"content":"3、事务的启动方式 建议你总是使用 set autocommit = 1, 通过显式语句的方式来启动事务。 在 autocommit = 1 的情况下，用 begin 显式启动的事务，如果执行 commit 则提交事务。如果执行 commit work and chain，则是提交事务并自动启动下一个事务。 你可以在 information_schema 库的 innodb_trx 这个表中查询长事务，比如下面这个语句，用于查找持续时间超过 60s 的事务。 mysql\u003e select * from information_schema.innodb_trx where TIME_TO_SEC(timediff(now(),trx_started))\u003e60 ","date":"0001-01-01 00:00:00","objectID":"/ooooo-notes/old-notes/geektime/mysql-45/03/:3:0","tags":null,"title":"","uri":"/ooooo-notes/old-notes/geektime/mysql-45/03/"},{"categories":null,"content":"4、问题 怎样避免长事务？ 在开发端： 设置set autocommit = 0。 确认是否有不必要的只读事务。 通过 SET MAX_EXECUTION_TIME 命令，来控制每个语句执行的最长时间，避免单个语句意外执行太长时间。 在数据库端： 控 information_schema.Innodb_trx 表，设置长事务阈值，超过就报警/或者 kill。 ercona 的 pt-kill 这个工具不错，推荐使用。 在业务功能测试阶段要求输出所有的 general_log，分析日志行为提前发现问题。 如果使用的是MySQL 5.6或者更新版本，把 innodb_undo_tablespaces 设置成2（或更大的值）。如果真的出现大事务导致回滚段过大，这样设置后清理起来更方便。 ","date":"0001-01-01 00:00:00","objectID":"/ooooo-notes/old-notes/geektime/mysql-45/03/:4:0","tags":null,"title":"","uri":"/ooooo-notes/old-notes/geektime/mysql-45/03/"},{"categories":null,"content":"1、索引的常见模型 ","date":"0001-01-01 00:00:00","objectID":"/ooooo-notes/old-notes/geektime/mysql-45/04/:1:0","tags":null,"title":"","uri":"/ooooo-notes/old-notes/geektime/mysql-45/04/"},{"categories":null,"content":"1、哈希表 以键-值（key-value）存储数据的结构。只适用于等值查询的场景。 ","date":"0001-01-01 00:00:00","objectID":"/ooooo-notes/old-notes/geektime/mysql-45/04/:1:1","tags":null,"title":"","uri":"/ooooo-notes/old-notes/geektime/mysql-45/04/"},{"categories":null,"content":"2、有序数组 查询效率高，但更新数据，成本高。只适用静态存储引擎。 上面数组的按照 ID_card 升序排列，如果查询条件是 where ID_card = '?'，可以用二分法查询。 ","date":"0001-01-01 00:00:00","objectID":"/ooooo-notes/old-notes/geektime/mysql-45/04/:1:2","tags":null,"title":"","uri":"/ooooo-notes/old-notes/geektime/mysql-45/04/"},{"categories":null,"content":"3、搜索树 InnoDB 引擎中使用 B+ 树（ N 叉树）。可以减少磁盘 IO。 ","date":"0001-01-01 00:00:00","objectID":"/ooooo-notes/old-notes/geektime/mysql-45/04/:1:3","tags":null,"title":"","uri":"/ooooo-notes/old-notes/geektime/mysql-45/04/"},{"categories":null,"content":"2、InnoDB的索引模型 我们有一个主键列为 ID 的表，表中有字段 k，并且在 k 上有索引，建表语句如下： mysql\u003e create table T( id int primary key, k int not null, name varchar(16), index (k))engine=InnoDB; 表中 R1 ~ R5 的 (ID,k) 值分别为 (100,1)、(200,2)、(300,3)、(500,5) 和 (600,6)。 在 InnoDB 引擎中，每个索引就是一颗 B+ 树。两颗索引树如下。 根据叶子节点的内容，索引类型分为主键索引和非主键索引。 主键索引的叶子节点存放的是整行数据。 非主键索引的叶子节点存放的是主键的值。 基于主键索引和普通索引的查询有什么区别？ 如果语句是 select * from T where ID=500，即主键查询方式，则只需要搜索 ID 这棵B+树； 如果语句是 select * from T where k=5，即普通索引查询方式，则需要先搜索 k 索引树，得到 ID 的值为 500，再到 ID 索引树搜索一次。这个过程称为回表。 ","date":"0001-01-01 00:00:00","objectID":"/ooooo-notes/old-notes/geektime/mysql-45/04/:2:0","tags":null,"title":"","uri":"/ooooo-notes/old-notes/geektime/mysql-45/04/"},{"categories":null,"content":"3、索引维护 B+ 树为了维护索引有序性，在插入新值的时候需要做必要的维护。以上面这个图为例，如果插入新的行 ID 值为 700，则只需要在 R5 的记录后面插入一个新记录。如果新插入的 ID 值为 400，就相对麻烦了，需要逻辑上挪动后面的数据，空出位置。 而更糟的情况是，如果 R5 所在的数据页已经满了，根据 B+ 树的算法，这时候需要申请一个新的数据页，然后挪动部分数据过去。这个过程称为页分裂。在这种情况下，性能自然会受影响。 除了性能外，页分裂操作还影响数据页的利用率。原本放在一个页的数据，现在分到两个页中，整体空间利用率降低大约50%。当相邻两个页由于删除了数据，利用率很低之后，会将数据页做合并。 总结： 如果主键是自增的，每次插入一条新的数据，就是追加操作，就不会触发页分裂。 主键长度越小，普通索引的叶子节点就越小，占用的空间也就越小。 ","date":"0001-01-01 00:00:00","objectID":"/ooooo-notes/old-notes/geektime/mysql-45/04/:3:0","tags":null,"title":"","uri":"/ooooo-notes/old-notes/geektime/mysql-45/04/"},{"categories":null,"content":"4、问题 对于上面例子中的 InnoDB 表 T，如果你要重建索引 k，你的两个 SQL 语句可以这么写： alter table T drop index k; alter table T add index(k); 如果你要重建主键索引，也可以这么写： alter table T drop primary key; alter table T add primary key(id); 可以执行 alter table T engine=InnoDB; 来重建索引。 ","date":"0001-01-01 00:00:00","objectID":"/ooooo-notes/old-notes/geektime/mysql-45/04/:4:0","tags":null,"title":"","uri":"/ooooo-notes/old-notes/geektime/mysql-45/04/"},{"categories":null,"content":"1、覆盖索引 创建表 T : mysql\u003e create table T ( ID int primary key, k int NOT NULL DEFAULT 0, s varchar(16) NOT NULL DEFAULT '', index k(k)) engine=InnoDB; insert into T values(100,1, 'aa'),(200,2,'bb'),(300,3,'cc'),(500,5,'ee'),(600,6,'ff'),(700,7,'gg'); 执行语句 select ID from T where k between 3 and 5，只需要扫描 k 索引树。因为结果只需要查询 ID，而 ID 在 k 索引树上，减少了回表操作。 索引树已经覆盖了查询结果，称之为覆盖索引。覆盖索引可以减少树的搜索次数，显著提升查询性能。 ","date":"0001-01-01 00:00:00","objectID":"/ooooo-notes/old-notes/geektime/mysql-45/05/:1:0","tags":null,"title":"","uri":"/ooooo-notes/old-notes/geektime/mysql-45/05/"},{"categories":null,"content":"2、最左前缀匹配 B+ 树这种索引结构，可以利用索引的最左前缀，来定位记录。 创建表 tuser ： CREATE TABLE `tuser` ( `id` int(11) NOT NULL, `id_card` varchar(32) DEFAULT NULL, `name` varchar(32) DEFAULT NULL, `age` int(11) DEFAULT NULL, `ismale` tinyint(1) DEFAULT NULL, PRIMARY KEY (`id`), KEY `id_card` (`id_card`), KEY `name_age` (`name`,`age`) ) ENGINE=InnoDB 当你的查询条件是 where name like ‘张%’，也是可以用到索引（name,age）的。 联合索引建立规则： 通过调整顺序，可以少维护一个索引，这是优先考虑的。 索引的空间。 ","date":"0001-01-01 00:00:00","objectID":"/ooooo-notes/old-notes/geektime/mysql-45/05/:2:0","tags":null,"title":"","uri":"/ooooo-notes/old-notes/geektime/mysql-45/05/"},{"categories":null,"content":"3、索引下推 可以对索引中存在的字段先做判断，减少回表次数。 当查询条件是 where name like '张%' and age=10 and ismale=1;，也是可以用到索引树（name,age）的。 ","date":"0001-01-01 00:00:00","objectID":"/ooooo-notes/old-notes/geektime/mysql-45/05/:3:0","tags":null,"title":"","uri":"/ooooo-notes/old-notes/geektime/mysql-45/05/"},{"categories":null,"content":"4、问题 有如下表： CREATE TABLE `geek` ( `a` int(11) NOT NULL, `b` int(11) NOT NULL, `c` int(11) NOT NULL, `d` int(11) NOT NULL, PRIMARY KEY (`a`,`b`), KEY `c` (`c`), KEY `ca` (`c`,`a`), KEY `cb` (`c`,`b`) ) ENGINE=InnoDB; 由于历史原因，这个表需要a、b做联合主键。 查询语句如下： select * from geek where c=N order by a limit 1; select * from geek where c=N order by b limit 1; 索引（c,a）、（c,b）是否都是必须的? 索引 (c,a) 不需要。 ","date":"0001-01-01 00:00:00","objectID":"/ooooo-notes/old-notes/geektime/mysql-45/05/:4:0","tags":null,"title":"","uri":"/ooooo-notes/old-notes/geektime/mysql-45/05/"},{"categories":null,"content":"1、全局表 根据加锁的范围，MySQL 里面的锁大致可以分成全局锁、表级锁和行锁三类。 MySQL 加全局读锁的命令：flush tables with read lock;（FTWRL）, 解锁命令：unlock tables;。让整个库处于只读状态。任何增删改语句都会被阻塞。 全局锁的典型使用场景是，做全库逻辑备份，也就是 select 所有的数据。 对于 MyISAM 引擎来说，备份只能加全局锁。 对于 Innodb 引擎来说，可以使用脚本 mysqldump，参数为 –-single-transaction，来启动一个事务，确保拿到一致性视图来备份数据。MyISAM 不支持事务，所以无法用此脚本。 全库只读，为什么不使用 set global readonly=true 的方式，主要原因有两点： 在有些系统中，readonly 会被用来做其他逻辑，比如判断一个库是主库还是备库。 异常处理机制有差异。 执行 FTWRL 命令后，客户端发生异常断开，MySQL 会自动释放这个全局锁。如果整库处理 readonly 状态，客户端发生异常，数据库会一直处于 readonly 状态。 ","date":"0001-01-01 00:00:00","objectID":"/ooooo-notes/old-notes/geektime/mysql-45/06/:1:0","tags":null,"title":"","uri":"/ooooo-notes/old-notes/geektime/mysql-45/06/"},{"categories":null,"content":"2、表级锁 MySQL 里面表级别的锁有两种：一种是表锁，一种是元数据锁（meta data lock，MDL)。 例如：如果在某个线程 A 中执行 lock tables t1 read, t2 write; 这个语句，则其他线程写 t1 、读写 t2 的语句都会被阻塞。线程 A 执行 unlock tables; 语句来解锁。 MDL 不需要显式使用，在访问一个表的时候会被自动加上。MDL 的作用是，保证读写的正确性。 在 MySQL 5.5 版本中引入了 MDL，当对一个表做增删改查操作的时候，加MDL读锁；当要对表做结构变更操作的时候，加MDL写锁。 给一个小表加个字段，导致整个库挂了。 session A 和 session B 都会加上 MDL读锁。 session C 要变更表结构，必要要加上 MDL写锁，此时只能阻塞。 session D 申请 MDL读锁 就会被 session C 阻塞。 MDL 锁必须要等整个事务提交后再释放。 如何安全地给小表加字段？ 首先要解决长事务，事务不提交，就会一直占着 MDL锁。在 MySQL 的 information_schema 库的 innodb_trx 表中，你可以查到当前执行中的事务。如果你要做 DDL 变更的表刚好有长事务在执行，要考虑先暂停 DDL，或者 kill 掉这个长事务。 如果要变更的表是一个热点表，虽然数据量不大，但是上面的请求很频繁，而你不得不加个字段, kill 可能未必管用，比较理想的机制是，在 alter table 语句里面设定等待时间，如果在这个等待时间里面能够拿到 MDL写锁 最好，拿不到也不要阻塞后面的业务语句，先放弃。之后再通过重试命令重复这个过程。 ALTER TABLE tbl_name NOWAIT add column ... ALTER TABLE tbl_name WAIT N add column ... ","date":"0001-01-01 00:00:00","objectID":"/ooooo-notes/old-notes/geektime/mysql-45/06/:2:0","tags":null,"title":"","uri":"/ooooo-notes/old-notes/geektime/mysql-45/06/"},{"categories":null,"content":"3、问题 备份一般都会在备库上执行，你在用 –-single-transaction 方法做逻辑备份的过程中，如果主库上的一个小表做了一个DDL，比如给一个表上加了一列。这时候，从备库上会看到什么现象呢？ 假设这个 DDL 是针对表 t1 的， 这里把备份过程中几个关键的语句列出来： Q1:SET SESSION TRANSACTION ISOLATION LEVEL REPEATABLE READ; Q2:START TRANSACTION WITH CONSISTENT SNAPSHOT; /* other tables */ Q3:SAVEPOINT sp; /* 时刻 1 */ Q4:show create table `t1`; /* 时刻 2 */ Q5:SELECT * FROM `t1`; /* 时刻 3 */ Q6:ROLLBACK TO SAVEPOINT sp; /* 时刻 4 */ /* other tables */ 在备份开始的时候，为了确保RR（可重复读）隔离级别，再设置一次RR隔离级别(Q1); 启动事务(Q2); 设置一个保存点，这个很重要(Q3); show create 是为了拿到表结构(Q4)，然后正式导数据 （Q5），回滚到SAVEPOINT sp，在这里的作用是释放 t1 的 MDL锁。 答案如下： 如果在 Q4 语句执行之前到达，现象：没有影响，备份拿到的是DDL后的表结构。 如果在\"时刻 2\"到达，则 Q5 执行的时候表结构被改过，报 Table definition has changed, please retry transaction，现象：mysqldump 终止； 如果在\"时刻2\"和\"时刻3\"之间到达，mysqldump 占着 t1 的 MDL读锁，binlog 被阻塞，现象：主从延迟，直到 Q6 执行完成。 从\"时刻4\"开始，mysqldump 释放了 MDL读锁，现象：没有影响，备份拿到的是 DDL 前的表结构。 ","date":"0001-01-01 00:00:00","objectID":"/ooooo-notes/old-notes/geektime/mysql-45/06/:3:0","tags":null,"title":"","uri":"/ooooo-notes/old-notes/geektime/mysql-45/06/"},{"categories":null,"content":"1、行锁 MySQL 的行锁是在引擎层由各个引擎自己实现的，并不是所有的引擎都支持行锁，比如 MyISAM 引擎就不支持，InnoDB 引擎支持行锁。 两阶段锁： 实际上事务B的 update 语句会被阻塞，直到事务A执行 commit 之后，事务B才能继续执行。 在 InnoDB 事务中，行锁是在需要的时候才加上的，但并不是不需要了就立刻释放，而是要等到事务结束时才释放。这个就是两阶段锁协议。 如果你的事务中需要锁多个行，要把最可能造成锁冲突、最可能影响并发度的锁尽量往后放。 比如，电影票在线交易业务： 从顾客A账户余额中扣除电影票价。 给影院B的账户余额增加这张电影票价。 记录一条交易日志。 要保证交易的原子性，就要把这三个操作放在一个事务中，按照 3 -\u003e 1 -\u003e 2 的顺序执行，就减少了事务之间的锁等待，提升了并发度。 ","date":"0001-01-01 00:00:00","objectID":"/ooooo-notes/old-notes/geektime/mysql-45/07/:1:0","tags":null,"title":"","uri":"/ooooo-notes/old-notes/geektime/mysql-45/07/"},{"categories":null,"content":"2、死锁和死锁检测 死锁例子： 事务A在等待事务B释放 id=2 的行锁，而事务B在等待事务A释放 id=1 的行锁。 事务A和事务B在互相等待对方的资源释放，就是进入了死锁状态。 当出现死锁以后，有两种策略 直接进入等待，直到超时，这个超时时间通过参数 innodb_lock_wait_timeout 来设置。 发起死锁检测，发现死锁后，主动回滚死锁链条中的某一个事务，让其他事务得以继续执行。通过参数 innodb_deadlock_detect 设置为 on。 在 InnoDB 中，innodb_lock_wait_timeout 的默认值是 50s，当出现死锁以后，第一个被锁住的线程要过 50s 才会超时退出，这个等待时间肯定是无法接受的，但如果设置为 1s，可能出现不是死锁的情况（大事务），造成误伤。 正常情况下，我们要采用第二种策略：主动死锁检测。innodb_deadlock_detect 的默认值为 on。 当更新同一行时，每个线程检查的时间复杂度为 O(n), 也就是说 1000 个线程，要操作 100w 次，所以死锁检测要耗费大量的 CPU 资源。 怎么解决由这种热点行更新导致的性能问题呢？ 一种头痛医头的方法，就是如果你能确保这个业务一定不会出现死锁，可以临时把死锁检测关掉。业务有损。 控制并发度。比如同一行同时最多只有 10 个线程在更新，那么死锁检测的成本很低，就不会出现这个问题。 并发控制要做在数据库服务端，如果你有中间件，可以考虑在中间件实现；如果你的团队有能修改 MySQL 源码的人，也可以做在 MySQL 里面。基本思路就是，对于相同行的更新，在进入引擎之前排队。这样在 InnoDB 内部就不会有大量的死锁检测工作了。 控制并发度也可以从业务设计上优化。考虑将一行改成逻辑上的多行来减少锁冲突。 还是以影院账户为例，可以考虑放在多条记录上，比如 10 个记录，影院的账户总额等于这 10 个记录的值的总和。这样每次要给影院账户加金额的时候，随机选其中一条记录来加。这样每次冲突概率变成原来的 1/10，可以减少锁等待个数，也就减少了死锁检测的 CPU 消耗。 如果账户余额可能会减少，比如退票逻辑，那么这时候就需要考虑当一部分行记录变成 0 的时候，代码要有特殊处理。 ","date":"0001-01-01 00:00:00","objectID":"/ooooo-notes/old-notes/geektime/mysql-45/07/:2:0","tags":null,"title":"","uri":"/ooooo-notes/old-notes/geektime/mysql-45/07/"},{"categories":null,"content":"3、问题 如果你要删除一个表里面的前 10000 行数据，有以下三种方法可以做到： 第一种，直接执行 delete from T limit 10000; 第二种，在一个连接中循环执行20次 delete from T limit 500; 第三种，在 20 个连接中同时执行 delete from T limit 500。 你会选择哪一种方法呢？为什么呢？ 第二种方式是相对较好的。 第一种方式单个语句占用时间长，锁的时间也比较长；而且大事务还会导致主从延迟。 第三种方式会人为造成锁冲突。 ","date":"0001-01-01 00:00:00","objectID":"/ooooo-notes/old-notes/geektime/mysql-45/07/:3:0","tags":null,"title":"","uri":"/ooooo-notes/old-notes/geektime/mysql-45/07/"},{"categories":null,"content":"1、事务 在第 3 篇文章和你讲事务隔离级别的时候提到过，如果是可重复读隔离级别，事务 T 启动的时候会创建一个视图 read-view，之后事务 T 执行期间，即使有其他事务修改了数据，事务 T 看到的仍然跟在启动时看到的一样。 例子： mysql\u003e CREATE TABLE `t` ( `id` int(11) NOT NULL, `k` int(11) DEFAULT NULL, PRIMARY KEY (`id`) ) ENGINE=InnoDB; insert into t(id, k) values(1,1),(2,2); 在可重复读隔离级别下，begin/start transaction 并不是事务的起点，只有执行到第一个语句时才会真正启动事务。如果你想要马上启动一个事务，可以使用 start transaction with consistent snapshot 这个命令。 在这个例子中，事务 C 没有显式地使用 begin/commit ，表示这个 update 语句本身就是一个事务，语句完成的时候会自动提交。 结果是事务 B 的 k 值为 3，事务A的 k 值为 1。 在 MySQL 里，有两个\"视图\"的概念: 一个是 view，它是一个用查询语句定义的虚拟表。 另一个是 InnoDB 在实现 MVCC 时用到的一致性读视图，即 consistent read view。 ","date":"0001-01-01 00:00:00","objectID":"/ooooo-notes/old-notes/geektime/mysql-45/08/:1:0","tags":null,"title":"","uri":"/ooooo-notes/old-notes/geektime/mysql-45/08/"},{"categories":null,"content":"2、“快照\"怎么工作的 在可重复读隔离级别下，事务在启动的时候就\"拍了个快照”。这个快照是基于整库的。 InnoDB 里面每个事务有一个唯一的事务 ID，叫作 transaction id 。它是在事务开始的时候向 InnoDB 的事务系统申请的，是按申请顺序严格递增的。 而每行数据也都是有多个版本的。每次事务更新数据的时候，都会生成一个新的数据版本，并且把transaction id赋值给这个数据版本的事务 ID，记为 row trx_id。也就是说，表中的一行记录，其实可能有多个版本(row)，每个版本有自己的 row trx_id。 一个记录被多个事务连续更新后的状态: 图中虚线框里是同一行数据的4个版本，当前最新版本是 V4，k 的值是 22，它是被transaction id 为 25 的事务更新的，因此它的 row trx_id 也是 25。 图中三个虚线箭头，就是 undo log，而 V1、V2、V3 并不是物理上真实存在的，而是每次需要的时候根据当前版本和 undo log 计算出来的。 一个事务只需要在启动的时候声明说，“以我启动的时刻为准，如果一个数据版本是在我启动之前生成的，就认；如果是我启动以后才生成的，我就不认，我必须要找到它的上一个版本”。如果\"上一个版本\"也不可见，那就得继续往前找。 在实现上， InnoDB 为每个事务构造了一个数组，用来保存这个事务启动瞬间，当前正在 活跃 (启动了但还没提交)的所有事务 ID。 数组里面事务 ID 的最小值记为低水位，当前系统里面已经创建过的事务 ID 的最大值加1记为高水位。 对于一个数据版本的 row trx_id, 有以下几种可能： 如果落在绿色部分，表示这个版本是已提交的事务或者是当前事务自己生成的，这个数据是可见的； 如果落在红色部分，表示这个版本是由将来启动的事务生成的，是肯定不可见的； 如果落在黄色部分，那就包括两种情况 a. 若 row trx_id 在数组中，表示这个版本是由还没提交的事务生成的，不可见； b. 若 row trx_id 不在数组中，表示这个版本是已经提交了的事务生成的，可见。 分析下图 1 中的三个事务，事务 A 为什么是 k=1 ？ 假设： 事务A开始前，系统里面只有一个活跃事务 ID 是 99； 事务 A、B、C 的版本号分别是 100、101、102，且当前系统里只有这四个事务； 三个事务开始前，(1,1）这一行数据的 row trx_id是 90。 这样，事务 A 的视图数组就是 [99,100] , 事务B的视图数组是 [99,100,101], 事务C的视图数组是 [99,100,101,102]。 事务A查询逻辑有关的操作: 总结： 一个数据版本，对于一个事务视图来说，除了自己的更新总是可见以外，有三种情况： 版本未提交，不可见； 版本已提交，但是是在视图创建后提交的，不可见； 版本已提交，而且是在视图创建前提交的，可见。 ","date":"0001-01-01 00:00:00","objectID":"/ooooo-notes/old-notes/geektime/mysql-45/08/:2:0","tags":null,"title":"","uri":"/ooooo-notes/old-notes/geektime/mysql-45/08/"},{"categories":null,"content":"3、更新逻辑 更新数据都是先读后写的，而这个读，只能读当前的值，称为\"当前读\"（current read）。 在执行事务 B 语句的时候，update 语句是当前读，这是 row trx_id 为 101，所以 select 语句能读到 k=3。 除了 update 语句外，select 语句如果加锁，也是当前读。 mysql\u003e select k from t where id=1 lock in share mode; # 读锁（S锁，共享锁） mysql\u003e select k from t where id=1 for update; # 写锁（X锁，排他锁） 假设事务 C 不是马上提交的，而是变成了下面的事务 C’，会怎么样呢？ 事务 C’ 没提交，也就是说 (1,2) 这个版本上的写锁还没释放。而事务B 是当前读，必须要读最新版本，而且必须加锁，因此就被锁住了，必须等到事务 C’ 释放这个锁，才能继续它的当前读。 总结： 可重复读的核心就是一致性读（consistent read）；而事务更新数据的时候，只能用当前读。如果当前的记录的行锁被其他事务占用的话，就需要进入锁等待。 读提交的逻辑和可重复读的逻辑类似，它们最主要的区别是： 在可重复读隔离级别下，只需要在事务开始的时候创建一致性视图，之后事务里的其他查询都共用这个一致性视图； 在读提交隔离级别下，每一个语句执行前都会重新算出一个新的视图。 注意，语句 start transaction with consistent snapshot; 在读提交隔离级别下，没有意义。 读提交时的状态图，注意是事务 C。 事务 A 查询语句返回的是 k=2，事务 B 查询结果 k=3。 ","date":"0001-01-01 00:00:00","objectID":"/ooooo-notes/old-notes/geektime/mysql-45/08/:2:1","tags":null,"title":"","uri":"/ooooo-notes/old-notes/geektime/mysql-45/08/"},{"categories":null,"content":"4、最重要的总结 一个数据版本，对于一个事务视图来说，除了自己的更新总是可见以外，有三种情况： 版本未提交，不可见； 版本已提交，但是是在视图创建后提交的，不可见； 版本已提交，而且是在视图创建前提交的，可见。 语句 update 、for update 、lock in share mode 是当前读。 ","date":"0001-01-01 00:00:00","objectID":"/ooooo-notes/old-notes/geektime/mysql-45/08/:3:0","tags":null,"title":"","uri":"/ooooo-notes/old-notes/geektime/mysql-45/08/"},{"categories":null,"content":"5、问题 我用下面的表结构和初始化语句作为试验环境，事务隔离级别是可重复读。现在，我要把所有\"字段c和id值相等的行\"的 c 值清零，但是却发现了一个“诡异”的、改不掉的情况。请你构造出这种情况，并说明其原理。 mysql\u003e CREATE TABLE `t` ( `id` int(11) NOT NULL, `c` int(11) DEFAULT NULL, PRIMARY KEY (`id`) ) ENGINE=InnoDB; insert into t(id, c) values(1,1),(2,2),(3,3),(4,4); 出现的情况： 第一种情况 2. 第二种情况 ","date":"0001-01-01 00:00:00","objectID":"/ooooo-notes/old-notes/geektime/mysql-45/08/:4:0","tags":null,"title":"","uri":"/ooooo-notes/old-notes/geektime/mysql-45/08/"},{"categories":null,"content":"1、查询过程 主键索引 ID 和 普通索引 K: 假设执行查询的语句是 select id from T where k=5, 使用二分法查询： 对于普通索引来说，查找到满足条件的第一个记录(5,500)后，需要查找下一个记录，直到碰到第一个不满足 k=5 条件的记录。 对于唯一索引来说，由于索引定义了唯一性，查找到第一个满足条件的记录后，就会停止继续检索。 两个索引的性能差别，微乎其微。 InnoDB 的数据是按数据页为单位来读写，也就是说，当找到 k=5 的记录的时候，它所在的数据页就都在内存里了，所以判断下一条记录是很快的。 ","date":"0001-01-01 00:00:00","objectID":"/ooooo-notes/old-notes/geektime/mysql-45/09/:1:0","tags":null,"title":"","uri":"/ooooo-notes/old-notes/geektime/mysql-45/09/"},{"categories":null,"content":"2、更新过程 当需要更新一个数据页时，如果数据页在内存中就直接更新，如果这个数据页还没有在内存中，在不影响数据一致性的前提下，InooDB 会将这些更新操作缓存在 change buffer 中，这样就不需要从磁盘中读入这个数据页了。在下次查询需要访问这个数据页的时候，将数据页读入内存，然后执行 change buffer 中与这个页有关的操作。通过这种方式就能保证这个数据逻辑的正确性。 将 change buffer 中的操作应用到原数据页，得到最新结果的过程称为 merge。除了访问这个数据页会触发 merge 外，系统有后台线程会定期 merge 。在数据库正常关闭（shutdown）的过程中，也会执行 merge 操作。 对于唯一索引来说，所有的更新操作都要先判断这个操作是否违反唯一性约束，所以必须要将数据页读入内存才能判断，这时 change buffer 不能使用了。 change buffer 用的是 buffer pool 里的内存，因此不能无限增大。change buffer 的大小，可以通过参数 innodb_change_buffer_max_size 来动态设置。这个参数设置为 50 的时候，表示 change buffer 的大小最多只能占用 buffer pool 的 50%。 ","date":"0001-01-01 00:00:00","objectID":"/ooooo-notes/old-notes/geektime/mysql-45/09/:2:0","tags":null,"title":"","uri":"/ooooo-notes/old-notes/geektime/mysql-45/09/"},{"categories":null,"content":"3、change buffer 的使用场景 change buffer 的主要目的就是将记录的变更动作缓存下来，所以在一个数据页做 merge 之前，change buffer 记录的变更越多（也就是这个页面上要更新的次数越多），收益就越大。 对于写多读少的业务来说， change buffer 的使用效果最好。这种业务模型常见的就是账单类、日志类的系统。 假设一个业务的更新模式是写入之后马上会做查询，那么即使满足了条件，将更新先记录在 change buffer，但之后由于马上要访问这个数据页，会立即触发 merge 过程。这样随机访问 IO 的次数不会减少，反而增加了 change buffer 的维护代价。 ","date":"0001-01-01 00:00:00","objectID":"/ooooo-notes/old-notes/geektime/mysql-45/09/:3:0","tags":null,"title":"","uri":"/ooooo-notes/old-notes/geektime/mysql-45/09/"},{"categories":null,"content":"4、索引选择和实践 在不影响业务的情况下，建议你尽量选择普通索引。 普通索引和 change buffer 的配合使用，对于数据量大的表的更新优化还是很明显的。 ","date":"0001-01-01 00:00:00","objectID":"/ooooo-notes/old-notes/geektime/mysql-45/09/:4:0","tags":null,"title":"","uri":"/ooooo-notes/old-notes/geektime/mysql-45/09/"},{"categories":null,"content":"5、change buffer 和 redo log 执行插入语句： mysql\u003e insert into t(id,k) values(id1,k1),(id2,k2); 我们假设\u0008当前 k 索引树的状态，查找到位置后，k1 所在的数据页在内存(InnoDB buffer pool)中，k2 所在的数据页不在内存中。下图是带 change buffer 的更新状态图。 这条插入语句做了如下的操作: Page 1 在内存中，直接更新内存。 Page 2 没有在内存中，就在内存的 change buffer 区域，记录下\"我要往 Page 2 插入一行\"这个信息。 将上述两个动作记入 redo log 中（图中3和4）。 图中的两个虚线箭头，是后台操作，不影响更新的响应时间。 现在要执行语句 select * from t where k in (k1, k2)，这两个读请求的流程图： 读 Page 1 的时候，直接从内存返回。 要读 Page 2 的时候，需要把 Page 2 从磁盘读入内存中，然后应用 change buffer 里面的操作日志，生成一个正确的版本并返回结果。 从上图可知，redo log 主要节省的是随机写磁盘的 IO 消耗（转成顺序写），而 change buffer 主要节省的则是随机读磁盘的 IO 消耗。 ","date":"0001-01-01 00:00:00","objectID":"/ooooo-notes/old-notes/geektime/mysql-45/09/:5:0","tags":null,"title":"","uri":"/ooooo-notes/old-notes/geektime/mysql-45/09/"},{"categories":null,"content":"6、问题 change buffer 一开始是写内存的，那么如果这个时候机器掉电重启，会不会导致 change buffer 丢失呢？change buffer 丢失可不是小事儿，再从磁盘读入数据可就没有了 merge 过程，就等于是数据丢失了。会不会出现这种情况呢？ 不会丢失。虽然是只更新内存，但是在事务提交的时候，我们把 change buffer 的操作也记录到 redo log 里了，所以崩溃恢复的时候，change buffer 也能找回来。 merge 的执行流程是这样的： 从磁盘读入数据页到内存（老版本的数据页）； 从 change buffer 里找出这个数据页的 change buffer 记录(可能有多个），依次应用，得到新版数据页； 写 redo log。这个 redo log 包含了数据的变更和 change buffer 的变更。 ","date":"0001-01-01 00:00:00","objectID":"/ooooo-notes/old-notes/geektime/mysql-45/09/:6:0","tags":null,"title":"","uri":"/ooooo-notes/old-notes/geektime/mysql-45/09/"},{"categories":null,"content":"1、选错索引 一个例子，建表语句如下： CREATE TABLE `x` ( `id` int(11) NOT NULL, `a` int(11) DEFAULT NULL, `b` int(11) DEFAULT NULL, PRIMARY KEY (`id`), KEY `a` (`a`), KEY `b` (`b`) ) ENGINE=InnoDB; 往表 x 中插入 10 万行记录，取值按整数递增，即：(1,1,1)，(2,2,2)，(3,3,3) 直到(100000,100000,100000)，存储过程如下： delimiter ;; create procedure idata() begin declare i int; set i=1; while(i\u003c=100000)do insert into x values(i, i, i); set i=i+1; end while; end;; delimiter ; call idata(); 分析一条 SQL 语句 select * from x where a between 10000 and 20000; 我们再做如下操作。 这时候，session B 的查询语句 select * from t where a between 10000 and 20000 就不会再选择索引 a 了。 查看慢查询： 临时开启慢查询： set global slow_query_log='ON'; set global slow_query_log_file='/var/lib/mysql/instance-1-slow.log'; 实验过程就是这三个语句： set long_query_time=0; # 记录所有的查询到慢查询日志中 select * from x where a between 10000 and 20000; # Q1 查询 select * from x force index(a) where a between 10000 and 20000; # Q2 强制使用索引 a 这三条 SQL 语句执行完成后的慢查询日志: Q1 扫描了 10 万行，显然是走了全表扫描，执行时间是 40 毫秒。Q2 扫描了 10001 行，执行了21毫秒，很显然 mysql 选错了索引。 ","date":"0001-01-01 00:00:00","objectID":"/ooooo-notes/old-notes/geektime/mysql-45/10/:1:0","tags":null,"title":"","uri":"/ooooo-notes/old-notes/geektime/mysql-45/10/"},{"categories":null,"content":"2、优化器的逻辑 在第一篇文章中，我们就提到过，选择索引是优化器的工作。 优化器选择索引会根据扫描行数、是否使用临时表、是否排序等因素进行综合判断。 当然，这个例子中只有扫描行数这个因素，对于扫描行数，MySQL 根据统计信息来估算记录数，抽样来得到索引的基数信息（区别度），如下图。 采样统计的时候，InnoDB 默认会选择 N 个数据页，统计这些页面上的不同值，得到一个平均值，然后乘以这个索引的页面数，就得到了这个索引的基数。 在 MySQL 中，有两种存储索引统计的方式，可以通过设置参数 innodb_stats_persistent 的值来选择： 设置为 on，表示统计信息会持久化存储。这时，默认的 N 是 20，M 是 10。 设置为 off，表示统计信息只存储在内存中。这时，默认的 N是 8，M 是 16。 MySQL 选错索引，是因为索引统计信息不准确，修正统计信息，执行 analyze table x; 命令。 另外一个语句： mysql\u003e select * from t where (a between 1 and 1000) and (b between 50000 and 100000) order by b limit 1; 从条件上看，这个查询没有符合条件的记录，因此会返回空集合。 如果优化器使用索引 a 的话，执行速度明显会快很多，执行 explain命令后： 返回结果中 key 字段显示，这次优化器选择了索引 b。 从这个结果中，你可以得到两个结论： 扫描行数的估计值依然不准确。 这个例子里 MySQL 又选错了索引。 ","date":"0001-01-01 00:00:00","objectID":"/ooooo-notes/old-notes/geektime/mysql-45/10/:2:0","tags":null,"title":"","uri":"/ooooo-notes/old-notes/geektime/mysql-45/10/"},{"categories":null,"content":"3、索引选择异常和处理 一种方法是，采用 force index 强行选择一个索引。 另一种方法是，我们可以考虑修改语句，引导 MySQL 使用我们期望的索引。 修改语句后： 之前优化器选择使用索引 b，是因为它认为使用索引 b 可以避免排序（ b 本身是索引，已经是有序的了，如果选择索引 b 的话，不需要再做排序，只需要遍历），所以即使扫描行数多，也判定为代价更小。 现在 order by b,a 这种写法，要求按照 b,a 排序，就意味着使用这两个索引都需要排序。因此，扫描行数成了影响决策的主要条件，于是此时优化器选了只需要扫描 1000 行的索引 a。 这种修改并不是通用的优化手段，只是刚好在这个语句里面有 limit 1,order by b limit 1 和 order by b,a limit 1 都会返回b 是最小的那一行，逻辑上一致，才可以这么做。 另一种修改语句： 在这个例子里，我们用 limit 100 让优化器意识到，使用 b 索引代价是很高的。其实是我们根据数据特征诱导了一下优化器，也不具备通用性。 第三种方法是，有些场景下，我们可以新建一个更合适的索引，来提供给优化器做选择，或删掉误用的索引。 第四种方法是，删掉索引 b。 ","date":"0001-01-01 00:00:00","objectID":"/ooooo-notes/old-notes/geektime/mysql-45/10/:3:0","tags":null,"title":"","uri":"/ooooo-notes/old-notes/geektime/mysql-45/10/"},{"categories":null,"content":"4、问题 前面我们在构造第一个例子的过程中，通过 session A 的配合，让 session B 删除数据后又重新插入了一遍数据，然后就发现 explain 结果中，rows 字段从 10001 变成 37000 多。 而如果没有 session A 的配合，只是单独执行 delete from t 、call idata()、explain 这三句话，会看到 rows 字段其实还是10000左右。 答案： delete 语句删掉了所有的数据，然后再通过 call idata() 插入了 10 万行数据，看上去是覆盖了原来的 10 万行。 但是，session A 开启了事务并没有提交，所以之前插入的 10 万行数据是不能删除的。这样，之前的数据每一行数据都有两个版本，旧版本是 delete 之前的数据，新版本是标记为 deleted 的数据。 这样，索引 a 上的数据其实就有两份。 表的行数，优化器直接用的是 show table status 的值。 ","date":"0001-01-01 00:00:00","objectID":"/ooooo-notes/old-notes/geektime/mysql-45/10/:4:0","tags":null,"title":"","uri":"/ooooo-notes/old-notes/geektime/mysql-45/10/"},{"categories":null,"content":"1、字符串索引 假设，你现在维护一个支持邮箱登录的系统，用户表是这么定义的 mysql\u003e create table SUser( ID bigint unsigned primary key, email varchar(64), ... )engine=innodb; 要使用邮箱登录，一定会出现类似下面的语句： mysql\u003e select f1, f2 from SUser where email='xxx'; email 这个字段上没有索引，那么这个语句就只能做全表扫描。 MySQL 是支持前缀索引的，你可以定义字符串的一部分作为索引，例如： mysql\u003e alter table SUser add index index1(email); ## 整个字符串 mysql\u003e alter table SUser add index index2(email(6)); ## 前 6 个字节 两个索引图差别如下： 使用前缀索引的优势，占用的空间会更小，但可能会增加额外的记录扫描次数。 例如分析执行语句 select id,name,email from SUser where email='zhangssxyz@xxx.com'; 的过程。 使用 index1 (整个字符串)： 这个过程中，只需要从 index1 索引树找到满足索引值是 ’zhangssxyz@xxx.com’ 的这条记录，再进行回表操作，就可以返回结果。 使用 index2 (前 6 个字节)： 这个过程中，需要从 index2 索引树找到满足索引值是 ’zhangs’ 的全部记录，再进行回表操作，逐一判断 email 是 ’zhangssxyz@xxx.com’，最后返回结果。可能有大量以 ’zhangs’ 开头的记录， 所以可能会增加额外的记录扫描次数。 使用前缀索引，定义好长度，就可以做到既节省空间，又不用额外增加太多的查询成本。 如果建立前缀索引，就要关注区分度，区分度越高越好。 可以使用如下语句，统计不同索引的个数： mysql\u003e select count(distinct email) as L from SUser; 例如，统计 4~7 个字节的前缀索引： mysql\u003e select count(distinct left(email,4)）as L4, count(distinct left(email,5)）as L5, count(distinct left(email,6)）as L6, count(distinct left(email,7)）as L7, from SUser; ","date":"0001-01-01 00:00:00","objectID":"/ooooo-notes/old-notes/geektime/mysql-45/11/:1:0","tags":null,"title":"","uri":"/ooooo-notes/old-notes/geektime/mysql-45/11/"},{"categories":null,"content":"2、其他方式 对于类似于邮箱这样的字段来说，使用前缀索引的效果可能还不错,遇到前缀的区分度不够好的情况时，我们要怎么办呢？ 例如对身份证号的解决方法。 第一种方式是使用倒序存储。 mysql\u003e select field_list from t where id_card = reverse('input_id_card_string'); 第二种方式是使用hash字段 ## 添加索引，用来存储 crc32。 mysql\u003e alter table t add id_card_crc int unsigned, add index(id_card_crc); # 查询时判断 crc32 值 mysql\u003e select field_list from t where id_card_crc=crc32('input_id_card_string') and id_card='input_id_card_string' 它们的相同点是，都不支持范围查询。 ","date":"0001-01-01 00:00:00","objectID":"/ooooo-notes/old-notes/geektime/mysql-45/11/:2:0","tags":null,"title":"","uri":"/ooooo-notes/old-notes/geektime/mysql-45/11/"},{"categories":null,"content":"3、问题 如果你在维护一个学校的学生信息数据库，学生登录名的统一格式是 ”学号@gmail.com\", 而学号的规则是：十五位的数字，其中前三位是所在城市编号、第四到第六位是学校编号、第七位到第十位是入学年份、最后五位是顺序编号。 系统登录的时候都需要学生输入登录名和密码，验证正确后才能继续使用系统。就只考虑登录验证这个行为的话，你会怎么设计这个登录名的索引呢？ 答案： 可以只存入学年份加顺序编号，它们的长度是 9 位。如果数字类型来存这 9 位数字。比如 201100001 ，这样只需要占 4 个字节。 ","date":"0001-01-01 00:00:00","objectID":"/ooooo-notes/old-notes/geektime/mysql-45/11/:3:0","tags":null,"title":"","uri":"/ooooo-notes/old-notes/geektime/mysql-45/11/"},{"categories":null,"content":"1、SQL语句为什么变\"慢\"了 在前面第 2 篇文章《日志系统：一条SQL更新语句是如何执行的？》中，InnoDB 在处理更新语句时，只做了写日志这一个磁盘操作，这个日志叫作 redo log。更新内存写完 redo log 后，就返回给客户端，本次更新成功。。 当内存数据页跟磁盘数据页内容不一致的时候，我们称这个内存页为\"脏页\"。内存数据写入到磁盘后，内存和磁盘上的数据页的内容就一致了，称为\"干净页\"。 MySQL 偶尔\"抖\"一下的那个瞬间，可能就是在刷脏页（flush）。 引发数据库的 flush 过程的几种情况： InnoDB 的 redo log 写满了。 系统内存不足。当需要新的内存页，而内存不够用的时候，就要淘汰一些数据页，空出内存给别的数据页使用。如果淘汰的是\"脏页\"，就要先将脏页写到磁盘。 MySQL 认为系统\"空闲\"的时候。 MySQL 正常关闭。 上面四种场景对性能的影响： 第 3 种情况和第 4 种场景是 MySQL 正常情况，不用太关心性能。 第 1 种是 “redo log 写满了，要 flush 脏页”，出现这种情况了，整个系统就不能再接受更新，如果你从监控上看，这时候更新数会跌为 0。 第 2 种是\"内存不够用了，要先将脏页写到磁盘\"，这种情况其实是常态。InnoDB用缓冲池（buffer pool）管理内存，缓冲池中的内存页有三种状态： 第一种是，还没有使用的； 第二种是，使用了并且是干净页； 第三种是，使用了并且是脏页。 InnoDB 的策略是尽量使用内存，因此对于一个长时间运行的库来说，未被使用的页面很少。刷脏页虽然是常态，但是出现以下这两种情况，都是会明显影响性能的： 一个查询要淘汰的脏页个数太多，会导致查询的响应时间明显变长。 日志写满，更新全部堵住，写性能跌为 0，这种情况对敏感业务来说，是不能接受的。 ","date":"0001-01-01 00:00:00","objectID":"/ooooo-notes/old-notes/geektime/mysql-45/12/:1:0","tags":null,"title":"","uri":"/ooooo-notes/old-notes/geektime/mysql-45/12/"},{"categories":null,"content":"2、InnoDB 刷脏页的控制策略 innodb_io_capacity 参数：告诉 InnoDB 所在主机的 IO 能力。 可以使用 fio 这个工具来测试 IO 能力： fio -filename=$filename -direct=1 -iodepth 1 -thread -rw=randrw -ioengine=psync -bs=16k -size=500M -numjobs=10 -runtime=10 -group_reporting -name=mytest InnoDB 的刷盘速度就是要参考这两个因素：一个是脏页比例，一个是redo log写盘速度。 参数 innodb_max_dirty_pages_pct 是脏页比例上限，默认值是 75%。 脏页比例是通过 Innodb_buffer_pool_pages_dirty/Innodb_buffer_pool_pages_total 得到的，具体的命令如下： select VARIABLE_VALUE into @a from global_status where VARIABLE_NAME = 'Innodb_buffer_pool_pages_dirty'; select VARIABLE_VALUE into @b from global_status where VARIABLE_NAME = 'Innodb_buffer_pool_pages_total'; select @a/@b; 要尽量避免刷脏页这种情况，你就要合理地设置 innodb_io_capacity 的值，并且平时要多关注脏页比例，不要让它经常接近 75%。 一个有趣的策略： 一旦一个查询请求需要在执行过程中先 flush 掉一个脏页时，这个查询就可能要比平时慢了。而 MySQL中 的一个机制，可能让你的查询会更慢：在准备刷一个脏页的时候，如果这个数据页旁边的数据页刚好是脏页，就会把这个\"邻居\"也带着一起刷掉；而且这个把\"邻居\"拖下水的逻辑还可以继续蔓延，也就是对于每个邻居数据页，如果跟它相邻的数据页也还是脏页的话，也会被放到一起刷。 在 InnoDB 中，innodb_flush_neighbors 参数就是用来控制这个行为的，值为 1 的时候会有上述的\"连坐\"机制，值为 0 时表示不找邻居，自己刷自己的。 如果是 SSD 这种， 建议 innodb_flush_neighbors = 0。 ","date":"0001-01-01 00:00:00","objectID":"/ooooo-notes/old-notes/geektime/mysql-45/12/:2:0","tags":null,"title":"","uri":"/ooooo-notes/old-notes/geektime/mysql-45/12/"},{"categories":null,"content":"3、问题 一个内存配置为 128GB、innodb_io_capacity设置为 20000 的大规格实例，正常会建议你将redo log 设置成 4 个 1GB 的文件。 但如果你在配置的时候不慎将 redo log 设置成了 1个 100M 的文件，会发生什么情况呢？又为什么会出现这样的情况呢？ 答案： redo log 太小，很快就会被写满，就必须要 flush，在这种情况下， change buffer 的优化也失效了，因为 flush 时，必须要进行 merge 操作。你看到的现象就是磁盘压力很小，但是数据库出现间歇性的性能下跌。 ","date":"0001-01-01 00:00:00","objectID":"/ooooo-notes/old-notes/geektime/mysql-45/12/:2:1","tags":null,"title":"","uri":"/ooooo-notes/old-notes/geektime/mysql-45/12/"},{"categories":null,"content":"MySQL 实战 45 讲 01、SQL 查询语句的执行过程 ","date":"0001-01-01 00:00:00","objectID":"/ooooo-notes/old-notes/geektime/mysql-45/readme/:1:0","tags":null,"title":"","uri":"/ooooo-notes/old-notes/geektime/mysql-45/readme/"}]