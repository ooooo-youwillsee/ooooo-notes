[{"categories":null,"content":" 在 window 上使用 openssl, 会遇到错误 failed to run custom build command for openssl-sys v0.9.102. ","date":"2024-06-09","objectID":"/ooooo-notes/rust-openssl-%E4%BE%9D%E8%B5%96/:0:0","tags":["rust"],"title":"rust openssl 依赖","uri":"/ooooo-notes/rust-openssl-%E4%BE%9D%E8%B5%96/"},{"categories":null,"content":"解决方法 下载 vcpkg, 打开 powershell 执行 ./bootstrap-vcpkg.bat 执行 ./vcpkg.exe install openssl:x64-windows-static 配置环境变量 OPENSSL_DIR=C:\\Users\\ooooo\\Development\\Vcpkg\\installed\\x64-windows-static 重新启动项目编译 ","date":"2024-06-09","objectID":"/ooooo-notes/rust-openssl-%E4%BE%9D%E8%B5%96/:1:0","tags":["rust"],"title":"rust openssl 依赖","uri":"/ooooo-notes/rust-openssl-%E4%BE%9D%E8%B5%96/"},{"categories":null,"content":"参考 stackoverflow github ","date":"2024-06-09","objectID":"/ooooo-notes/rust-openssl-%E4%BE%9D%E8%B5%96/:2:0","tags":["rust"],"title":"rust openssl 依赖","uri":"/ooooo-notes/rust-openssl-%E4%BE%9D%E8%B5%96/"},{"categories":null,"content":" 介绍常用的依赖库，持续更新… async_trait: 异步支持 once_cell: OnceCell 和 Lazy clap: 命令行支持 axum: http 服务 tokio: 异步运行时 serde: 序列化 serde_json: json 序列化 log: 日志门面 env_logger: 日志实现 anyhow: Result chrono: 日期和时间 quick-xml: 读写 xml sqlx: 异步 sql 访问 dotenvy: 支持加载 .env 文件 cargo-watch: 热加载代码，二进制程序 cargo-expand: 展开宏代码，二进制程序 evalexpr: 解析表达式 reqwest: http 客户端 tower: 请求和响应抽象层 tower-http: http 中间件实现 ","date":"2024-06-01","objectID":"/ooooo-notes/rust-%E5%B8%B8%E7%94%A8%E4%BE%9D%E8%B5%96%E5%BA%93/:0:0","tags":["rust"],"title":"rust 常用依赖库","uri":"/ooooo-notes/rust-%E5%B8%B8%E7%94%A8%E4%BE%9D%E8%B5%96%E5%BA%93/"},{"categories":null,"content":" 解决 rust 依赖加载太慢的问题。 ","date":"2024-05-24","objectID":"/ooooo-notes/rust-%E9%95%9C%E5%83%8F%E9%85%8D%E7%BD%AE/:0:0","tags":["rust"],"title":"rust 镜像配置","uri":"/ooooo-notes/rust-%E9%95%9C%E5%83%8F%E9%85%8D%E7%BD%AE/"},{"categories":null,"content":"配置文件 文件路径：用户名/.cargo/config.toml [source.crates-io] registry = \"https://github.com/rust-lang/crates.io-index\" # 指定镜像 replace-with = 'ustc' # rustcc 1号源 #[source.rustcc] #registry = \"git://crates.rustcc.com/crates.io-index\" # rustcc 2号源 [source.rustcc2] registry = \"git://crates.rustcc.cn/crates.io-index\" # 清华大学 [source.tuna] registry = \"https://mirrors.tuna.tsinghua.edu.cn/git/crates.io-index.git\" # 中国科学技术大学 [source.ustc] registry = \"git://mirrors.ustc.edu.cn/crates.io-index\" # 上海交通大学 [source.sjtu] registry = \"https://mirrors.sjtug.sjtu.edu.cn/git/crates.io-index\" # 阿里云 [source.rustcc] registry = \"https://code.aliyun.com/rustcc/crates.io-index.git\" [http] check-revoke = false [net] git-fetch-with-cli = true ","date":"2024-05-24","objectID":"/ooooo-notes/rust-%E9%95%9C%E5%83%8F%E9%85%8D%E7%BD%AE/:1:0","tags":["rust"],"title":"rust 镜像配置","uri":"/ooooo-notes/rust-%E9%95%9C%E5%83%8F%E9%85%8D%E7%BD%AE/"},{"categories":null,"content":" jdk 基于 8 版本 在平时的开发中，我们经常会用到 ConcurrentHashMap, 是并发安全的。 ","date":"2024-05-23","objectID":"/ooooo-notes/09-concurrenthashmap/:0:0","tags":["jdk","source code","源码分析 jdk 系列"],"title":"09 ConcurrentHashMap","uri":"/ooooo-notes/09-concurrenthashmap/"},{"categories":null,"content":"使用方式 public class ConcurrentHashMapTest { @Test void test() { Map\u003cString, String\u003e map = new ConcurrentHashMap\u003c\u003e(); map.put(\"1\", \"1\"); assertThat(map.get(\"1\")).isEqualTo(\"1\"); map.remove(\"1\"); assertThat(map.size()).isEqualTo(0); } } ","date":"2024-05-23","objectID":"/ooooo-notes/09-concurrenthashmap/:1:0","tags":["jdk","source code","源码分析 jdk 系列"],"title":"09 ConcurrentHashMap","uri":"/ooooo-notes/09-concurrenthashmap/"},{"categories":null,"content":"put 添加元素。 源码位置: java.util.concurrent.ConcurrentHashMap#putVal final V putVal(K key, V value, boolean onlyIfAbsent) { if (key == null || value == null) throw new NullPointerException(); // 计算 hash 值 int hash = spread(key.hashCode()); int binCount = 0; for (Node\u003cK,V\u003e[] tab = table;;) { Node\u003cK,V\u003e f; int n, i, fh; if (tab == null || (n = tab.length) == 0) // 使用 CAS 来初始化 table tab = initTable(); else if ((f = tabAt(tab, i = (n - 1) \u0026 hash)) == null) { // f: 桶里的第一个元素，如果为空，使用 CAS 来初始化 if (casTabAt(tab, i, null, new Node\u003cK,V\u003e(hash, key, value, null))) break; } else if ((fh = f.hash) == MOVED) // 扩容操作，太复杂了，不用关心 tab = helpTransfer(tab, f); else { V oldVal = null; // 对首元素加锁 synchronized (f) { if (tabAt(tab, i) == f) { // fh \u003e= 0 表示链表节点 if (fh \u003e= 0) { binCount = 1; // 添加新节点，替换旧节点 for (Node\u003cK,V\u003e e = f;; ++binCount) { K ek; if (e.hash == hash \u0026\u0026 ((ek = e.key) == key || (ek != null \u0026\u0026 key.equals(ek)))) { oldVal = e.val; if (!onlyIfAbsent) e.val = value; break; } Node\u003cK,V\u003e pred = e; if ((e = e.next) == null) { pred.next = new Node\u003cK,V\u003e(hash, key, value, null); break; } } } // 红黑树节点 else if (f instanceof TreeBin) { Node\u003cK,V\u003e p; binCount = 2; if ((p = ((TreeBin\u003cK,V\u003e)f).putTreeVal(hash, key, value)) != null) { oldVal = p.val; if (!onlyIfAbsent) p.val = value; } } } } if (binCount != 0) { // 链表节点大于8，将链表转为红黑树 if (binCount \u003e= TREEIFY_THRESHOLD) treeifyBin(tab, i); if (oldVal != null) return oldVal; break; } } } // 添加个数 addCount(1L, binCount); return null; } ","date":"2024-05-23","objectID":"/ooooo-notes/09-concurrenthashmap/:2:0","tags":["jdk","source code","源码分析 jdk 系列"],"title":"09 ConcurrentHashMap","uri":"/ooooo-notes/09-concurrenthashmap/"},{"categories":null,"content":"remove 删除元素。 源码位置: java.util.concurrent.ConcurrentHashMap#replaceNode // 参数 value = null, vc = null final V replaceNode(Object key, V value, Object cv) { // 计算 hash 值 int hash = spread(key.hashCode()); for (Node\u003cK,V\u003e[] tab = table;;) { Node\u003cK,V\u003e f; int n, i, fh; // 桶里的首元素不存在 if (tab == null || (n = tab.length) == 0 || (f = tabAt(tab, i = (n - 1) \u0026 hash)) == null) break; else if ((fh = f.hash) == MOVED) // 扩容，太复杂，不用关心 tab = helpTransfer(tab, f); else { V oldVal = null; boolean validated = false; // 对首元素加锁 synchronized (f) { if (tabAt(tab, i) == f) { // fh \u003e= 0 表示链表节点 if (fh \u003e= 0) { validated = true; // 删除节点 for (Node\u003cK,V\u003e e = f, pred = null;;) { K ek; if (e.hash == hash \u0026\u0026 ((ek = e.key) == key || (ek != null \u0026\u0026 key.equals(ek)))) { V ev = e.val; if (cv == null || cv == ev || (ev != null \u0026\u0026 cv.equals(ev))) { oldVal = ev; if (value != null) e.val = value; else if (pred != null) pred.next = e.next; else setTabAt(tab, i, e.next); } break; } pred = e; if ((e = e.next) == null) break; } } // 红黑树节点 else if (f instanceof TreeBin) { validated = true; TreeBin\u003cK,V\u003e t = (TreeBin\u003cK,V\u003e)f; TreeNode\u003cK,V\u003e r, p; if ((r = t.root) != null \u0026\u0026 (p = r.findTreeNode(hash, key, null)) != null) { V pv = p.val; // 注意删除操作时，cv 和 value 都为 null if (cv == null || cv == pv || (pv != null \u0026\u0026 cv.equals(pv))) { oldVal = pv; if (value != null) p.val = value; else if (t.removeTreeNode(p)) setTabAt(tab, i, untreeify(t.first)); } } } } } if (validated) { if (oldVal != null) { if (value == null) // 减少 count addCount(-1L, -1); return oldVal; } break; } } } return null; } ","date":"2024-05-23","objectID":"/ooooo-notes/09-concurrenthashmap/:3:0","tags":["jdk","source code","源码分析 jdk 系列"],"title":"09 ConcurrentHashMap","uri":"/ooooo-notes/09-concurrenthashmap/"},{"categories":null,"content":"get 获取元素，无锁。 源码位置: java.util.concurrent.ConcurrentHashMap#get public V get(Object key) { Node\u003cK,V\u003e[] tab; Node\u003cK,V\u003e e, p; int n, eh; K ek; // 计算 hash 值 int h = spread(key.hashCode()); // e: 桶里第一个元素 if ((tab = table) != null \u0026\u0026 (n = tab.length) \u003e 0 \u0026\u0026 (e = tabAt(tab, (n - 1) \u0026 h)) != null) { // 是首元素，返回 val if ((eh = e.hash) == h) { if ((ek = e.key) == key || (ek != null \u0026\u0026 key.equals(ek))) return e.val; } // 表示为 TREEBIN (红黑树节点) else if (eh \u003c 0) return (p = e.find(h, key)) != null ? p.val : null; // 遍历查找 (链表节点) while ((e = e.next) != null) { if (e.hash == h \u0026\u0026 ((ek = e.key) == key || (ek != null \u0026\u0026 key.equals(ek)))) return e.val; } } return null; } ","date":"2024-05-23","objectID":"/ooooo-notes/09-concurrenthashmap/:4:0","tags":["jdk","source code","源码分析 jdk 系列"],"title":"09 ConcurrentHashMap","uri":"/ooooo-notes/09-concurrenthashmap/"},{"categories":null,"content":"size 获取大小，无锁。 源码位置: java.util.concurrent.ConcurrentHashMap#size public int size() { long n = sumCount(); return ((n \u003c 0L) ? 0 : (n \u003e (long)Integer.MAX_VALUE) ? Integer.MAX_VALUE : (int)n); } // 遍历每个桶的大小 final long sumCount() { CounterCell[] as = counterCells; CounterCell a; long sum = baseCount; if (as != null) { for (int i = 0; i \u003c as.length; ++i) { if ((a = as[i]) != null) sum += a.value; } } return sum; } ","date":"2024-05-23","objectID":"/ooooo-notes/09-concurrenthashmap/:5:0","tags":["jdk","source code","源码分析 jdk 系列"],"title":"09 ConcurrentHashMap","uri":"/ooooo-notes/09-concurrenthashmap/"},{"categories":null,"content":" jdk 基于 8 版本 在平时的开发中，我们经常会用到 CopyOnWriteArrayList, 利用写时复制的机制来保证并发安全, 适合多读少写的场景。 ","date":"2024-05-22","objectID":"/ooooo-notes/08-copyonwritearraylist/:0:0","tags":["jdk","source code","源码分析 jdk 系列"],"title":"08 CopyOnWriteArrayList","uri":"/ooooo-notes/08-copyonwritearraylist/"},{"categories":null,"content":"使用方式 public class CopyOnWriteArrayListTest { @Test public void test() { List\u003cString\u003e data = new CopyOnWriteArrayList\u003c\u003e(); data.add(\"1\"); assertThat(data.get(0)).isEqualTo(\"1\"); data.remove(\"1\"); assertThat(data.isEmpty()).isEqualTo(true); } } ","date":"2024-05-22","objectID":"/ooooo-notes/08-copyonwritearraylist/:1:0","tags":["jdk","source code","源码分析 jdk 系列"],"title":"08 CopyOnWriteArrayList","uri":"/ooooo-notes/08-copyonwritearraylist/"},{"categories":null,"content":"add 添加元素，写时复制。 源码位置: java.util.concurrent.CopyOnWriteArrayList#add(E) public boolean add(E e) { final ReentrantLock lock = this.lock; // 加锁 lock.lock(); try { // 获取数组 Object[] elements = getArray(); int len = elements.length; // 复制到新数组 Object[] newElements = Arrays.copyOf(elements, len + 1); newElements[len] = e; // 修改数组 setArray(newElements); return true; } finally { lock.unlock(); } } ","date":"2024-05-22","objectID":"/ooooo-notes/08-copyonwritearraylist/:2:0","tags":["jdk","source code","源码分析 jdk 系列"],"title":"08 CopyOnWriteArrayList","uri":"/ooooo-notes/08-copyonwritearraylist/"},{"categories":null,"content":"get 获取元素，不加锁。 源码位置: java.util.concurrent.CopyOnWriteArrayList#get(int) public E get(int index) { // 获取数据，这里不加锁 return get(getArray(), index); } ","date":"2024-05-22","objectID":"/ooooo-notes/08-copyonwritearraylist/:3:0","tags":["jdk","source code","源码分析 jdk 系列"],"title":"08 CopyOnWriteArrayList","uri":"/ooooo-notes/08-copyonwritearraylist/"},{"categories":null,"content":"remove 删除元素，写时复制。 源码位置: java.util.concurrent.CopyOnWriteArrayList#remove(int) public E remove(int index) { final ReentrantLock lock = this.lock; // 加锁 lock.lock(); try { // 获取数组 Object[] elements = getArray(); int len = elements.length; E oldValue = get(elements, index); int numMoved = len - index - 1; if (numMoved == 0) // 复制数组 setArray(Arrays.copyOf(elements, len - 1)); else { Object[] newElements = new Object[len - 1]; // 复制数组 System.arraycopy(elements, 0, newElements, 0, index); // 复制数组 System.arraycopy(elements, index + 1, newElements, index, numMoved); setArray(newElements); } return oldValue; } finally { lock.unlock(); } } ","date":"2024-05-22","objectID":"/ooooo-notes/08-copyonwritearraylist/:4:0","tags":["jdk","source code","源码分析 jdk 系列"],"title":"08 CopyOnWriteArrayList","uri":"/ooooo-notes/08-copyonwritearraylist/"},{"categories":null,"content":" jdk 基于 8 版本 在平时的开发中，我们经常会用到 ThreadPoolExecutor, 需要了解源码。 ","date":"2024-05-21","objectID":"/ooooo-notes/07-threadpoolexecutor/:0:0","tags":["jdk","source code","源码分析 jdk 系列"],"title":"07 ThreadPoolExecutor","uri":"/ooooo-notes/07-threadpoolexecutor/"},{"categories":null,"content":"使用方式 public class ThreadPoolTest { @Test void test() { ThreadPoolExecutor executor = new ThreadPoolExecutor( 2, 10, 60, TimeUnit.SECONDS, new LinkedBlockingQueue\u003c\u003e(), new CustomizableThreadFactory(\"test-\"), new ThreadPoolExecutor.CallerRunsPolicy()); executor.submit(() -\u003e { System.out.println(\"xxxx\"); }); executor.shutdown(); } } ","date":"2024-05-21","objectID":"/ooooo-notes/07-threadpoolexecutor/:1:0","tags":["jdk","source code","源码分析 jdk 系列"],"title":"07 ThreadPoolExecutor","uri":"/ooooo-notes/07-threadpoolexecutor/"},{"categories":null,"content":"new 创建线程池。 源码位置: java.util.concurrent.ThreadPoolExecutor#ThreadPoolExecutor // 参数说明 // corePoolSize: 核心线程池大小 // maximumPoolSize: 最大线程池大小 // keepAliveTime: 非核心线程存活时间 // workQueue: 任务队列 // threadFactory: 线程工厂，可以给线程命名 // handler: 拒绝策略 public ThreadPoolExecutor(int corePoolSize, int maximumPoolSize, long keepAliveTime, TimeUnit unit, BlockingQueue\u003cRunnable\u003e workQueue, ThreadFactory threadFactory, RejectedExecutionHandler handler) { // 下面都是简单的赋值，没啥要说明的 if (corePoolSize \u003c 0 || maximumPoolSize \u003c= 0 || maximumPoolSize \u003c corePoolSize || keepAliveTime \u003c 0) throw new IllegalArgumentException(); if (workQueue == null || threadFactory == null || handler == null) throw new NullPointerException(); this.acc = System.getSecurityManager() == null ? null : AccessController.getContext(); this.corePoolSize = corePoolSize; this.maximumPoolSize = maximumPoolSize; this.workQueue = workQueue; this.keepAliveTime = unit.toNanos(keepAliveTime); this.threadFactory = threadFactory; this.handler = handler; } 参数说明： 新创建的线程池，线程池不会预热，所以线程数为零。 当有任务提交了，判断核心线程数是否有剩余。 如果核心线程数有，启动核心线程来执行任务，如果核心线程数没有，加入到任务队列。 如果任务队列没有满，把任务放入到任务队列中，方法结束。如果任务队列满了，判断非核心线程数是否有剩余。 如果非核心线程数有，启动非核心线程数来执行任务，如果非核心线程数没有，执行拒绝策略。 ","date":"2024-05-21","objectID":"/ooooo-notes/07-threadpoolexecutor/:2:0","tags":["jdk","source code","源码分析 jdk 系列"],"title":"07 ThreadPoolExecutor","uri":"/ooooo-notes/07-threadpoolexecutor/"},{"categories":null,"content":"submit 提交任务 源码位置: java.util.concurrent.AbstractExecutorService#submit(java.lang.Runnable) public Future\u003c?\u003e submit(Runnable task) { if (task == null) throw new NullPointerException(); // 创建 future, RunnableFuture\u003cVoid\u003e ftask = newTaskFor(task, null); // 执行 task execute(ftask); return ftask; } 源码位置: java.util.concurrent.ThreadPoolExecutor#execute public void execute(Runnable command) { ... int c = ctl.get(); // 获取当前的线程数, 小于核心线程数 if (workerCountOf(c) \u003c corePoolSize) { // 尝试添加新线程来执行任务 if (addWorker(command, true)) return; // 添加失败，核心线程数满了 c = ctl.get(); } // 尝试加入任务队列 if (isRunning(c) \u0026\u0026 workQueue.offer(command)) { int recheck = ctl.get(); // 如果不是 running 状态，移除任务 if (!isRunning(recheck) \u0026\u0026 remove(command)) // 执行拒绝策略 reject(command); else if (workerCountOf(recheck) == 0) // 添加非核心线程 addWorker(null, false); } // 任务队列满了，尝试添加非核心线程来执行任务 else if (!addWorker(command, false)) // 非核心线程数满了，执行拒绝策略 reject(command); } 源码位置: java.util.concurrent.ThreadPoolExecutor#addWorker private boolean addWorker(Runnable firstTask, boolean core) { // 使用 CAS 来增加当前线程数 retry: for (;;) { ... for (;;) { int wc = workerCountOf(c); if (wc \u003e= CAPACITY || wc \u003e= (core ? corePoolSize : maximumPoolSize)) return false; if (compareAndIncrementWorkerCount(c)) break retry; c = ctl.get(); // Re-read ctl if (runStateOf(c) != rs) continue retry; } } ... try { // 创建新的 worker w = new Worker(firstTask); final Thread t = w.thread; if (t != null) { final ReentrantLock mainLock = this.mainLock; mainLock.lock(); try { int rs = runStateOf(ctl.get()); // 添加到 workers 中 if (rs \u003c SHUTDOWN || (rs == SHUTDOWN \u0026\u0026 firstTask == null)) { if (t.isAlive()) // precheck that t is startable throw new IllegalThreadStateException(); workers.add(w); int s = workers.size(); // 更新当前最大线程数 if (s \u003e largestPoolSize) largestPoolSize = s; workerAdded = true; } } finally { mainLock.unlock(); } // 添加成功，启动线程 if (workerAdded) { t.start(); workerStarted = true; } } } finally { ... } return workerStarted; } ","date":"2024-05-21","objectID":"/ooooo-notes/07-threadpoolexecutor/:3:0","tags":["jdk","source code","源码分析 jdk 系列"],"title":"07 ThreadPoolExecutor","uri":"/ooooo-notes/07-threadpoolexecutor/"},{"categories":null,"content":"runWorker 执行任务。 源码位置: java.util.concurrent.ThreadPoolExecutor#runWorker final void runWorker(Worker w) { ... try { // 从任务队列中获取任务, 如果 task 为 null，当前线程会终止 while (task != null || (task = getTask()) != null) { w.lock(); ... // 如果是 STOP 状态，打断当前线程，停止执行 if ((runStateAtLeast(ctl.get(), STOP) || (Thread.interrupted() \u0026\u0026 runStateAtLeast(ctl.get(), STOP))) \u0026\u0026 !wt.isInterrupted()) wt.interrupt(); try { // 空实现 beforeExecute(wt, task); Throwable thrown = null; try { // 执行任务 task.run(); } catch (RuntimeException x) { ... } finally { // 空实现 afterExecute(task, thrown); } } finally { ... } } completedAbruptly = false; } finally { processWorkerExit(w, completedAbruptly); } } 源码位置: java.util.concurrent.ThreadPoolExecutor#getTask // 获取任务，如果为 null，当前线程就要终止 private Runnable getTask() { boolean timedOut = false; // Did the last poll() time out? for (;;) { int c = ctl.get(); int rs = runStateOf(c); // 如果 SHUTDOWN 状态，任务队列为空，应该减少 worker if (rs \u003e= SHUTDOWN \u0026\u0026 (rs \u003e= STOP || workQueue.isEmpty())) { decrementWorkerCount(); return null; } int wc = workerCountOf(c); // 控制是否要结束线程 boolean timed = allowCoreThreadTimeOut || wc \u003e corePoolSize; // timedOut 为 true，表示没有任务了 if ((wc \u003e maximumPoolSize || (timed \u0026\u0026 timedOut)) \u0026\u0026 (wc \u003e 1 || workQueue.isEmpty())) { // CAS 减少 worker if (compareAndDecrementWorkerCount(c)) return null; continue; } try { // 获取任务，对于非核心线程是 poll 方法, 对核心线程是 take 方法 Runnable r = timed ? workQueue.poll(keepAliveTime, TimeUnit.NANOSECONDS) : workQueue.take(); if (r != null) return r; timedOut = true; } catch (InterruptedException retry) { timedOut = false; } } } ","date":"2024-05-21","objectID":"/ooooo-notes/07-threadpoolexecutor/:4:0","tags":["jdk","source code","源码分析 jdk 系列"],"title":"07 ThreadPoolExecutor","uri":"/ooooo-notes/07-threadpoolexecutor/"},{"categories":null,"content":" spring bean 初始化过程涉及到很多 spring 的扩展接口，源码必懂。 ","date":"2024-05-21","objectID":"/ooooo-notes/spring-bean-%E5%88%9D%E5%A7%8B%E5%8C%96/:0:0","tags":["spring boot","source code","源码分析 spring boot 系列"],"title":"spring bean 初始化","uri":"/ooooo-notes/spring-bean-%E5%88%9D%E5%A7%8B%E5%8C%96/"},{"categories":null,"content":"getBean 源码位置: org.springframework.beans.factory.BeanFactory#getBean // BeanFactory 是接口，由 AbstractBeanFactory 类来实现 @Override public Object getBean(String name) throws BeansException { return doGetBean(name, null, null, false); } 源码位置: org.springframework.beans.factory.support.AbstractBeanFactory#doGetBean // 代码非常长，只分析单例对象 protected \u003cT\u003e T doGetBean( String name, @Nullable Class\u003cT\u003e requiredType, @Nullable Object[] args, boolean typeCheckOnly) throws BeansException { // 转换 beanName, 因为你传入的 name 可能只是别名 String beanName = transformedBeanName(name); Object beanInstance; // 从缓存中获取 bean Object sharedInstance = getSingleton(beanName); if (sharedInstance != null \u0026\u0026 args == null) { // 返回真实的 bean， 可能是 FactoryBean beanInstance = getObjectForBeanInstance(sharedInstance, name, beanName, null); } else { ... // 从父容器中获取 bean BeanFactory parentBeanFactory = getParentBeanFactory(); if (parentBeanFactory != null \u0026\u0026 !containsBeanDefinition(beanName)) { ... } // 标记 bean 正在创建中 if (!typeCheckOnly) { markBeanAsCreated(beanName); } try { // 获取 beanDefinition RootBeanDefinition mbd = getMergedLocalBeanDefinition(beanName); // 初始化 bean 的依赖, 注解 @DependsOn，不用关心 String[] dependsOn = mbd.getDependsOn(); if (dependsOn != null) { ... } // 初始化单例 bean if (mbd.isSingleton()) { sharedInstance = getSingleton(beanName, () -\u003e { try { // 创建 bean, 后面继续分析 return createBean(beanName, mbd, args); } catch (BeansException ex) { ... } }); // 获取真实的 bean，可能是 FactoryBean beanInstance = getObjectForBeanInstance(sharedInstance, name, beanName, mbd); } // 省略了 PROTOTYPE，@Scope 类型的 bean 的代码 ... } ... } // 根据 requiredType 转换 bean 的类型, TypeConverter return adaptBeanInstance(name, beanInstance, requiredType); } ","date":"2024-05-21","objectID":"/ooooo-notes/spring-bean-%E5%88%9D%E5%A7%8B%E5%8C%96/:1:0","tags":["spring boot","source code","源码分析 spring boot 系列"],"title":"spring bean 初始化","uri":"/ooooo-notes/spring-bean-%E5%88%9D%E5%A7%8B%E5%8C%96/"},{"categories":null,"content":"createBean 源码位置: org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory#createBean @Override protected Object createBean(String beanName, RootBeanDefinition mbd, @Nullable Object[] args) throws BeanCreationException { RootBeanDefinition mbdToUse = mbd; // 解析 bean 的 class Class\u003c?\u003e resolvedClass = resolveBeanClass(mbd, beanName); if (resolvedClass != null \u0026\u0026 !mbd.hasBeanClass() \u0026\u0026 mbd.getBeanClassName() != null) { mbdToUse = new RootBeanDefinition(mbd); mbdToUse.setBeanClass(resolvedClass); } ... try { // 执行 InstantiationAwareBeanPostProcessor 钩子，这里可以返回代理对象 Object bean = resolveBeforeInstantiation(beanName, mbdToUse); if (bean != null) { return bean; } } catch (Throwable ex) { throw new BeanCreationException(mbdToUse.getResourceDescription(), beanName, \"BeanPostProcessor before instantiation of bean failed\", ex); } try { // 创建 bean, 继续分析 Object beanInstance = doCreateBean(beanName, mbdToUse, args); return beanInstance; } catch (BeanCreationException | ImplicitlyAppearedSingletonException ex) { ... } } ","date":"2024-05-21","objectID":"/ooooo-notes/spring-bean-%E5%88%9D%E5%A7%8B%E5%8C%96/:2:0","tags":["spring boot","source code","源码分析 spring boot 系列"],"title":"spring bean 初始化","uri":"/ooooo-notes/spring-bean-%E5%88%9D%E5%A7%8B%E5%8C%96/"},{"categories":null,"content":"doCreateBean 源码位置: org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory#doCreateBean protected Object doCreateBean(String beanName, RootBeanDefinition mbd, @Nullable Object[] args) throws BeanCreationException { // Instantiate the bean. BeanWrapper instanceWrapper = null; if (mbd.isSingleton()) { instanceWrapper = this.factoryBeanInstanceCache.remove(beanName); } if (instanceWrapper == null) { // 创建 beanWrapper instanceWrapper = createBeanInstance(beanName, mbd, args); } Object bean = instanceWrapper.getWrappedInstance(); Class\u003c?\u003e beanType = instanceWrapper.getWrappedClass(); if (beanType != NullBean.class) { mbd.resolvedTargetType = beanType; } // Allow post-processors to modify the merged bean definition. synchronized (mbd.postProcessingLock) { if (!mbd.postProcessed) { try { // 执行 MergedBeanDefinitionPostProcessor 钩子 applyMergedBeanDefinitionPostProcessors(mbd, beanType, beanName); } catch (Throwable ex) { ... } mbd.postProcessed = true; } } // 是否允许循环引用, 默认不允许 boolean earlySingletonExposure = (mbd.isSingleton() \u0026\u0026 this.allowCircularReferences \u0026\u0026 isSingletonCurrentlyInCreation(beanName)); if (earlySingletonExposure) { // 加入到 singletonFactory 中，当有循环引用时，调用 ObjectFactory#getObject 获取 bean addSingletonFactory(beanName, () -\u003e getEarlyBeanReference(beanName, mbd, bean)); } Object exposedObject = bean; try { // 填充 bean 属性， 执行 @Autowired, @Value populateBean(beanName, mbd, instanceWrapper); // 初始化 bean，执行 BeanPostProcessor, InitializingBean exposedObject = initializeBean(beanName, exposedObject, mbd); } catch (Throwable ex) { ... } ... try { // 注册 DisposableBean, 销毁前的回调方法 registerDisposableBeanIfNecessary(beanName, bean, mbd); } catch (BeanDefinitionValidationException ex) { ... } // 返回最终的 bean return exposedObject; } ","date":"2024-05-21","objectID":"/ooooo-notes/spring-bean-%E5%88%9D%E5%A7%8B%E5%8C%96/:3:0","tags":["spring boot","source code","源码分析 spring boot 系列"],"title":"spring bean 初始化","uri":"/ooooo-notes/spring-bean-%E5%88%9D%E5%A7%8B%E5%8C%96/"},{"categories":null,"content":" spring security 的代码比较难，之前我在 ProcessOn 上做了源码导读，所以这里只说关键点。 ","date":"2024-05-20","objectID":"/ooooo-notes/spring-security-%E5%8E%9F%E7%90%86/:0:0","tags":["spring boot","source code","源码分析 spring boot 系列"],"title":"spring security 原理","uri":"/ooooo-notes/spring-security-%E5%8E%9F%E7%90%86/"},{"categories":null,"content":"理解关键点 认证的逻辑有多个 filter 来完成，常用的 filter 如 UsernamePasswordAuthenticationFilter， RememberMeAuthenticationFilter。 认证成功，就会生成 Authentication 对象，可以从 SecurityContextHolder 获取。 有两个核心配置类，HttpSecurity 和 WebSecurity，这两个都是用来配置 springSecurityFilterChain，只不过暴露的方法不一样。 ","date":"2024-05-20","objectID":"/ooooo-notes/spring-security-%E5%8E%9F%E7%90%86/:1:0","tags":["spring boot","source code","源码分析 spring boot 系列"],"title":"spring security 原理","uri":"/ooooo-notes/spring-security-%E5%8E%9F%E7%90%86/"},{"categories":null,"content":"关键代码 第一步执行下面方法，添加 SecurityConfigurer。 源码位置: org.springframework.security.config.annotation.web.configuration.WebSecurityConfiguration#setFilterChainProxySecurityConfigurer @Autowired(required = false) public void setFilterChainProxySecurityConfigurer(ObjectPostProcessor\u003cObject\u003e objectPostProcessor, @Value(\"#{@autowiredWebSecurityConfigurersIgnoreParents.getWebSecurityConfigurers()}\") List\u003cSecurityConfigurer\u003cFilter, WebSecurity\u003e\u003e webSecurityConfigurers) throws Exception { // 初始化 webSecurity, objectPostProcessor 是 AutowireBeanFactoryObjectPostProcessor 类 this.webSecurity = objectPostProcessor.postProcess(new WebSecurity(objectPostProcessor)); // 配置 debug，在开发阶段建议开启 if (this.debugEnabled != null) { this.webSecurity.debug(this.debugEnabled); } ... // 添加 SecurityConfigurer（我们实现的 WebSecurityConfigurerAdapter 就是这类） for (SecurityConfigurer\u003cFilter, WebSecurity\u003e webSecurityConfigurer : webSecurityConfigurers) { this.webSecurity.apply(webSecurityConfigurer); } this.webSecurityConfigurers = webSecurityConfigurers; } 第二步执行 build，构建 filter。 源码位置: org.springframework.security.config.annotation.web.configuration.WebSecurityConfiguration#springSecurityFilterChain @Bean(name = AbstractSecurityWebApplicationInitializer.DEFAULT_FILTER_NAME) public Filter springSecurityFilterChain() throws Exception { ... // 添加 securityFilterChain, 默认为空 for (SecurityFilterChain securityFilterChain : this.securityFilterChains) { this.webSecurity.addSecurityFilterChainBuilder(() -\u003e securityFilterChain); for (Filter filter : securityFilterChain.getFilters()) { if (filter instanceof FilterSecurityInterceptor) { this.webSecurity.securityInterceptor((FilterSecurityInterceptor) filter); break; } } } // 默认为空 for (WebSecurityCustomizer customizer : this.webSecurityCustomizers) { customizer.customize(this.webSecurity); } // 执行 build 方法，里面就会执行 SecurityConfigurer#configure 方法，关键点 return this.webSecurity.build(); } ","date":"2024-05-20","objectID":"/ooooo-notes/spring-security-%E5%8E%9F%E7%90%86/:2:0","tags":["spring boot","source code","源码分析 spring boot 系列"],"title":"spring security 原理","uri":"/ooooo-notes/spring-security-%E5%8E%9F%E7%90%86/"},{"categories":null,"content":" jdk 基于 8 版本 在平时的开发中，我们经常会用到 ThreadLocal，本质是一个 Map\u003cThread, V\u003e 结构。 ","date":"2024-05-19","objectID":"/ooooo-notes/06-threadlocal/:0:0","tags":["jdk","source code","源码分析 jdk 系列"],"title":"06 ThreadLocal","uri":"/ooooo-notes/06-threadlocal/"},{"categories":null,"content":"使用示例 例子来自于官网 @Configuration @EnableWebSocket public class WebSocketConfig implements WebSocketConfigurer { @Override public void registerWebSocketHandlers(WebSocketHandlerRegistry registry) { registry.addHandler(myHandler(), \"/myHandler\"); } @Bean public WebSocketHandler myHandler() { return new MyHandler(); } } 说明： WebSocketConfigurer 配置 websocket。 WebSocketHandler 处理 websocket 的连接。 ","date":"2024-05-18","objectID":"/ooooo-notes/spring-websocket-%E5%8E%9F%E7%90%86/:1:0","tags":["spring boot","source code","源码分析 spring boot 系列"],"title":"spring websocket 原理","uri":"/ooooo-notes/spring-websocket-%E5%8E%9F%E7%90%86/"},{"categories":null,"content":"WebSocketServletAutoConfiguration 添加了 tomcat 对 websocket 的支持，也就是 WsSci。 源码位置: org.springframework.boot.autoconfigure.websocket.servlet.WebSocketServletAutoConfiguration @ConditionalOnWebApplication(type = Type.SERVLET) @AutoConfigureBefore(ServletWebServerFactoryAutoConfiguration.class) public class WebSocketServletAutoConfiguration { @Configuration(proxyBeanMethods = false) @ConditionalOnClass({ Tomcat.class, WsSci.class }) static class TomcatWebSocketConfiguration { @Bean @ConditionalOnMissingBean(name = \"websocketServletWebServerCustomizer\") TomcatWebSocketServletWebServerCustomizer websocketServletWebServerCustomizer() { return new TomcatWebSocketServletWebServerCustomizer(); } } ... } ","date":"2024-05-18","objectID":"/ooooo-notes/spring-websocket-%E5%8E%9F%E7%90%86/:2:0","tags":["spring boot","source code","源码分析 spring boot 系列"],"title":"spring websocket 原理","uri":"/ooooo-notes/spring-websocket-%E5%8E%9F%E7%90%86/"},{"categories":null,"content":"@EnableWebSocket 最关键的配置入口。 源码位置: org.springframework.web.socket.config.annotation.EnableWebSocket // 导入 websocket 配置类, 父类为 WebSocketConfigurationSupport @Import(DelegatingWebSocketConfiguration.class) public @interface EnableWebSocket { } 源码位置: org.springframework.web.socket.config.annotation.WebSocketConfigurationSupport public class WebSocketConfigurationSupport { // 注册 WebSocketHandlerMapping @Bean public HandlerMapping webSocketHandlerMapping(@Nullable TaskScheduler defaultSockJsTaskScheduler) { // 初始化 registry ServletWebSocketHandlerRegistry registry = initHandlerRegistry(); ... // 很重要, 后面继续解析 return registry.getHandlerMapping(); } private ServletWebSocketHandlerRegistry initHandlerRegistry() { if (this.handlerRegistry == null) { this.handlerRegistry = new ServletWebSocketHandlerRegistry(); // 由子类 DelegatingWebSocketConfiguration 来实现 registerWebSocketHandlers(this.handlerRegistry); } return this.handlerRegistry; } } 源码位置: org.springframework.web.socket.config.annotation.ServletWebSocketHandlerRegistry#getHandlerMapping public AbstractHandlerMapping getHandlerMapping() { Map\u003cString, Object\u003e urlMap = new LinkedHashMap\u003c\u003e(); // 遍历所有的 websocket 的配置 for (ServletWebSocketHandlerRegistration registration : this.registrations) { // HttpRequestHandler 实现类为 WebSocketHttpRequestHandler，负责处理 websocket 请求, 很重要 MultiValueMap\u003cHttpRequestHandler, String\u003e mappings = registration.getMappings(); mappings.forEach((httpHandler, patterns) -\u003e { for (String pattern : patterns) { urlMap.put(pattern, httpHandler); } }); } // WebSocketHandlerMapping 负责拦截 websocket 的 url，然后由 WebSocketHttpRequestHandler 处理请求 WebSocketHandlerMapping hm = new WebSocketHandlerMapping(); hm.setUrlMap(urlMap); hm.setOrder(this.order); if (this.urlPathHelper != null) { hm.setUrlPathHelper(this.urlPathHelper); } return hm; } ","date":"2024-05-18","objectID":"/ooooo-notes/spring-websocket-%E5%8E%9F%E7%90%86/:3:0","tags":["spring boot","source code","源码分析 spring boot 系列"],"title":"spring websocket 原理","uri":"/ooooo-notes/spring-websocket-%E5%8E%9F%E7%90%86/"},{"categories":null,"content":"WebSocketHttpRequestHandler 处理 websocket 请求 源码位置: org.springframework.web.socket.server.support.WebSocketHttpRequestHandler#handleRequest @Override public void handleRequest(HttpServletRequest servletRequest, HttpServletResponse servletResponse) throws ServletException, IOException { ... try { ... // 执行 HandshakeInterceptor#beforeHandshake 方法 if (!chain.applyBeforeHandshake(request, response, attributes)) { return; } // 执行 websocket 握手, 最终会调用 TomcatRequestUpgradeStrategy#upgradeInternal this.handshakeHandler.doHandshake(request, response, this.wsHandler, attributes); // 执行 HandshakeInterceptor#afterHandshake 方法 chain.applyAfterHandshake(request, response, null); } catch (HandshakeFailureException ex) { failure = ex; } catch (Exception ex) { failure = new HandshakeFailureException(\"Uncaught failure for request \" + request.getURI(), ex); } finally { ... } } ","date":"2024-05-18","objectID":"/ooooo-notes/spring-websocket-%E5%8E%9F%E7%90%86/:4:0","tags":["spring boot","source code","源码分析 spring boot 系列"],"title":"spring websocket 原理","uri":"/ooooo-notes/spring-websocket-%E5%8E%9F%E7%90%86/"},{"categories":null,"content":" 在 spring 中，最常用的两种扩展就是 BeanFactoryPostProcessor 和 BeanPostProcessor, 当然还有其他的扩展，比如 ApplicationListener, SpringApplicationRunListener 等等。 ","date":"2024-05-17","objectID":"/ooooo-notes/spring-%E5%B8%B8%E7%94%A8%E6%89%A9%E5%B1%95%E7%82%B9/:0:0","tags":["spring boot","source code","源码分析 spring boot 系列"],"title":"spring 常用扩展点","uri":"/ooooo-notes/spring-%E5%B8%B8%E7%94%A8%E6%89%A9%E5%B1%95%E7%82%B9/"},{"categories":null,"content":"BeanFactoryPostProcessor 源码位置: org.springframework.beans.factory.config.BeanFactoryPostProcessor @FunctionalInterface public interface BeanFactoryPostProcessor { // 可以用 beanFactory 来修改 beanDefinition, 注册 singletonBean void postProcessBeanFactory(ConfigurableListableBeanFactory beanFactory) throws BeansException; } 源码位置: org.springframework.beans.factory.support.BeanDefinitionRegistryPostProcessor // 当前类是 BeanFactoryPostProcessor 的子接口 public interface BeanDefinitionRegistryPostProcessor extends BeanFactoryPostProcessor { // 可以用 registry 来注册和修改 beanDefinition void postProcessBeanDefinitionRegistry(BeanDefinitionRegistry registry) throws BeansException; } ","date":"2024-05-17","objectID":"/ooooo-notes/spring-%E5%B8%B8%E7%94%A8%E6%89%A9%E5%B1%95%E7%82%B9/:1:0","tags":["spring boot","source code","源码分析 spring boot 系列"],"title":"spring 常用扩展点","uri":"/ooooo-notes/spring-%E5%B8%B8%E7%94%A8%E6%89%A9%E5%B1%95%E7%82%B9/"},{"categories":null,"content":"BeanPostProcessor 源码位置: org.springframework.beans.factory.config.BeanPostProcessor public interface BeanPostProcessor { // 在 InitializingBean#afterPropertiesSet 执行前 default Object postProcessBeforeInitialization(Object bean, String beanName) throws BeansException { // 1. 可以返回代理对象 // 2. 可以设置bean的属性 (这里) return bean; } // 在 InitializingBean#afterPropertiesSet 执行后 default Object postProcessAfterInitialization(Object bean, String beanName) throws BeansException { // 1. 可以返回代理对象 // 2. 可以设置bean的属性 return bean; } } 源码位置: org.springframework.beans.factory.config.InstantiationAwareBeanPostProcessor // 当前类是 BeanPostProcessor 的子接口 public interface InstantiationAwareBeanPostProcessor extends BeanPostProcessor { // 执行实例化之前 default Object postProcessBeforeInstantiation(Class\u003c?\u003e beanClass, String beanName) throws BeansException { return null; } // 执行实例化之后 default boolean postProcessAfterInstantiation(Object bean, String beanName) throws BeansException { return true; } // 填充属性 default PropertyValues postProcessProperties(PropertyValues pvs, Object bean, String beanName) throws BeansException { return null; } // 过期方法，和 postProcessProperties 一样 @Deprecated default PropertyValues postProcessPropertyValues( PropertyValues pvs, PropertyDescriptor[] pds, Object bean, String beanName) throws BeansException { return pvs; } } 源码位置: org.springframework.beans.factory.config.SmartInstantiationAwareBeanPostProcessor // 当前类是 BeanPostProcessor 的子接口 public interface SmartInstantiationAwareBeanPostProcessor extends InstantiationAwareBeanPostProcessor { // 预测 bean 的类型，这个方法用的很少 default Class\u003c?\u003e predictBeanType(Class\u003c?\u003e beanClass, String beanName) throws BeansException { return null; } // 决定候选的构造函数, 这个方法用的很少 default Constructor\u003c?\u003e[] determineCandidateConstructors(Class\u003c?\u003e beanClass, String beanName) throws BeansException { return null; } // 返回 early 访问的 bean 引用 (解决循环引用) default Object getEarlyBeanReference(Object bean, String beanName) throws BeansException { // 可以返回代理对象 return bean; } } ","date":"2024-05-17","objectID":"/ooooo-notes/spring-%E5%B8%B8%E7%94%A8%E6%89%A9%E5%B1%95%E7%82%B9/:2:0","tags":["spring boot","source code","源码分析 spring boot 系列"],"title":"spring 常用扩展点","uri":"/ooooo-notes/spring-%E5%B8%B8%E7%94%A8%E6%89%A9%E5%B1%95%E7%82%B9/"},{"categories":null,"content":"其他扩展 ApplicationListener: 监听 spring 事件，比如 ContextRefreshedEvent, ContextStartedEvent。 SpringApplicationRunListener: 监听 spring boot 事件，比如 contextLoaded，started。 ApplicationRunner, CommandLineRunner: 服务启动的回调。 InitializingBean: 初始化 bean 的回调。 DisposableBean: 销毁 bean 的回调。 *Aware: 注入 bean 的回调，比如 ApplicationContextAware, EnvironmentAware。 FactoryBean: 创建 bean 的工厂类。 ","date":"2024-05-17","objectID":"/ooooo-notes/spring-%E5%B8%B8%E7%94%A8%E6%89%A9%E5%B1%95%E7%82%B9/:3:0","tags":["spring boot","source code","源码分析 spring boot 系列"],"title":"spring 常用扩展点","uri":"/ooooo-notes/spring-%E5%B8%B8%E7%94%A8%E6%89%A9%E5%B1%95%E7%82%B9/"},{"categories":null,"content":"自动配置类 源码位置: org.springframework.boot.autoconfigure.web.servlet.ServletWebServerFactoryAutoConfiguration // 导入 tomcat，jetty，undertow 的配置 @Import({ ServletWebServerFactoryAutoConfiguration.BeanPostProcessorsRegistrar.class, ServletWebServerFactoryConfiguration.EmbeddedTomcat.class, ServletWebServerFactoryConfiguration.EmbeddedJetty.class, ServletWebServerFactoryConfiguration.EmbeddedUndertow.class }) public class ServletWebServerFactoryAutoConfiguration { @Bean public ServletWebServerFactoryCustomizer servletWebServerFactoryCustomizer(ServerProperties serverProperties, ObjectProvider\u003cWebListenerRegistrar\u003e webListenerRegistrars, ObjectProvider\u003cCookieSameSiteSupplier\u003e cookieSameSiteSuppliers) { // 配置 serverProperties return new ServletWebServerFactoryCustomizer(serverProperties, webListenerRegistrars.orderedStream().collect(Collectors.toList()), cookieSameSiteSuppliers.orderedStream().collect(Collectors.toList())); } ... } ","date":"2024-05-16","objectID":"/ooooo-notes/%E9%80%82%E9%85%8D%E5%A4%9A%E7%A7%8D-servlet-%E5%AE%B9%E5%99%A8/:1:0","tags":["spring boot","source code","源码分析 spring boot 系列"],"title":"适配多种 servlet 容器","uri":"/ooooo-notes/%E9%80%82%E9%85%8D%E5%A4%9A%E7%A7%8D-servlet-%E5%AE%B9%E5%99%A8/"},{"categories":null,"content":"创建 webServer 源码位置: org.springframework.boot.web.servlet.context.ServletWebServerApplicationContext#onRefresh @Override protected void onRefresh() { super.onRefresh(); try { // 创建 webServer createWebServer(); } catch (Throwable ex) { throw new ApplicationContextException(\"Unable to start web server\", ex); } } 源码位置: org.springframework.boot.web.servlet.context.ServletWebServerApplicationContext#createWebServer private void createWebServer() { WebServer webServer = this.webServer; ServletContext servletContext = getServletContext(); if (webServer == null \u0026\u0026 servletContext == null) { // 获取 ServletWebServerFactory (在自动配置类中) ServletWebServerFactory factory = getWebServerFactory(); // 创建 webServer this.webServer = factory.getWebServer(getSelfInitializer()); ... // 会执行回调来执行 webServer#start 方法 getBeanFactory().registerSingleton(\"webServerStartStop\", new WebServerStartStopLifecycle(this, this.webServer)); } ... } ","date":"2024-05-16","objectID":"/ooooo-notes/%E9%80%82%E9%85%8D%E5%A4%9A%E7%A7%8D-servlet-%E5%AE%B9%E5%99%A8/:2:0","tags":["spring boot","source code","源码分析 spring boot 系列"],"title":"适配多种 servlet 容器","uri":"/ooooo-notes/%E9%80%82%E9%85%8D%E5%A4%9A%E7%A7%8D-servlet-%E5%AE%B9%E5%99%A8/"},{"categories":null,"content":" spring boot 启动流程必须懂。 启动类示例: @SpringBootApplication public class HiApplication { public static void main(String[] args) { // 先执行 SpringApplication 的构造方法，然后执行 run 方法 SpringApplication.run(HiApplication.class, args); } } ","date":"2024-05-15","objectID":"/ooooo-notes/spring-boot-%E5%90%AF%E5%8A%A8%E6%B5%81%E7%A8%8B/:0:0","tags":["spring boot","source code","源码分析 spring boot 系列"],"title":"spring boot 启动流程","uri":"/ooooo-notes/spring-boot-%E5%90%AF%E5%8A%A8%E6%B5%81%E7%A8%8B/"},{"categories":null,"content":"SpringApplication#run 源码位置: org.springframework.boot.SpringApplication#SpringApplication // SpringApplication 的构造方法 public SpringApplication(ResourceLoader resourceLoader, Class\u003c?\u003e... primarySources) { ... // 决定 web 类型，如 servlet, reactive this.webApplicationType = WebApplicationType.deduceFromClasspath(); // 加载 BootstrapRegistryInitializer 扩展 this.bootstrapRegistryInitializers = new ArrayList\u003c\u003e( getSpringFactoriesInstances(BootstrapRegistryInitializer.class)); // 加载 ApplicationContextInitializer 扩展 setInitializers((Collection) getSpringFactoriesInstances(ApplicationContextInitializer.class)); // 加载 ApplicationListener 扩展，这个很重要 setListeners((Collection) getSpringFactoriesInstances(ApplicationListener.class)); this.mainApplicationClass = deduceMainApplicationClass(); } 源码位置: org.springframework.boot.SpringApplication#run public ConfigurableApplicationContext run(String... args) { long startTime = System.nanoTime(); // 会执行 BootstrapRegistryInitializer#initialize 方法 DefaultBootstrapContext bootstrapContext = createBootstrapContext(); ConfigurableApplicationContext context = null; configureHeadlessProperty(); // 加载 SpringApplicationRunListener 扩展 SpringApplicationRunListeners listeners = getRunListeners(args); // 执行 SpringApplicationRunListener#starting 方法 listeners.starting(bootstrapContext, this.mainApplicationClass); try { ApplicationArguments applicationArguments = new DefaultApplicationArguments(args); // 准备 environment, 后面继续解析 ConfigurableEnvironment environment = prepareEnvironment(listeners, bootstrapContext, applicationArguments); configureIgnoreBeanInfo(environment); // 打印 banner Banner printedBanner = printBanner(environment); // 创建 applicationContext, 后面继续解析 context = createApplicationContext(); context.setApplicationStartup(this.applicationStartup); // 准备 context, 后面继续解析 prepareContext(bootstrapContext, context, environment, listeners, applicationArguments, printedBanner); // 刷新 context, 后面继续解析 refreshContext(context); // 空实现 afterRefresh(context, applicationArguments); ... // 执行 SpringApplicationRunListener#started 方法 listeners.started(context, timeTakenToStartup); // 执行 ApplicationRunner 和 CommandLineRunner callRunners(context, applicationArguments); } catch (Throwable ex) { // 执行 SpringApplicationRunListener#failed 方法 handleRunFailure(context, ex, listeners); throw new IllegalStateException(ex); } try { Duration timeTakenToReady = Duration.ofNanos(System.nanoTime() - startTime); // 执行 SpringApplicationRunListener#ready 方法 listeners.ready(context, timeTakenToReady); } catch (Throwable ex) { // 执行 SpringApplicationRunListener#failed 方法 handleRunFailure(context, ex, null); throw new IllegalStateException(ex); } return context; } ","date":"2024-05-15","objectID":"/ooooo-notes/spring-boot-%E5%90%AF%E5%8A%A8%E6%B5%81%E7%A8%8B/:1:0","tags":["spring boot","source code","源码分析 spring boot 系列"],"title":"spring boot 启动流程","uri":"/ooooo-notes/spring-boot-%E5%90%AF%E5%8A%A8%E6%B5%81%E7%A8%8B/"},{"categories":null,"content":"prepareEnvironment 准备环境 源码位置: org.springframework.boot.SpringApplication#prepareEnvironment private ConfigurableEnvironment prepareEnvironment(SpringApplicationRunListeners listeners, DefaultBootstrapContext bootstrapContext, ApplicationArguments applicationArguments) { // 根据 webType 来创建环境 ConfigurableEnvironment environment = getOrCreateEnvironment(); // 环境中添加默认配置和命令行配置 configureEnvironment(environment, applicationArguments.getSourceArgs()); // 没干啥事, 不重要 ConfigurationPropertySources.attach(environment); // 执行 SpringApplicationRunListener#environmentPrepared 方法 listeners.environmentPrepared(bootstrapContext, environment); // 默认配置移到最后 DefaultPropertiesPropertySource.moveToEnd(environment); Assert.state(!environment.containsProperty(\"spring.main.environment-prefix\"), \"Environment prefix cannot be set via properties.\"); // 绑定 spring.main 的属性到 SpringApplication bindToSpringApplication(environment); if (!this.isCustomEnvironment) { environment = convertEnvironment(environment); } // 没干啥事, 不重要 ConfigurationPropertySources.attach(environment); return environment; } ","date":"2024-05-15","objectID":"/ooooo-notes/spring-boot-%E5%90%AF%E5%8A%A8%E6%B5%81%E7%A8%8B/:2:0","tags":["spring boot","source code","源码分析 spring boot 系列"],"title":"spring boot 启动流程","uri":"/ooooo-notes/spring-boot-%E5%90%AF%E5%8A%A8%E6%B5%81%E7%A8%8B/"},{"categories":null,"content":"createApplicationContext 创建上下文 源码位置: org.springframework.boot.SpringApplication#createApplicationContext protected ConfigurableApplicationContext createApplicationContext() { // 根据 webType 来创建 applicationContext return this.applicationContextFactory.create(this.webApplicationType); } 源码位置: org.springframework.boot.web.servlet.context.AnnotationConfigServletWebServerApplicationContext.Factory#create // 根据 webType 来创建 applicationContext @Override public ConfigurableApplicationContext create(WebApplicationType webApplicationType) { return (webApplicationType != WebApplicationType.SERVLET) ? null : new AnnotationConfigServletWebServerApplicationContext(); } // 构造函数 public AnnotationConfigServletWebServerApplicationContext() { // 非常重要，构造方法中注册了 ConfigurationClassPostProcessor，AutowiredAnnotationBeanPostProcessor this.reader = new AnnotatedBeanDefinitionReader(this); // 非常重要，扫描方法 ClassPathBeanDefinitionScanner#scan this.scanner = new ClassPathBeanDefinitionScanner(this); } ","date":"2024-05-15","objectID":"/ooooo-notes/spring-boot-%E5%90%AF%E5%8A%A8%E6%B5%81%E7%A8%8B/:3:0","tags":["spring boot","source code","源码分析 spring boot 系列"],"title":"spring boot 启动流程","uri":"/ooooo-notes/spring-boot-%E5%90%AF%E5%8A%A8%E6%B5%81%E7%A8%8B/"},{"categories":null,"content":"prepareContext 准备上下文 源码位置: org.springframework.boot.SpringApplication#prepareContext private void prepareContext(DefaultBootstrapContext bootstrapContext, ConfigurableApplicationContext context, ConfigurableEnvironment environment, SpringApplicationRunListeners listeners, ApplicationArguments applicationArguments, Banner printedBanner) { // 设置环境 context.setEnvironment(environment); // 设置 beanNameGenerator，resourceLoader，conversionService postProcessApplicationContext(context); // 执行 ApplicationContextInitializer#initialize applyInitializers(context); // 执行 SpringApplicationRunListener#contextPrepared 方法 listeners.contextPrepared(context); // 发布 BootstrapContextClosedEvent 事件，不重要 bootstrapContext.close(context); ... // lazy 初始化 bean if (this.lazyInitialization) { context.addBeanFactoryPostProcessor(new LazyInitializationBeanFactoryPostProcessor()); } // 不重要 context.addBeanFactoryPostProcessor(new PropertySourceOrderingBeanFactoryPostProcessor(context)); // Load the sources Set\u003cObject\u003e sources = getAllSources(); Assert.notEmpty(sources, \"Sources must not be empty\"); // 加载启动类, 这样启动类就会作为一个 bean, 会被 ConfigurationClassPostProcessor 处理 load(context, sources.toArray(new Object[0])); // 执行 SpringApplicationRunListener#contextLoaded 方法 listeners.contextLoaded(context); } ","date":"2024-05-15","objectID":"/ooooo-notes/spring-boot-%E5%90%AF%E5%8A%A8%E6%B5%81%E7%A8%8B/:4:0","tags":["spring boot","source code","源码分析 spring boot 系列"],"title":"spring boot 启动流程","uri":"/ooooo-notes/spring-boot-%E5%90%AF%E5%8A%A8%E6%B5%81%E7%A8%8B/"},{"categories":null,"content":"refreshContext 刷新上下文 源码位置: org.springframework.boot.SpringApplication#refreshContext private void refreshContext(ConfigurableApplicationContext context) { // 注册 shutdown 钩子 if (this.registerShutdownHook) { shutdownHook.registerApplicationContext(context); } // 刷新, 实现类为 ServletWebServerApplicationContext refresh(context); } 源码位置: org.springframework.boot.web.servlet.context.ServletWebServerApplicationContext#refresh @Override public final void refresh() throws BeansException, IllegalStateException { try { // 父类为 AbstractApplicationContext super.refresh(); } catch (RuntimeException ex) { WebServer webServer = this.webServer; if (webServer != null) { webServer.stop(); } throw ex; } } 源码位置: org.springframework.context.support.AbstractApplicationContext#refresh @Override public void refresh() throws BeansException, IllegalStateException { synchronized (this.startupShutdownMonitor) { ... // 初始化 ServletPropertySources prepareRefresh(); // 做一些准备操作，如忽略依赖接口，注册可解析的依赖, 注册默认的Bean prepareBeanFactory(beanFactory); try { // 子类实现为 AnnotationConfigServletWebServerApplicationContext // 读取和扫描 beanDefinition(spring boot 没有用) postProcessBeanFactory(beanFactory); // 执行 BeanFactoryPostProcessor (按照 PriorityOrdered，Ordered，nonOrdered)，不解析 // 里面会用到 ConfigurationClassPostProcessor 扫描启动类 invokeBeanFactoryPostProcessors(beanFactory); // 注册 BeanPostProcessor (按照 PriorityOrdered，Ordered，nonOrdered)，不解析 registerBeanPostProcessors(beanFactory); // 注册 MessageSource（国际化）不用关心 initMessageSource(); // 注册 ApplicationEventMulticaster initApplicationEventMulticaster(); // 子类实现为 AnnotationConfigServletWebServerApplicationContext // 创建了 servlet 容器 onRefresh(); // 注册 ApplicationListener registerListeners(); // 初始化 beanFactory, 初始化 non-lazy-init bean, 非常重要，会在【bean 初始化】章节解析 finishBeanFactoryInitialization(beanFactory); // 执行 LifecycleProcessor#onRefresh 方法，发布 ContextRefreshedEvent 事件 finishRefresh(); } catch (BeansException ex) { ... } finally { ... } } } ","date":"2024-05-15","objectID":"/ooooo-notes/spring-boot-%E5%90%AF%E5%8A%A8%E6%B5%81%E7%A8%8B/:5:0","tags":["spring boot","source code","源码分析 spring boot 系列"],"title":"spring boot 启动流程","uri":"/ooooo-notes/spring-boot-%E5%90%AF%E5%8A%A8%E6%B5%81%E7%A8%8B/"},{"categories":null,"content":" spring mvc 原理真的必须懂, 之前写的源码导读。 ","date":"2024-05-14","objectID":"/ooooo-notes/spring-mvc-%E8%AF%B7%E6%B1%82%E6%B5%81%E7%A8%8B/:0:0","tags":["spring boot","source code","源码分析 spring boot 系列"],"title":"spring mvc 请求流程","uri":"/ooooo-notes/spring-mvc-%E8%AF%B7%E6%B1%82%E6%B5%81%E7%A8%8B/"},{"categories":null,"content":"DispatcherServlet 源码位置: org.springframework.web.servlet.DispatcherServlet#doDispatch protected void doDispatch(HttpServletRequest request, HttpServletResponse response) throws Exception { ... try { ... try { // 检查是否为文件上传 processedRequest = checkMultipart(request); // 获取 HandlerMapping mappedHandler = getHandler(processedRequest); if (mappedHandler == null) { // 40 noHandlerFound(processedRequest, response); return; } // 获取 HandlerAdapter HandlerAdapter ha = getHandlerAdapter(mappedHandler.getHandler()); ... // 执行 HandlerInterceptor#preHandle if (!mappedHandler.applyPreHandle(processedRequest, response)) { return; } // 执行请求 mv = ha.handle(processedRequest, response, mappedHandler.getHandler()); ... // 执行 HandlerInterceptor#postHandle mappedHandler.applyPostHandle(processedRequest, response, mv); } catch (Exception ex) { dispatchException = ex; } catch (Throwable err) { dispatchException = new NestedServletException(\"Handler dispatch failed\", err); } // 处理异常，然后执行 HandlerInterceptor#afterCompletion processDispatchResult(processedRequest, response, mappedHandler, mv, dispatchException); } catch (Exception ex) { // 执行 HandlerInterceptor#afterCompletion triggerAfterCompletion(processedRequest, response, mappedHandler, ex); } catch (Throwable err) { // 执行 HandlerInterceptor#afterCompletion triggerAfterCompletion(processedRequest, response, mappedHandler, new NestedServletException(\"Handler processing failed\", err)); } finally { ... } } 总结： 根据 url 查找对应的 HandlerMapping。 根据 HandlerMapping 查找对应的 HandlerAdapter。 执行 HandlerInterceptor#preHandle。 执行请求。 执行 HandlerInterceptor#postHandle。 执行 HandlerExceptionResolver#resolveException。 执行 HandlerInterceptor#afterCompletion。 ","date":"2024-05-14","objectID":"/ooooo-notes/spring-mvc-%E8%AF%B7%E6%B1%82%E6%B5%81%E7%A8%8B/:1:0","tags":["spring boot","source code","源码分析 spring boot 系列"],"title":"spring mvc 请求流程","uri":"/ooooo-notes/spring-mvc-%E8%AF%B7%E6%B1%82%E6%B5%81%E7%A8%8B/"},{"categories":null,"content":"HandlerMapping 常用实现类 RequestMappingHandlerMapping ","date":"2024-05-14","objectID":"/ooooo-notes/spring-mvc-%E8%AF%B7%E6%B1%82%E6%B5%81%E7%A8%8B/:2:0","tags":["spring boot","source code","源码分析 spring boot 系列"],"title":"spring mvc 请求流程","uri":"/ooooo-notes/spring-mvc-%E8%AF%B7%E6%B1%82%E6%B5%81%E7%A8%8B/"},{"categories":null,"content":"HandlerAdapter 常用实现类 RequestMappingHandlerAdapter ","date":"2024-05-14","objectID":"/ooooo-notes/spring-mvc-%E8%AF%B7%E6%B1%82%E6%B5%81%E7%A8%8B/:3:0","tags":["spring boot","source code","源码分析 spring boot 系列"],"title":"spring mvc 请求流程","uri":"/ooooo-notes/spring-mvc-%E8%AF%B7%E6%B1%82%E6%B5%81%E7%A8%8B/"},{"categories":null,"content":" spring cache 是最常见的功能之一，有必要了解其原理。 ","date":"2024-05-13","objectID":"/ooooo-notes/spring-cache-%E5%8E%9F%E7%90%86/:0:0","tags":["spring boot","source code","源码分析 spring boot 系列"],"title":"spring cache 原理","uri":"/ooooo-notes/spring-cache-%E5%8E%9F%E7%90%86/"},{"categories":null,"content":"CacheAutoConfiguration 自动配置类 源码位置: org.springframework.boot.autoconfigure.cache.CacheAutoConfiguration // 导入 CacheConfigurationImportSelector 配置类 @Import({ CacheConfigurationImportSelector.class, ... }) public class CacheAutoConfiguration { // 自定义 CacheManager @Bean @ConditionalOnMissingBean public CacheManagerCustomizers cacheManagerCustomizers(ObjectProvider\u003cCacheManagerCustomizer\u003c?\u003e\u003e customizers) { return new CacheManagerCustomizers(customizers.orderedStream().collect(Collectors.toList())); } ... static class CacheConfigurationImportSelector implements ImportSelector { @Override public String[] selectImports(AnnotationMetadata importingClassMetadata) { // CacheType 定义了多种缓存的实现，比如 redis，caffine，simple CacheType[] types = CacheType.values(); String[] imports = new String[types.length]; for (int i = 0; i \u003c types.length; i++) { // 获取对应的配置类，每个配置类都有注解 @Conditional(CacheCondition.class) imports[i] = CacheConfigurations.getConfigurationClass(types[i]); } return imports; } } } ","date":"2024-05-13","objectID":"/ooooo-notes/spring-cache-%E5%8E%9F%E7%90%86/:1:0","tags":["spring boot","source code","源码分析 spring boot 系列"],"title":"spring cache 原理","uri":"/ooooo-notes/spring-cache-%E5%8E%9F%E7%90%86/"},{"categories":null,"content":"CacheCondition 选择不同的缓存 源码位置: org.springframework.boot.autoconfigure.cache.CacheCondition class CacheCondition extends SpringBootCondition { @Override public ConditionOutcome getMatchOutcome(ConditionContext context, AnnotatedTypeMetadata metadata) { ... try { // 获取配置的 cacheType BindResult\u003cCacheType\u003e specified = Binder.get(environment).bind(\"spring.cache.type\", CacheType.class); if (!specified.isBound()) { return ConditionOutcome.match(message.because(\"automatic cache type\")); } // 加载指定的缓存 CacheType required = CacheConfigurations.getType(((AnnotationMetadata) metadata).getClassName()); if (specified.get() == required) { return ConditionOutcome.match(message.because(specified.get() + \" cache type\")); } } catch (BindException ex) { } return ConditionOutcome.noMatch(message.because(\"unknown cache type\")); } } ","date":"2024-05-13","objectID":"/ooooo-notes/spring-cache-%E5%8E%9F%E7%90%86/:2:0","tags":["spring boot","source code","源码分析 spring boot 系列"],"title":"spring cache 原理","uri":"/ooooo-notes/spring-cache-%E5%8E%9F%E7%90%86/"},{"categories":null,"content":"@EnableCaching 启用缓存 启用缓存功能，可以在启动类上使用注解 @EnableCaching。 源码位置: org.springframework.cache.annotation.EnableCaching @Import(CachingConfigurationSelector.class) public @interface EnableCaching { boolean proxyTargetClass() default false; AdviceMode mode() default AdviceMode.PROXY; int order() default Ordered.LOWEST_PRECEDENCE; } 源码位置: org.springframework.cache.annotation.CachingConfigurationSelector // 选择不同的代理模式 public class CachingConfigurationSelector extends AdviceModeImportSelector\u003cEnableCaching\u003e { @Override public String[] selectImports(AdviceMode adviceMode) { switch (adviceMode) { case PROXY: // 默认是这个 return getProxyImports(); case ASPECTJ: return getAspectJImports(); default: return null; } } private String[] getProxyImports() { List\u003cString\u003e result = new ArrayList\u003c\u003e(3); // 加载 aop 配置类 result.add(AutoProxyRegistrar.class.getName()); // 加载 CacheInterceptor result.add(ProxyCachingConfiguration.class.getName()); if (jsr107Present \u0026\u0026 jcacheImplPresent) { result.add(PROXY_JCACHE_CONFIGURATION_CLASS); } return StringUtils.toStringArray(result); } } ","date":"2024-05-13","objectID":"/ooooo-notes/spring-cache-%E5%8E%9F%E7%90%86/:3:0","tags":["spring boot","source code","源码分析 spring boot 系列"],"title":"spring cache 原理","uri":"/ooooo-notes/spring-cache-%E5%8E%9F%E7%90%86/"},{"categories":null,"content":"CacheInterceptor 拦截器 源码位置: org.springframework.cache.interceptor.CacheInterceptor#invoke public Object invoke(final MethodInvocation invocation) throws Throwable { ... CacheOperationInvoker aopAllianceInvoker = () -\u003e { try { return invocation.proceed(); } catch (Throwable ex) { throw new CacheOperationInvoker.ThrowableWrapper(ex); } }; ... try { return execute(aopAllianceInvoker, target, method, invocation.getArguments()); } catch (CacheOperationInvoker.ThrowableWrapper th) { throw th.getOriginal(); } } 源码位置: org.springframework.cache.interceptor.CacheAspectSupport#execute // 缓存值为 null，会执行 @Cachable // 任何时候都会执行 @CachePut 和 @CacheEvict private Object execute(final CacheOperationInvoker invoker, Method method, CacheOperationContexts contexts) { if (contexts.isSynchronized()) { // 默认情况下不是同步操作，这里不解析了 ... } // 处理注解 @CacheEvict(beforeInvocation=true) processCacheEvicts(contexts.get(CacheEvictOperation.class), true, CacheOperationExpressionEvaluator.NO_RESULT); // 从缓存中获取值 Cache.ValueWrapper cacheHit = findCachedItem(contexts.get(CacheableOperation.class)); // Collect puts from any @Cacheable miss, if no cached item is found List\u003cCachePutRequest\u003e cachePutRequests = new ArrayList\u003c\u003e(); if (cacheHit == null) { // 缓存值为 null，说明要执行 @Cacheable collectPutRequests(contexts.get(CacheableOperation.class), CacheOperationExpressionEvaluator.NO_RESULT, cachePutRequests); } Object cacheValue; Object returnValue; // 缓存值不为 null，直接使用 if (cacheHit != null \u0026\u0026 !hasCachePut(contexts)) { // If there are no put requests, just use the cache hit cacheValue = cacheHit.get(); returnValue = wrapCacheValue(method, cacheValue); } else { // 调用真实方法获取值 returnValue = invokeOperation(invoker); cacheValue = unwrapReturnValue(returnValue); } // Collect any explicit @CachePuts // 处理注解 @CachePut collectPutRequests(contexts.get(CachePutOperation.class), cacheValue, cachePutRequests); // Process any collected put requests, either from @CachePut or a @Cacheable miss for (CachePutRequest cachePutRequest : cachePutRequests) { cachePutRequest.apply(cacheValue); } // 处理注解 @CacheEvict(beforeInvocation=false) (默认情况) processCacheEvicts(contexts.get(CacheEvictOperation.class), false, cacheValue); return returnValue; } ","date":"2024-05-13","objectID":"/ooooo-notes/spring-cache-%E5%8E%9F%E7%90%86/:4:0","tags":["spring boot","source code","源码分析 spring boot 系列"],"title":"spring cache 原理","uri":"/ooooo-notes/spring-cache-%E5%8E%9F%E7%90%86/"},{"categories":null,"content":"为什么学 现在 rust 特别火，我觉得有必须学一学，虽然这门语言上手难，一旦学会了写代码的体验非常好。 ","date":"2024-05-12","objectID":"/ooooo-notes/%E4%BB%8E%E9%9B%B6%E5%AD%A6-rust/:1:0","tags":["rust","从零学技术系列"],"title":"从零学 rust","uri":"/ooooo-notes/%E4%BB%8E%E9%9B%B6%E5%AD%A6-rust/"},{"categories":null,"content":"怎么学 推荐一些资料 Rust程序设计（第2版） Rust实战 Asynchronous Programming in Rust The Rustonomicon 代码： robinson web引擎 toydb 分布式数据库 r9cc c99编译器 ","date":"2024-05-12","objectID":"/ooooo-notes/%E4%BB%8E%E9%9B%B6%E5%AD%A6-rust/:2:0","tags":["rust","从零学技术系列"],"title":"从零学 rust","uri":"/ooooo-notes/%E4%BB%8E%E9%9B%B6%E5%AD%A6-rust/"},{"categories":null,"content":"固定窗口算法 type FixedWindowRateLimiter struct { // 固定窗口大小, 单位ms windowInterval time.Duration // 限制 limit int // 窗口开始时间 prevTime time.Time // 当前限制 curLimit int } func (s *FixedWindowRateLimiter) acquire() (bool, error) { // 不在一个时间窗口，重置 if time.Until(s.prevTime) \u003e s.windowInterval { s.curLimit = 0 } s.curLimit++ s.prevTime = time.Now() return s.curLimit \u003c s.limit, nil } ","date":"2024-04-12","objectID":"/ooooo-notes/%E5%B8%B8%E8%A7%81%E7%9A%84%E9%99%90%E6%B5%81%E7%AE%97%E6%B3%95/:1:0","tags":["rateLimiter"],"title":"常见的限流算法","uri":"/ooooo-notes/%E5%B8%B8%E8%A7%81%E7%9A%84%E9%99%90%E6%B5%81%E7%AE%97%E6%B3%95/"},{"categories":null,"content":"令牌桶算法 type TokenBucketRateLimiter struct { // 桶大小 bucketSize int // 速率，单位ms rate int // 剩余令牌数 remainTokens int // 时间 prevTime time.Time } func (t *TokenBucketRateLimiter) acquire() (bool, error) { // 计算新的令牌数 newTokens := int(time.Until(t.prevTime).Milliseconds()) * t.rate t.remainTokens += newTokens if t.remainTokens \u003e= t.bucketSize { t.remainTokens = t.bucketSize } t.remainTokens-- t.prevTime = time.Now() return t.remainTokens \u003e 0, nil } ","date":"2024-04-12","objectID":"/ooooo-notes/%E5%B8%B8%E8%A7%81%E7%9A%84%E9%99%90%E6%B5%81%E7%AE%97%E6%B3%95/:2:0","tags":["rateLimiter"],"title":"常见的限流算法","uri":"/ooooo-notes/%E5%B8%B8%E8%A7%81%E7%9A%84%E9%99%90%E6%B5%81%E7%AE%97%E6%B3%95/"},{"categories":null,"content":"漏桶算法 type LeakyBucketRateLimiter struct { // 速率，单位ms rate int // 剩余令牌数 remainTokens int // 时间 prevTime time.Time } func (l *LeakyBucketRateLimiter) acquire() (bool, error) { // 不是同一毫秒，重置令牌数 if time.Now().UnixMilli() != l.prevTime.UnixMilli() { l.remainTokens = l.rate } l.remainTokens-- l.prevTime = time.Now() return l.remainTokens \u003e 0, nil } ","date":"2024-04-12","objectID":"/ooooo-notes/%E5%B8%B8%E8%A7%81%E7%9A%84%E9%99%90%E6%B5%81%E7%AE%97%E6%B3%95/:3:0","tags":["rateLimiter"],"title":"常见的限流算法","uri":"/ooooo-notes/%E5%B8%B8%E8%A7%81%E7%9A%84%E9%99%90%E6%B5%81%E7%AE%97%E6%B3%95/"},{"categories":null,"content":"示例代码 demo-ratelimiter ","date":"2024-04-12","objectID":"/ooooo-notes/%E5%B8%B8%E8%A7%81%E7%9A%84%E9%99%90%E6%B5%81%E7%AE%97%E6%B3%95/:4:0","tags":["rateLimiter"],"title":"常见的限流算法","uri":"/ooooo-notes/%E5%B8%B8%E8%A7%81%E7%9A%84%E9%99%90%E6%B5%81%E7%AE%97%E6%B3%95/"},{"categories":null,"content":" gossip 协议是为实现最终一致性提出的。 ","date":"2024-04-12","objectID":"/ooooo-notes/%E7%AE%80%E5%8D%95%E5%AE%9E%E7%8E%B0-gossip-%E5%8D%8F%E8%AE%AE/:0:0","tags":["gossip","protocol"],"title":"简单实现 gossip 协议","uri":"/ooooo-notes/%E7%AE%80%E5%8D%95%E5%AE%9E%E7%8E%B0-gossip-%E5%8D%8F%E8%AE%AE/"},{"categories":null,"content":"实现思路 每个节点都有基本属性，如 id, addr, port。 每个节点都有成员列表 members，存储一部分数据 data。 通过定时向随机节点发送请求，同步成员列表给随机节点，这样就能达到成员列表最终一致性。 客户端访问数据，先根据 key 来计算 hash 值，对成员列表取余，确定是哪个节点上，要么返回数据要么转发请求。 ","date":"2024-04-12","objectID":"/ooooo-notes/%E7%AE%80%E5%8D%95%E5%AE%9E%E7%8E%B0-gossip-%E5%8D%8F%E8%AE%AE/:1:0","tags":["gossip","protocol"],"title":"简单实现 gossip 协议","uri":"/ooooo-notes/%E7%AE%80%E5%8D%95%E5%AE%9E%E7%8E%B0-gossip-%E5%8D%8F%E8%AE%AE/"},{"categories":null,"content":"实现代码 每个节点的定义: type Node struct { Id ID Addr string Port int // 上次联系时间 lastContact time.Time // 所有的节点列表 members map[ID]*Node // 每个节点都会携带一部分数据 data map[string]string } 向随机节点发送同步请求： func (n *Node) sync() { // 随机选择一个节点 targetNode := n.selectRandomNode() if targetNode == nil { log.Printf(\"%s, not sync, targetNode is nil\", n) return } // 发送请求 err := n.call(targetNode, n.members) if err != nil { // 节点超时, 应该加入失败节点列表，然后广播所有节点判断是否应该剔除 log.Printf(\"self: %s call fail, targetNode is %v\", n, targetNode) return } log.Printf(\"self: %s sync success, targetNode is %v\", n, targetNode) targetNode.lastContact = time.Now() } 随机节点处理请求： // 模拟被调用方的逻辑 func (n *Node) call(targetNode *Node, members map[ID]*Node) error { // 更新成员 for id, m := range members { if _, ok := targetNode.members[id]; !ok { targetNode.members[id] = m } } // 更新时间 targetNode.members[n.Id] = n targetNode.members[n.Id].lastContact = time.Now() return nil } ","date":"2024-04-12","objectID":"/ooooo-notes/%E7%AE%80%E5%8D%95%E5%AE%9E%E7%8E%B0-gossip-%E5%8D%8F%E8%AE%AE/:2:0","tags":["gossip","protocol"],"title":"简单实现 gossip 协议","uri":"/ooooo-notes/%E7%AE%80%E5%8D%95%E5%AE%9E%E7%8E%B0-gossip-%E5%8D%8F%E8%AE%AE/"},{"categories":null,"content":"示例代码 demo-gossip-protocol ","date":"2024-04-12","objectID":"/ooooo-notes/%E7%AE%80%E5%8D%95%E5%AE%9E%E7%8E%B0-gossip-%E5%8D%8F%E8%AE%AE/:3:0","tags":["gossip","protocol"],"title":"简单实现 gossip 协议","uri":"/ooooo-notes/%E7%AE%80%E5%8D%95%E5%AE%9E%E7%8E%B0-gossip-%E5%8D%8F%E8%AE%AE/"},{"categories":null,"content":" redisson 基于 org.redisson:redisson-spring-data-27:3.27.2 版本 在 java 中，操作 redis 一般都会选择 redisson 框架, 我们需要了解常用功能的实现原理, 这次来介绍 RedissonSortedSet。 ","date":"2024-04-11","objectID":"/ooooo-notes/04-redissonsortedset/:0:0","tags":["redisson","source code","源码分析 redisson 系列"],"title":"04 RedissonSortedSet","uri":"/ooooo-notes/04-redissonsortedset/"},{"categories":null,"content":"使用方式 @Test void testOrderedSet() { RSortedSet\u003cString\u003e set = redissonClient.getSortedSet(\"set\"); set.clear(); set.add(\"3\"); set.add(\"2\"); set.add(\"1\"); for (String s : set.readAll()) { System.out.println(s); } } RedissonSortedSet 是通过 list 数据结构来实现的。但如果 value 是 string 类型的，可以使用 RedissonLexSortedSet 来优化操作。 ","date":"2024-04-11","objectID":"/ooooo-notes/04-redissonsortedset/:1:0","tags":["redisson","source code","源码分析 redisson 系列"],"title":"04 RedissonSortedSet","uri":"/ooooo-notes/04-redissonsortedset/"},{"categories":null,"content":"add 源码位置: org.redisson.RedissonSortedSet#add @Override public boolean add(V value) { // 加分布式锁，防止并发 lock.lock(); try { // 通过二分法查找插入位置 BinarySearchResult\u003cV\u003e res = binarySearch(value, codec); if (res.getIndex() \u003c 0) { int index = -(res.getIndex() + 1); // 编码 ByteBuf encodedValue = encode(value); // 执行 lua 脚本，插入元素 commandExecutor.get(commandExecutor.evalWriteNoRetryAsync(list.getRawName(), codec, RedisCommands.EVAL_VOID, \"local len = redis.call('llen', KEYS[1]);\" + \"if tonumber(ARGV[1]) \u003c len then \" // 获取插入位置的元素值 + \"local pivot = redis.call('lindex', KEYS[1], ARGV[1]);\" // 插入值 + \"redis.call('linsert', KEYS[1], 'before', pivot, ARGV[2]);\" + \"return;\" + \"end;\" + \"redis.call('rpush', KEYS[1], ARGV[2]);\", Arrays.\u003cObject\u003easList(list.getRawName()), index, encodedValue)); return true; } else { return false; } } finally { lock.unlock(); } } ","date":"2024-04-11","objectID":"/ooooo-notes/04-redissonsortedset/:2:0","tags":["redisson","source code","源码分析 redisson 系列"],"title":"04 RedissonSortedSet","uri":"/ooooo-notes/04-redissonsortedset/"},{"categories":null,"content":" redisson 基于 org.redisson:redisson-spring-data-27:3.27.2 版本 在 java 中，操作 redis 一般都会选择 redisson 框架, 我们需要了解常用功能的实现原理, 这次来介绍 RedissonPriorityQueue。 ","date":"2024-04-11","objectID":"/ooooo-notes/05-redissonpriorityqueue/:0:0","tags":["redisson","source code","源码分析 redisson 系列"],"title":"05 RedissonPriorityQueue","uri":"/ooooo-notes/05-redissonpriorityqueue/"},{"categories":null,"content":"使用方式 @Test void testPriorityQueue() { RPriorityQueue\u003cString\u003e queue = redissonClient.getPriorityQueue(\"queue\"); queue.clear(); queue.add(\"3\"); queue.add(\"2\"); queue.add(\"1\"); for (String s : queue.readAll()) { System.out.println(s); } } RedissonPriorityQueue 是通过 list 数据结构来实现的。 ","date":"2024-04-11","objectID":"/ooooo-notes/05-redissonpriorityqueue/:1:0","tags":["redisson","source code","源码分析 redisson 系列"],"title":"05 RedissonPriorityQueue","uri":"/ooooo-notes/05-redissonpriorityqueue/"},{"categories":null,"content":"add 源码位置: org.redisson.RedissonPriorityQueue#add @Override public boolean add(V value) { // 分布式锁，防止并发 lock.lock(); try { // 通过二分查找插入位置 BinarySearchResult\u003cV\u003e res = binarySearch(value); int index = 0; if (res.getIndex() \u003c 0) { index = -(res.getIndex() + 1); } else { index = res.getIndex() + 1; } get(commandExecutor.evalWriteNoRetryAsync(getRawName(), codec, RedisCommands.EVAL_VOID, \"local len = redis.call('llen', KEYS[1]);\" + \"if tonumber(ARGV[1]) \u003c len then \" // 获取插入位置的值 + \"local pivot = redis.call('lindex', KEYS[1], ARGV[1]);\" // 插入值 + \"redis.call('linsert', KEYS[1], 'before', pivot, ARGV[2]);\" + \"return;\" + \"end;\" + \"redis.call('rpush', KEYS[1], ARGV[2]);\", Arrays.asList(getRawName()), index, encode(value))); return true; } finally { lock.unlock(); } } RedissonPriorityQueue 和 RedissonSortedSet 的内部实现基本是一样的。 ","date":"2024-04-11","objectID":"/ooooo-notes/05-redissonpriorityqueue/:2:0","tags":["redisson","source code","源码分析 redisson 系列"],"title":"05 RedissonPriorityQueue","uri":"/ooooo-notes/05-redissonpriorityqueue/"},{"categories":null,"content":" redisson 基于 org.redisson:redisson-spring-data-27:3.27.2 版本 在 java 中，操作 redis 一般都会选择 redisson 框架, 我们需要了解常用功能的实现原理, 这次来介绍 RedissonMultiLock。 ","date":"2024-04-10","objectID":"/ooooo-notes/03-redissonmultilock/:0:0","tags":["redisson","source code","源码分析 redisson 系列"],"title":"03 RedissonMultiLock","uri":"/ooooo-notes/03-redissonmultilock/"},{"categories":null,"content":"使用方式 @Test void testMultiLock() { RLock lock1 = redissonClient.getLock(\"lock1\"); RLock lock2 = redissonClient.getLock(\"lock2\"); RLock lock = redissonClient.getMultiLock(lock1, lock2); try { lock.lock(); ThreadUtil.sleep(30, TimeUnit.SECONDS); System.out.println(\"xxx\"); } finally { lock.unlock(); } } 在实际使用过程中，可能一次性获取多个锁， 那么你应该使用 RedissonMultiLock 来简化你的操作。 ","date":"2024-04-10","objectID":"/ooooo-notes/03-redissonmultilock/:1:0","tags":["redisson","source code","源码分析 redisson 系列"],"title":"03 RedissonMultiLock","uri":"/ooooo-notes/03-redissonmultilock/"},{"categories":null,"content":"lock 源码位置: org.redisson.RedissonMultiLock#tryLock // lock 最终会调用 tryLock 方法 @Override public boolean tryLock(long waitTime, long leaseTime, TimeUnit unit) throws InterruptedException { ... int failedLocksLimit = failedLocksLimit(); List\u003cRLock\u003e acquiredLocks = new ArrayList\u003c\u003e(locks.size()); // 遍历获取每个锁 for (ListIterator\u003cRLock\u003e iterator = locks.listIterator(); iterator.hasNext();) { RLock lock = iterator.next(); boolean lockAcquired; try { if (waitTime \u003c= 0 \u0026\u0026 leaseTime \u003c= 0) { lockAcquired = lock.tryLock(); } else { long awaitTime = Math.min(lockWaitTime, remainTime); // 尝试获取锁 lockAcquired = lock.tryLock(awaitTime, newLeaseTime, TimeUnit.MILLISECONDS); } } catch (RedisResponseTimeoutException e) { unlockInner(Arrays.asList(lock)); lockAcquired = false; } catch (Exception e) { lockAcquired = false; } if (lockAcquired) { // 获取锁成功 acquiredLocks.add(lock); } else { // 判断容错次数, failedLocksLimit() 默认为 0 if (locks.size() - acquiredLocks.size() == failedLocksLimit()) { break; } // 获取锁失败 if (failedLocksLimit == 0) { // 释放之前的锁 unlockInner(acquiredLocks); if (waitTime \u003c= 0) { return false; } failedLocksLimit = failedLocksLimit(); acquiredLocks.clear(); // reset iterator while (iterator.hasPrevious()) { iterator.previous(); } } else { failedLocksLimit--; } } ... } return true; } ","date":"2024-04-10","objectID":"/ooooo-notes/03-redissonmultilock/:2:0","tags":["redisson","source code","源码分析 redisson 系列"],"title":"03 RedissonMultiLock","uri":"/ooooo-notes/03-redissonmultilock/"},{"categories":null,"content":"unlock 源码位置: org.redisson.RedissonMultiLock#unlock @Override public void unlock() { // 遍历每个锁，然后解锁 locks.forEach(Lock::unlock); } ","date":"2024-04-10","objectID":"/ooooo-notes/03-redissonmultilock/:3:0","tags":["redisson","source code","源码分析 redisson 系列"],"title":"03 RedissonMultiLock","uri":"/ooooo-notes/03-redissonmultilock/"},{"categories":null,"content":" redisson 基于 org.redisson:redisson-spring-data-27:3.27.2 版本 在 java 中，操作 redis 一般都会选择 redisson 框架, 我们需要了解常用功能的实现原理, 这次来介绍 RedissonSpinLock。 ","date":"2024-04-09","objectID":"/ooooo-notes/02-redissonspinlock/:0:0","tags":["redisson","source code","源码分析 redisson 系列"],"title":"02 RedissonSpinLock","uri":"/ooooo-notes/02-redissonspinlock/"},{"categories":null,"content":"使用方式 @Test void testSpinLock() { RLock lock = redissonClient.getSpinLock(\"lock\"); try { lock.lock(); ThreadUtil.sleep(30, TimeUnit.SECONDS); System.out.println(\"xxx\"); } finally { lock.unlock(); } } ","date":"2024-04-09","objectID":"/ooooo-notes/02-redissonspinlock/:1:0","tags":["redisson","source code","源码分析 redisson 系列"],"title":"02 RedissonSpinLock","uri":"/ooooo-notes/02-redissonspinlock/"},{"categories":null,"content":"lock 源码位置: org.redisson.RedissonSpinLock#lock @Override public void lock() { try { lockInterruptibly(-1, null); } catch (InterruptedException e) { throw new IllegalStateException(); } } @Override public void lockInterruptibly(long leaseTime, TimeUnit unit) throws InterruptedException { long threadId = Thread.currentThread().getId(); // 尝试获取锁, 和 RedissonLock 逻辑一样，不解析了 Long ttl = tryAcquire(leaseTime, unit, threadId); // lock acquired if (ttl == null) { return; } // 使用指数级睡眠策略 LockOptions.BackOffPolicy backOffPolicy = backOff.create(); while (ttl != null) { long nextSleepPeriod = backOffPolicy.getNextSleepPeriod(); // 睡眠一段时间 Thread.sleep(nextSleepPeriod); // 然后再尝试获取锁 ttl = tryAcquire(leaseTime, unit, threadId); } } ","date":"2024-04-09","objectID":"/ooooo-notes/02-redissonspinlock/:2:0","tags":["redisson","source code","源码分析 redisson 系列"],"title":"02 RedissonSpinLock","uri":"/ooooo-notes/02-redissonspinlock/"},{"categories":null,"content":"unlock RedissonSpinLock 和 RedissonLock 的解锁代码是一样的，所以就不解析了。 ","date":"2024-04-09","objectID":"/ooooo-notes/02-redissonspinlock/:3:0","tags":["redisson","source code","源码分析 redisson 系列"],"title":"02 RedissonSpinLock","uri":"/ooooo-notes/02-redissonspinlock/"},{"categories":null,"content":" redisson 基于 org.redisson:redisson-spring-data-27:3.27.2 版本 在 java 中，操作 redis 一般都会选择 redisson 框架, 我们需要了解常用功能的实现原理, 这次来介绍 RedissonLock。 ","date":"2024-04-08","objectID":"/ooooo-notes/01-redissonlock/:0:0","tags":["redisson","source code","源码分析 redisson 系列"],"title":"01 RedissonLock","uri":"/ooooo-notes/01-redissonlock/"},{"categories":null,"content":"使用方式 @Test void testDistributedLock() { RLock lock = redissonClient.getLock(\"lock\"); try { lock.lock(); ThreadUtil.sleep(30, TimeUnit.SECONDS); System.out.println(\"xxx\"); } finally { lock.unlock(); } } 上面是最常见分布式锁使用示例, redisson 的锁分为好几种，我们先以 RedissonLock 来说明。 分布式锁 ","date":"2024-04-08","objectID":"/ooooo-notes/01-redissonlock/:1:0","tags":["redisson","source code","源码分析 redisson 系列"],"title":"01 RedissonLock","uri":"/ooooo-notes/01-redissonlock/"},{"categories":null,"content":"lock 源码位置: org.redisson.RedissonLock#lock() // leaseTime：续约时间, 默认为 30 秒 // interruptibly: 支持可打断 private void lock(long leaseTime, TimeUnit unit, boolean interruptibly) throws InterruptedException { // 获取当前线程 id long threadId = Thread.currentThread().getId(); // 尝试获取锁 Long ttl = tryAcquire(-1, leaseTime, unit, threadId); // ttl 为 null，表示获取锁成功 if (ttl == null) { return; } // 订阅这个锁，一旦锁释放就会得到通知 CompletableFuture\u003cRedissonLockEntry\u003e future = subscribe(threadId); pubSub.timeout(future); RedissonLockEntry entry; if (interruptibly) { entry = commandExecutor.getInterrupted(future); } else { entry = commandExecutor.get(future); } try { while (true) { // 尝试获取锁 ttl = tryAcquire(-1, leaseTime, unit, threadId); // lock acquired if (ttl == null) { break; } // waiting for message if (ttl \u003e= 0) { try { // 等待锁释放 entry.getLatch().tryAcquire(ttl, TimeUnit.MILLISECONDS); } catch (InterruptedException e) { if (interruptibly) { throw e; } entry.getLatch().tryAcquire(ttl, TimeUnit.MILLISECONDS); } } else { if (interruptibly) { entry.getLatch().acquire(); } else { entry.getLatch().acquireUninterruptibly(); } } } } finally { // 取消订阅 unsubscribe(entry, threadId); } } 上面的逻辑可以结合图来理解。 源码位置: org.redisson.RedissonLock#tryAcquireAsync // leaseTime 为 -1，表示会一直持有锁，除非调用 unlock 解锁 private RFuture\u003cLong\u003e tryAcquireAsync(long waitTime, long leaseTime, TimeUnit unit, long threadId) { RFuture\u003cLong\u003e ttlRemainingFuture; if (leaseTime \u003e 0) { // 执行 lua 脚本进行加锁 ttlRemainingFuture = tryLockInnerAsync(waitTime, leaseTime, unit, threadId, RedisCommands.EVAL_LONG); } else { // 执行 lua 脚本进行加锁 ttlRemainingFuture = tryLockInnerAsync(waitTime, internalLockLeaseTime, TimeUnit.MILLISECONDS, threadId, RedisCommands.EVAL_LONG); } // 处理异常情况，如果加锁失败，就释放锁 CompletionStage\u003cLong\u003e s = handleNoSync(threadId, ttlRemainingFuture); ttlRemainingFuture = new CompletableFutureWrapper\u003c\u003e(s); CompletionStage\u003cLong\u003e f = ttlRemainingFuture.thenApply(ttlRemaining -\u003e { // ttlRemaining 为 null，加锁成功 if (ttlRemaining == null) { if (leaseTime \u003e 0) { internalLockLeaseTime = unit.toMillis(leaseTime); } else { // 定期续约锁 scheduleExpirationRenewal(threadId); } } return ttlRemaining; }); return new CompletableFutureWrapper\u003c\u003e(f); } 源码位置: org.redisson.RedissonLock#tryLockInnerAsync // 执行 lua 脚本进行加锁 \u003cT\u003e RFuture\u003cT\u003e tryLockInnerAsync(long waitTime, long leaseTime, TimeUnit unit, long threadId, RedisStrictCommand\u003cT\u003e command) { return evalWriteSyncedAsync(getRawName(), LongCodec.INSTANCE, command, // 不存在锁 \"if ((redis.call('exists', KEYS[1]) == 0) \" + // 可重入锁 \"or (redis.call('hexists', KEYS[1], ARGV[2]) == 1)) then \" + // 加一 \"redis.call('hincrby', KEYS[1], ARGV[2], 1); \" + // 设置过期时间 \"redis.call('pexpire', KEYS[1], ARGV[1]); \" + // 返回 null，表示加锁成功 \"return nil; \" + \"end; \" + // 获取锁过期时间 \"return redis.call('pttl', KEYS[1]);\", Collections.singletonList(getRawName()), unit.toMillis(leaseTime), getLockName(threadId)); } ","date":"2024-04-08","objectID":"/ooooo-notes/01-redissonlock/:2:0","tags":["redisson","source code","源码分析 redisson 系列"],"title":"01 RedissonLock","uri":"/ooooo-notes/01-redissonlock/"},{"categories":null,"content":"unlock 源码位置: org.redisson.RedissonBaseLock#unlock public void unlock() { try { // 获取当前线程 id, 然后解锁, 最终调用 unlockInnerAsync get(unlockAsync(Thread.currentThread().getId())); } catch (RedisException e) { if (e.getCause() instanceof IllegalMonitorStateException) { throw (IllegalMonitorStateException) e.getCause(); } else { throw e; } } } 源码位置: org.redisson.RedissonBaseLock#unlockInnerAsync(long) protected final RFuture\u003cBoolean\u003e unlockInnerAsync(long threadId) { String id = getServiceManager().generateId(); MasterSlaveServersConfig config = getServiceManager().getConfig(); int timeout = (config.getTimeout() + config.getRetryInterval()) * config.getRetryAttempts(); timeout = Math.max(timeout, 1); // 执行 lua 脚本进行解锁 RFuture\u003cBoolean\u003e r = unlockInnerAsync(threadId, id, timeout); CompletionStage\u003cBoolean\u003e ff = r.thenApply(v -\u003e { CommandAsyncExecutor ce = commandExecutor; if (ce instanceof CommandBatchService) { ce = new CommandBatchService(commandExecutor); } // 删除标记 ce.writeAsync(getRawName(), LongCodec.INSTANCE, RedisCommands.DEL, getUnlockLatchName(id)); if (ce instanceof CommandBatchService) { ((CommandBatchService) ce).executeAsync(); } return v; }); return new CompletableFutureWrapper\u003c\u003e(ff); } 源码位置: org.redisson.RedissonLock#unlockInnerAsync protected RFuture\u003cBoolean\u003e unlockInnerAsync(long threadId, String requestId, int timeout) { return evalWriteSyncedAsync(getRawName(), LongCodec.INSTANCE, RedisCommands.EVAL_BOOLEAN, // 判断是否已经解锁过 \"local val = redis.call('get', KEYS[3]); \" + \"if val ~= false then \" + \"return tonumber(val);\" + \"end; \" + // 锁已经不存在，无需解锁 \"if (redis.call('hexists', KEYS[1], ARGV[3]) == 0) then \" + \"return nil;\" + \"end; \" + // 处理可重入锁 \"local counter = redis.call('hincrby', KEYS[1], ARGV[3], -1); \" + \"if (counter \u003e 0) then \" + \"redis.call('pexpire', KEYS[1], ARGV[2]); \" + \"redis.call('set', KEYS[3], 0, 'px', ARGV[5]); \" + \"return 0; \" + \"else \" + // 解锁 \"redis.call('del', KEYS[1]); \" + // 发布解锁消息 \"redis.call(ARGV[4], KEYS[2], ARGV[1]); \" + // 标记已经解锁 \"redis.call('set', KEYS[3], 1, 'px', ARGV[5]); \" + \"return 1; \" + \"end; \", Arrays.asList(getRawName(), getChannelName(), getUnlockLatchName(requestId)), LockPubSub.UNLOCK_MESSAGE, internalLockLeaseTime, getLockName(threadId), getSubscribeService().getPublishCommand(), timeout); } ","date":"2024-04-08","objectID":"/ooooo-notes/01-redissonlock/:3:0","tags":["redisson","source code","源码分析 redisson 系列"],"title":"01 RedissonLock","uri":"/ooooo-notes/01-redissonlock/"},{"categories":null,"content":" jdk 基于 8 版本 在平时的开发中，我们很少会用到 TreeMap, 但是还是需要了解源码。 TreeMap 基于红黑树来实现按照 key 排序，关于这个算法，这里不做解释。 ","date":"2024-04-03","objectID":"/ooooo-notes/05-treemap/:0:0","tags":["jdk","source code","源码分析 jdk 系列"],"title":"05 TreeMap","uri":"/ooooo-notes/05-treemap/"},{"categories":null,"content":"使用方式 public class TreeMapTest { @Test void test() { Map\u003cString, String\u003e map = new TreeMap\u003c\u003e(); map.put(\"1\", \"a\"); map.put(\"2\", \"b\"); assertThat(map.remove(\"1\")).isEqualTo(\"a\"); assertThat(map.put(\"2\", \"c\")).isEqualTo(\"b\"); assertThat(map.get(\"2\")).isEqualTo(\"c\"); } } 因为是 Map 接口的实现类 ，所以使用方式是差不多的。只不过在遍历过程中，是按照 key 值排序的。 ","date":"2024-04-03","objectID":"/ooooo-notes/05-treemap/:1:0","tags":["jdk","source code","源码分析 jdk 系列"],"title":"05 TreeMap","uri":"/ooooo-notes/05-treemap/"},{"categories":null,"content":"put 源码位置: java.util.TreeMap#put public V put(K key, V value) { Entry\u003cK,V\u003e t = root; if (t == null) { compare(key, key); // type (and possibly null) check root = new Entry\u003c\u003e(key, value, null); size = 1; modCount++; return null; } int cmp; Entry\u003cK,V\u003e parent; // split comparator and comparable paths // 通过 Comparator 来比较 key 值 Comparator\u003c? super K\u003e cpr = comparator; if (cpr != null) { // 下面是排序二叉树的标准代码，不解释 do { parent = t; cmp = cpr.compare(key, t.key); if (cmp \u003c 0) t = t.left; else if (cmp \u003e 0) t = t.right; else return t.setValue(value); } while (t != null); } else { // 通过 Comparable 来比较 key 值 if (key == null) throw new NullPointerException(); @SuppressWarnings(\"unchecked\") Comparable\u003c? super K\u003e k = (Comparable\u003c? super K\u003e) key; // 下面是排序二叉树的标准代码，不解释 do { parent = t; cmp = k.compareTo(t.key); if (cmp \u003c 0) t = t.left; else if (cmp \u003e 0) t = t.right; else return t.setValue(value); } while (t != null); } Entry\u003cK,V\u003e e = new Entry\u003c\u003e(key, value, parent); if (cmp \u003c 0) parent.left = e; else parent.right = e; // 红黑树插入操作 fixAfterInsertion(e); size++; modCount++; return null; } ","date":"2024-04-03","objectID":"/ooooo-notes/05-treemap/:2:0","tags":["jdk","source code","源码分析 jdk 系列"],"title":"05 TreeMap","uri":"/ooooo-notes/05-treemap/"},{"categories":null,"content":"remove 源码位置: java.util.TreeMap#remove public V remove(Object key) { // 按照排序二叉树的方式来查找 key 值 Entry\u003cK,V\u003e p = getEntry(key); if (p == null) return null; V oldValue = p.value; // 红黑树删除操作 deleteEntry(p); return oldValue; } ","date":"2024-04-03","objectID":"/ooooo-notes/05-treemap/:3:0","tags":["jdk","source code","源码分析 jdk 系列"],"title":"05 TreeMap","uri":"/ooooo-notes/05-treemap/"},{"categories":null,"content":"get 源码位置: java.util.TreeMap#get public V get(Object key) { // 按照排序二叉树的方式来查找 key 值 Entry\u003cK,V\u003e p = getEntry(key); return (p==null ? null : p.value); } ","date":"2024-04-03","objectID":"/ooooo-notes/05-treemap/:4:0","tags":["jdk","source code","源码分析 jdk 系列"],"title":"05 TreeMap","uri":"/ooooo-notes/05-treemap/"},{"categories":null,"content":" jdk 基于 8 版本 在平时的开发中，我们会经常用到 LinkedHashMap, 非常有必要了解源码。 LinkedHashMap 基于 HashMap 来实现, 内部借助双向链表来维持访问顺序，可以用来实现 LRU 算法。 ","date":"2024-04-02","objectID":"/ooooo-notes/04-linkedhashmap/:0:0","tags":["jdk","source code","源码分析 jdk 系列"],"title":"04 LinkedHashMap","uri":"/ooooo-notes/04-linkedhashmap/"},{"categories":null,"content":"使用方式 public class LinkedHashMapTest { @Test void test() { Map\u003cString, String\u003e map = new LinkedHashMap\u003c\u003e(); map.put(\"1\", \"a\"); map.put(\"2\", \"b\"); assertThat(map.remove(\"1\")).isEqualTo(\"a\"); assertThat(map.put(\"2\", \"c\")).isEqualTo(\"b\"); assertThat(map.get(\"2\")).isEqualTo(\"c\"); } } 对于使用方式来说，LinkedHashMap 和 HashMap 是一样的，只不过 LinkedHashMap 在遍历过程中是有序的, 实现原理是在添加元素时，需要把元素移动到双向链表的尾部，然后遍历时直接取双向链表。 ","date":"2024-04-02","objectID":"/ooooo-notes/04-linkedhashmap/:1:0","tags":["jdk","source code","源码分析 jdk 系列"],"title":"04 LinkedHashMap","uri":"/ooooo-notes/04-linkedhashmap/"},{"categories":null,"content":"put 对于 put 方法，LinkedHashMap 实际上只实现了 newNode, afterNodeAccess, afterNodeInsertion。 源码位置: java.util.LinkedHashMap#newNode Node\u003cK,V\u003e newNode(int hash, K key, V value, Node\u003cK,V\u003e e) { LinkedHashMap.Entry\u003cK,V\u003e p = new LinkedHashMap.Entry\u003cK,V\u003e(hash, key, value, e); // 将新节点移动到 tail 位置，tail 节点是最新的节点 linkNodeLast(p); return p; } 源码位置: java.util.LinkedHashMap#afterNodeAccess // 把 e 节点移动到 tail 位置，tail 节点是最新的节点 void afterNodeAccess(Node\u003cK,V\u003e e) { // move node to last LinkedHashMap.Entry\u003cK,V\u003e last; // accessOrder 默认为 false if (accessOrder \u0026\u0026 (last = tail) != e) { // 获取 p 节点，b 前驱节点，a 后继结点 LinkedHashMap.Entry\u003cK,V\u003e p = (LinkedHashMap.Entry\u003cK,V\u003e)e, b = p.before, a = p.after; p.after = null; if (b == null) head = a; else b.after = a; if (a != null) a.before = b; else last = b; if (last == null) head = p; else { p.before = last; last.after = p; } tail = p; ++modCount; } } 源码位置: java.util.LinkedHashMap#afterNodeInsertion // 判断是否要删除 head 节点, head 节点是最老的节点 void afterNodeInsertion(boolean evict) { // possibly remove eldest LinkedHashMap.Entry\u003cK,V\u003e first; // removeEldestEntry 由子类或者匿名内部类实现 if (evict \u0026\u0026 (first = head) != null \u0026\u0026 removeEldestEntry(first)) { K key = first.key; // 删除指定节点 removeNode(hash(key), key, null, false, true); } } ","date":"2024-04-02","objectID":"/ooooo-notes/04-linkedhashmap/:2:0","tags":["jdk","source code","源码分析 jdk 系列"],"title":"04 LinkedHashMap","uri":"/ooooo-notes/04-linkedhashmap/"},{"categories":null,"content":"remove 对于 remove 方法，LinkedHashMap 实际上只实现了 afterNodeRemoval。 源码位置: java.util.LinkedHashMap#afterNodeRemoval // 删除 e 节点 void afterNodeRemoval(Node\u003cK,V\u003e e) { // unlink LinkedHashMap.Entry\u003cK,V\u003e p = (LinkedHashMap.Entry\u003cK,V\u003e)e, b = p.before, a = p.after; p.before = p.after = null; if (b == null) head = a; else b.after = a; if (a == null) tail = b; else a.before = b; } ","date":"2024-04-02","objectID":"/ooooo-notes/04-linkedhashmap/:3:0","tags":["jdk","source code","源码分析 jdk 系列"],"title":"04 LinkedHashMap","uri":"/ooooo-notes/04-linkedhashmap/"},{"categories":null,"content":"get 对于 get 方法，LinkedHashMap 实际上只实现了 afterNodeAccess。 源码位置: java.util.LinkedHashMap#get public V get(Object key) { Node\u003cK,V\u003e e; if ((e = getNode(hash(key), key)) == null) return null; // 如果按照访问顺序排序，则需要把刚刚的节点移动到 tail 位置，tail 节点是最新的节点 if (accessOrder) afterNodeAccess(e); return e.value; } ","date":"2024-04-02","objectID":"/ooooo-notes/04-linkedhashmap/:4:0","tags":["jdk","source code","源码分析 jdk 系列"],"title":"04 LinkedHashMap","uri":"/ooooo-notes/04-linkedhashmap/"},{"categories":null,"content":" jdk 基于 8 版本 在平时的开发中，我们会经常用到 HashMap, 非常有必要了解源码。 HashMap 基于拉链法和红黑树来实现，关于这两个算法，这里不做解释。 ","date":"2024-04-01","objectID":"/ooooo-notes/03-hashmap/:0:0","tags":["jdk","source code","源码分析 jdk 系列"],"title":"03 HashMap","uri":"/ooooo-notes/03-hashmap/"},{"categories":null,"content":"使用方式 public class HashMapTest { @Test void test() { Map\u003cString, String\u003e map = new HashMap\u003c\u003e(); map.put(\"1\", \"a\"); map.put(\"2\", \"b\"); assertThat(map.remove(\"1\")).isEqualTo(\"a\"); assertThat(map.put(\"2\", \"c\")).isEqualTo(\"b\"); assertThat(map.get(\"2\")).isEqualTo(\"c\"); } } ","date":"2024-04-01","objectID":"/ooooo-notes/03-hashmap/:1:0","tags":["jdk","source code","源码分析 jdk 系列"],"title":"03 HashMap","uri":"/ooooo-notes/03-hashmap/"},{"categories":null,"content":"put 源码位置: java.util.HashMap#put 在 HashMap 中，先利用拉链法来添加节点(这里是尾插法)，当链表长度大于 8 了，就会将链表转为红黑树。 public V put(K key, V value) { // 先计算 key 的 hash 值，然后再 putVal return putVal(hash(key), key, value, false, true); } // 调用 hashCode static final int hash(Object key) { int h; return (key == null) ? 0 : (h = key.hashCode()) ^ (h \u003e\u003e\u003e 16); } 源码位置: java.util.HashMap#putVal final V putVal(int hash, K key, V value, boolean onlyIfAbsent, boolean evict) { Node\u003cK,V\u003e[] tab; Node\u003cK,V\u003e p; int n, i; // 计算 tab 的长度，默认为 16 if ((tab = table) == null || (n = tab.length) == 0) n = (tab = resize()).length; if ((p = tab[i = (n - 1) \u0026 hash]) == null) // 说明当前桶没有节点，需要创建新节点 tab[i] = newNode(hash, key, value, null); else { // 说明当前桶有节点, p 表示桶里第一个节点 Node\u003cK,V\u003e e; K k; if (p.hash == hash \u0026\u0026 ((k = p.key) == key || (key != null \u0026\u0026 key.equals(k)))) // 找到节点了 e = p; else if (p instanceof TreeNode) // 添加到红黑树中 e = ((TreeNode\u003cK,V\u003e)p).putTreeVal(this, tab, hash, key, value); else { // 遍历链表 for (int binCount = 0; ; ++binCount) { if ((e = p.next) == null) { p.next = newNode(hash, key, value, null); // 链表长度达到8，会把链表转为红黑树 if (binCount \u003e= TREEIFY_THRESHOLD - 1) // -1 for 1st treeifyBin(tab, hash); break; } // 判断当前节点 if (e.hash == hash \u0026\u0026 ((k = e.key) == key || (key != null \u0026\u0026 key.equals(k)))) break; // 进行下一次循环 p = e; } } // e 表示旧值 if (e != null) { // existing mapping for key V oldValue = e.value; if (!onlyIfAbsent || oldValue == null) e.value = value; // LinkedHashMap 会使用这个方法 afterNodeAccess(e); return oldValue; } } // 标记修改 ++modCount; // 动态扩容 if (++size \u003e threshold) resize(); // LinkedHashMap 会使用这个方法 afterNodeInsertion(evict); return null; } ","date":"2024-04-01","objectID":"/ooooo-notes/03-hashmap/:2:0","tags":["jdk","source code","源码分析 jdk 系列"],"title":"03 HashMap","uri":"/ooooo-notes/03-hashmap/"},{"categories":null,"content":"remove remove 和 put 非常类似，都需要找到对应的节点 源码位置: java.util.HashMap#remove(java.lang.Object) public V remove(Object key) { Node\u003cK,V\u003e e; // 先计算 key 的 hash 值，然后 removeNode return (e = removeNode(hash(key), key, null, false, true)) == null ? null : e.value; } 源码位置: java.util.HashMap#removeNode final Node\u003cK,V\u003e removeNode(int hash, Object key, Object value, boolean matchValue, boolean movable) { Node\u003cK,V\u003e[] tab; Node\u003cK,V\u003e p; int n, index; // 判断桶里第一个节点是否存在, p 表示桶里第一个节点 if ((tab = table) != null \u0026\u0026 (n = tab.length) \u003e 0 \u0026\u0026 (p = tab[index = (n - 1) \u0026 hash]) != null) { Node\u003cK,V\u003e node = null, e; K k; V v; if (p.hash == hash \u0026\u0026 ((k = p.key) == key || (key != null \u0026\u0026 key.equals(k)))) // 找到了节点 node = p; else if ((e = p.next) != null) { if (p instanceof TreeNode) // 从红黑色中获取节点 node = ((TreeNode\u003cK,V\u003e)p).getTreeNode(hash, key); else { // 遍历链表获取节点 do { if (e.hash == hash \u0026\u0026 ((k = e.key) == key || (key != null \u0026\u0026 key.equals(k)))) { node = e; break; } p = e; } while ((e = e.next) != null); } } // node 表示找到的节点 if (node != null \u0026\u0026 (!matchValue || (v = node.value) == value || (value != null \u0026\u0026 value.equals(v)))) { if (node instanceof TreeNode) // 删除红黑树节点 ((TreeNode\u003cK,V\u003e)node).removeTreeNode(this, tab, movable); else if (node == p) // 删除头结点 tab[index] = node.next; else // 删除链表节点 p.next = node.next; // 标记修改 ++modCount; --size; // LinkedHashMap 会使用这个方法 afterNodeRemoval(node); return node; } } return null; } ","date":"2024-04-01","objectID":"/ooooo-notes/03-hashmap/:3:0","tags":["jdk","source code","源码分析 jdk 系列"],"title":"03 HashMap","uri":"/ooooo-notes/03-hashmap/"},{"categories":null,"content":"get get 和 put 非常类似，都需要找到对应的节点 源码位置: java.util.HashMap#get public V get(Object key) { Node\u003cK,V\u003e e; return (e = getNode(hash(key), key)) == null ? null : e.value; } final Node\u003cK,V\u003e getNode(int hash, Object key) { Node\u003cK,V\u003e[] tab; Node\u003cK,V\u003e first, e; int n; K k; // 判断桶里第一个节点是否存在，first 表示第一个节点 if ((tab = table) != null \u0026\u0026 (n = tab.length) \u003e 0 \u0026\u0026 (first = tab[(n - 1) \u0026 hash]) != null) { if (first.hash == hash \u0026\u0026 // always check first node ((k = first.key) == key || (key != null \u0026\u0026 key.equals(k)))) // 找到节点了 return first; if ((e = first.next) != null) { if (first instanceof TreeNode) // 从红黑树中获取节点 return ((TreeNode\u003cK,V\u003e)first).getTreeNode(hash, key); // 遍历链表获取节点 do { if (e.hash == hash \u0026\u0026 ((k = e.key) == key || (key != null \u0026\u0026 key.equals(k)))) return e; } while ((e = e.next) != null); } } return null; } ","date":"2024-04-01","objectID":"/ooooo-notes/03-hashmap/:4:0","tags":["jdk","source code","源码分析 jdk 系列"],"title":"03 HashMap","uri":"/ooooo-notes/03-hashmap/"},{"categories":null,"content":" java 的线程池可以充当一个任务执行器的，但是有时候不符合我们的要求，所以需要自定义开发。 满足1：可以根据任务数量来动态调整核心线程数和最大线程数。 满足2：支持重复执行的任务。 ","date":"2024-03-24","objectID":"/ooooo-notes/%E6%97%B6%E9%97%B4%E8%BD%AE%E5%92%8C%E7%BA%BF%E7%A8%8B%E6%B1%A0%E5%AE%9E%E7%8E%B0%E4%BB%BB%E5%8A%A1%E6%89%A7%E8%A1%8C%E5%99%A8/:0:0","tags":["java"],"title":"时间轮和线程池实现任务执行器","uri":"/ooooo-notes/%E6%97%B6%E9%97%B4%E8%BD%AE%E5%92%8C%E7%BA%BF%E7%A8%8B%E6%B1%A0%E5%AE%9E%E7%8E%B0%E4%BB%BB%E5%8A%A1%E6%89%A7%E8%A1%8C%E5%99%A8/"},{"categories":null,"content":"RepeatTask public class RepeatTask extends AbstractTask { private final long maxDelay; public RepeatTask(Runnable runnable, long maxDelay, TimeUnit timeUnit) { super(runnable); this.maxDelay = timeUnit.toMillis(maxDelay); } @Override public void run() { long prevTime = System.currentTimeMillis(); try { super.run(); } finally { long diff = System.currentTimeMillis() - prevTime; diff = Long.max(maxDelay - diff, 0); taskExecutor.schedule(this, diff, TimeUnit.MILLISECONDS); } } } 通过计算执行的时间来判断下一次的延时时间，从而实现重复执行。 ","date":"2024-03-24","objectID":"/ooooo-notes/%E6%97%B6%E9%97%B4%E8%BD%AE%E5%92%8C%E7%BA%BF%E7%A8%8B%E6%B1%A0%E5%AE%9E%E7%8E%B0%E4%BB%BB%E5%8A%A1%E6%89%A7%E8%A1%8C%E5%99%A8/:1:0","tags":["java"],"title":"时间轮和线程池实现任务执行器","uri":"/ooooo-notes/%E6%97%B6%E9%97%B4%E8%BD%AE%E5%92%8C%E7%BA%BF%E7%A8%8B%E6%B1%A0%E5%AE%9E%E7%8E%B0%E4%BB%BB%E5%8A%A1%E6%89%A7%E8%A1%8C%E5%99%A8/"},{"categories":null,"content":"DefaultTaskExecutor private final HashedWheelTimer timer; private final ThreadPoolExecutor threadPoolExecutor; private final TaskExecutorPoolSizeAdjuster poolSizeAdjuster; public DefaultTaskExecutor(ThreadPoolExecutor threadPoolExecutor, TaskExecutorPoolSizeAdjuster poolSizeAdjuster) { assert threadPoolExecutor != null; this.timer = new HashedWheelTimer(new DefaultThreadFactory(\"TaskExecutor-Timer\"), 10, TimeUnit.MILLISECONDS, 100, true, -1, threadPoolExecutor); this.threadPoolExecutor = threadPoolExecutor; this.poolSizeAdjuster = poolSizeAdjuster; init(); } private void init() { if (poolSizeAdjuster != null) { submit(new RepeatTask(() -\u003e { int maxPoolSize = poolSizeAdjuster.calctMaximumPoolSize(); threadPoolExecutor.setMaximumPoolSize(maxPoolSize); int corePoolSize = poolSizeAdjuster.calcCorePoolSize(); threadPoolExecutor.setCorePoolSize(corePoolSize); }, 10, TimeUnit.SECONDS)); } } 使用 HashedWheelTimer 来调度延时任务。 在构造方法中传入 TaskExecutorPoolSizeAdjuster 来动态调整线程数。 ","date":"2024-03-24","objectID":"/ooooo-notes/%E6%97%B6%E9%97%B4%E8%BD%AE%E5%92%8C%E7%BA%BF%E7%A8%8B%E6%B1%A0%E5%AE%9E%E7%8E%B0%E4%BB%BB%E5%8A%A1%E6%89%A7%E8%A1%8C%E5%99%A8/:2:0","tags":["java"],"title":"时间轮和线程池实现任务执行器","uri":"/ooooo-notes/%E6%97%B6%E9%97%B4%E8%BD%AE%E5%92%8C%E7%BA%BF%E7%A8%8B%E6%B1%A0%E5%AE%9E%E7%8E%B0%E4%BB%BB%E5%8A%A1%E6%89%A7%E8%A1%8C%E5%99%A8/"},{"categories":null,"content":"示例代码 demo-task-executor ","date":"2024-03-24","objectID":"/ooooo-notes/%E6%97%B6%E9%97%B4%E8%BD%AE%E5%92%8C%E7%BA%BF%E7%A8%8B%E6%B1%A0%E5%AE%9E%E7%8E%B0%E4%BB%BB%E5%8A%A1%E6%89%A7%E8%A1%8C%E5%99%A8/:3:0","tags":["java"],"title":"时间轮和线程池实现任务执行器","uri":"/ooooo-notes/%E6%97%B6%E9%97%B4%E8%BD%AE%E5%92%8C%E7%BA%BF%E7%A8%8B%E6%B1%A0%E5%AE%9E%E7%8E%B0%E4%BB%BB%E5%8A%A1%E6%89%A7%E8%A1%8C%E5%99%A8/"},{"categories":null,"content":" 在 java 中有四种引用类型，分为强引用，软引用，弱引用，虚引用，这里介绍如何使用软引用来实现一个缓存。 ","date":"2024-03-20","objectID":"/ooooo-notes/%E5%AE%9E%E7%8E%B0%E4%B8%80%E4%B8%AA%E8%BD%AF%E5%BC%95%E7%94%A8%E7%BC%93%E5%AD%98/:0:0","tags":["java"],"title":"实现一个软引用缓存","uri":"/ooooo-notes/%E5%AE%9E%E7%8E%B0%E4%B8%80%E4%B8%AA%E8%BD%AF%E5%BC%95%E7%94%A8%E7%BC%93%E5%AD%98/"},{"categories":null,"content":"实现代码 public class SoftReferenceCache\u003cK, V\u003e implements Cache\u003cK, V\u003e { private Map\u003cK, SoftValue\u003cV\u003e\u003e map; private ReferenceQueue\u003cV\u003e referenceQueue; public SoftReferenceCache() { this.map = new HashMap\u003c\u003e(); this.referenceQueue = new ReferenceQueue\u003c\u003e(); } @Override public void put(K key, V value) { removeSoftValue(); this.map.put(key, new SoftValue\u003c\u003e(key, value, referenceQueue)); } @Override public V get(K key) { removeSoftValue(); SoftValue\u003cV\u003e softValue = this.map.get(key); return softValue.getValue(); } // 这里没有使用额外的线程来定时执行方法 protected void removeSoftValue() { while (true) { SoftValue\u003cV\u003e softValue = (SoftValue\u003cV\u003e) referenceQueue.poll(); if (softValue == null) { break; } System.out.println(\"remove unnecessary softValue: \" + softValue); map.remove(softValue.getKey()); } } private class SoftValue\u003cV\u003e extends SoftReference\u003cV\u003e { // 从引用队列中获取此对象，就能知道是哪个key和value要回收了。 private K key; public SoftValue(K key, V value, ReferenceQueue\u003cV\u003e referenceQueue) { super(value, referenceQueue); this.key = key; } public K getKey() { return key; } public V getValue() { return super.get(); } @Override public String toString() { return \"SoftValue{\" + \"key=\" + key + '}'; } } } ","date":"2024-03-20","objectID":"/ooooo-notes/%E5%AE%9E%E7%8E%B0%E4%B8%80%E4%B8%AA%E8%BD%AF%E5%BC%95%E7%94%A8%E7%BC%93%E5%AD%98/:1:0","tags":["java"],"title":"实现一个软引用缓存","uri":"/ooooo-notes/%E5%AE%9E%E7%8E%B0%E4%B8%80%E4%B8%AA%E8%BD%AF%E5%BC%95%E7%94%A8%E7%BC%93%E5%AD%98/"},{"categories":null,"content":"测试代码 注意: 我在 build.gradle 文件中添加了 test 的 jvm 参数 jvmArgs = [\"-Xmx10m\", “-Xms10m”], 来模拟内存不足来触发回收软引用。 @Test void testSoftReferenceCache() { Cache\u003cString, String\u003e cache = new SoftReferenceCache\u003c\u003e(); for (int i = 0; i \u003c 1_000_000; i++) { System.gc(); cache.put(\"key\" + i, \"value\" + i); } for (int i = 0; i \u003c 10; i++) { cache.get(\"key\" + i); } } ","date":"2024-03-20","objectID":"/ooooo-notes/%E5%AE%9E%E7%8E%B0%E4%B8%80%E4%B8%AA%E8%BD%AF%E5%BC%95%E7%94%A8%E7%BC%93%E5%AD%98/:2:0","tags":["java"],"title":"实现一个软引用缓存","uri":"/ooooo-notes/%E5%AE%9E%E7%8E%B0%E4%B8%80%E4%B8%AA%E8%BD%AF%E5%BC%95%E7%94%A8%E7%BC%93%E5%AD%98/"},{"categories":null,"content":"示例代码 demo-java-soft-reference ","date":"2024-03-20","objectID":"/ooooo-notes/%E5%AE%9E%E7%8E%B0%E4%B8%80%E4%B8%AA%E8%BD%AF%E5%BC%95%E7%94%A8%E7%BC%93%E5%AD%98/:3:0","tags":["java"],"title":"实现一个软引用缓存","uri":"/ooooo-notes/%E5%AE%9E%E7%8E%B0%E4%B8%80%E4%B8%AA%E8%BD%AF%E5%BC%95%E7%94%A8%E7%BC%93%E5%AD%98/"},{"categories":null,"content":" jdk 基于 8 版本 在平时的开发中，我们会经常用到 ArrayList, 非常有必要了解源码。 ","date":"2024-03-18","objectID":"/ooooo-notes/01-arraylist/:0:0","tags":["jdk","source code","源码分析 jdk 系列"],"title":"01 ArrayList","uri":"/ooooo-notes/01-arraylist/"},{"categories":null,"content":"使用方式 public class ArrayListTest { @Test void test() { List\u003cString\u003e ids = new ArrayList\u003c\u003e(); assertThat(ids.add(\"1\")).isEqualTo(true); assertThat(ids.add(\"2\")).isEqualTo(true); assertThat(ids.add(\"3\")).isEqualTo(true); assertThat(ids.remove(\"2\")).isEqualTo(true); assertThat(ids.set(0, \"4\")).isEqualTo(\"1\"); assertThat(ids.get(0)).isEqualTo(\"4\"); } } ","date":"2024-03-18","objectID":"/ooooo-notes/01-arraylist/:1:0","tags":["jdk","source code","源码分析 jdk 系列"],"title":"01 ArrayList","uri":"/ooooo-notes/01-arraylist/"},{"categories":null,"content":"add 添加元素到 ArrayList 中，如果空间不够，则触发 newCapacity = oldCapacity + (oldCapacity \u003e\u003e 1)。 源码位置: java.util.ArrayList#add public boolean add(E e) { // 确保有足够的空间 ensureCapacityInternal(size + 1); // Increments modCount!! // 存入元素 elementData[size++] = e; return true; } 源码位置: java.util.ArrayList#ensureCapacityInternal private void ensureCapacityInternal(int minCapacity) { // 先计算容量，扩展容量 ensureExplicitCapacity(calculateCapacity(elementData, minCapacity)); } // 计算容量，最小为10 (DEFAULT_CAPACITY = 10) private static int calculateCapacity(Object[] elementData, int minCapacity) { if (elementData == DEFAULTCAPACITY_EMPTY_ELEMENTDATA) { return Math.max(DEFAULT_CAPACITY, minCapacity); } return minCapacity; } 源码位置: java.util.ArrayList#ensureExplicitCapacity // 确保容量足够 private void ensureExplicitCapacity(int minCapacity) { modCount++; // overflow-conscious code if (minCapacity - elementData.length \u003e 0) // 空间不够，需要扩容 grow(minCapacity); } private void grow(int minCapacity) { // overflow-conscious code int oldCapacity = elementData.length; int newCapacity = oldCapacity + (oldCapacity \u003e\u003e 1); // 处理溢出情况 if (newCapacity - minCapacity \u003c 0) newCapacity = minCapacity; // 处理最大情况 if (newCapacity - MAX_ARRAY_SIZE \u003e 0) newCapacity = hugeCapacity(minCapacity); // 复制元素到新数组中 elementData = Arrays.copyOf(elementData, newCapacity); } ","date":"2024-03-18","objectID":"/ooooo-notes/01-arraylist/:2:0","tags":["jdk","source code","源码分析 jdk 系列"],"title":"01 ArrayList","uri":"/ooooo-notes/01-arraylist/"},{"categories":null,"content":"remove 删除元素，有两个重载，一个是根据元素来删除，一个是根据下标来删除，下面以根据下标来删除说明 源码位置: java.util.ArrayList#remove(int) public E remove(int index) { // 下标检查 rangeCheck(index); modCount++; // 获取旧值 E oldValue = elementData(index); int numMoved = size - index - 1; if (numMoved \u003e 0) // 把 index+1 之后的元素移动到 index 位置 System.arraycopy(elementData, index+1, elementData, index, numMoved); // gc elementData[--size] = null; // clear to let GC do its work return oldValue; } ","date":"2024-03-18","objectID":"/ooooo-notes/01-arraylist/:3:0","tags":["jdk","source code","源码分析 jdk 系列"],"title":"01 ArrayList","uri":"/ooooo-notes/01-arraylist/"},{"categories":null,"content":"set 设置元素 源码位置: java.util.ArrayList#set public E set(int index, E element) { // 下标检查 rangeCheck(index); // 获取旧值 E oldValue = elementData(index); // 设置新值 elementData[index] = element; return oldValue; } ","date":"2024-03-18","objectID":"/ooooo-notes/01-arraylist/:4:0","tags":["jdk","source code","源码分析 jdk 系列"],"title":"01 ArrayList","uri":"/ooooo-notes/01-arraylist/"},{"categories":null,"content":"get 获取元素 源码位置: java.util.ArrayList#get public E get(int index) { // 下标检查 rangeCheck(index); // 获取旧值 return elementData(index); } ","date":"2024-03-18","objectID":"/ooooo-notes/01-arraylist/:5:0","tags":["jdk","source code","源码分析 jdk 系列"],"title":"01 ArrayList","uri":"/ooooo-notes/01-arraylist/"},{"categories":null,"content":" jdk 基于 8 版本 在平时的开发中，我们会经常用到 LinkedList, 非常有必要了解源码。 ","date":"2024-03-18","objectID":"/ooooo-notes/02-linkedlist/:0:0","tags":["jdk","source code","源码分析 jdk 系列"],"title":"02 LinkedList","uri":"/ooooo-notes/02-linkedlist/"},{"categories":null,"content":"使用方式 public class LinkedListTest { @Test void test() { List\u003cString\u003e ids = new LinkedList\u003c\u003e(); assertThat(ids.add(\"1\")).isEqualTo(true); assertThat(ids.add(\"2\")).isEqualTo(true); assertThat(ids.add(\"3\")).isEqualTo(true); assertThat(ids.remove(\"2\")).isEqualTo(true); assertThat(ids.set(0, \"4\")).isEqualTo(\"1\"); assertThat(ids.get(0)).isEqualTo(\"4\"); } } ","date":"2024-03-18","objectID":"/ooooo-notes/02-linkedlist/:1:0","tags":["jdk","source code","源码分析 jdk 系列"],"title":"02 LinkedList","uri":"/ooooo-notes/02-linkedlist/"},{"categories":null,"content":"add 添加元素 源码位置: java.util.LinkedList#add(E) public boolean add(E e) { // 添加到链表末尾 linkLast(e); return true; } void linkLast(E e) { final Node\u003cE\u003e l = last; // 新建节点 final Node\u003cE\u003e newNode = new Node\u003c\u003e(l, e, null); // 尾结点为新节点 last = newNode; if (l == null) first = newNode; else // 旧尾结点指向新节点 l.next = newNode; size++; modCount++; } ","date":"2024-03-18","objectID":"/ooooo-notes/02-linkedlist/:2:0","tags":["jdk","source code","源码分析 jdk 系列"],"title":"02 LinkedList","uri":"/ooooo-notes/02-linkedlist/"},{"categories":null,"content":"remove 删除元素，有两个重载，一个是根据元素来删除，一个是根据下标来删除，下面以根据下标来删除说明 源码位置: java.util.LinkedList#remove(int) public E remove(int index) { // 下标检查 checkElementIndex(index); // 先查找下标对应的节点，然后删除 return unlink(node(index)); } 源码位置: java.util.LinkedList#node // 获取下标对应的节点 Node\u003cE\u003e node(int index) { // 判断是否从头结点开始查找 if (index \u003c (size \u003e\u003e 1)) { Node\u003cE\u003e x = first; for (int i = 0; i \u003c index; i++) x = x.next; return x; } else { Node\u003cE\u003e x = last; for (int i = size - 1; i \u003e index; i--) x = x.prev; return x; } } 源码位置: java.util.LinkedList#unlink // 删除节点 E unlink(Node\u003cE\u003e x) { // assert x != null; final E element = x.item; // 获取后继节点 final Node\u003cE\u003e next = x.next; // 获取前驱节点 final Node\u003cE\u003e prev = x.prev; // 连接前驱节点和后继节点 if (prev == null) { first = next; } else { prev.next = next; x.prev = null; } if (next == null) { last = prev; } else { next.prev = prev; x.next = null; } x.item = null; size--; modCount++; return element; } ","date":"2024-03-18","objectID":"/ooooo-notes/02-linkedlist/:3:0","tags":["jdk","source code","源码分析 jdk 系列"],"title":"02 LinkedList","uri":"/ooooo-notes/02-linkedlist/"},{"categories":null,"content":"set 设置元素 源码位置: java.util.LinkedList#set public E set(int index, E element) { // 下标检查 checkElementIndex(index); // 找到对应下标的节点, 在 remove 方法中已解析 Node\u003cE\u003e x = node(index); E oldVal = x.item; // 赋值 x.item = element; return oldVal; } ","date":"2024-03-18","objectID":"/ooooo-notes/02-linkedlist/:4:0","tags":["jdk","source code","源码分析 jdk 系列"],"title":"02 LinkedList","uri":"/ooooo-notes/02-linkedlist/"},{"categories":null,"content":"get 源码位置: java.util.LinkedList#get public E get(int index) { // 下标检查 checkElementIndex(index); // 找到对应下标的节点, 在 remove 方法中已解析 return node(index).item; } ","date":"2024-03-18","objectID":"/ooooo-notes/02-linkedlist/:5:0","tags":["jdk","source code","源码分析 jdk 系列"],"title":"02 LinkedList","uri":"/ooooo-notes/02-linkedlist/"},{"categories":null,"content":"今天好多人 There are many people on the subway ","date":"2024-03-06","objectID":"/ooooo-notes/%E8%8B%B1%E8%AF%AD%E6%97%A5%E5%B8%B8%E5%AF%B9%E8%AF%9D/:1:0","tags":["english","英语日常对话"],"title":"英语日常对话","uri":"/ooooo-notes/%E8%8B%B1%E8%AF%AD%E6%97%A5%E5%B8%B8%E5%AF%B9%E8%AF%9D/"},{"categories":null,"content":"地铁要等好久 It’s a long wait for the subway. ","date":"2024-03-06","objectID":"/ooooo-notes/%E8%8B%B1%E8%AF%AD%E6%97%A5%E5%B8%B8%E5%AF%B9%E8%AF%9D/:2:0","tags":["english","英语日常对话"],"title":"英语日常对话","uri":"/ooooo-notes/%E8%8B%B1%E8%AF%AD%E6%97%A5%E5%B8%B8%E5%AF%B9%E8%AF%9D/"},{"categories":null,"content":"中午不知道吃啥 I don’t know what to eat for lunch. ","date":"2024-03-06","objectID":"/ooooo-notes/%E8%8B%B1%E8%AF%AD%E6%97%A5%E5%B8%B8%E5%AF%B9%E8%AF%9D/:3:0","tags":["english","英语日常对话"],"title":"英语日常对话","uri":"/ooooo-notes/%E8%8B%B1%E8%AF%AD%E6%97%A5%E5%B8%B8%E5%AF%B9%E8%AF%9D/"},{"categories":null,"content":" 只要涉及到数据库操作，必定就会使用 @Transactional 注解，其中有一个属性就是 propagation(传播类型)，掌握它的用法很重要。演示代码见末尾。 ","date":"2024-02-23","objectID":"/ooooo-notes/spring-tx-%E4%BC%A0%E6%92%AD%E7%B1%BB%E5%9E%8B/:0:0","tags":["spring boot","source code","源码分析 spring boot 系列"],"title":"spring tx 传播类型","uri":"/ooooo-notes/spring-tx-%E4%BC%A0%E6%92%AD%E7%B1%BB%E5%9E%8B/"},{"categories":null,"content":"演示事务传播 ","date":"2024-02-23","objectID":"/ooooo-notes/spring-tx-%E4%BC%A0%E6%92%AD%E7%B1%BB%E5%9E%8B/:1:0","tags":["spring boot","source code","源码分析 spring boot 系列"],"title":"spring tx 传播类型","uri":"/ooooo-notes/spring-tx-%E4%BC%A0%E6%92%AD%E7%B1%BB%E5%9E%8B/"},{"categories":null,"content":"基础代码 定义了所有的传播类型，第二个参数来控制是否抛出异常。 @Transactional(propagation = Propagation.REQUIRED) public void REQUIRED(User user, boolean throwException) { insertUser(user, throwException); } @Transactional(propagation = Propagation.REQUIRES_NEW) public void REQUIRES_NEW(User user, boolean throwException) { insertUser(user, throwException); } @Transactional(propagation = Propagation.NESTED) public void NESTED(User user, boolean throwException) { insertUser(user, throwException); } @Transactional(propagation = Propagation.NOT_SUPPORTED) public void NOT_SUPPORTED(User user, boolean throwException) { insertUser(user, throwException); } @Transactional(propagation = Propagation.SUPPORTS) public void SUPPORTS(User user, boolean throwException) { insertUser(user, throwException); } @Transactional(propagation = Propagation.NEVER) public void NEVER(User user, boolean throwException) { insertUser(user, throwException); } @Transactional(propagation = Propagation.MANDATORY) public void MANDATORY(User user, boolean throwException) { insertUser(user, throwException); } ","date":"2024-02-23","objectID":"/ooooo-notes/spring-tx-%E4%BC%A0%E6%92%AD%E7%B1%BB%E5%9E%8B/:1:1","tags":["spring boot","source code","源码分析 spring boot 系列"],"title":"spring tx 传播类型","uri":"/ooooo-notes/spring-tx-%E4%BC%A0%E6%92%AD%E7%B1%BB%E5%9E%8B/"},{"categories":null,"content":"REQUIRED_REQUIRED @Transactional(propagation = Propagation.REQUIRED) public void REQUIRED_REQUIRED() { serviceA.REQUIRED(new User(\"111\"), false); try { serviceA.REQUIRED(new User(\"222\"), true); } catch (Exception ignored) { } } 结论：不会插入数据, 会抛出异常 分析：第一次调用创建新的事务状态，第二次调用因为是 REQUIRED, 所以会共用之前的事务状态，这样两次调用是同一个事务状态。 第二次调用发生异常，事务状态要回滚，而第一次调用没有异常，事务状态要提交，导致事务状态冲突。 ","date":"2024-02-23","objectID":"/ooooo-notes/spring-tx-%E4%BC%A0%E6%92%AD%E7%B1%BB%E5%9E%8B/:1:2","tags":["spring boot","source code","源码分析 spring boot 系列"],"title":"spring tx 传播类型","uri":"/ooooo-notes/spring-tx-%E4%BC%A0%E6%92%AD%E7%B1%BB%E5%9E%8B/"},{"categories":null,"content":"REQUIRED_REQUIRES_NEW @Transactional(propagation = Propagation.REQUIRED) public void REQUIRED_REQUIRES_NEW() { serviceA.REQUIRED(new User(\"111\"), false); try { serviceA.REQUIRES_NEW(new User(\"222\"), true); } catch (Exception ignored) { } } 结论：会插入 111 数据 分析：第一次调用创建新的事务状态，第二次调用因为是 REQUIRES_NEW, 所以会创建新的事务状态，这样两次调用不是同一个事务状态。 第二次调用发生异常，事务状态要回滚，而第一次调用没有异常，事务状态要提交。 ","date":"2024-02-23","objectID":"/ooooo-notes/spring-tx-%E4%BC%A0%E6%92%AD%E7%B1%BB%E5%9E%8B/:1:3","tags":["spring boot","source code","源码分析 spring boot 系列"],"title":"spring tx 传播类型","uri":"/ooooo-notes/spring-tx-%E4%BC%A0%E6%92%AD%E7%B1%BB%E5%9E%8B/"},{"categories":null,"content":"REQUIRED_NESTED @Transactional(propagation = Propagation.REQUIRED) public void REQUIRED_NESTED() { serviceA.REQUIRED(new User(\"111\"), false); try { serviceA.NESTED(new User(\"222\"), true); } catch (Exception ignored) { } } 结论：会插入 111 数据 分析：第一次调用创建新的事务状态，第二次调用因为是 NESTED, 所以会设置保存点，这样两次调用是同一个事务状态。 第二次调用发生异常，事务状态要回滚到保存点，而第一次调用没有异常，事务状态要提交。 ","date":"2024-02-23","objectID":"/ooooo-notes/spring-tx-%E4%BC%A0%E6%92%AD%E7%B1%BB%E5%9E%8B/:1:4","tags":["spring boot","source code","源码分析 spring boot 系列"],"title":"spring tx 传播类型","uri":"/ooooo-notes/spring-tx-%E4%BC%A0%E6%92%AD%E7%B1%BB%E5%9E%8B/"},{"categories":null,"content":"REQUIRED_NOT_SUPPORTED @Transactional(propagation = Propagation.REQUIRED) public void REQUIRED_NOT_SUPPORTED() { serviceA.REQUIRED(new User(\"111\"), false); try { serviceA.NOT_SUPPORTED(new User(\"222\"), true); } catch (Exception ignored) { } } 结论：会插入 111 数据, 222 数据 分析：第一次调用创建新的事务状态，第二次调用因为是 NOT_SUPPORTED, 所以会挂起事务，这样只有第一次调用是有事务。 第二次调用发生异常，因为没有事务，所以不会回滚，而第一次调用没有异常，事务状态要提交。 ","date":"2024-02-23","objectID":"/ooooo-notes/spring-tx-%E4%BC%A0%E6%92%AD%E7%B1%BB%E5%9E%8B/:1:5","tags":["spring boot","source code","源码分析 spring boot 系列"],"title":"spring tx 传播类型","uri":"/ooooo-notes/spring-tx-%E4%BC%A0%E6%92%AD%E7%B1%BB%E5%9E%8B/"},{"categories":null,"content":"REQUIRED_SUPPORTS @Transactional(propagation = Propagation.REQUIRED) public void REQUIRED_SUPPORTS() { serviceA.REQUIRED(new User(\"111\"), false); try { serviceA.SUPPORTS(new User(\"222\"), true); } catch (Exception ignored) { } } 结论：不会插入数据, 会抛出异常 分析：第一次调用创建新的事务状态，第二次调用因为是 SUPPORTS, 所以会共用之前的事务状态，这样两次调用是同一个事务状态。 第二次调用发生异常，事务状态要回滚，而第一次调用没有异常，事务状态要提交，导致事务状态冲突。 ","date":"2024-02-23","objectID":"/ooooo-notes/spring-tx-%E4%BC%A0%E6%92%AD%E7%B1%BB%E5%9E%8B/:1:6","tags":["spring boot","source code","源码分析 spring boot 系列"],"title":"spring tx 传播类型","uri":"/ooooo-notes/spring-tx-%E4%BC%A0%E6%92%AD%E7%B1%BB%E5%9E%8B/"},{"categories":null,"content":"REQUIRED_NEVER @Transactional(propagation = Propagation.REQUIRED) public void REQUIRED_NEVER() { serviceA.REQUIRED(new User(\"111\"), false); try { serviceA.NEVER(new User(\"222\"), true); } catch (Exception ignored) { } } 结论：会插入 111 数据 分析：第一次调用创建新的事务状态，第二次调用因为是 NEVER, 所以会抛出异常不会继续执行代码。 第一次调用没有异常，事务状态要提交。 ","date":"2024-02-23","objectID":"/ooooo-notes/spring-tx-%E4%BC%A0%E6%92%AD%E7%B1%BB%E5%9E%8B/:1:7","tags":["spring boot","source code","源码分析 spring boot 系列"],"title":"spring tx 传播类型","uri":"/ooooo-notes/spring-tx-%E4%BC%A0%E6%92%AD%E7%B1%BB%E5%9E%8B/"},{"categories":null,"content":"REQUIRED_MANDATORY @Transactional(propagation = Propagation.REQUIRED) public void REQUIRED_MANDATORY() { serviceA.REQUIRED(new User(\"111\"), false); try { serviceA.MANDATORY(new User(\"222\"), true); } catch (Exception ignored) { } } 结论：不会插入数据, 会抛出异常 分析：第一次调用创建新的事务状态，第二次调用因为是 REQUIRED, 所以会共用之前的事务状态，这样两次调用是同一个事务状态。 第二次调用发生异常，事务状态要回滚，而第一次调用没有异常，事务状态要提交，导致事务状态冲突。 ","date":"2024-02-23","objectID":"/ooooo-notes/spring-tx-%E4%BC%A0%E6%92%AD%E7%B1%BB%E5%9E%8B/:1:8","tags":["spring boot","source code","源码分析 spring boot 系列"],"title":"spring tx 传播类型","uri":"/ooooo-notes/spring-tx-%E4%BC%A0%E6%92%AD%E7%B1%BB%E5%9E%8B/"},{"categories":null,"content":"事务传播原理 源码位置: org.springframework.transaction.support.AbstractPlatformTransactionManager#getTransaction // 每一个 @Transactional 都会执行下面的方法，来获取事务状态 public final TransactionStatus getTransaction(@Nullable TransactionDefinition definition) throws TransactionException { ... // 获取当前事务 Object transaction = doGetTransaction(); // 判断事务是否存在 if (isExistingTransaction(transaction)) { // 重点解析 return handleExistingTransaction(def, transaction, debugEnabled); } // 下面是不存在事务的情况 if (def.getPropagationBehavior() == TransactionDefinition.PROPAGATION_MANDATORY) { throw new IllegalTransactionStateException( \"No existing transaction found for transaction marked with propagation 'mandatory'\"); } else if (def.getPropagationBehavior() == TransactionDefinition.PROPAGATION_REQUIRED || def.getPropagationBehavior() == TransactionDefinition.PROPAGATION_REQUIRES_NEW || def.getPropagationBehavior() == TransactionDefinition.PROPAGATION_NESTED) { SuspendedResourcesHolder suspendedResources = suspend(null); ... try { // 开启新的事务 return startTransaction(def, transaction, debugEnabled, suspendedResources); } catch (RuntimeException | Error ex) { resume(null, suspendedResources); throw ex; } } else { ... // 不开始事务 return prepareTransactionStatus(def, null, true, newSynchronization, debugEnabled, null); } } 源码位置: org.springframework.transaction.support.AbstractPlatformTransactionManager#handleExistingTransaction private TransactionStatus handleExistingTransaction( TransactionDefinition definition, Object transaction, boolean debugEnabled) throws TransactionException { // 下面是存在事务的情况 if (definition.getPropagationBehavior() == TransactionDefinition.PROPAGATION_NEVER) { throw new IllegalTransactionStateException( \"Existing transaction found for transaction marked with propagation 'never'\"); } if (definition.getPropagationBehavior() == TransactionDefinition.PROPAGATION_NOT_SUPPORTED) { ... // 挂起当前事务，以非事务来执行 Object suspendedResources = suspend(transaction); boolean newSynchronization = (getTransactionSynchronization() == SYNCHRONIZATION_ALWAYS); return prepareTransactionStatus( definition, null, false, newSynchronization, debugEnabled, suspendedResources); } if (definition.getPropagationBehavior() == TransactionDefinition.PROPAGATION_REQUIRES_NEW) { ... // 挂起当前事务 SuspendedResourcesHolder suspendedResources = suspend(transaction); try { // 开启新事务 return startTransaction(definition, transaction, debugEnabled, suspendedResources); } catch (RuntimeException | Error beginEx) { resumeAfterBeginException(transaction, suspendedResources, beginEx); throw beginEx; } } if (definition.getPropagationBehavior() == TransactionDefinition.PROPAGATION_NESTED) { ... if (useSavepointForNestedTransaction()) { ... DefaultTransactionStatus status = prepareTransactionStatus(definition, transaction, false, false, debugEnabled, null); // 在当前事务上，创建保存点 status.createAndHoldSavepoint(); return status; } else { // 不支持保存点，就开启新事务 return startTransaction(definition, transaction, debugEnabled, null); } } ... // 继续使用当前事务 return prepareTransactionStatus(definition, transaction, false, newSynchronization, debugEnabled, null); } 说明：在 startTransaction 方法中，每次都会获取新连接来开启事务。 ","date":"2024-02-23","objectID":"/ooooo-notes/spring-tx-%E4%BC%A0%E6%92%AD%E7%B1%BB%E5%9E%8B/:2:0","tags":["spring boot","source code","源码分析 spring boot 系列"],"title":"spring tx 传播类型","uri":"/ooooo-notes/spring-tx-%E4%BC%A0%E6%92%AD%E7%B1%BB%E5%9E%8B/"},{"categories":null,"content":"代码 demo-spring-transaction-propagation ","date":"2024-02-23","objectID":"/ooooo-notes/spring-tx-%E4%BC%A0%E6%92%AD%E7%B1%BB%E5%9E%8B/:3:0","tags":["spring boot","source code","源码分析 spring boot 系列"],"title":"spring tx 传播类型","uri":"/ooooo-notes/spring-tx-%E4%BC%A0%E6%92%AD%E7%B1%BB%E5%9E%8B/"},{"categories":null,"content":" 当我们使用缓存时，必定会遇到缓存一致性问题，也就是在读写请求过程中数据库和缓存中的数据不一致。 下面将分析为什么会造成不一致, 所有的代码参考末尾。 ","date":"2024-01-07","objectID":"/ooooo-notes/%E7%BC%93%E5%AD%98%E4%B8%80%E8%87%B4%E6%80%A7%E9%97%AE%E9%A2%98/:0:0","tags":["cache"],"title":"缓存一致性问题","uri":"/ooooo-notes/%E7%BC%93%E5%AD%98%E4%B8%80%E8%87%B4%E6%80%A7%E9%97%AE%E9%A2%98/"},{"categories":null,"content":"先更新数据库，后更新缓存 数据库的值默认为 0 读操作： public String get(Long id) { // 从缓存中加载 String userName = userCache.queryUserNameById(id); if (userName == null) { // 从数据库中加载 userName = userDB.queryUserNameById(id); // 设置到缓存中 TestUtil.sleep(200); // 表示 gc，请求延迟 userCache.setUserNameById(id, userName); } return userName; } 写操作： public void set(Long id, String username) { // 更新数据库 TestUtil.sleep(100); // 表示 gc, 请求延迟 userDB.setUserNameById(id, username); // 更新缓存 userCache.setUserNameById(id, null); } 实际执行过程: 读操作（从缓存中读取数据，发现为空，所以查询数据库，得到 0） 写操作（更新数据库值为 1，删除缓存值） 读操作（更新缓存值为 0） 不一致（数据库值为 1，缓存值为 0） 从上面可以分析，更新数据库和更新缓存的顺序，无论谁先谁后都会造成数据不一致。 ","date":"2024-01-07","objectID":"/ooooo-notes/%E7%BC%93%E5%AD%98%E4%B8%80%E8%87%B4%E6%80%A7%E9%97%AE%E9%A2%98/:1:0","tags":["cache"],"title":"缓存一致性问题","uri":"/ooooo-notes/%E7%BC%93%E5%AD%98%E4%B8%80%E8%87%B4%E6%80%A7%E9%97%AE%E9%A2%98/"},{"categories":null,"content":"同时更新数据 写操作（A）： // username = 1 public void set1(Long id, String username) { // 更新缓存 TestUtil.sleep(100); userCache.setUserNameById(id, username); // 更新数据库 userDB.setUserNameById(id, username); } 写操作（B）： // username = 2 public void set2(Long id, String username) { // 更新缓存 userCache.setUserNameById(id, username); // 更新数据库 TestUtil.sleep(200); userDB.setUserNameById(id, username); } 实际执行过程： B（更新缓存值为 2） A（更新缓存值为 1，更新数据库值为 1） B（更新数据库值为 2） 不一致（数据库值为 2，缓存值为 1） 从上面可以分析，更新数据库和更新缓存的顺序，无论谁先谁后都会造成数据不一致。 ","date":"2024-01-07","objectID":"/ooooo-notes/%E7%BC%93%E5%AD%98%E4%B8%80%E8%87%B4%E6%80%A7%E9%97%AE%E9%A2%98/:2:0","tags":["cache"],"title":"缓存一致性问题","uri":"/ooooo-notes/%E7%BC%93%E5%AD%98%E4%B8%80%E8%87%B4%E6%80%A7%E9%97%AE%E9%A2%98/"},{"categories":null,"content":"解决方法 使用分布式锁来确保更新数据库和更新缓存是原子性。 ","date":"2024-01-07","objectID":"/ooooo-notes/%E7%BC%93%E5%AD%98%E4%B8%80%E8%87%B4%E6%80%A7%E9%97%AE%E9%A2%98/:3:0","tags":["cache"],"title":"缓存一致性问题","uri":"/ooooo-notes/%E7%BC%93%E5%AD%98%E4%B8%80%E8%87%B4%E6%80%A7%E9%97%AE%E9%A2%98/"},{"categories":null,"content":"代码 demo-cache-consistency-question ","date":"2024-01-07","objectID":"/ooooo-notes/%E7%BC%93%E5%AD%98%E4%B8%80%E8%87%B4%E6%80%A7%E9%97%AE%E9%A2%98/:4:0","tags":["cache"],"title":"缓存一致性问题","uri":"/ooooo-notes/%E7%BC%93%E5%AD%98%E4%B8%80%E8%87%B4%E6%80%A7%E9%97%AE%E9%A2%98/"},{"categories":null,"content":" 在 servlet 3.0 的规范中，有异步servlet特性，这个可以增大吞吐量。我们有必要看看 spring 是如何适配这个特性的。 ","date":"2024-01-06","objectID":"/ooooo-notes/%E5%BC%82%E6%AD%A5-servlet-%E5%8E%9F%E7%90%86/:0:0","tags":["spring boot","source code","源码分析 spring boot 系列"],"title":"异步 servlet 原理","uri":"/ooooo-notes/%E5%BC%82%E6%AD%A5-servlet-%E5%8E%9F%E7%90%86/"},{"categories":null,"content":"实现异步 servlet 在 spring mvc 中，实现异步servlet有多种方式，比如 DeferredResult、Callable，相关代码见末尾。 ","date":"2024-01-06","objectID":"/ooooo-notes/%E5%BC%82%E6%AD%A5-servlet-%E5%8E%9F%E7%90%86/:1:0","tags":["spring boot","source code","源码分析 spring boot 系列"],"title":"异步 servlet 原理","uri":"/ooooo-notes/%E5%BC%82%E6%AD%A5-servlet-%E5%8E%9F%E7%90%86/"},{"categories":null,"content":"DeferredResult 方式 @GetMapping(\"/test2\") public DeferredResult\u003cString\u003e test2() { before(); DeferredResult\u003cString\u003e result = new DeferredResult\u003c\u003e(); executor.submit(() -\u003e { process(); result.setResult(\"test2\"); }); after(); return result; } 相关日志: deferredResult-log ","date":"2024-01-06","objectID":"/ooooo-notes/%E5%BC%82%E6%AD%A5-servlet-%E5%8E%9F%E7%90%86/:1:1","tags":["spring boot","source code","源码分析 spring boot 系列"],"title":"异步 servlet 原理","uri":"/ooooo-notes/%E5%BC%82%E6%AD%A5-servlet-%E5%8E%9F%E7%90%86/"},{"categories":null,"content":"Callable 方式 @GetMapping(\"/test4\") public Callable\u003cString\u003e test4() { before(); Callable\u003cString\u003e callable = () -\u003e { process(); return \"test4\"; }; after(); return callable; } 相关日志: callable-log ","date":"2024-01-06","objectID":"/ooooo-notes/%E5%BC%82%E6%AD%A5-servlet-%E5%8E%9F%E7%90%86/:1:2","tags":["spring boot","source code","源码分析 spring boot 系列"],"title":"异步 servlet 原理","uri":"/ooooo-notes/%E5%BC%82%E6%AD%A5-servlet-%E5%8E%9F%E7%90%86/"},{"categories":null,"content":"源码解读 在 spring 中，有一个特殊的接口 HandlerMethodReturnValueHandler，专门来处理请求的返回值。 ","date":"2024-01-06","objectID":"/ooooo-notes/%E5%BC%82%E6%AD%A5-servlet-%E5%8E%9F%E7%90%86/:2:0","tags":["spring boot","source code","源码分析 spring boot 系列"],"title":"异步 servlet 原理","uri":"/ooooo-notes/%E5%BC%82%E6%AD%A5-servlet-%E5%8E%9F%E7%90%86/"},{"categories":null,"content":"处理 DeferredResult 源码位置: org.springframework.web.servlet.mvc.method.annotation.DeferredResultMethodReturnValueHandler public class DeferredResultMethodReturnValueHandler implements HandlerMethodReturnValueHandler { @Override public boolean supportsReturnType(MethodParameter returnType) { // 判断类型 Class\u003c?\u003e type = returnType.getParameterType(); return (DeferredResult.class.isAssignableFrom(type) || ListenableFuture.class.isAssignableFrom(type) || CompletionStage.class.isAssignableFrom(type)); } @Override public void handleReturnValue(@Nullable Object returnValue, MethodParameter returnType, ModelAndViewContainer mavContainer, NativeWebRequest webRequest) throws Exception { ... DeferredResult\u003c?\u003e result; if (returnValue instanceof DeferredResult) { result = (DeferredResult\u003c?\u003e) returnValue; } else if (returnValue instanceof ListenableFuture) { result = adaptListenableFuture((ListenableFuture\u003c?\u003e) returnValue); } else if (returnValue instanceof CompletionStage) { result = adaptCompletionStage((CompletionStage\u003c?\u003e) returnValue); } else { // Should not happen... throw new IllegalStateException(\"Unexpected return value type: \" + returnValue); } // 开始异步处理 WebAsyncUtils.getAsyncManager(webRequest).startDeferredResultProcessing(result, mavContainer); } } ","date":"2024-01-06","objectID":"/ooooo-notes/%E5%BC%82%E6%AD%A5-servlet-%E5%8E%9F%E7%90%86/:2:1","tags":["spring boot","source code","源码分析 spring boot 系列"],"title":"异步 servlet 原理","uri":"/ooooo-notes/%E5%BC%82%E6%AD%A5-servlet-%E5%8E%9F%E7%90%86/"},{"categories":null,"content":"处理 Callable 源码位置: org.springframework.web.servlet.mvc.method.annotation.CallableMethodReturnValueHandler public class CallableMethodReturnValueHandler implements HandlerMethodReturnValueHandler { @Override public boolean supportsReturnType(MethodParameter returnType) { // 判断类型 return Callable.class.isAssignableFrom(returnType.getParameterType()); } @Override public void handleReturnValue(@Nullable Object returnValue, MethodParameter returnType, ModelAndViewContainer mavContainer, NativeWebRequest webRequest) throws Exception { ... Callable\u003c?\u003e callable = (Callable\u003c?\u003e) returnValue; // 开始异步处理 WebAsyncUtils.getAsyncManager(webRequest).startCallableProcessing(callable, mavContainer); } } 从上面两个类可以看出，最终都是调用了 WebAsyncManager 类的 startDeferredResultProcessing 或者 startCallableProcessing 方法， 这两个方法的内部实现都是差不多的，下面以 startCallableProcessing 为例。 ","date":"2024-01-06","objectID":"/ooooo-notes/%E5%BC%82%E6%AD%A5-servlet-%E5%8E%9F%E7%90%86/:2:2","tags":["spring boot","source code","源码分析 spring boot 系列"],"title":"异步 servlet 原理","uri":"/ooooo-notes/%E5%BC%82%E6%AD%A5-servlet-%E5%8E%9F%E7%90%86/"},{"categories":null,"content":"WebAsyncManager 源码位置: org.springframework.web.context.request.async.WebAsyncManager#startCallableProcessing public void startCallableProcessing(final WebAsyncTask\u003c?\u003e webAsyncTask, Object... processingContext) throws Exception { ... List\u003cCallableProcessingInterceptor\u003e interceptors = new ArrayList\u003c\u003e(); interceptors.add(webAsyncTask.getInterceptor()); interceptors.addAll(this.callableInterceptors.values()); interceptors.add(timeoutCallableInterceptor); final Callable\u003c?\u003e callable = webAsyncTask.getCallable(); final CallableInterceptorChain interceptorChain = new CallableInterceptorChain(interceptors); // 设置超时处理器 this.asyncWebRequest.addTimeoutHandler(() -\u003e { if (logger.isDebugEnabled()) { logger.debug(\"Async request timeout for \" + formatRequestUri()); } Object result = interceptorChain.triggerAfterTimeout(this.asyncWebRequest, callable); if (result != CallableProcessingInterceptor.RESULT_NONE) { setConcurrentResultAndDispatch(result); } }); // 设置错误处理 this.asyncWebRequest.addErrorHandler(ex -\u003e { if (!this.errorHandlingInProgress) { if (logger.isDebugEnabled()) { logger.debug(\"Async request error for \" + formatRequestUri() + \": \" + ex); } Object result = interceptorChain.triggerAfterError(this.asyncWebRequest, callable, ex); result = (result != CallableProcessingInterceptor.RESULT_NONE ? result : ex); setConcurrentResultAndDispatch(result); } }); // 设置完成处理器 this.asyncWebRequest.addCompletionHandler(() -\u003e interceptorChain.triggerAfterCompletion(this.asyncWebRequest, callable)); // 执行钩子 interceptorChain.applyBeforeConcurrentHandling(this.asyncWebRequest, callable); // 开启异步处理，就是 request#startAsync (servlet api) startAsyncProcessing(processingContext); try { Future\u003c?\u003e future = this.taskExecutor.submit(() -\u003e { Object result = null; try { // 执行钩子 applyPreProcess interceptorChain.applyPreProcess(this.asyncWebRequest, callable); // 处理请求 result = callable.call(); } catch (Throwable ex) { result = ex; } finally { // 执行钩子 applyPostProcess result = interceptorChain.applyPostProcess(this.asyncWebRequest, callable, result); } // 设置结果, 然后 dispatch, 当前这个请求就会再次处理，会被 RequestMappingHandlerAdapter#invokeHandlerMethod 拦截 setConcurrentResultAndDispatch(result); }); interceptorChain.setTaskFuture(future); } catch (RejectedExecutionException ex) { Object result = interceptorChain.applyPostProcess(this.asyncWebRequest, callable, ex); setConcurrentResultAndDispatch(result); throw ex; } } 源码位置: org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter#invokeHandlerMethod protected ModelAndView invokeHandlerMethod(HttpServletRequest request, HttpServletResponse response, HandlerMethod handlerMethod) throws Exception { ... WebAsyncManager asyncManager = WebAsyncUtils.getAsyncManager(request); // 检查是否有异步结果 if (asyncManager.hasConcurrentResult()) { Object result = asyncManager.getConcurrentResult(); mavContainer = (ModelAndViewContainer) asyncManager.getConcurrentResultContext()[0]; asyncManager.clearConcurrentResult(); // 这里会返回一个新的 handlerMethod, 这个很重要 invocableMethod = invocableMethod.wrapConcurrentResult(result); } // 返回 json invocableMethod.invokeAndHandle(webRequest, mavContainer); if (asyncManager.isConcurrentHandlingStarted()) { return null; } // 对于 json 请求来说，这里不会执行 return getModelAndView(mavContainer, modelFactory, webRequest); } finally { webRequest.requestCompleted(); } } ","date":"2024-01-06","objectID":"/ooooo-notes/%E5%BC%82%E6%AD%A5-servlet-%E5%8E%9F%E7%90%86/:2:3","tags":["spring boot","source code","源码分析 spring boot 系列"],"title":"异步 servlet 原理","uri":"/ooooo-notes/%E5%BC%82%E6%AD%A5-servlet-%E5%8E%9F%E7%90%86/"},{"categories":null,"content":"task 线程池 Callable 的执行，需要线程池，默认配置类如下: 源码位置: org.springframework.boot.autoconfigure.task.TaskExecutionAutoConfiguration @Lazy @Bean(name = { APPLICATION_TASK_EXECUTOR_BEAN_NAME, AsyncAnnotationBeanPostProcessor.DEFAULT_TASK_EXECUTOR_BEAN_NAME }) @ConditionalOnMissingBean(Executor.class) public ThreadPoolTaskExecutor applicationTaskExecutor(TaskExecutorBuilder builder) { return builder.build(); } ","date":"2024-01-06","objectID":"/ooooo-notes/%E5%BC%82%E6%AD%A5-servlet-%E5%8E%9F%E7%90%86/:2:4","tags":["spring boot","source code","源码分析 spring boot 系列"],"title":"异步 servlet 原理","uri":"/ooooo-notes/%E5%BC%82%E6%AD%A5-servlet-%E5%8E%9F%E7%90%86/"},{"categories":null,"content":"代码 demo-spring-async-servlet ","date":"2024-01-06","objectID":"/ooooo-notes/%E5%BC%82%E6%AD%A5-servlet-%E5%8E%9F%E7%90%86/:3:0","tags":["spring boot","source code","源码分析 spring boot 系列"],"title":"异步 servlet 原理","uri":"/ooooo-notes/%E5%BC%82%E6%AD%A5-servlet-%E5%8E%9F%E7%90%86/"},{"categories":null,"content":" ⭕ 进行中 ✅ 已完成 ❌ 已废弃 ❓ 有必要 ❗ 重要性 📝 记笔记 🖊️ 写代码 ","date":"2024-01-01","objectID":"/ooooo-notes/2024%E5%B9%B4%E8%AE%A1%E5%88%92/:0:0","tags":["learning"],"title":"2024年学习计划","uri":"/ooooo-notes/2024%E5%B9%B4%E8%AE%A1%E5%88%92/"},{"categories":null,"content":"1. 关于阅读 0️⃣1️⃣. 《操作系统导论》 ✅ 0️⃣2️⃣. 《深入理解计算机系统（原书第3版）》 ✅ 0️⃣3️⃣. 《MySQL技术内幕》 ⭕ 0️⃣4️⃣. 《RocketMQ技术内幕 第二版》 ⭕ 0️⃣5️⃣. 《Vim实用技巧（第2版）》 ⭕ 0️⃣6️⃣. 《C和指针》 ✅ 0️⃣7️⃣. 《C专家编程》 0️⃣8️⃣. 《C陷阱与缺陷》 0️⃣9️⃣. Rust 编程第一课 ⭕ 1️⃣0️⃣. 长安的荔枝 ✅ 1️⃣1️⃣. x86汇编语言（第2版） 1️⃣2️⃣. Rust实战 ✅ 1️⃣3️⃣. 期货及衍生品基础（第三版） ⭕ ","date":"2024-01-01","objectID":"/ooooo-notes/2024%E5%B9%B4%E8%AE%A1%E5%88%92/:1:0","tags":["learning"],"title":"2024年学习计划","uri":"/ooooo-notes/2024%E5%B9%B4%E8%AE%A1%E5%88%92/"},{"categories":null,"content":"2. 关于源码笔记 0️⃣1️⃣. 《rocketmq 源码》 ⭕ 0️⃣2️⃣. 《dubbo 源码》 ✅ 0️⃣3️⃣. 《grpc-go 源码》 0️⃣4️⃣. 《spring boot 源码》 ⭕ 0️⃣5️⃣. 《netty 源码》 0️⃣6️⃣. 《tomcat 源码》 ","date":"2024-01-01","objectID":"/ooooo-notes/2024%E5%B9%B4%E8%AE%A1%E5%88%92/:2:0","tags":["learning"],"title":"2024年学习计划","uri":"/ooooo-notes/2024%E5%B9%B4%E8%AE%A1%E5%88%92/"},{"categories":null,"content":" dubbo 基于 3.2.6 版本 如果我们将 dubbo 应用部署在 k8s 环境中，我们就可以使用 k8s 作为注册中心。 ","date":"2023-12-25","objectID":"/ooooo-notes/13-%E9%9B%86%E6%88%90-k8s/:0:0","tags":["dubbo","source code","源码分析 dubbo 系列"],"title":"13 集成 k8s","uri":"/ooooo-notes/13-%E9%9B%86%E6%88%90-k8s/"},{"categories":null,"content":"服务调用流程 provider 使用 KubernetesServiceDiscovery#doRegister 注册服务实例，元数据信息会存放在 pod 对象上. consumer 使用 ServiceNameMapping#getMapping 来获取 consumerUrl 对应的 serviceName. consumer 使用 KubernetesServiceDiscovery#getInstances 来获取 serviceName 对应的服务实例. 获取的服务实例上面就会有元数据信息，然后就会使用元数据信息来获取服务实例的所有 url 列表. 根据这些 url 列表来创建对应的 invoker，比如 DubboInvoker, TripleInvoker. ","date":"2023-12-25","objectID":"/ooooo-notes/13-%E9%9B%86%E6%88%90-k8s/:1:0","tags":["dubbo","source code","源码分析 dubbo 系列"],"title":"13 集成 k8s","uri":"/ooooo-notes/13-%E9%9B%86%E6%88%90-k8s/"},{"categories":null,"content":"KubernetesServiceDiscovery#doRegister 注册实例 源码位置: org.apache.dubbo.registry.kubernetes.KubernetesServiceDiscovery#KubernetesServiceDiscovery // KubernetesServiceDiscovery 构造方法 public KubernetesServiceDiscovery(ApplicationModel applicationModel, URL registryURL) { super(applicationModel, registryURL); Config config = KubernetesConfigUtils.createKubernetesConfig(registryURL); // 初始化 k8s client this.kubernetesClient = new KubernetesClientBuilder().withConfig(config).build(); // HostName 一般默认就是 podName this.currentHostname = System.getenv(\"HOSTNAME\"); this.registryURL = registryURL; this.namespace = config.getNamespace(); // 默认需要注册 this.enableRegister = registryURL.getParameter(KubernetesClientConst.ENABLE_REGISTER, true); boolean availableAccess; try { // 检查 k8s 是否可用 availableAccess = kubernetesClient.pods().withName(currentHostname).get() != null; } catch (Throwable e) { availableAccess = false; } if (!availableAccess) { ... } else { // todo 暂时不解析 KubernetesMeshEnvListener.injectKubernetesEnv(kubernetesClient, namespace); } } 源码位置: org.apache.dubbo.registry.kubernetes.KubernetesServiceDiscovery#doRegister // 注册实例 @Override public void doRegister(ServiceInstance serviceInstance) throws RuntimeException { if (enableRegister) { kubernetesClient .pods() .inNamespace(namespace) // 选择当前 pod .withName(currentHostname) .edit(pod -\u003e new PodBuilder(pod) .editOrNewMetadata() // 添加到注解 .addToAnnotations(KUBERNETES_PROPERTIES_KEY, JsonUtils.toJson(serviceInstance.getMetadata())) .endMetadata() .build()); if (logger.isInfoEnabled()) { logger.info(\"Write Current Service Instance Metadata to Kubernetes pod. \" + \"Current pod name: \" + currentHostname); } } } 源码位置: org.apache.dubbo.registry.kubernetes.KubernetesServiceDiscovery#doUpdate // 实例信息改变之后，重新注册 @Override public void doUpdate(ServiceInstance oldServiceInstance, ServiceInstance newServiceInstance) throws RuntimeException { reportMetadata(newServiceInstance.getServiceMetadata()); this.doRegister(newServiceInstance); } ","date":"2023-12-25","objectID":"/ooooo-notes/13-%E9%9B%86%E6%88%90-k8s/:2:0","tags":["dubbo","source code","源码分析 dubbo 系列"],"title":"13 集成 k8s","uri":"/ooooo-notes/13-%E9%9B%86%E6%88%90-k8s/"},{"categories":null,"content":"KubernetesServiceDiscovery#getInstances 获取实例 源码位置: org.apache.dubbo.registry.kubernetes.KubernetesServiceDiscovery#getInstances @Override public List\u003cServiceInstance\u003e getInstances(String serviceName) throws NullPointerException { Endpoints endpoints = null; // 从 informer 中获取 SharedIndexInformer\u003cEndpoints\u003e endInformer = ENDPOINTS_INFORMER.get(serviceName); if (endInformer != null) { // get endpoints directly from informer local store List\u003cEndpoints\u003e endpointsList = endInformer.getStore().list(); if (endpointsList.size() \u003e 0) { endpoints = endpointsList.get(0); } } if (endpoints == null) { // 直接获取 endpoints = kubernetesClient .endpoints() .inNamespace(namespace) .withName(serviceName) .get(); } // 根据 k8s 的 endpoint 来获取 return toServiceInstance(endpoints, serviceName); } 源码位置: org.apache.dubbo.registry.kubernetes.KubernetesServiceDiscovery#toServiceInstance // 方法的逻辑：查询出所有的 pod 和 endpoint，以 endpoint 为准，然后对比，挑选出可用的 pod，最终包装为 serviceInstance private List\u003cServiceInstance\u003e toServiceInstance(Endpoints endpoints, String serviceName) { Map\u003cString, String\u003e serviceSelector = getServiceSelector(serviceName); if (serviceSelector == null) { return new LinkedList\u003c\u003e(); } // 获取 pod Map\u003cString, Pod\u003e pods = kubernetesClient .pods() .inNamespace(namespace) .withLabels(serviceSelector) .list() .getItems() .stream() .collect( Collectors.toMap( pod -\u003e pod.getMetadata().getName(), pod -\u003e pod)); List\u003cServiceInstance\u003e instances = new LinkedList\u003c\u003e(); Set\u003cInteger\u003e instancePorts = new HashSet\u003c\u003e(); // 获取 port for (EndpointSubset endpointSubset : endpoints.getSubsets()) { instancePorts.addAll( endpointSubset.getPorts() .stream().map(EndpointPort::getPort) .collect(Collectors.toSet())); } for (EndpointSubset endpointSubset : endpoints.getSubsets()) { for (EndpointAddress address : endpointSubset.getAddresses()) { // 检查 endpoint 和 pod 是否关联， Pod pod = pods.get(address.getTargetRef().getName()); String ip = address.getIp(); // 如果 pod 为 null，说明这个 pod 删除了 if (pod == null) { logger.warn(REGISTRY_UNABLE_MATCH_KUBERNETES, \"\", \"\", \"Unable to match Kubernetes Endpoint address with Pod. \" + \"EndpointAddress Hostname: \" + address.getTargetRef().getName()); continue; } // 遍历所有 port，新建 ServiceInstance instancePorts.forEach(port -\u003e { ServiceInstance serviceInstance = new DefaultServiceInstance(serviceName, ip, port, ScopeModelUtil.getApplicationModel(getUrl().getScopeModel())); // 从 pod 上获取之前的元数据信息 String properties = pod.getMetadata().getAnnotations().get(KUBERNETES_PROPERTIES_KEY); if (StringUtils.isNotEmpty(properties)) { serviceInstance.getMetadata().putAll(JsonUtils.toJavaObject(properties, Map.class)); instances.add(serviceInstance); } else { ... } }); } } return instances; } ","date":"2023-12-25","objectID":"/ooooo-notes/13-%E9%9B%86%E6%88%90-k8s/:3:0","tags":["dubbo","source code","源码分析 dubbo 系列"],"title":"13 集成 k8s","uri":"/ooooo-notes/13-%E9%9B%86%E6%88%90-k8s/"},{"categories":null,"content":" dubbo 基于 3.2.6 版本 dubbo 集成 spring 的实现方式： 提供 ServiceAnnotationPostProcessor 来扫描 @DubboService 注解，导出服务 提供 ReferenceAnnotationBeanPostProcessor 来扫描 @DubboReference 注解，引用服务 提供 SpringExtensionInjector 来获取 spring 的 bean 提供 DubboInfraBeanRegisterPostProcessor 来注册相关类，加载 spring 配置 ","date":"2023-12-24","objectID":"/ooooo-notes/12-%E9%9B%86%E6%88%90-spring/:0:0","tags":["dubbo","source code","源码分析 dubbo 系列"],"title":"12 集成 spring","uri":"/ooooo-notes/12-%E9%9B%86%E6%88%90-spring/"},{"categories":null,"content":"ServiceAnnotationPostProcessor 对一个 HelloService, 会注册两个 beanDefinition，分别为 HelloService ServiceBean\u003cHelloService\u003e 源码位置: org.apache.dubbo.config.spring.beans.factory.annotation.ServiceAnnotationPostProcessor#postProcessBeanFactory @Override public void postProcessBeanFactory(ConfigurableListableBeanFactory beanFactory) throws BeansException { // 扫描方法上的 @DubboService，这个很少用 String[] beanNames = beanFactory.getBeanDefinitionNames(); for (String beanName : beanNames) { BeanDefinition beanDefinition = beanFactory.getBeanDefinition(beanName); Map\u003cString, Object\u003e annotationAttributes = getServiceAnnotationAttributes(beanDefinition); if (annotationAttributes != null) { // process @DubboService at java-config @bean method processAnnotatedBeanDefinition(beanName, (AnnotatedBeanDefinition) beanDefinition, annotationAttributes); } } if (!scanned) { // 扫描类上的 @DubboService，这个继续解析 scanServiceBeans(resolvedPackagesToScan, registry); } } 源码位置: org.apache.dubbo.config.spring.beans.factory.annotation.ServiceAnnotationPostProcessor#scanServiceBeans private void scanServiceBeans(Set\u003cString\u003e packagesToScan, BeanDefinitionRegistry registry) { // 标记已扫描 scanned = true; if (CollectionUtils.isEmpty(packagesToScan)) { return; } // 创建扫描器 DubboClassPathBeanDefinitionScanner scanner = new DubboClassPathBeanDefinitionScanner(registry, environment, resourceLoader); BeanNameGenerator beanNameGenerator = resolveBeanNameGenerator(registry); scanner.setBeanNameGenerator(beanNameGenerator); // 添加注解过滤器，比如 @DubboService for (Class\u003c? extends Annotation\u003e annotationType : serviceAnnotationTypes) { scanner.addIncludeFilter(new AnnotationTypeFilter(annotationType)); } ScanExcludeFilter scanExcludeFilter = new ScanExcludeFilter(); scanner.addExcludeFilter(scanExcludeFilter); // 对每个包都进行扫描 for (String packageToScan : packagesToScan) { // Registers @Service Bean first，这个会注册 spring bean scanner.scan(packageToScan); // Finds all BeanDefinitionHolders of @Service whether @ComponentScan scans or not. Set\u003cBeanDefinitionHolder\u003e beanDefinitionHolders = findServiceBeanDefinitionHolders(scanner, packageToScan, registry, beanNameGenerator); // 有 @DubboService 的 beanDefintion if (!CollectionUtils.isEmpty(beanDefinitionHolders)) { if (logger.isInfoEnabled()) { List\u003cString\u003e serviceClasses = new ArrayList\u003c\u003e(beanDefinitionHolders.size()); for (BeanDefinitionHolder beanDefinitionHolder : beanDefinitionHolders) { serviceClasses.add(beanDefinitionHolder.getBeanDefinition().getBeanClassName()); } logger.info(\"Found \" + beanDefinitionHolders.size() + \" classes annotated by Dubbo @Service under package [\" + packageToScan + \"]: \" + serviceClasses); } for (BeanDefinitionHolder beanDefinitionHolder : beanDefinitionHolders) { // 处理 beanDefinition，很重要 processScannedBeanDefinition(beanDefinitionHolder); servicePackagesHolder.addScannedClass(beanDefinitionHolder.getBeanDefinition().getBeanClassName()); } } else { ... } // 标记已扫描 servicePackagesHolder.addScannedPackage(packageToScan); } } 源码位置: org.apache.dubbo.config.spring.beans.factory.annotation.ServiceAnnotationPostProcessor#processScannedBeanDefinition private void processScannedBeanDefinition(BeanDefinitionHolder beanDefinitionHolder) { Class\u003c?\u003e beanClass = resolveClass(beanDefinitionHolder); // 找到 @DubboService Annotation service = findServiceAnnotation(beanClass); // The attributes of @Service annotation Map\u003cString, Object\u003e serviceAnnotationAttributes = AnnotationUtils.getAttributes(service, true); String serviceInterface = resolveInterfaceName(serviceAnnotationAttributes, beanClass); String annotatedServiceBeanName = beanDefinitionHolder.getBeanName(); // ServiceBean Bean name String beanName = generateServiceBeanName(serviceAnnotationAttributes, serviceInterface); // 构建 ServiceBeanDefinition, 也就是 dubbo 的 ServiceBean, 里面的 ref 属性会引用 spring bean AbstractBeanDefinition serviceBeanDefinition = buildServiceBeanDefinition(serviceAnnotationAttributes, serviceInterface, annotatedServiceBeanName); // 注册 ServiceBeanDefinition registerServiceBeanDefinition(","date":"2023-12-24","objectID":"/ooooo-notes/12-%E9%9B%86%E6%88%90-spring/:1:0","tags":["dubbo","source code","源码分析 dubbo 系列"],"title":"12 集成 spring","uri":"/ooooo-notes/12-%E9%9B%86%E6%88%90-spring/"},{"categories":null,"content":"ReferenceAnnotationBeanPostProcessor 源码位置: org.apache.dubbo.config.spring.beans.factory.annotation.ReferenceAnnotationBeanPostProcessor#postProcessBeanFactory @Override public void postProcessBeanFactory(ConfigurableListableBeanFactory beanFactory) throws BeansException { // 遍历所有的 beanName String[] beanNames = beanFactory.getBeanDefinitionNames(); for (String beanName : beanNames) { Class\u003c?\u003e beanType; // 解析出 beanType if (beanFactory.isFactoryBean(beanName)) { ... beanType = ClassUtils.resolveClass(beanClassName, getClassLoader()); } else { beanType = beanFactory.getType(beanName); } if (beanType != null) { AnnotatedInjectionMetadata metadata = findInjectionMetadata(beanName, beanType, null); try { // 注入字段和方法 prepareInjection(metadata); } catch (BeansException e) { throw e; } catch (Exception e) { throw new IllegalStateException(\"Prepare dubbo reference injection element failed\", e); } } } ... } 源码位置: org.apache.dubbo.config.spring.beans.factory.annotation.ReferenceAnnotationBeanPostProcessor#prepareInjection protected void prepareInjection(AnnotatedInjectionMetadata metadata) throws BeansException { try { //find and register bean definition for @DubboReference/@Reference // 遍历字段 for (AnnotatedFieldElement fieldElement : metadata.getFieldElements()) { if (fieldElement.injectedObject != null) { continue; } Class\u003c?\u003e injectedType = fieldElement.field.getType(); AnnotationAttributes attributes = fieldElement.attributes; // 注册 @DubboReference bean, 也就是 dubbo 的 ReferenceBean String referenceBeanName = registerReferenceBean(fieldElement.getPropertyName(), injectedType, attributes, fieldElement.field); //associate fieldElement and reference bean // 设置关联 fieldElement.injectedObject = referenceBeanName; injectedFieldReferenceBeanCache.put(fieldElement, referenceBeanName); } // 遍历方法 for (AnnotatedMethodElement methodElement : metadata.getMethodElements()) { if (methodElement.injectedObject != null) { continue; } Class\u003c?\u003e injectedType = methodElement.getInjectedType(); AnnotationAttributes attributes = methodElement.attributes; // 注册 @DubboReference bean, 也就是 dubbo 的 ReferenceBean String referenceBeanName = registerReferenceBean(methodElement.getPropertyName(), injectedType, attributes, methodElement.method); //associate methodElement and reference bean // 设置关联 methodElement.injectedObject = referenceBeanName; injectedMethodReferenceBeanCache.put(methodElement, referenceBeanName); } } catch (ClassNotFoundException e) { throw new BeanCreationException(\"prepare reference annotation failed\", e); } } ","date":"2023-12-24","objectID":"/ooooo-notes/12-%E9%9B%86%E6%88%90-spring/:2:0","tags":["dubbo","source code","源码分析 dubbo 系列"],"title":"12 集成 spring","uri":"/ooooo-notes/12-%E9%9B%86%E6%88%90-spring/"},{"categories":null,"content":"SpringExtensionInjector 我们常常需要扩展自己的 filter，如果在这个类中需要获取 spring 的 bean，就会用到这个扩展类。 源码位置: org.apache.dubbo.config.spring.extension.SpringExtensionInjector#getInstance // type: 字段类型 // name: 字段名称 // 字段是需要 setter 方法 public \u003cT\u003e T getInstance(Class\u003cT\u003e type, String name) { if (context == null) { // ignore if spring context is not bound return null; } //check @SPI annotation if (type.isInterface() \u0026\u0026 type.isAnnotationPresent(SPI.class)) { return null; } // 最终调用 spring 的 BeanFactory 来获取 bean T bean = getOptionalBean(context, name, type); if (bean != null) { return bean; } //logger.warn(\"No spring extension (bean) named:\" + name + \", try to find an extension (bean) of type \" + type.getName()); return null; } ","date":"2023-12-24","objectID":"/ooooo-notes/12-%E9%9B%86%E6%88%90-spring/:3:0","tags":["dubbo","source code","源码分析 dubbo 系列"],"title":"12 集成 spring","uri":"/ooooo-notes/12-%E9%9B%86%E6%88%90-spring/"},{"categories":null,"content":"DubboInfraBeanRegisterPostProcessor 源码位置: org.apache.dubbo.config.spring.context.DubboInfraBeanRegisterPostProcessor#postProcessBeanFactory @Override public void postProcessBeanFactory(ConfigurableListableBeanFactory beanFactory) throws BeansException { if (registry != null) { // 注册 ReferenceAnnotationBeanPostProcessor, 负责扫描 @DubboReference ReferenceAnnotationBeanPostProcessor referenceAnnotationBeanPostProcessor = beanFactory.getBean( ReferenceAnnotationBeanPostProcessor.BEAN_NAME, ReferenceAnnotationBeanPostProcessor.class); beanFactory.addBeanPostProcessor(referenceAnnotationBeanPostProcessor); // register PropertySourcesPlaceholderConfigurer bean if not exits DubboBeanUtils.registerPlaceholderConfigurerBeanIfNotExists(beanFactory, registry); } ApplicationModel applicationModel = DubboBeanUtils.getApplicationModel(beanFactory); ModuleModel moduleModel = DubboBeanUtils.getModuleModel(beanFactory); // 初始化 SpringExtensionInjector SpringExtensionInjector.get(applicationModel).init(applicationContext); SpringExtensionInjector.get(moduleModel).init(applicationContext); DubboBeanUtils.getInitializationContext(beanFactory).setApplicationContext(applicationContext); // 将 spring 的 environment 传递到 dubbo 的 environment 中，重要 ConfigurableEnvironment environment = (ConfigurableEnvironment) applicationContext.getEnvironment(); SortedMap\u003cString, String\u003e dubboProperties = EnvironmentUtils.filterDubboProperties(environment); applicationModel.modelEnvironment().getAppConfigMap().putAll(dubboProperties); ... } ","date":"2023-12-24","objectID":"/ooooo-notes/12-%E9%9B%86%E6%88%90-spring/:4:0","tags":["dubbo","source code","源码分析 dubbo 系列"],"title":"12 集成 spring","uri":"/ooooo-notes/12-%E9%9B%86%E6%88%90-spring/"},{"categories":null,"content":" dubbo 基于 3.2.6 版本 在 dubbo 中，filter 是非常核心的组件之一，很多功能都是依靠 filter 来实现的，下面我来介绍几种常用的 filter 实现。 ","date":"2023-12-23","objectID":"/ooooo-notes/11-%E5%B8%B8%E7%94%A8%E8%BF%87%E6%BB%A4%E5%99%A8/:0:0","tags":["dubbo","source code","源码分析 dubbo 系列"],"title":"11 常用过滤器","uri":"/ooooo-notes/11-%E5%B8%B8%E7%94%A8%E8%BF%87%E6%BB%A4%E5%99%A8/"},{"categories":null,"content":"ConsumerContextFilter (consumer 传递隐式参数) 源码位置: org.apache.dubbo.rpc.cluster.filter.support.ConsumerContextFilter @Override public Result invoke(Invoker\u003c?\u003e invoker, Invocation invocation) throws RpcException { ... RpcContext context = RpcContext.getClientAttachment(); context.setAttachment(REMOTE_APPLICATION_KEY, invoker.getUrl().getApplication()); if (invocation instanceof RpcInvocation) { ((RpcInvocation) invocation).setInvoker(invoker); } // 添加 ServerAttachment 参数 ((RpcInvocation) invocation).addObjectAttachments(RpcContext.getServerAttachment().getObjectAttachments()); // 添加 ClientAttachment 参数 Map\u003cString, Object\u003e contextAttachments = RpcContext.getClientAttachment().getObjectAttachments(); if (CollectionUtils.isNotEmptyMap(contextAttachments)) { ((RpcInvocation) invocation).addObjectAttachments(contextAttachments); } ... RpcContext.removeClientResponseContext(); return invoker.invoke(invocation); } ","date":"2023-12-23","objectID":"/ooooo-notes/11-%E5%B8%B8%E7%94%A8%E8%BF%87%E6%BB%A4%E5%99%A8/:1:0","tags":["dubbo","source code","源码分析 dubbo 系列"],"title":"11 常用过滤器","uri":"/ooooo-notes/11-%E5%B8%B8%E7%94%A8%E8%BF%87%E6%BB%A4%E5%99%A8/"},{"categories":null,"content":"ContextFilter (provider 接受隐式参数) 源码位置: org.apache.dubbo.rpc.filter.ContextFilter @Override public Result invoke(Invoker\u003c?\u003e invoker, Invocation invocation) throws RpcException { Map\u003cString, Object\u003e attachments = invocation.getObjectAttachments(); ... // 设置 RemoteApplicationName String remoteApplication = invocation.getAttachment(REMOTE_APPLICATION_KEY); if (StringUtils.isNotEmpty(remoteApplication)) { RpcContext.getServiceContext().setRemoteApplicationName(remoteApplication); } else { RpcContext.getServiceContext().setRemoteApplicationName(context.getAttachment(REMOTE_APPLICATION_KEY)); } // 添加 invocation 中的 attachments（consumer 端传递的隐式参数） if (CollectionUtils.isNotEmptyMap(attachments)) { if (context.getObjectAttachments().size() \u003e 0) { context.getObjectAttachments().putAll(attachments); } else { context.setObjectAttachments(attachments); } } try { context.clearAfterEachInvoke(false); return invoker.invoke(invocation); } finally { context.clearAfterEachInvoke(true); if (context.isAsyncStarted()) { removeContext(); } } } ","date":"2023-12-23","objectID":"/ooooo-notes/11-%E5%B8%B8%E7%94%A8%E8%BF%87%E6%BB%A4%E5%99%A8/:2:0","tags":["dubbo","source code","源码分析 dubbo 系列"],"title":"11 常用过滤器","uri":"/ooooo-notes/11-%E5%B8%B8%E7%94%A8%E8%BF%87%E6%BB%A4%E5%99%A8/"},{"categories":null,"content":"ClassLoaderFilter (provider 设置类加载器) 源码位置: org.apache.dubbo.rpc.filter.ClassLoaderFilter public Result invoke(Invoker\u003c?\u003e invoker, Invocation invocation) throws RpcException { // 获取之前的 classloader ClassLoader stagedClassLoader = Thread.currentThread().getContextClassLoader(); // 获取当前的 classloader ClassLoader effectiveClassLoader; if (invocation.getServiceModel() != null) { effectiveClassLoader = invocation.getServiceModel().getClassLoader(); } else { effectiveClassLoader = invoker.getClass().getClassLoader(); } if (effectiveClassLoader != null) { invocation.put(STAGED_CLASSLOADER_KEY, stagedClassLoader); invocation.put(WORKING_CLASSLOADER_KEY, effectiveClassLoader); Thread.currentThread().setContextClassLoader(effectiveClassLoader); } try { return invoker.invoke(invocation); } finally { // 还原 classloader Thread.currentThread().setContextClassLoader(stagedClassLoader); } } ","date":"2023-12-23","objectID":"/ooooo-notes/11-%E5%B8%B8%E7%94%A8%E8%BF%87%E6%BB%A4%E5%99%A8/:3:0","tags":["dubbo","source code","源码分析 dubbo 系列"],"title":"11 常用过滤器","uri":"/ooooo-notes/11-%E5%B8%B8%E7%94%A8%E8%BF%87%E6%BB%A4%E5%99%A8/"},{"categories":null,"content":"ActiveLimitFilter (consumer 限流) 代码不分析，主要逻辑是获取 active 参数来判断。 ","date":"2023-12-23","objectID":"/ooooo-notes/11-%E5%B8%B8%E7%94%A8%E8%BF%87%E6%BB%A4%E5%99%A8/:4:0","tags":["dubbo","source code","源码分析 dubbo 系列"],"title":"11 常用过滤器","uri":"/ooooo-notes/11-%E5%B8%B8%E7%94%A8%E8%BF%87%E6%BB%A4%E5%99%A8/"},{"categories":null,"content":"ExecuteLimitFilter (provider 限流) 代码不分析，主要逻辑是获取 executes 参数来判断 ","date":"2023-12-23","objectID":"/ooooo-notes/11-%E5%B8%B8%E7%94%A8%E8%BF%87%E6%BB%A4%E5%99%A8/:5:0","tags":["dubbo","source code","源码分析 dubbo 系列"],"title":"11 常用过滤器","uri":"/ooooo-notes/11-%E5%B8%B8%E7%94%A8%E8%BF%87%E6%BB%A4%E5%99%A8/"},{"categories":null,"content":"ExceptionFilter (provider 异常处理) 源码位置: org.apache.dubbo.rpc.filter.ExceptionFilter @Override public void onResponse(Result appResponse, Invoker\u003c?\u003e invoker, Invocation invocation) { // 有异常，并且不是泛化调用 if (appResponse.hasException() \u0026\u0026 GenericService.class != invoker.getInterface()) { try { Throwable exception = appResponse.getException(); // directly throw if it's checked exception // 不是 RuntimeException, 直接 return, 传递到 consumer 端 if (!(exception instanceof RuntimeException) \u0026\u0026 (exception instanceof Exception)) { return; } // directly throw if the exception appears in the signature // 检查方法上声明的异常 try { Method method = invoker.getInterface().getMethod(RpcUtils.getMethodName(invocation), invocation.getParameterTypes()); Class\u003c?\u003e[] exceptionClasses = method.getExceptionTypes(); for (Class\u003c?\u003e exceptionClass : exceptionClasses) { if (exception.getClass().equals(exceptionClass)) { return; } } } catch (NoSuchMethodException e) { return; } // 检查接口和异常类是同一个jar，直接 return，返回给 consumer 端 String serviceFile = ReflectUtils.getCodeBase(invoker.getInterface()); String exceptionFile = ReflectUtils.getCodeBase(exception.getClass()); if (serviceFile == null || exceptionFile == null || serviceFile.equals(exceptionFile)) { return; } // directly throw if it's JDK exception String className = exception.getClass().getName(); // 检查是 JDK 异常，直接 return，返回给 consumer 端 if (className.startsWith(\"java.\") || className.startsWith(\"javax.\")) { return; } // directly throw if it's dubbo exception // 检查是 RpcException，直接 return, 返回给 consumer 端 if (exception instanceof RpcException) { return; } // otherwise, wrap with RuntimeException and throw back to the client // 包装为 RuntimeException appResponse.setException(new RuntimeException(StringUtils.toString(exception))); } catch (Throwable e) { ... } } } ","date":"2023-12-23","objectID":"/ooooo-notes/11-%E5%B8%B8%E7%94%A8%E8%BF%87%E6%BB%A4%E5%99%A8/:6:0","tags":["dubbo","source code","源码分析 dubbo 系列"],"title":"11 常用过滤器","uri":"/ooooo-notes/11-%E5%B8%B8%E7%94%A8%E8%BF%87%E6%BB%A4%E5%99%A8/"},{"categories":null,"content":"GenericFilter (provider 泛化调用) 源码位置: org.apache.dubbo.rpc.filter.GenericFilter @Override public Result invoke(Invoker\u003c?\u003e invoker, Invocation inv) throws RpcException { // 检查是否为泛化调用 if ((inv.getMethodName().equals($INVOKE) || inv.getMethodName().equals($INVOKE_ASYNC)) \u0026\u0026 inv.getArguments() != null \u0026\u0026 inv.getArguments().length == 3 \u0026\u0026 !GenericService.class.isAssignableFrom(invoker.getInterface())) { // 获取泛化调用的 方法名，参数类型，参数值 String name = ((String) inv.getArguments()[0]).trim(); String[] types = (String[]) inv.getArguments()[1]; Object[] args = (Object[]) inv.getArguments()[2]; try { Method method = findMethodByMethodSignature(invoker.getInterface(), name, types, inv.getServiceModel()); Class\u003c?\u003e[] params = method.getParameterTypes(); ... String generic = inv.getAttachment(GENERIC_KEY); // 获取 GENERIC_KEY 参数 if (StringUtils.isBlank(generic)) { generic = getGenericValueFromRpcContext(); } if (StringUtils.isEmpty(generic) || ProtocolUtils.isDefaultGenericSerialization(generic) || ProtocolUtils.isGenericReturnRawResult(generic)) { // 默认序列化方式，比如 Map 装换为 JavaBean args = PojoUtils.realize(args, params, method.getGenericParameterTypes()); } } else if (ProtocolUtils.isGsonGenericSerialization(generic)) { // gson 序列化 args = getGsonGenericArgs(args, method.getGenericParameterTypes()); } else if (ProtocolUtils.isJavaGenericSerialization(generic)) { // java 序列化 ... } else if (ProtocolUtils.isBeanGenericSerialization(generic)) { // bean 序列化，参数需要实现 JavaBeanDescriptor 接口 ... } else if (ProtocolUtils.isProtobufGenericSerialization(generic)) { // protobuf 序列化 } // 构建新的 invocation RpcInvocation rpcInvocation = new RpcInvocation(inv.getTargetServiceUniqueName(), ... // 调用 return invoker.invoke(rpcInvocation); } catch (NoSuchMethodException | ClassNotFoundException e) { throw new RpcException(e.getMessage(), e); } } // 不是泛化调用，直接调用 return invoker.invoke(inv); } ","date":"2023-12-23","objectID":"/ooooo-notes/11-%E5%B8%B8%E7%94%A8%E8%BF%87%E6%BB%A4%E5%99%A8/:7:0","tags":["dubbo","source code","源码分析 dubbo 系列"],"title":"11 常用过滤器","uri":"/ooooo-notes/11-%E5%B8%B8%E7%94%A8%E8%BF%87%E6%BB%A4%E5%99%A8/"},{"categories":null,"content":"AopAutoConfiguration 自动配置类 源码位置: org.springframework.boot.autoconfigure.aop.AopAutoConfiguration @Configuration(proxyBeanMethods = false) // 自动激活 aop 配置, 还会激活 @EnableAspectJAutoProxy 注解 @ConditionalOnProperty(prefix = \"spring.aop\", name = \"auto\", havingValue = \"true\", matchIfMissing = true) public class AopAutoConfiguration { @Configuration(proxyBeanMethods = false) @ConditionalOnClass(Advice.class) static class AspectJAutoProxyingConfiguration { @Configuration(proxyBeanMethods = false) @EnableAspectJAutoProxy(proxyTargetClass = false) // aop 使用 jdk proxy @ConditionalOnProperty(prefix = \"spring.aop\", name = \"proxy-target-class\", havingValue = \"false\") static class JdkDynamicAutoProxyConfiguration { } @Configuration(proxyBeanMethods = false) @EnableAspectJAutoProxy(proxyTargetClass = true) // aop 使用 cglib proxy @ConditionalOnProperty(prefix = \"spring.aop\", name = \"proxy-target-class\", havingValue = \"true\", matchIfMissing = true) static class CglibAutoProxyConfiguration { } } @Configuration(proxyBeanMethods = false) @ConditionalOnMissingClass(\"org.aspectj.weaver.Advice\") // cglib proxy 激活 @ConditionalOnProperty(prefix = \"spring.aop\", name = \"proxy-target-class\", havingValue = \"true\", matchIfMissing = true) static class ClassProxyingConfiguration { @Bean static BeanFactoryPostProcessor forceAutoProxyCreatorToUseClassProxying() { return (beanFactory) -\u003e { if (beanFactory instanceof BeanDefinitionRegistry) { BeanDefinitionRegistry registry = (BeanDefinitionRegistry) beanFactory; // 注册 aop 相关类 AopConfigUtils.registerAutoProxyCreatorIfNecessary(registry); // 强制使用 proxyTargetClass = true AopConfigUtils.forceAutoProxyCreatorToUseClassProxying(registry); } }; } } } 源码位置: org.springframework.aop.config.AopConfigUtils#registerAutoProxyCreatorIfNecessary // 注册 aop 相关类 @Nullable public static BeanDefinition registerAutoProxyCreatorIfNecessary( BeanDefinitionRegistry registry, @Nullable Object source) { return registerOrEscalateApcAsRequired(InfrastructureAdvisorAutoProxyCreator.class, registry, source); } private static BeanDefinition registerOrEscalateApcAsRequired( Class\u003c?\u003e cls, BeanDefinitionRegistry registry, @Nullable Object source) { Assert.notNull(registry, \"BeanDefinitionRegistry must not be null\"); // 检查之前是否注册过 if (registry.containsBeanDefinition(AUTO_PROXY_CREATOR_BEAN_NAME)) { BeanDefinition apcDefinition = registry.getBeanDefinition(AUTO_PROXY_CREATOR_BEAN_NAME); // 和当前注册类不一样 if (!cls.getName().equals(apcDefinition.getBeanClassName())) { int currentPriority = findPriorityForClass(apcDefinition.getBeanClassName()); int requiredPriority = findPriorityForClass(cls); // 比较优先级 if (currentPriority \u003c requiredPriority) { // 重新设置 beanClassName apcDefinition.setBeanClassName(cls.getName()); } } return null; } // 之前没有注册过 RootBeanDefinition beanDefinition = new RootBeanDefinition(cls); beanDefinition.setSource(source); // order 是最大值 beanDefinition.getPropertyValues().add(\"order\", Ordered.HIGHEST_PRECEDENCE); // 基础类 beanDefinition.setRole(BeanDefinition.ROLE_INFRASTRUCTURE); // 注册 beanDefinition registry.registerBeanDefinition(AUTO_PROXY_CREATOR_BEAN_NAME, beanDefinition); return beanDefinition; } 源码位置: org.springframework.aop.config.AopConfigUtils // 从上到下，优先级依次变高 static { // Set up the escalation list... APC_PRIORITY_LIST.add(InfrastructureAdvisorAutoProxyCreator.class); APC_PRIORITY_LIST.add(AspectJAwareAdvisorAutoProxyCreator.class); APC_PRIORITY_LIST.add(AnnotationAwareAspectJAutoProxyCreator.class); } ","date":"2023-12-18","objectID":"/ooooo-notes/spring-aop-%E5%8E%9F%E7%90%86/:1:0","tags":["spring boot","source code","源码分析 spring boot 系列"],"title":"spring aop 原理","uri":"/ooooo-notes/spring-aop-%E5%8E%9F%E7%90%86/"},{"categories":null,"content":"@EnableAspectJAutoProxy 源码位置: org.springframework.context.annotation.EnableAspectJAutoProxy @Target(ElementType.TYPE) @Retention(RetentionPolicy.RUNTIME) @Documented // 导入配置类 @Import(AspectJAutoProxyRegistrar.class) public @interface EnableAspectJAutoProxy { /** * Indicate whether subclass-based (CGLIB) proxies are to be created as opposed * to standard Java interface-based proxies. The default is {@code false}. */ // 如果为 true，则为 cglib proxy boolean proxyTargetClass() default false; /** * Indicate that the proxy should be exposed by the AOP framework as a {@code ThreadLocal} * for retrieval via the {@link org.springframework.aop.framework.AopContext} class. * Off by default, i.e. no guarantees that {@code AopContext} access will work. * @since 4.3.1 */ boolean exposeProxy() default false; } 源码位置: org.springframework.context.annotation.AspectJAutoProxyRegistrar class AspectJAutoProxyRegistrar implements ImportBeanDefinitionRegistrar { /** * Register, escalate, and configure the AspectJ auto proxy creator based on the value * of the @{@link EnableAspectJAutoProxy#proxyTargetClass()} attribute on the importing * {@code @Configuration} class. */ @Override public void registerBeanDefinitions( AnnotationMetadata importingClassMetadata, BeanDefinitionRegistry registry) { // 激活 aop 相关类 AnnotationAwareAspectJAutoProxyCreator AopConfigUtils.registerAspectJAnnotationAutoProxyCreatorIfNecessary(registry); AnnotationAttributes enableAspectJAutoProxy = AnnotationConfigUtils.attributesFor(importingClassMetadata, EnableAspectJAutoProxy.class); if (enableAspectJAutoProxy != null) { if (enableAspectJAutoProxy.getBoolean(\"proxyTargetClass\")) { // 强制使用 proxyTargetClass = true, 使用 cglib 来实现代理 AopConfigUtils.forceAutoProxyCreatorToUseClassProxying(registry); } if (enableAspectJAutoProxy.getBoolean(\"exposeProxy\")) { // 强制使用 exposeProxy = true, 可以用 AopContext#currentProxy 获取代理对象 AopConfigUtils.forceAutoProxyCreatorToExposeProxy(registry); } } } } ","date":"2023-12-18","objectID":"/ooooo-notes/spring-aop-%E5%8E%9F%E7%90%86/:2:0","tags":["spring boot","source code","源码分析 spring boot 系列"],"title":"spring aop 原理","uri":"/ooooo-notes/spring-aop-%E5%8E%9F%E7%90%86/"},{"categories":null,"content":"AnnotationAwareAspectJAutoProxyCreator 这里以 AnnotationAwareAspectJAutoProxyCreator 为例, 当我们添加了 org.springframework.boot:spring-boot-starter-aop 依赖后就会激活这个类。 源码位置: org.springframework.aop.framework.autoproxy.AbstractAutoProxyCreator#postProcessBeforeInstantiation // bean 实例化之前会执行这个方法 @Override public Object postProcessBeforeInstantiation(Class\u003c?\u003e beanClass, String beanName) { Object cacheKey = getCacheKey(beanClass, beanName); if (!StringUtils.hasLength(beanName) || !this.targetSourcedBeans.contains(beanName)) { if (this.advisedBeans.containsKey(cacheKey)) { return null; } // 基础类 或者 需要跳过 if (isInfrastructureClass(beanClass) || shouldSkip(beanClass, beanName)) { this.advisedBeans.put(cacheKey, Boolean.FALSE); return null; } } // Create proxy here if we have a custom TargetSource. // Suppresses unnecessary default instantiation of the target bean: // The TargetSource will handle target instances in a custom fashion. // 获取自定义的 targetSource, 默认为空，所以不会在这个方法中生成代理对象 TargetSource targetSource = getCustomTargetSource(beanClass, beanName); if (targetSource != null) { if (StringUtils.hasLength(beanName)) { this.targetSourcedBeans.add(beanName); } // 获取这个类的 advisor Object[] specificInterceptors = getAdvicesAndAdvisorsForBean(beanClass, beanName, targetSource); // 创建代理对象 Object proxy = createProxy(beanClass, beanName, specificInterceptors, targetSource); this.proxyTypes.put(cacheKey, proxy.getClass()); return proxy; } // 返回 null，会继续实例化 return null; } 源码位置: org.springframework.aop.framework.autoproxy.AbstractAutoProxyCreator#postProcessAfterInitialization // bean 初始化之后会执行这个方法 @Override public Object postProcessAfterInitialization(@Nullable Object bean, String beanName) { if (bean != null) { Object cacheKey = getCacheKey(bean.getClass(), beanName); // 如果没有 early getBean, 这里就是 null if (this.earlyProxyReferences.remove(cacheKey) != bean) { // 创建代理对象, 后面继续解析 return wrapIfNecessary(bean, beanName, cacheKey); } } return bean; } 源码位置: org.springframework.aop.framework.autoproxy.AbstractAutoProxyCreator#wrapIfNecessary protected Object wrapIfNecessary(Object bean, String beanName, Object cacheKey) { // 有自定义的 targetSource, 则跳过 if (StringUtils.hasLength(beanName) \u0026\u0026 this.targetSourcedBeans.contains(beanName)) { return bean; } // 不需要代理 if (Boolean.FALSE.equals(this.advisedBeans.get(cacheKey))) { return bean; } // 基础类 或者 应该跳过 if (isInfrastructureClass(bean.getClass()) || shouldSkip(bean.getClass(), beanName)) { this.advisedBeans.put(cacheKey, Boolean.FALSE); return bean; } // Create proxy if we have advice. // 获取 advisor, 这个会获取 @Aspect，Advisor，后面会继续解析 Object[] specificInterceptors = getAdvicesAndAdvisorsForBean(bean.getClass(), beanName, null); if (specificInterceptors != DO_NOT_PROXY) { this.advisedBeans.put(cacheKey, Boolean.TRUE); // 创建代理对象，后面会继续解析 Object proxy = createProxy( bean.getClass(), beanName, specificInterceptors, new SingletonTargetSource(bean)); this.proxyTypes.put(cacheKey, proxy.getClass()); return proxy; } // 标记不需要代理 this.advisedBeans.put(cacheKey, Boolean.FALSE); return bean; } 源码位置: org.springframework.aop.framework.autoproxy.AbstractAdvisorAutoProxyCreator#getAdvicesAndAdvisorsForBean // 获取 advisor @Override @Nullable protected Object[] getAdvicesAndAdvisorsForBean( Class\u003c?\u003e beanClass, String beanName, @Nullable TargetSource targetSource) { // 找到合适的 advisor List\u003cAdvisor\u003e advisors = findEligibleAdvisors(beanClass, beanName); if (advisors.isEmpty()) { // 返回 null，不需要代理 return DO_NOT_PROXY; } return advisors.toArray(); } // 找到合适的 advisor protected List\u003cAdvisor\u003e findEligibleAdvisors(Class\u003c?\u003e beanClass, String beanName) { // 找到所有的 advisor bean, 包括 @Aspect List\u003cAdvisor\u003e candidateAdvisors = findCandidateAdvisors(); // 判断 advisor 是否能应用到 bean, ClassFilter 和 MethodMatcher List\u003cAdvisor\u003e eligibleAdvisors = findAdvisorsThatCanApply(candidateAdvisors, beanClass, beanName); // 子类扩展，目前会添加 ExposeInvocationInterceptor extendAdvisors(eligibleAdvisors); if (!eligibleAdvisors.isEmpty()) { // 对 advisor 排序 eligibleAdvisors = sortAdvisors(eligibleAdvisors); } ","date":"2023-12-18","objectID":"/ooooo-notes/spring-aop-%E5%8E%9F%E7%90%86/:3:0","tags":["spring boot","source code","源码分析 spring boot 系列"],"title":"spring aop 原理","uri":"/ooooo-notes/spring-aop-%E5%8E%9F%E7%90%86/"},{"categories":null,"content":" dubbo 基于 3.2.6 版本 在 dubbo 2.x 中，最常用的协议就是 dubbo 协议，我们有必要弄懂整个实现过程。 ","date":"2023-12-07","objectID":"/ooooo-notes/09-dubbo-%E5%8D%8F%E8%AE%AE/:0:0","tags":["dubbo","source code","源码分析 dubbo 系列"],"title":"09 dubbo 协议","uri":"/ooooo-notes/09-dubbo-%E5%8D%8F%E8%AE%AE/"},{"categories":null,"content":"export 导出服务 源码位置: org.apache.dubbo.rpc.protocol.dubbo.DubboProtocol#export @Override public \u003cT\u003e Exporter\u003cT\u003e export(Invoker\u003cT\u003e invoker) throws RpcException { checkDestroyed(); URL url = invoker.getUrl(); String key = serviceKey(url); // 添加到 exporterMap DubboExporter\u003cT\u003e exporter = new DubboExporter\u003cT\u003e(invoker, key, exporterMap); ... // 打开服务，会监听端口 openServer(url); // 优化序列化，不用太关心 optimizeSerialization(url); return exporter; } 源码位置: org.apache.dubbo.rpc.protocol.dubbo.DubboProtocol#openServer private void openServer(URL url) { checkDestroyed(); String key = url.getAddress(); // 判断是否为 server 端 boolean isServer = url.getParameter(IS_SERVER_KEY, true); if (isServer) { // 延迟初始化 ProtocolServer server = serverMap.get(key); if (server == null) { synchronized (this) { server = serverMap.get(key); if (server == null) { // 创建服务 serverMap.put(key, createServer(url)); return; } } } // server supports reset, use together with override server.reset(url); } } 源码位置: org.apache.dubbo.rpc.protocol.dubbo.DubboProtocol#createServer private ProtocolServer createServer(URL url) { url = URLBuilder.from(url) .addParameterIfAbsent(CHANNEL_READONLYEVENT_SENT_KEY, Boolean.TRUE.toString()) // 心跳 .addParameterIfAbsent(HEARTBEAT_KEY, String.valueOf(DEFAULT_HEARTBEAT)) // 编解码 .addParameter(CODEC_KEY, DubboCodec.NAME) .build(); // 使用 netty String transporter = url.getParameter(SERVER_KEY, DEFAULT_REMOTING_SERVER); if (StringUtils.isNotEmpty(transporter) \u0026\u0026 !url.getOrDefaultFrameworkModel().getExtensionLoader(Transporter.class).hasExtension(transporter)) { throw new RpcException(\"Unsupported server type: \" + transporter + \", url: \" + url); } ExchangeServer server; try { // 绑定端口, 设置 requestHandler，因为 client 和 server 都是同一个 requestHandler, 最后再解析 server = Exchangers.bind(url, requestHandler); } catch (RemotingException e) { throw new RpcException(\"Fail to start server(url: \" + url + \") \" + e.getMessage(), e); } ... return protocolServer; } ","date":"2023-12-07","objectID":"/ooooo-notes/09-dubbo-%E5%8D%8F%E8%AE%AE/:1:0","tags":["dubbo","source code","源码分析 dubbo 系列"],"title":"09 dubbo 协议","uri":"/ooooo-notes/09-dubbo-%E5%8D%8F%E8%AE%AE/"},{"categories":null,"content":"refer 引用服务 源码位置: org.apache.dubbo.rpc.protocol.dubbo.DubboProtocol#refer @Override public \u003cT\u003e Invoker\u003cT\u003e refer(Class\u003cT\u003e type, URL url) throws RpcException { checkDestroyed(); return protocolBindingRefer(type, url); } @Override public \u003cT\u003e Invoker\u003cT\u003e protocolBindingRefer(Class\u003cT\u003e serviceType, URL url) throws RpcException { checkDestroyed(); // 优化序列化，不需要关心 optimizeSerialization(url); // 获取 clients, 创建 dubboInvoker DubboInvoker\u003cT\u003e invoker = new DubboInvoker\u003cT\u003e(serviceType, url, getClients(url), invokers); invokers.add(invoker); return invoker; } 源码位置: org.apache.dubbo.rpc.protocol.dubbo.DubboProtocol#getClients private ClientsProvider getClients(URL url) { // 获取连接数，0表示共享一个连接 int connections = url.getParameter(CONNECTIONS_KEY, 0); // whether to share connection // if not configured, connection is shared, otherwise, one connection for one service if (connections == 0) { ... // 获取共享client，最终调用 initClient 方法 return getSharedClient(url, connections); } // 获取多个client List\u003cExchangeClient\u003e clients = IntStream.range(0, connections) .mapToObj((i) -\u003e initClient(url)) .collect(Collectors.toList()); return new ExclusiveClientsProvider(clients); } 源码位置: org.apache.dubbo.rpc.protocol.dubbo.DubboProtocol#initClient private ExchangeClient initClient(URL url) { // 使用 netty String str = url.getParameter(CLIENT_KEY, url.getParameter(SERVER_KEY, DEFAULT_REMOTING_CLIENT)); ... try { ScopeModel scopeModel = url.getScopeModel(); int heartbeat = UrlUtils.getHeartbeat(url); // Replace InstanceAddressURL with ServiceConfigURL. url = new ServiceConfigURL(DubboCodec.NAME, url.getUsername(), url.getPassword(), url.getHost(), url.getPort(), url.getPath(), url.getAllParameters()); // 编解码 url = url.addParameter(CODEC_KEY, DubboCodec.NAME); // 心跳 url = url.addParameterIfAbsent(HEARTBEAT_KEY, Integer.toString(heartbeat)); url = url.setScopeModel(scopeModel); // connection should be lazy return url.getParameter(LAZY_CONNECT_KEY, false) ? new LazyConnectExchangeClient(url, requestHandler) // 连接端口，设置 requestHandler : Exchangers.connect(url, requestHandler); } catch (RemotingException e) { throw new RpcException(\"Fail to create remoting client for service(\" + url + \"): \" + e.getMessage(), e); } } ","date":"2023-12-07","objectID":"/ooooo-notes/09-dubbo-%E5%8D%8F%E8%AE%AE/:2:0","tags":["dubbo","source code","源码分析 dubbo 系列"],"title":"09 dubbo 协议","uri":"/ooooo-notes/09-dubbo-%E5%8D%8F%E8%AE%AE/"},{"categories":null,"content":"requestHandler client 和 server 共用的请求处理器 源码位置: org.apache.dubbo.remoting.exchange.support.ExchangeHandlerAdapter#reply @Override public CompletableFuture\u003cObject\u003e reply(ExchangeChannel channel, Object message) throws RemotingException { ... Invocation inv = (Invocation) message; // 获取 invoker Invoker\u003c?\u003e invoker = inv.getInvoker() == null ? getInvoker(channel, inv) : inv.getInvoker(); // switch TCCL if (invoker.getUrl().getServiceModel() != null) { Thread.currentThread().setContextClassLoader(invoker.getUrl().getServiceModel().getClassLoader()); } // 判断回调方法是否存在 if (Boolean.TRUE.toString().equals(inv.getObjectAttachmentWithoutConvert(IS_CALLBACK_SERVICE_INVOKE))) { String methodsStr = invoker.getUrl().getParameters().get(\"methods\"); boolean hasMethod = false; if (methodsStr == null || !methodsStr.contains(\",\")) { hasMethod = inv.getMethodName().equals(methodsStr); } else { String[] methods = methodsStr.split(\",\"); for (String method : methods) { if (inv.getMethodName().equals(method)) { hasMethod = true; break; } } } if (!hasMethod) { logger.warn(PROTOCOL_FAILED_REFER_INVOKER, \"\", \"\", new IllegalStateException(\"The methodName \" + inv.getMethodName() + \" not found in callback service interface ,invoke will be ignored.\" + \" please update the api interface. url is:\" + invoker.getUrl()) + \" ,invocation is :\" + inv); return null; } } RpcContext.getServiceContext().setRemoteAddress(channel.getRemoteAddress()); // 调用业务接口，返回 AsyncRpcResult Result result = invoker.invoke(inv); return result.thenApply(Function.identity()); } ","date":"2023-12-07","objectID":"/ooooo-notes/09-dubbo-%E5%8D%8F%E8%AE%AE/:3:0","tags":["dubbo","source code","源码分析 dubbo 系列"],"title":"09 dubbo 协议","uri":"/ooooo-notes/09-dubbo-%E5%8D%8F%E8%AE%AE/"},{"categories":null,"content":" dubbo 基于 3.2.6 版本 在 dubbo 3.x 中，新增了一种协议，那就是 triple 协议，可以兼容 grpc 协议, 这两个协议的底层都是 http2 协议。 triple 协议实现的比较复杂，所以我会把关键代码贴出来。 ","date":"2023-12-07","objectID":"/ooooo-notes/10-triple-%E5%8D%8F%E8%AE%AE/:0:0","tags":["dubbo","source code","源码分析 dubbo 系列"],"title":"10 triple 协议","uri":"/ooooo-notes/10-triple-%E5%8D%8F%E8%AE%AE/"},{"categories":null,"content":"export 导出服务 源码位置: org.apache.dubbo.rpc.protocol.tri.TripleProtocol#export @Override public \u003cT\u003e Exporter\u003cT\u003e export(Invoker\u003cT\u003e invoker) throws RpcException { URL url = invoker.getUrl(); String key = serviceKey(url); ... invokers.add(invoker); // 添加到 pathResolver, 这个很关键 Invoker\u003c?\u003e previous = pathResolver.add(url.getServiceKey(), invoker); if (previous != null) { ... } ... // 初始化线程池 ExecutorRepository.getInstance(url.getOrDefaultApplicationModel()).createExecutorIfAbsent(ExecutorUtil.setThreadName(url, SERVER_THREAD_POOL_NAME)); // 绑定端口，开启服务, 注意 DefaultPuHandler 是空实现，这是和 DubboProtocol 实现的主要区别 PortUnificationExchanger.bind(url, new DefaultPuHandler()); // 序列化，不用关心 optimizeSerialization(url); return exporter; } 源码位置: org.apache.dubbo.remoting.transport.netty4.NettyPortUnificationTransporter#bind // PortUnificationExchanger#bind 最终会调用此方法 // 在 NettyPortUnificationServer 的父类构造函数中会调用 doOpen 方法 @Override public AbstractPortUnificationServer bind(URL url, ChannelHandler handler) throws RemotingException { return new NettyPortUnificationServer(url, handler); } 源码位置: org.apache.dubbo.remoting.transport.netty4.NettyPortUnificationServer#doOpen // 下面的代码是标准的 netty 代码， 我们只需要关注其中的 channelHandler 就可以了 public void doOpen() throws Throwable { bootstrap = new ServerBootstrap(); ... bootstrap.group(bossGroup, workerGroup) .channel(NettyEventLoopFactory.serverSocketChannelClass()) .option(ChannelOption.SO_REUSEADDR, Boolean.TRUE) .childOption(ChannelOption.TCP_NODELAY, Boolean.TRUE) .childOption(ChannelOption.ALLOCATOR, PooledByteBufAllocator.DEFAULT) .childHandler(new ChannelInitializer\u003cSocketChannel\u003e() { @Override protected void initChannel(SocketChannel ch) throws Exception { // Do not add idle state handler here, because it should be added in the protocol handler. final ChannelPipeline p = ch.pipeline(); // 负责心跳，不用关心 NettyChannelHandler nettyChannelHandler = new NettyChannelHandler(dubboChannels, getUrl(), NettyPortUnificationServer.this); // puHandler 是最重要的 channelHandler, 负责检测是 grpc 还是 triple 协议 NettyPortUnificationServerHandler puHandler = new NettyPortUnificationServerHandler(getUrl(), true, getProtocols(), NettyPortUnificationServer.this, getSupportedUrls(), getSupportedHandlers()); p.addLast(\"channel-handler\", nettyChannelHandler); p.addLast(\"negotiation-protocol\", puHandler); } }); ... } 源码位置: org.apache.dubbo.remoting.transport.netty4.NettyPortUnificationServerHandler#decode // 当接受到请求时，netty 会回调这个方法 @Override protected void decode(ChannelHandlerContext ctx, ByteBuf in, List\u003cObject\u003e out) throws Exception { NettyChannel channel = NettyChannel.getOrAddChannel(ctx.channel(), url, handler); ... if (providerConnectionConfig != null \u0026\u0026 isSsl(in)) { // 检测 SSL，就是判断前5个字符 enableSsl(ctx, providerConnectionConfig); } else { // 检测是 grpc 还是 triple for (final WireProtocol protocol : protocols) { in.markReaderIndex(); ChannelBuffer buf = new NettyBackedChannelBuffer(in); final ProtocolDetector.Result result = protocol.detector().detect(buf); in.resetReaderIndex(); switch (result) { case UNRECOGNIZED: continue; case RECOGNIZED: String protocolName = url.getOrDefaultFrameworkModel().getExtensionLoader(WireProtocol.class) .getExtensionName(protocol); // 获取 handler 和 url， 不用关心 ChannelHandler localHandler = this.handlerMapper.getOrDefault(protocolName, handler); URL localURL = this.urlMapper.getOrDefault(protocolName, url); channel.setUrl(localURL); NettyConfigOperator operator = new NettyConfigOperator(channel, localHandler); // 配置 channelHandler，非常重要，后面继续解析 protocol.configServerProtocolHandler(url, operator); // 移除当前 channelHandler，下一次就不需要在检测了 ctx.pipeline().remove(this); case NEED_MORE_DATA: return; default: return; } } ... } } 源码位置: org.apache.dubbo.rpc.protocol.tri.TripleHttp2Protocol#configServerProtocolHandler // 配置 http2 相关的 channelHandler @Override public void configServerProtocolHandler(URL url, ChannelOperator operator) { ... final Http2FrameCodec codec = TripleHttp2FrameCodecBuilder.forServer() ... .build(); ExecutorSupport executorSupport = Executo","date":"2023-12-07","objectID":"/ooooo-notes/10-triple-%E5%8D%8F%E8%AE%AE/:1:0","tags":["dubbo","source code","源码分析 dubbo 系列"],"title":"10 triple 协议","uri":"/ooooo-notes/10-triple-%E5%8D%8F%E8%AE%AE/"},{"categories":null,"content":"TripleHttp2FrameServerHandler 接受客户端的消息 源码位置: org.apache.dubbo.rpc.protocol.tri.transport.TripleHttp2FrameServerHandler#TripleHttp2FrameServerHandler // http2 中的每个 stream 都会接受到回调方法 @Override public void channelRead(ChannelHandlerContext ctx, Object msg) throws Exception { if (msg instanceof Http2HeadersFrame) { // 处理 Http2HeadersFrame， 读取 serviceName，methodName onHeadersRead(ctx, (Http2HeadersFrame) msg); } else if (msg instanceof Http2DataFrame) { // 处理 Http2DataFrame, 读取数据 onDataRead(ctx, (Http2DataFrame) msg); } else if (msg instanceof ReferenceCounted) { // ignored ReferenceCountUtil.release(msg); } } // 处理 Http2DataFrame public void onDataRead(ChannelHandlerContext ctx, Http2DataFrame msg) throws Exception { tripleServerStream.transportObserver.onData(msg.content(), msg.isEndStream()); } // 处理 Http2HeadersFrame public void onHeadersRead(ChannelHandlerContext ctx, Http2HeadersFrame msg) throws Exception { Executor executor = executorSupport.getExecutor(msg.headers()); tripleServerStream.setExecutor(executor); // 调用 ServerTransportObserver#onHeader 方法 tripleServerStream.transportObserver.onHeader(msg.headers(), msg.isEndStream()); } 源码位置: org.apache.dubbo.rpc.protocol.tri.stream.TripleServerStream.ServerTransportObserver#onHeader @Override public void onHeader(Http2Headers headers, boolean endStream) { executor.execute(() -\u003e processHeader(headers, endStream)); } private void processHeader(Http2Headers headers, boolean endStream) { ... String[] parts = path.split(\"/\"); if (parts.length != 3) { responseErr(TriRpcStatus.UNIMPLEMENTED.withDescription(\"Bad path format:\" + path)); return; } String serviceName = parts[1]; String originalMethodName = parts[2]; // 从 pathResolver 中获取 invoker Invoker\u003c?\u003e invoker = getInvoker(headers, serviceName); ... // headers 转换为 map Map\u003cString, Object\u003e requestMetadata = headersToMap(headers, () -\u003e { return Optional.ofNullable(headers.get(TripleHeaderEnum.TRI_HEADER_CONVERT.getHeader())) .map(CharSequence::toString) .orElse(null); }); boolean hasStub = pathResolver.hasNativeStub(path); if (hasStub) { listener = new StubAbstractServerCall(invoker, TripleServerStream.this, frameworkModel, acceptEncoding, serviceName, originalMethodName, executor); } else { // 常用的就是这个，下面以这个为例子 listener = new ReflectionAbstractServerCall(invoker, TripleServerStream.this, frameworkModel, acceptEncoding, serviceName, originalMethodName, filters, executor); } // must before onHeader deframer = new TriDecoder(deCompressor, new ServerDecoderListener(listener)); // 根据 methodDescriptor 来获取最终调用的 listener，非常重要 listener.onHeader(requestMetadata); } 源码位置: org.apache.dubbo.rpc.protocol.tri.call.AbstractServerCall#onHeader @Override public void onHeader(Map\u003cString, Object\u003e requestMetadata) { this.requestMetadata = requestMetadata; ... startCall(); } // 注意 startCall 应该调用子类的方法，在这里忽略，直接分析父类的方法逻辑 protected void startCall() { // 构建 RpcInvocation RpcInvocation invocation = buildInvocation(methodDescriptor); // 非常重要 listener = startInternalCall(invocation, methodDescriptor, invoker); } 源码位置: org.apache.dubbo.rpc.protocol.tri.call.AbstractServerCall#startInternalCall 设置调用监听器, 比如 UnaryServerCallListener, ServerStreamServerCallListener, BiStreamServerCallListener protected ServerCall.Listener startInternalCall( RpcInvocation invocation, MethodDescriptor methodDescriptor, Invoker\u003c?\u003e invoker) { this.cancellationContext = RpcContext.getCancellationContext(); ServerCallToObserverAdapter\u003cObject\u003e responseObserver = new ServerCallToObserverAdapter\u003c\u003e(this, cancellationContext); try { ServerCall.Listener listener; switch (methodDescriptor.getRpcType()) { case UNARY: listener = new UnaryServerCallListener(invocation, invoker, responseObserver, packableMethod.needWrapper()); request(2); break; case SERVER_STREAM: listener = new ServerStreamServerCallListener(invocation, invoker, responseObserver); request(2); break; case BI_STREAM: case CLIENT_STREAM: listener = new BiStreamServerCallListener(invocation, invoker, responseObserver); request(1); break; def","date":"2023-12-07","objectID":"/ooooo-notes/10-triple-%E5%8D%8F%E8%AE%AE/:2:0","tags":["dubbo","source code","源码分析 dubbo 系列"],"title":"10 triple 协议","uri":"/ooooo-notes/10-triple-%E5%8D%8F%E8%AE%AE/"},{"categories":null,"content":"refer 引用服务 源码位置: org.apache.dubbo.rpc.protocol.tri.TripleProtocol#refer @Override public \u003cT\u003e Invoker\u003cT\u003e refer(Class\u003cT\u003e type, URL url) throws RpcException { // 序列化优化，不用关心 optimizeSerialization(url); ExecutorService streamExecutor = getOrCreateStreamExecutor( url.getOrDefaultApplicationModel(), url); // 连接端口，注意 DefaultPuHandler 是空实现，这是和 DubboProtocol 实现的主要区别 AbstractConnectionClient connectionClient = PortUnificationExchanger.connect(url, new DefaultPuHandler()); // 包装为 tripleInvoker TripleInvoker\u003cT\u003e invoker = new TripleInvoker\u003c\u003e(type, url, acceptEncodings, connectionClient, invokers, streamExecutor); invokers.add(invoker); return invoker; } 源码位置: org.apache.dubbo.remoting.transport.netty4.NettyPortUnificationTransporter#connect // PortUnificationExchanger#connect 最终会调用此方法 @Override public AbstractConnectionClient connect(URL url, ChannelHandler handler) throws RemotingException { ConnectionManager manager = url.getOrDefaultFrameworkModel().getExtensionLoader(ConnectionManager.class).getExtension(MultiplexProtocolConnectionManager.NAME); // 连接, 最终会调用 NettyConnectionManager#connect 方法 return manager.connect(url, handler); } 源码位置: org.apache.dubbo.remoting.transport.netty4.NettyConnectionManager#connect // 在 NettyConnectionClient 的父类构造方法中会调用 doOpen 和 doConnect 方法 @Override public AbstractConnectionClient connect(URL url, ChannelHandler handler) { try { return new NettyConnectionClient(url, handler); } catch (RemotingException e) { throw new RuntimeException(e); } } 源码位置: org.apache.dubbo.remoting.transport.netty4.NettyConnectionClient#doOpen @Override protected void doOpen() throws Throwable { initConnectionClient(); // 初始化 netty 的 bootstrap, 设置了 http2 的编解码 initBootstrap(); } @Override protected void doConnect() throws RemotingException { ... createConnectingPromise(); // 连接端口 final ChannelFuture promise = bootstrap.connect(); ...忽略错误处理逻辑 } ","date":"2023-12-07","objectID":"/ooooo-notes/10-triple-%E5%8D%8F%E8%AE%AE/:3:0","tags":["dubbo","source code","源码分析 dubbo 系列"],"title":"10 triple 协议","uri":"/ooooo-notes/10-triple-%E5%8D%8F%E8%AE%AE/"},{"categories":null,"content":"TripleInvoker 当客户端调用方法时，就会执行 TripleInvoker#doInvoke 方法, 接下来分析这部分逻辑。 源码位置: org.apache.dubbo.rpc.protocol.tri.TripleInvoker#doInvoke // 根据不同的方式来调用，比如 invokeUnary, invokeServerStream, invokeBiOrClientStream @Override protected Result doInvoke(final Invocation invocation) { ... ClientCall call = new TripleClientCall(connectionClient, callbackExecutor, getUrl().getOrDefaultFrameworkModel(), writeQueue); AsyncRpcResult result; try { switch (methodDescriptor.getRpcType()) { case UNARY: // 重点分析这个 result = invokeUnary(methodDescriptor, invocation, call, callbackExecutor); break; case SERVER_STREAM: result = invokeServerStream(methodDescriptor, invocation, call); break; case CLIENT_STREAM: case BI_STREAM: result = invokeBiOrClientStream(methodDescriptor, invocation, call); break; default: throw new IllegalStateException(\"Can not reach here\"); } return result; } catch (Throwable t) { ...省略处理错误逻辑 } } 源码位置: org.apache.dubbo.rpc.protocol.tri.TripleInvoker#invokeUnary // 一元请求逻辑 AsyncRpcResult invokeUnary(MethodDescriptor methodDescriptor, Invocation invocation, ClientCall call, ExecutorService callbackExecutor) { ... final AsyncRpcResult result; DeadlineFuture future = DeadlineFuture.newFuture(getUrl().getPath(), methodDescriptor.getMethodName(), getUrl().getAddress(), timeout, callbackExecutor); RequestMetadata request = createRequest(methodDescriptor, invocation, timeout); final Object pureArgument; // 封装参数 if (methodDescriptor instanceof StubMethodDescriptor) { pureArgument = invocation.getArguments()[0]; } else { if (methodDescriptor.isGeneric()) { Object[] args = new Object[3]; args[0] = RpcUtils.getMethodName(invocation); args[1] = Arrays.stream(RpcUtils.getParameterTypes(invocation)).map(Class::getName).collect(Collectors.toList()); args[2] = RpcUtils.getArguments(invocation); pureArgument = args; } else { pureArgument = invocation.getArguments(); } } result = new AsyncRpcResult(future, invocation); ... ClientCall.Listener callListener = new UnaryClientCallListener(future); // start 方法非常重要，创建了 TripleClientStream, 并设置 channelHandler final StreamObserver\u003cObject\u003e requestObserver = call.start(request, callListener); // 发送请求 requestObserver.onNext(pureArgument); requestObserver.onCompleted(); return result; } 源码位置: org.apache.dubbo.rpc.protocol.tri.call.TripleClientCall#start @Override public StreamObserver\u003cObject\u003e start(RequestMetadata metadata, ClientCall.Listener responseListener) { this.requestMetadata = metadata; this.listener = responseListener; // 在构造方法中调用 initHttp2StreamChannel 方法 this.stream = new TripleClientStream(frameworkModel, executor, (Channel) connectionClient.getChannel(true), this, writeQueue); return new ClientCallToObserverAdapter\u003c\u003e(this); } 源码位置: org.apache.dubbo.rpc.protocol.tri.stream.TripleClientStream#initHttp2StreamChannel // 初始化 http2 stream 的 channelHandler private TripleStreamChannelFuture initHttp2StreamChannel(Channel parent) { TripleStreamChannelFuture streamChannelFuture = new TripleStreamChannelFuture(parent); Http2StreamChannelBootstrap bootstrap = new Http2StreamChannelBootstrap(parent); bootstrap.handler(new ChannelInboundHandlerAdapter() { @Override public void handlerAdded(ChannelHandlerContext ctx) throws Exception { Channel channel = ctx.channel(); // 负责发送请求 channel.pipeline().addLast(new TripleCommandOutBoundHandler()); // 负责接受响应 channel.pipeline().addLast(new TripleHttp2ClientResponseHandler(createTransportListener())); } }); CreateStreamQueueCommand cmd = CreateStreamQueueCommand.create(bootstrap, streamChannelFuture); this.writeQueue.enqueue(cmd); return streamChannelFuture; } ","date":"2023-12-07","objectID":"/ooooo-notes/10-triple-%E5%8D%8F%E8%AE%AE/:4:0","tags":["dubbo","source code","源码分析 dubbo 系列"],"title":"10 triple 协议","uri":"/ooooo-notes/10-triple-%E5%8D%8F%E8%AE%AE/"},{"categories":null,"content":"TripleHttp2ClientResponseHandler 接受服务端的消息 源码位置: org.apache.dubbo.rpc.protocol.tri.transport.TripleHttp2ClientResponseHandler#channelRead0 // 负责接受响应 protected void channelRead0(ChannelHandlerContext ctx, Http2StreamFrame msg) throws Exception { if (msg instanceof Http2HeadersFrame) { final Http2HeadersFrame headers = (Http2HeadersFrame) msg; transportListener.onHeader(headers.headers(), headers.isEndStream()); } else if (msg instanceof Http2DataFrame) { final Http2DataFrame data = (Http2DataFrame) msg; transportListener.onData(data.content(), data.isEndStream()); } else { super.channelRead(ctx, msg); } } ","date":"2023-12-07","objectID":"/ooooo-notes/10-triple-%E5%8D%8F%E8%AE%AE/:5:0","tags":["dubbo","source code","源码分析 dubbo 系列"],"title":"10 triple 协议","uri":"/ooooo-notes/10-triple-%E5%8D%8F%E8%AE%AE/"},{"categories":null,"content":"测试类 org.apache.dubbo.rpc.protocol.tri.TripleProtocolTest#testDemoProtocol 在调试过程，可能会出现超时，可以添加下面代码来解决。 URL consumerUrl = URL.valueOf( \"tri://127.0.0.1:\" + availablePort + \"/\" + IGreeter.class.getName()); // 添加下面代码 RpcContext.getClientAttachment().getObjectAttachments().put(\"timeout\", 180000); ","date":"2023-12-07","objectID":"/ooooo-notes/10-triple-%E5%8D%8F%E8%AE%AE/:6:0","tags":["dubbo","source code","源码分析 dubbo 系列"],"title":"10 triple 协议","uri":"/ooooo-notes/10-triple-%E5%8D%8F%E8%AE%AE/"},{"categories":null,"content":" dubbo 基于 3.2.6 版本 接口级别引用是 dubbo 2.x 版本的方式，其主流程和之前的章节【引用服务】没有差别，主要区别在于注册中心的逻辑不一样。 ","date":"2023-12-06","objectID":"/ooooo-notes/08-%E5%BC%95%E7%94%A8%E6%9C%8D%E5%8A%A1%E6%8E%A5%E5%8F%A3%E7%BA%A7%E5%88%AB/:0:0","tags":["dubbo","source code","源码分析 dubbo 系列"],"title":"08 引用服务（接口级别）","uri":"/ooooo-notes/08-%E5%BC%95%E7%94%A8%E6%9C%8D%E5%8A%A1%E6%8E%A5%E5%8F%A3%E7%BA%A7%E5%88%AB/"},{"categories":null,"content":"RegistryProtocol#doCreateInvoker 创建 invoker 源码位置: org.apache.dubbo.registry.integration.RegistryProtocol#doCreateInvoker protected \u003cT\u003e ClusterInvoker\u003cT\u003e doCreateInvoker(DynamicDirectory\u003cT\u003e directory, Cluster cluster, Registry registry, Class\u003cT\u003e type) { directory.setRegistry(registry); directory.setProtocol(protocol); ... directory.buildRouterChain(urlToRegistry); // 订阅 url, directory 实现类 为 RegistryDirectory directory.subscribe(toSubscribeUrl(urlToRegistry)); return (ClusterInvoker\u003cT\u003e) cluster.join(directory, true); } 源码位置: org.apache.dubbo.registry.integration.RegistryDirectory#subscribe @Override public void subscribe(URL url) { ... ApplicationModel applicationModel = url.getApplicationModel(); String registryClusterName = registry.getUrl().getParameter(RegistryConstants.REGISTRY_CLUSTER_KEY, registry.getUrl().getParameter(PROTOCOL_KEY)); MetricsEventBus.post(RegistryEvent.toSubscribeEvent(applicationModel,registryClusterName), () -\u003e { // 调用父类 DynamicDirectory#subscribe super.subscribe(url); return null; } ); // 开启配置监听，不解析 if (moduleModel.modelEnvironment().getConfiguration().convert(Boolean.class, org.apache.dubbo.registry.Constants.ENABLE_CONFIGURATION_LISTEN, true)) { consumerConfigurationListener.addNotifyListener(this); referenceConfigurationListener = new ReferenceConfigurationListener(moduleModel, this, url); } } 源码位置: org.apache.dubbo.registry.integration.DynamicDirectory#subscribe public void subscribe(URL url) { setSubscribeUrl(url); // 这里以 ZookeeperRegistry 为例 registry.subscribe(url, this); } ","date":"2023-12-06","objectID":"/ooooo-notes/08-%E5%BC%95%E7%94%A8%E6%9C%8D%E5%8A%A1%E6%8E%A5%E5%8F%A3%E7%BA%A7%E5%88%AB/:1:0","tags":["dubbo","source code","源码分析 dubbo 系列"],"title":"08 引用服务（接口级别）","uri":"/ooooo-notes/08-%E5%BC%95%E7%94%A8%E6%9C%8D%E5%8A%A1%E6%8E%A5%E5%8F%A3%E7%BA%A7%E5%88%AB/"},{"categories":null,"content":"ZookeeperRegistry#subscribe 订阅服务 源码位置: `` @Override public void subscribe(URL url, NotifyListener listener) { ... // 移除 url removeFailedSubscribed(url, listener); try { // Sending a subscription request to the server side // 调用子类的订阅方法，这里以 ZookeeperRegistry 为例 doSubscribe(url, listener); } catch (Exception e) { Throwable t = e; List\u003cURL\u003e urls = getCacheUrls(url); if (CollectionUtils.isNotEmpty(urls)) { notify(url, listener, urls); } else { // If the startup detection is opened, the Exception is thrown directly. boolean check = getUrl().getParameter(Constants.CHECK_KEY, true) \u0026\u0026 url.getParameter(Constants.CHECK_KEY, true); boolean skipFailback = t instanceof SkipFailbackWrapperException; // 检查 check 参数，如果为 true，表示第一次订阅服务要成功 if (check || skipFailback) { if (skipFailback) { t = t.getCause(); } throw new IllegalStateException(\"Failed to subscribe \" + url + \", cause: \" + t.getMessage(), t); } else { logger.error(REGISTRY_FAILED_NOTIFY_EVENT, \"\", \"\", \"Failed to subscribe \" + url + \", waiting for retry, cause: \" + t.getMessage(), t); } } // 记录失败的url， 稍后定时任务会再次订阅 addFailedSubscribed(url, listener); } } 源码位置: org.apache.dubbo.registry.zookeeper.ZookeeperRegistry#doSubscribe @Override public void doSubscribe(final URL url, final NotifyListener listener) { try { checkDestroyed(); // 订阅所有接口，dubbo-admin 服务会使用，这个不分析 if (ANY_VALUE.equals(url.getServiceInterface())) { ... } else { CountDownLatch latch = new CountDownLatch(1); try { List\u003cURL\u003e urls = new ArrayList\u003c\u003e(); /* Iterate over the category value in URL. With default settings, the path variable can be when url is a consumer URL: /dubbo/[service name]/providers, /dubbo/[service name]/configurators /dubbo/[service name]/routers */ // 监听每一个路径 for (String path : toCategoriesPath(url)) { ConcurrentMap\u003cNotifyListener, ChildListener\u003e listeners = ConcurrentHashMapUtils.computeIfAbsent(zkListeners, url, k -\u003e new ConcurrentHashMap\u003c\u003e()); // 这里把 listener 添加进去了，等 url 更改时，再执行回调函数 ChildListener zkListener = ConcurrentHashMapUtils.computeIfAbsent(listeners, listener, k -\u003e new RegistryChildListenerImpl(url, k, latch)); if (zkListener instanceof RegistryChildListenerImpl) { // latch 为了监听到 urls 时，通知主线程 ((RegistryChildListenerImpl) zkListener).setLatch(latch); } // 创建根路径，比如 /dubbo/${interfaceName}/consumers zkClient.create(path, false, true); // 获取所有的子路径，用于第一次初始化 urls List\u003cString\u003e children = zkClient.addChildListener(path, zkListener); if (children != null) { // The invocation point that may cause 1-1. urls.addAll(toUrlsWithEmpty(url, path, children)); } } // 执行回调函数 notify(url, listener, urls); } finally { // tells the listener to run only after the sync notification of main thread finishes. latch.countDown(); } } } catch (Throwable e) { ... } } ","date":"2023-12-06","objectID":"/ooooo-notes/08-%E5%BC%95%E7%94%A8%E6%9C%8D%E5%8A%A1%E6%8E%A5%E5%8F%A3%E7%BA%A7%E5%88%AB/:2:0","tags":["dubbo","source code","源码分析 dubbo 系列"],"title":"08 引用服务（接口级别）","uri":"/ooooo-notes/08-%E5%BC%95%E7%94%A8%E6%9C%8D%E5%8A%A1%E6%8E%A5%E5%8F%A3%E7%BA%A7%E5%88%AB/"},{"categories":null,"content":" dubbo 基于 3.2.6 版本 接口级别导出是 dubbo 2.x 版本的方式，其主流程和之前的章节【导出服务】没有差别，主要区别在于注册中心的逻辑不一样。 ","date":"2023-12-05","objectID":"/ooooo-notes/07-%E5%AF%BC%E5%87%BA%E6%9C%8D%E5%8A%A1%E6%8E%A5%E5%8F%A3%E7%BA%A7%E5%88%AB/:0:0","tags":["dubbo","source code","源码分析 dubbo 系列"],"title":"07 导出服务（接口级别）","uri":"/ooooo-notes/07-%E5%AF%BC%E5%87%BA%E6%9C%8D%E5%8A%A1%E6%8E%A5%E5%8F%A3%E7%BA%A7%E5%88%AB/"},{"categories":null,"content":"RegistryProtocol#export 导出服务 源码位置: org.apache.dubbo.registry.integration.RegistryProtocol#export @Override public \u003cT\u003e Exporter\u003cT\u003e export(final Invoker\u003cT\u003e originInvoker) throws RpcException { ... // 获取 registry，比如 ZookeeperRegistry (接口级注册) final Registry registry = getRegistry(registryUrl); final URL registeredProviderUrl = getUrlToRegistry(providerUrl, registryUrl); // decide if we need to delay publish (provider itself and registry should both need to register) // 如果是接口级别，register 为 true boolean register = providerUrl.getParameter(REGISTER_KEY, true) \u0026\u0026 registryUrl.getParameter(REGISTER_KEY, true); if (register) { // 注册 providerUrl，最终调用 ZookeeperRegistry#registry 方法 register(registry, registeredProviderUrl); } ... return new DestroyableExporter\u003c\u003e(exporter); } ","date":"2023-12-05","objectID":"/ooooo-notes/07-%E5%AF%BC%E5%87%BA%E6%9C%8D%E5%8A%A1%E6%8E%A5%E5%8F%A3%E7%BA%A7%E5%88%AB/:1:0","tags":["dubbo","source code","源码分析 dubbo 系列"],"title":"07 导出服务（接口级别）","uri":"/ooooo-notes/07-%E5%AF%BC%E5%87%BA%E6%9C%8D%E5%8A%A1%E6%8E%A5%E5%8F%A3%E7%BA%A7%E5%88%AB/"},{"categories":null,"content":"ZookeeperRegistry#register 注册服务 源码位置: org.apache.dubbo.registry.support.FailbackRegistry#register // 注册 url, 这个类是 ZookeeperRegistry 的 父类 @Override public void register(URL url) { ... super.register(url); // 移除 url removeFailedRegistered(url); removeFailedUnregistered(url); try { // Sending a registration request to the server side // 调用子类的注册方法, 这里以 ZookeeperRegistry 为例 doRegister(url); } catch (Exception e) { Throwable t = e; // If the startup detection is opened, the Exception is thrown directly. boolean check = getUrl().getParameter(Constants.CHECK_KEY, true) \u0026\u0026 url.getParameter(Constants.CHECK_KEY, true) \u0026\u0026 (url.getPort() != 0); boolean skipFailback = t instanceof SkipFailbackWrapperException; // 检查 check 参数，如果是 true，表示第一次一定要注册成功 if (check || skipFailback) { if (skipFailback) { t = t.getCause(); } throw new IllegalStateException(\"Failed to register \" + url + \" to registry \" + getUrl().getAddress() + \", cause: \" + t.getMessage(), t); } else { logger.error(INTERNAL_ERROR, \"unknown error in registry module\", \"\", \"Failed to register \" + url + \", waiting for retry, cause: \" + t.getMessage(), t); } // Record a failed registration request to a failed list, retry regularly // 添加失败的 url，稍后定时任务会重新注册 addFailedRegistered(url); } } 源码位置: org.apache.dubbo.registry.zookeeper.ZookeeperRegistry#doRegister @Override public void doRegister(URL url) { try { checkDestroyed(); // 创建 zookeeper 临时节点， 路径为 /dubbo/${interfaceName}/providers/ zkClient.create(toUrlPath(url), url.getParameter(DYNAMIC_KEY, true), true); } catch (Throwable e) { throw new RpcException(\"Failed to register \" + url + \" to zookeeper \" + getUrl() + \", cause: \" + e.getMessage(), e); } } ","date":"2023-12-05","objectID":"/ooooo-notes/07-%E5%AF%BC%E5%87%BA%E6%9C%8D%E5%8A%A1%E6%8E%A5%E5%8F%A3%E7%BA%A7%E5%88%AB/:2:0","tags":["dubbo","source code","源码分析 dubbo 系列"],"title":"07 导出服务（接口级别）","uri":"/ooooo-notes/07-%E5%AF%BC%E5%87%BA%E6%9C%8D%E5%8A%A1%E6%8E%A5%E5%8F%A3%E7%BA%A7%E5%88%AB/"},{"categories":null,"content":"为什么学 现在很多分布式系统都会使用到分布式一致性协议，比如 nacos、zookeeper、consul、etcd、tikv 等等，而 raft 可以说是最简单的分布式一致性协议。 ","date":"2023-12-05","objectID":"/ooooo-notes/%E4%BB%8E%E9%9B%B6%E5%AD%A6-raft/:1:0","tags":["raft","从零学技术系列"],"title":"从零学 raft","uri":"/ooooo-notes/%E4%BB%8E%E9%9B%B6%E5%AD%A6-raft/"},{"categories":null,"content":"怎么学 raft 协议动画演示 raft 协议论文 找一个 raft 协议的源码实现，比如 consul-raft 动手实现 raft 协议 ~ MIT课程 ","date":"2023-12-05","objectID":"/ooooo-notes/%E4%BB%8E%E9%9B%B6%E5%AD%A6-raft/:2:0","tags":["raft","从零学技术系列"],"title":"从零学 raft","uri":"/ooooo-notes/%E4%BB%8E%E9%9B%B6%E5%AD%A6-raft/"},{"categories":null,"content":" dubbo 基于 3.2.6 版本 在 dubbo 中引用服务的源码是非常复杂的，这里只介绍主要流程。 ","date":"2023-11-27","objectID":"/ooooo-notes/06-%E5%BC%95%E7%94%A8%E6%9C%8D%E5%8A%A1/:0:0","tags":["dubbo","source code","源码分析 dubbo 系列"],"title":"06 引用服务","uri":"/ooooo-notes/06-%E5%BC%95%E7%94%A8%E6%9C%8D%E5%8A%A1/"},{"categories":null,"content":"DefaultModuleDeployer#referServices 引用服务 源码位置: org.apache.dubbo.config.deploy.DefaultModuleDeployer#referServices private void referServices() { // 遍历所有的 reference configManager.getReferences().forEach(rc -\u003e { try { ReferenceConfig\u003c?\u003e referenceConfig = (ReferenceConfig\u003c?\u003e) rc; // 刷新配置 if (!referenceConfig.isRefreshed()) { referenceConfig.refresh(); } if (rc.shouldInit()) { // 异步引用 if (referAsync || rc.shouldReferAsync()) { ExecutorService executor = executorRepository.getServiceReferExecutor(); CompletableFuture\u003cVoid\u003e future = CompletableFuture.runAsync(() -\u003e { try { referenceCache.get(rc, false); } catch (Throwable t) { logger.error(CONFIG_FAILED_EXPORT_SERVICE, \"\", \"\", \"Failed to async export service config: \" + getIdentifier() + \" , catch error : \" + t.getMessage(), t); } }, executor); asyncReferringFutures.add(future); } else { // 同步引用 referenceCache.get(rc, false); } } } catch (Throwable t) { ... } }); } 源码位置: org.apache.dubbo.config.utils.SimpleReferenceCache#get // 同步引用 public \u003cT\u003e T get(ReferenceConfigBase\u003cT\u003e rc, boolean check) { String key = generator.generateKey(rc); Class\u003c?\u003e type = rc.getInterfaceClass(); boolean singleton = rc.getSingleton() == null || rc.getSingleton(); T proxy = null; // Check existing proxy of the same 'key' and 'type' first. if (singleton) { // 单例对象，从缓存 referenceKeyMap 中获取 proxy = get(key, (Class\u003cT\u003e) type); } else { logger.warn(CONFIG_API_WRONG_USE, \"\", \"\", \"Using non-singleton ReferenceConfig and ReferenceCache at the same time may cause memory leak. \" + \"Call ReferenceConfig#get() directly for non-singleton ReferenceConfig instead of using ReferenceCache#get(ReferenceConfig)\"); } // 第一次获取 if (proxy == null) { // 添加到 referenceTypeMap List\u003cReferenceConfigBase\u003c?\u003e\u003e referencesOfType = ConcurrentHashMapUtils.computeIfAbsent(referenceTypeMap, type, _t -\u003e Collections.synchronizedList(new ArrayList\u003c\u003e())); referencesOfType.add(rc); // 添加到 referenceKeyMap List\u003cReferenceConfigBase\u003c?\u003e\u003e referenceConfigList = ConcurrentHashMapUtils.computeIfAbsent(referenceKeyMap, key, _k -\u003e Collections.synchronizedList(new ArrayList\u003c\u003e())); referenceConfigList.add(rc); // 获取代理对象 proxy = rc.get(check); } return proxy; } ","date":"2023-11-27","objectID":"/ooooo-notes/06-%E5%BC%95%E7%94%A8%E6%9C%8D%E5%8A%A1/:1:0","tags":["dubbo","source code","源码分析 dubbo 系列"],"title":"06 引用服务","uri":"/ooooo-notes/06-%E5%BC%95%E7%94%A8%E6%9C%8D%E5%8A%A1/"},{"categories":null,"content":"ReferenceConfig#get 获取代理对象 源码位置: org.apache.dubbo.config.ReferenceConfig#get // 获取代理对象 public T get(boolean check) { ... if (ref == null) { if (getScopeModel().isLifeCycleManagedExternally()) { // prepare model for reference getScopeModel().getDeployer().prepare(); } else { // ensure start module, compatible with old api usage // 启动模块 getScopeModel().getDeployer().start(); } // 初始化 init(check); } return ref; } 源码位置: org.apache.dubbo.config.ReferenceConfig#init // 初始化 protected synchronized void init(boolean check) { if (initialized \u0026\u0026 ref != null) { return; } try { // 刷新配置 if (!this.isRefreshed()) { this.refresh(); } //auto detect proxy type String proxyType = getProxy(); if (StringUtils.isBlank(proxyType) \u0026\u0026 DubboStub.class.isAssignableFrom(interfaceClass)) { setProxy(CommonConstants.NATIVE_STUB); } // init serviceMetadata initServiceMetadata(consumer); serviceMetadata.setServiceType(getServiceInterfaceClass()); // TODO, uncomment this line once service key is unified serviceMetadata.generateServiceKey(); // 添加配置，如 application, consumer, interface Map\u003cString, String\u003e referenceParameters = appendConfig(); ModuleServiceRepository repository = getScopeModel().getServiceRepository(); ServiceDescriptor serviceDescriptor; if (CommonConstants.NATIVE_STUB.equals(getProxy())) { serviceDescriptor = StubSuppliers.getServiceDescriptor(interfaceName); repository.registerService(serviceDescriptor); } else { serviceDescriptor = repository.registerService(interfaceClass); } // 创建 consumerModel consumerModel = new ConsumerModel(serviceMetadata.getServiceKey(), proxy, serviceDescriptor, getScopeModel(), serviceMetadata, createAsyncMethodInfo(), interfaceClassLoader); // Compatible with dependencies on ServiceModel#getReferenceConfig() , and will be removed in a future version. consumerModel.setConfig(this); // 注册 consumerModel repository.registerConsumer(consumerModel); serviceMetadata.getAttachments().putAll(referenceParameters); // 创建代理对象，这个最重要 ref = createProxy(referenceParameters); serviceMetadata.setTarget(ref); serviceMetadata.addAttribute(PROXY_CLASS_REF, ref); // 设置销毁回调函数 consumerModel.setDestroyRunner(getDestroyRunner()); consumerModel.setProxyObject(ref); consumerModel.initMethodModels(); // 检查可用性，dubbo3 默认为 false if (check) { checkInvokerAvailable(0); } } catch (Throwable t) { logAndCleanup(t); throw t; } // 标记已初始化 initialized = true; } ","date":"2023-11-27","objectID":"/ooooo-notes/06-%E5%BC%95%E7%94%A8%E6%9C%8D%E5%8A%A1/:1:1","tags":["dubbo","source code","源码分析 dubbo 系列"],"title":"06 引用服务","uri":"/ooooo-notes/06-%E5%BC%95%E7%94%A8%E6%9C%8D%E5%8A%A1/"},{"categories":null,"content":"ReferenceConfig#createProxy 创建代理 源码位置: org.apache.dubbo.config.ReferenceConfig#createProxy // 创建代理对象 private T createProxy(Map\u003cString, String\u003e referenceParameters) { urls.clear(); // mesh mode 这一节不解析，以后会继续解析 meshModeHandleUrl(referenceParameters); if (StringUtils.isNotEmpty(url)) { // user specified URL, could be peer-to-peer address, or register center's address. // url 不为空, 说明是直连 parseUrl(referenceParameters); } else { // if protocols not in jvm checkRegistry // 从注册中心来获取 urls aggregateUrlFromRegistry(referenceParameters); } // 根据 urls 来创建 invoker createInvoker(); ... // 发布服务定义 MetadataUtils.publishServiceDefinition(consumerUrl, consumerModel.getServiceModel(), getApplicationModel()); // create service proxy // 获取代理对象 return (T) proxyFactory.getProxy(invoker, ProtocolUtils.isGeneric(generic)); } 源码位置: org.apache.dubbo.config.ReferenceConfig#aggregateUrlFromRegistry // 通过注册中心来获取 urls private void aggregateUrlFromRegistry(Map\u003cString, String\u003e referenceParameters) { checkRegistry(); // 加载所有的注册中心 List\u003cURL\u003e us = ConfigValidationUtils.loadRegistries(this, false); if (CollectionUtils.isNotEmpty(us)) { // 遍历所有的注册中心地址 for (URL u : us) { // 加载监控地址 URL monitorUrl = ConfigValidationUtils.loadMonitor(this, u); if (monitorUrl != null) { u = u.putAttribute(MONITOR_KEY, monitorUrl); } u = u.setScopeModel(getScopeModel()); u = u.setServiceModel(consumerModel); if (isInjvm() != null \u0026\u0026 isInjvm()) { u = u.addParameter(LOCAL_PROTOCOL, true); } // 把 referenceParameters 添加到 registryUrl 的 REFER_KEY 参数中 urls.add(u.putAttribute(REFER_KEY, referenceParameters)); } } ... } 源码位置: org.apache.dubbo.config.ReferenceConfig#createInvoker // 根据 urls 来创建 invoker // 一个 url 表示一种注册中心 private void createInvoker() { // 单注册中心 if (urls.size() == 1) { URL curUrl = urls.get(0); // 利用 SPI 机制生成对应的 invoker, 这时 url 是 registryUrl, 所以实现类就是 RegistryProtocol invoker = protocolSPI.refer(interfaceClass, curUrl); // registry url, mesh-enable and unloadClusterRelated is true, not need Cluster. if (!UrlUtils.isRegistry(curUrl) \u0026\u0026 !curUrl.getParameter(UNLOAD_CLUSTER_RELATED, false)) { List\u003cInvoker\u003c?\u003e\u003e invokers = new ArrayList\u003c\u003e(); invokers.add(invoker); invoker = Cluster.getCluster(getScopeModel(), Cluster.DEFAULT).join(new StaticDirectory(curUrl, invokers), true); } } else { // 多注册中心 List\u003cInvoker\u003c?\u003e\u003e invokers = new ArrayList\u003c\u003e(); URL registryUrl = null; for (URL url : urls) { // For multi-registry scenarios, it is not checked whether each referInvoker is available. // Because this invoker may become available later. // 每个 url 都是创建一个 invoker invokers.add(protocolSPI.refer(interfaceClass, url)); if (UrlUtils.isRegistry(url)) { // use last registry url registryUrl = url; } } ...省略 invokers 聚合的代码 } } ","date":"2023-11-27","objectID":"/ooooo-notes/06-%E5%BC%95%E7%94%A8%E6%9C%8D%E5%8A%A1/:1:2","tags":["dubbo","source code","源码分析 dubbo 系列"],"title":"06 引用服务","uri":"/ooooo-notes/06-%E5%BC%95%E7%94%A8%E6%9C%8D%E5%8A%A1/"},{"categories":null,"content":"RegistryProtocol#refer 引用服务 源码位置: org.apache.dubbo.registry.integration.RegistryProtocol#refer // 引用服务 @Override @SuppressWarnings(\"unchecked\") public \u003cT\u003e Invoker\u003cT\u003e refer(Class\u003cT\u003e type, URL url) throws RpcException { url = getRegistryUrl(url); // 获取注册中心 Registry registry = getRegistry(url); if (RegistryService.class.equals(type)) { return proxyFactory.getInvoker((T) registry, type, url); } // qs 是 consumer 配置, 在之前已经把 consumer 的配置存入 REFER_KEY 中 Map\u003cString, String\u003e qs = (Map\u003cString, String\u003e) url.getAttribute(REFER_KEY); ... // 获取 cluster，默认是 failover Cluster cluster = Cluster.getCluster(url.getScopeModel(), qs.get(CLUSTER_KEY)); // 引用服务 return doRefer(cluster, registry, type, url, qs); } 源码位置: org.apache.dubbo.registry.integration.RegistryProtocol#doRefer // 引用服务 protected \u003cT\u003e Invoker\u003cT\u003e doRefer(Cluster cluster, Registry registry, Class\u003cT\u003e type, URL url, Map\u003cString, String\u003e parameters) { Map\u003cString, Object\u003e consumerAttribute = new HashMap\u003c\u003e(url.getAttributes()); consumerAttribute.remove(REFER_KEY); String p = isEmpty(parameters.get(PROTOCOL_KEY)) ? CONSUMER : parameters.get(PROTOCOL_KEY); // 构建 consumerUrl URL consumerUrl = new ServiceConfigURL( p, null, null, parameters.get(REGISTER_IP_KEY), 0, getPath(parameters, type), parameters, consumerAttribute ); url = url.putAttribute(CONSUMER_URL_KEY, consumerUrl); // 实现类为 ServiceDiscoveryMigrationInvoker ClusterInvoker\u003cT\u003e migrationInvoker = getMigrationInvoker(this, cluster, registry, type, url, consumerUrl); // 执行 RegistryProtocolListener 钩子函数，最终会执行 MigrationRuleListener#onRefer 方法 return interceptInvoker(migrationInvoker, url, consumerUrl); } ","date":"2023-11-27","objectID":"/ooooo-notes/06-%E5%BC%95%E7%94%A8%E6%9C%8D%E5%8A%A1/:1:3","tags":["dubbo","source code","源码分析 dubbo 系列"],"title":"06 引用服务","uri":"/ooooo-notes/06-%E5%BC%95%E7%94%A8%E6%9C%8D%E5%8A%A1/"},{"categories":null,"content":"MigrationRuleListener#onRefer 执行钩子函数 源码位置: `` // 执行钩子函数 @Override public void onRefer(RegistryProtocol registryProtocol, ClusterInvoker\u003c?\u003e invoker, URL consumerUrl, URL registryURL) { MigrationRuleHandler\u003c?\u003e migrationRuleHandler = ConcurrentHashMapUtils.computeIfAbsent(handlers, (MigrationInvoker\u003c?\u003e) invoker, _key -\u003e { ((MigrationInvoker\u003c?\u003e) invoker).setMigrationRuleListener(this); return new MigrationRuleHandler\u003c\u003e((MigrationInvoker\u003c?\u003e) invoker, consumerUrl); }); // 迁移规则 migrationRuleHandler.doMigrate(rule); } 源码位置: org.apache.dubbo.registry.client.migration.MigrationRuleHandler#doMigrate // 迁移规则 public synchronized void doMigrate(MigrationRule rule) { // 这里会走到这个分支 if (migrationInvoker instanceof ServiceDiscoveryMigrationInvoker) { // 刷新 invoker refreshInvoker(MigrationStep.FORCE_APPLICATION, 1.0f, rule); return; } ...省略获取 step 的代码 } 源码位置: org.apache.dubbo.registry.client.migration.MigrationRuleHandler#refreshInvoker // 刷新 invoker private boolean refreshInvoker(MigrationStep step, Float threshold, MigrationRule newRule) { if (step == null || threshold == null) { throw new IllegalStateException(\"Step or threshold of migration rule cannot be null\"); } MigrationStep originStep = currentStep; // 这里的 step 为 FORCE_APPLICATION if ((currentStep == null || currentStep != step) || !currentThreshold.equals(threshold)) { boolean success = true; switch (step) { case APPLICATION_FIRST: migrationInvoker.migrateToApplicationFirstInvoker(newRule); break; case FORCE_APPLICATION: // 强制使用服务级别引用 success = migrationInvoker.migrateToForceApplicationInvoker(newRule); break; case FORCE_INTERFACE: default: success = migrationInvoker.migrateToForceInterfaceInvoker(newRule); } ... return success; } // ignore if step is same with previous, will continue override rule for MigrationInvoker return true; } 源码位置: org.apache.dubbo.registry.client.migration.MigrationInvoker#migrateToForceApplicationInvoker @Override public boolean migrateToForceApplicationInvoker(MigrationRule newRule) { CountDownLatch latch = new CountDownLatch(1); // 刷新 invoker, 这个很重要 refreshServiceDiscoveryInvoker(latch); if (invoker == null) { // invoker is absent, ignore threshold check this.currentAvailableInvoker = serviceDiscoveryInvoker; return true; } // wait and compare threshold // 等待初次 invoker 创建成功 waitAddressNotify(newRule, latch); ... return false; } 源码位置: `` protected void refreshServiceDiscoveryInvoker(CountDownLatch latch) { clearListener(serviceDiscoveryInvoker); // 需要刷新 if (needRefresh(serviceDiscoveryInvoker)) { if (logger.isDebugEnabled()) { logger.debug(\"Re-subscribing instance addresses, current interface \" + type.getName()); } if (serviceDiscoveryInvoker != null) { serviceDiscoveryInvoker.destroy(); } // 获取 invoker, registryProtocol 的实现类为 RegistryProtocol serviceDiscoveryInvoker = registryProtocol.getServiceDiscoveryInvoker(cluster, registry, type, url); } // 设置监听器 setListener(serviceDiscoveryInvoker, () -\u003e { latch.countDown(); ... if (step == APPLICATION_FIRST) { calcPreferredInvoker(rule); } }); } ","date":"2023-11-27","objectID":"/ooooo-notes/06-%E5%BC%95%E7%94%A8%E6%9C%8D%E5%8A%A1/:1:4","tags":["dubbo","source code","源码分析 dubbo 系列"],"title":"06 引用服务","uri":"/ooooo-notes/06-%E5%BC%95%E7%94%A8%E6%9C%8D%E5%8A%A1/"},{"categories":null,"content":"RegistryProtocol#getServiceDiscoveryInvoker 获取 invoker 源码位置: org.apache.dubbo.registry.integration.RegistryProtocol#getServiceDiscoveryInvoker // 获取 invoker public \u003cT\u003e ClusterInvoker\u003cT\u003e getServiceDiscoveryInvoker(Cluster cluster, Registry registry, Class\u003cT\u003e type, URL url) { // 创建 directory DynamicDirectory\u003cT\u003e directory = new ServiceDiscoveryRegistryDirectory\u003c\u003e(type, url); // 创建 invoker return doCreateInvoker(directory, cluster, registry, type); } 源码位置: org.apache.dubbo.registry.integration.RegistryProtocol#doCreateInvoker // 创建 invoker protected \u003cT\u003e ClusterInvoker\u003cT\u003e doCreateInvoker(DynamicDirectory\u003cT\u003e directory, Cluster cluster, Registry registry, Class\u003cT\u003e type) { directory.setRegistry(registry); directory.setProtocol(protocol); // all attributes of REFER_KEY Map\u003cString, String\u003e parameters = new HashMap\u003c\u003e(directory.getConsumerUrl().getParameters()); // 创建 consumerUrl URL urlToRegistry = new ServiceConfigURL( parameters.get(PROTOCOL_KEY) == null ? CONSUMER : parameters.get(PROTOCOL_KEY), parameters.remove(REGISTER_IP_KEY), 0, getPath(parameters, type), parameters ); urlToRegistry = urlToRegistry.setScopeModel(directory.getConsumerUrl().getScopeModel()); urlToRegistry = urlToRegistry.setServiceModel(directory.getConsumerUrl().getServiceModel()); // 注册 consumerUrl，有助于排查问题 if (directory.isShouldRegister()) { directory.setRegisteredConsumerUrl(urlToRegistry); registry.register(directory.getRegisteredConsumerUrl()); } // 创建 routerChain, 例如 TagStateRouter directory.buildRouterChain(urlToRegistry); // 订阅服务 directory.subscribe(toSubscribeUrl(urlToRegistry)); return (ClusterInvoker\u003cT\u003e) cluster.join(directory, true); } 源码位置: org.apache.dubbo.registry.client.ServiceDiscoveryRegistryDirectory#subscribe // 订阅服务 @Override public void subscribe(URL url) { // 开启配置监听，key: ${applicationName}.configurators if (moduleModel.modelEnvironment().getConfiguration().convert(Boolean.class, Constants.ENABLE_CONFIGURATION_LISTEN, true)) { enableConfigurationListen = true; getConsumerConfigurationListener(moduleModel).addNotifyListener(this); referenceConfigurationListener = new ReferenceConfigurationListener(this.moduleModel, this, url); } else { enableConfigurationListen = false; } // 调用父类 DynamicDirectory#subscribe super.subscribe(url); } 源码位置: org.apache.dubbo.registry.integration.DynamicDirectory#subscribe // 订阅服务 public void subscribe(URL url) { setSubscribeUrl(url); // 这里的 registry 是 ServiceDiscoveryRegistry registry.subscribe(url, this); } ","date":"2023-11-27","objectID":"/ooooo-notes/06-%E5%BC%95%E7%94%A8%E6%9C%8D%E5%8A%A1/:1:5","tags":["dubbo","source code","源码分析 dubbo 系列"],"title":"06 引用服务","uri":"/ooooo-notes/06-%E5%BC%95%E7%94%A8%E6%9C%8D%E5%8A%A1/"},{"categories":null,"content":"ServiceDiscoveryRegistry#subscribe 订阅服务 源码位置: org.apache.dubbo.registry.client.ServiceDiscoveryRegistry#subscribe // 订阅服务 @Override public final void subscribe(URL url, NotifyListener listener) { // 不用订阅 if (!shouldSubscribe(url)) { // Should Not Subscribe return; } // 订阅 doSubscribe(url, listener); } // 订阅 @Override public void doSubscribe(URL url, NotifyListener listener) { url = addRegistryClusterKey(url); // 添加 url 到 metadataInfo serviceDiscovery.subscribe(url, listener); // 从 provided-by 中解析 serviceNames Set\u003cString\u003e mappingByUrl = ServiceNameMapping.getMappingByUrl(url); String key = ServiceNameMapping.buildMappingKey(url); // 说明这个 url 是第一次订阅 if (mappingByUrl == null) { // 获取锁 Lock mappingLock = serviceNameMapping.getMappingLock(key); try { mappingLock.lock(); // 从缓存中获取 url 对应的 serviceNames mappingByUrl = serviceNameMapping.getMapping(url); try { // 创建 mappingListener，当 mapping 改变时，会执行回调方法 MappingListener mappingListener = new DefaultMappingListener(url, mappingByUrl, listener); // 获取 url 对应的 serviceNames, 并开始监听 mapping // 对应的 provider 源码是 ServiceConfig#exported 方法 mappingByUrl = serviceNameMapping.getAndListen(this.getUrl(), url, mappingListener); mappingListeners.put(url.getProtocolServiceKey(), mappingListener); } catch (Exception e) { logger.warn(INTERNAL_ERROR, \"\", \"\", \"Cannot find app mapping for service \" + url.getServiceInterface() + \", will not migrate.\", e); } ... } finally { mappingLock.unlock(); } } // 根据 serviceNames 来获取 url subscribeURLs(url, listener, mappingByUrl); } 源码位置: org.apache.dubbo.registry.client.ServiceDiscoveryRegistry#subscribeURLs // 根据 serviceNames 来获取 url protected void subscribeURLs(URL url, NotifyListener listener, Set\u003cString\u003e serviceNames) { // 排序 serviceNames, 用来当做 key serviceNames = toTreeSet(serviceNames); String serviceNamesKey = toStringKeys(serviceNames); String serviceKey = url.getServiceKey(); logger.info(String.format(\"Trying to subscribe from apps %s for service key %s, \", serviceNamesKey, serviceKey)); // register ServiceInstancesChangedListener Lock appSubscriptionLock = getAppSubscription(serviceNamesKey); try { // 加锁 appSubscriptionLock.lock(); ServiceInstancesChangedListener serviceInstancesChangedListener = serviceListeners.get(serviceNamesKey); if (serviceInstancesChangedListener == null) { // 创建 serviceInstancesChangedListener // 当 serviceName 下的 instance 发生改变时，执行回调函数 serviceInstancesChangedListener = serviceDiscovery.createListener(serviceNames); // 对每一个 serviceName 都获取 instances，然后执行回调函数 for (String serviceName : serviceNames) { List\u003cServiceInstance\u003e serviceInstances = serviceDiscovery.getInstances(serviceName); if (CollectionUtils.isNotEmpty(serviceInstances)) { // 这个方法很重要，重点分析 serviceInstancesChangedListener.onEvent(new ServiceInstancesChangedEvent(serviceName, serviceInstances)); } } // 添加缓存 serviceListeners.put(serviceNamesKey, serviceInstancesChangedListener); } ... } finally { appSubscriptionLock.unlock(); } } 源码位置: org.apache.dubbo.registry.client.event.listener.ServiceInstancesChangedListener#onEvent public void onEvent(ServiceInstancesChangedEvent event) { // 判断 event if (destroyed.get() || !accept(event) || isRetryAndExpired(event)) { return; } // 处理 event doOnEvent(event); } 源码位置: org.apache.dubbo.registry.client.event.listener.ServiceInstancesChangedListener#doOnEvent // 处理 event private synchronized void doOnEvent(ServiceInstancesChangedEvent event) { if (destroyed.get() || !accept(event) || isRetryAndExpired(event)) { return; } // 刷新实例 refreshInstance(event); Map\u003cString, List\u003cServiceInstance\u003e\u003e revisionToInstances = new HashMap\u003c\u003e(); Map\u003cServiceInfo, Set\u003cString\u003e\u003e localServiceToRevisions = new HashMap\u003c\u003e(); // grouping all instances of this app(service name) by revision // 按照 revsion 来分类 instance for (Map.Entry\u003cString, List\u003cServiceInstance\u003e\u003e entry : allInstances.entrySet()) { List\u003cServiceInstance\u003e instances = entry.getValue(); for (ServiceInstance instance : instances) { String revision = getExportedServicesRevision(instance); if (revision == null || EMPTY_RE","date":"2023-11-27","objectID":"/ooooo-notes/06-%E5%BC%95%E7%94%A8%E6%9C%8D%E5%8A%A1/:1:6","tags":["dubbo","source code","源码分析 dubbo 系列"],"title":"06 引用服务","uri":"/ooooo-notes/06-%E5%BC%95%E7%94%A8%E6%9C%8D%E5%8A%A1/"},{"categories":null,"content":"ServiceDiscoveryRegistryDirectory#notify 执行回调 源码位置: org.apache.dubbo.registry.client.ServiceDiscoveryRegistryDirectory#notify // 回调 urls @Override public synchronized void notify(List\u003cURL\u003e instanceUrls) { if (isDestroyed()) { return; } // Set the context of the address notification thread. RpcServiceContext.getServiceContext().setConsumerUrl(getConsumerUrl()); // 3.x added for extend URL address // 执行 AddressListener 回调 ExtensionLoader\u003cAddressListener\u003e addressListenerExtensionLoader = getUrl().getOrDefaultModuleModel().getExtensionLoader(AddressListener.class); List\u003cAddressListener\u003e supportedListeners = addressListenerExtensionLoader.getActivateExtension(getUrl(), (String[]) null); if (supportedListeners != null \u0026\u0026 !supportedListeners.isEmpty()) { for (AddressListener addressListener : supportedListeners) { instanceUrls = addressListener.notify(instanceUrls, getConsumerUrl(), this); } } // 刷新 invoker refreshOverrideAndInvoker(instanceUrls); } ","date":"2023-11-27","objectID":"/ooooo-notes/06-%E5%BC%95%E7%94%A8%E6%9C%8D%E5%8A%A1/:1:7","tags":["dubbo","source code","源码分析 dubbo 系列"],"title":"06 引用服务","uri":"/ooooo-notes/06-%E5%BC%95%E7%94%A8%E6%9C%8D%E5%8A%A1/"},{"categories":null,"content":" dubbo 基于 3.2.6 版本 在 dubbo 中导出服务的源码是非常复杂的，这里只介绍主要流程。 ","date":"2023-11-25","objectID":"/ooooo-notes/05-%E5%AF%BC%E5%87%BA%E6%9C%8D%E5%8A%A1/:0:0","tags":["dubbo","source code","源码分析 dubbo 系列"],"title":"05 导出服务","uri":"/ooooo-notes/05-%E5%AF%BC%E5%87%BA%E6%9C%8D%E5%8A%A1/"},{"categories":null,"content":"DefaultModuleDeployer#exportServices 导出服务 源码位置: org.apache.dubbo.config.deploy.DefaultModuleDeployer#exportServices // 导出服务 private void exportServices() { // 遍历 serviceConfig for (ServiceConfigBase sc : configManager.getServices()) { exportServiceInternal(sc); } } private void exportServiceInternal(ServiceConfigBase sc) { ServiceConfig\u003c?\u003e serviceConfig = (ServiceConfig\u003c?\u003e) sc; // 刷新服务 if (!serviceConfig.isRefreshed()) { serviceConfig.refresh(); } if (sc.isExported()) { return; } // 异步导出服务 if (exportAsync || sc.shouldExportAsync()) { ExecutorService executor = executorRepository.getServiceExportExecutor(); CompletableFuture\u003cVoid\u003e future = CompletableFuture.runAsync(() -\u003e { try { if (!sc.isExported()) { sc.export(); exportedServices.add(sc); } } catch (Throwable t) { logger.error(CONFIG_FAILED_EXPORT_SERVICE, \"\", \"\", \"Failed to async export service config: \" + getIdentifier() + \" , catch error : \" + t.getMessage(), t); } }, executor); asyncExportingFutures.add(future); } else { // 同步导出服务 if (!sc.isExported()) { sc.export(RegisterTypeEnum.AUTO_REGISTER_BY_DEPLOYER); exportedServices.add(sc); } } } 源码位置: org.apache.dubbo.config.ServiceConfig#export // 导出服务 @Override public void export(RegisterTypeEnum registerType) { if (this.exported) { return; } if (getScopeModel().isLifeCycleManagedExternally()) { // prepare model for reference getScopeModel().getDeployer().prepare(); } else { // ensure start module, compatible with old api usage getScopeModel().getDeployer().start(); } synchronized (this) { if (this.exported) { return; } // 刷新配置 if (!this.isRefreshed()) { this.refresh(); } if (this.shouldExport()) { // 初始化，这里是初始化 serviceListeners 和 serviceMetadata this.init(); if (shouldDelay()) { // should register if delay export // 延迟导出 doDelayExport(); } else if (Integer.valueOf(-1).equals(getDelay()) \u0026\u0026 Boolean.parseBoolean(ConfigurationUtils.getProperty( getScopeModel(), CommonConstants.DUBBO_MANUAL_REGISTER_KEY, \"false\"))) { // should not register by default doExport(RegisterTypeEnum.MANUAL_REGISTER); } else { // 导出服务 doExport(registerType); } } } } 源码位置: org.apache.dubbo.config.ServiceConfig#doExport protected synchronized void doExport(RegisterTypeEnum registerType) { if (unexported) { throw new IllegalStateException(\"The service \" + interfaceClass.getName() + \" has already unexported!\"); } if (exported) { return; } if (StringUtils.isEmpty(path)) { path = interfaceName; } // 导出 urls，这个很重要 doExportUrls(registerType); // 标记已导出，执行 serviceNameMapping.map(url)，这个很重要 exported(); } ","date":"2023-11-25","objectID":"/ooooo-notes/05-%E5%AF%BC%E5%87%BA%E6%9C%8D%E5%8A%A1/:1:0","tags":["dubbo","source code","源码分析 dubbo 系列"],"title":"05 导出服务","uri":"/ooooo-notes/05-%E5%AF%BC%E5%87%BA%E6%9C%8D%E5%8A%A1/"},{"categories":null,"content":"doExportUrls 导出接口 源码位置: org.apache.dubbo.config.ServiceConfig#doExportUrls // 导出 urls // 因为 dubbo 支持多协议服务，所以需要对每个协议执行导出服务 private void doExportUrls(RegisterTypeEnum registerType) { // 获取 ServiceRepository，调用的时候，可以通过这个来获取接口的元数据信息 ModuleServiceRepository repository = getScopeModel().getServiceRepository(); ServiceDescriptor serviceDescriptor; final boolean serverService = ref instanceof ServerService; if (serverService) { serviceDescriptor = ((ServerService) ref).getServiceDescriptor(); // 注册服务元数据 repository.registerService(serviceDescriptor); } else { // 注册服务元数据 serviceDescriptor = repository.registerService(getInterfaceClass()); } // 创建 ProviderModel providerModel = new ProviderModel(serviceMetadata.getServiceKey(), ref, serviceDescriptor, getScopeModel(), serviceMetadata, interfaceClassLoader); // Compatible with dependencies on ServiceModel#getServiceConfig(), and will be removed in a future version providerModel.setConfig(this); providerModel.setDestroyRunner(getDestroyRunner()); // 注册 provider repository.registerProvider(providerModel); // 加载注册中心配置, 很重要 List\u003cURL\u003e registryURLs = ConfigValidationUtils.loadRegistries(this, true); // 遍历协议 for (ProtocolConfig protocolConfig : protocols) { String pathKey = URL.buildKey(getContextPath(protocolConfig) .map(p -\u003e p + \"/\" + path) .orElse(path), group, version); // stub service will use generated service name if (!serverService) { // In case user specified path, register service one more time to map it to path. repository.registerService(pathKey, interfaceClass); } // 对每个协议导出服务，很重要 doExportUrlsFor1Protocol(protocolConfig, registryURLs, registerType); } // 设置 urls providerModel.setServiceUrls(urls); } ","date":"2023-11-25","objectID":"/ooooo-notes/05-%E5%AF%BC%E5%87%BA%E6%9C%8D%E5%8A%A1/:1:1","tags":["dubbo","source code","源码分析 dubbo 系列"],"title":"05 导出服务","uri":"/ooooo-notes/05-%E5%AF%BC%E5%87%BA%E6%9C%8D%E5%8A%A1/"},{"categories":null,"content":"loadRegistries 加载注册中心 源码位置: org.apache.dubbo.config.utils.ConfigValidationUtils#loadRegistries // 加载注册中心配置 public static List\u003cURL\u003e loadRegistries(AbstractInterfaceConfig interfaceConfig, boolean provider) { // check \u0026\u0026 override if necessary List\u003cURL\u003e registryList = new ArrayList\u003c\u003e(); ApplicationConfig application = interfaceConfig.getApplication(); List\u003cRegistryConfig\u003e registries = interfaceConfig.getRegistries(); if (CollectionUtils.isNotEmpty(registries)) { // 遍历 registries for (RegistryConfig config : registries) { // try to refresh registry in case it is set directly by user using config.setRegistries() // 刷新配置 if (!config.isRefreshed()) { config.refresh(); } String address = config.getAddress(); if (StringUtils.isEmpty(address)) { address = ANYHOST_VALUE; } // 是可用的地址 if (!RegistryConfig.NO_AVAILABLE.equalsIgnoreCase(address)) { // 组合参数 Map\u003cString, String\u003e map = new HashMap\u003cString, String\u003e(); AbstractConfig.appendParameters(map, application); AbstractConfig.appendParameters(map, config); map.put(PATH_KEY, RegistryService.class.getName()); AbstractInterfaceConfig.appendRuntimeParameters(map); if (!map.containsKey(PROTOCOL_KEY)) { map.put(PROTOCOL_KEY, DUBBO_PROTOCOL); } List\u003cURL\u003e urls = UrlUtils.parseURLs(address, map); // 组合 url for (URL url : urls) { url = URLBuilder.from(url) // 保留原始的协议类型，对服务级别注册有用 .addParameter(REGISTRY_KEY, url.getProtocol()) // 提取注册类型，设置协议，这个很重要，在 dubbo 3.0 有服务级别和接口级别两种注册方式 .setProtocol(extractRegistryType(url)) .setScopeModel(interfaceConfig.getScopeModel()) .build(); // provider delay register state will be checked in RegistryProtocol#export if (provider || url.getParameter(SUBSCRIBE_KEY, true)) { registryList.add(url); } } } } } // 兼容处理，不解析 return genCompatibleRegistries(interfaceConfig.getScopeModel(), registryList, provider); } ","date":"2023-11-25","objectID":"/ooooo-notes/05-%E5%AF%BC%E5%87%BA%E6%9C%8D%E5%8A%A1/:1:2","tags":["dubbo","source code","源码分析 dubbo 系列"],"title":"05 导出服务","uri":"/ooooo-notes/05-%E5%AF%BC%E5%87%BA%E6%9C%8D%E5%8A%A1/"},{"categories":null,"content":"doExportUrlsFor1Protocol 对单协议导出 源码位置: org.apache.dubbo.config.ServiceConfig#doExportUrlsFor1Protocol // 对单个协议导出 private void doExportUrlsFor1Protocol(ProtocolConfig protocolConfig, List\u003cURL\u003e registryURLs, RegisterTypeEnum registerType) { Map\u003cString, String\u003e map = buildAttributes(protocolConfig); // remove null key and null value map.keySet().removeIf(key -\u003e StringUtils.isEmpty(key) || StringUtils.isEmpty(map.get(key))); // init serviceMetadata attachments serviceMetadata.getAttachments().putAll(map); // 构建 url URL url = buildUrl(protocolConfig, map); // 设置单独的 executor, 之后会用这个 executor 去执行请求 processServiceExecutor(url); // 导出 url exportUrl(url, registryURLs, registerType); initServiceMethodMetrics(url); } 源码位置: org.apache.dubbo.config.ServiceConfig#exportUrl // 导出 url private void exportUrl(URL url, List\u003cURL\u003e registryURLs, RegisterTypeEnum registerType) { // 获取 scope, scope 分为 remote 和 local, 默认为空，表示两种都会导出 String scope = url.getParameter(SCOPE_KEY); // don't export when none is configured if (!SCOPE_NONE.equalsIgnoreCase(scope)) { // export to local if the config is not remote (export to remote only when config is remote) if (!SCOPE_REMOTE.equalsIgnoreCase(scope)) { // 导出本地服务, 不解析这个 exportLocal(url); } // export to remote if the config is not local (export to local only when config is local) if (!SCOPE_LOCAL.equalsIgnoreCase(scope)) { // export to extra protocol is used in remote export // extProtocol 默认为空 String extProtocol = url.getParameter(\"ext.protocol\", \"\"); List\u003cString\u003e protocols = new ArrayList\u003c\u003e(); if (StringUtils.isNotBlank(extProtocol)) { // export original url url = URLBuilder.from(url). addParameter(IS_PU_SERVER_KEY, Boolean.TRUE.toString()). removeParameter(\"ext.protocol\"). build(); } // 导出远程服务, 这个很重要 url = exportRemote(url, registryURLs, registerType); if (!isGeneric(generic) \u0026\u0026 !getScopeModel().isInternal()) { // 元数据中心，发布服务定义，这个很重要 MetadataUtils.publishServiceDefinition(url, providerModel.getServiceModel(), getApplicationModel()); } if (StringUtils.isNotBlank(extProtocol)) { String[] extProtocols = extProtocol.split(\",\", -1); protocols.addAll(Arrays.asList(extProtocols)); } // export extra protocols // 导出额外协议 for (String protocol : protocols) { if (StringUtils.isNotBlank(protocol)) { URL localUrl = URLBuilder.from(url). setProtocol(protocol). build(); // 导出远程服务, 这个很重要 localUrl = exportRemote(localUrl, registryURLs, registerType); if (!isGeneric(generic) \u0026\u0026 !getScopeModel().isInternal()) { // 元数据中心，发布服务定义，这个很重要 MetadataUtils.publishServiceDefinition(localUrl, providerModel.getServiceModel(), getApplicationModel()); } this.urls.add(localUrl); } } } } this.urls.add(url); } ","date":"2023-11-25","objectID":"/ooooo-notes/05-%E5%AF%BC%E5%87%BA%E6%9C%8D%E5%8A%A1/:1:3","tags":["dubbo","source code","源码分析 dubbo 系列"],"title":"05 导出服务","uri":"/ooooo-notes/05-%E5%AF%BC%E5%87%BA%E6%9C%8D%E5%8A%A1/"},{"categories":null,"content":"exportRemote 导出接口 源码位置: org.apache.dubbo.config.ServiceConfig#exportRemote // 对每一个注册中心，都导出接口 private URL exportRemote(URL url, List\u003cURL\u003e registryURLs, RegisterTypeEnum registerType) { if (CollectionUtils.isNotEmpty(registryURLs) \u0026\u0026 registerType != RegisterTypeEnum.NEVER_REGISTER) { // 遍历 registryUrl for (URL registryURL : registryURLs) { // dubbo3 中是 service-registry if (SERVICE_REGISTRY_PROTOCOL.equals(registryURL.getProtocol())) { url = url.addParameterIfAbsent(SERVICE_NAME_MAPPING_KEY, \"true\"); } //if protocol is only injvm ,not register // 如果是 injvm 协议，跳过 if (LOCAL_PROTOCOL.equalsIgnoreCase(url.getProtocol())) { continue; } // 添加 dynamic 参数，表示这个接口是临时的，当注册中心注销时，需要注销这个接口 url = url.addParameterIfAbsent(DYNAMIC_KEY, registryURL.getParameter(DYNAMIC_KEY)); // 添加 monitor 参数 URL monitorUrl = ConfigValidationUtils.loadMonitor(this, registryURL); if (monitorUrl != null) { url = url.putAttribute(MONITOR_KEY, monitorUrl); } // For providers, this is used to enable custom proxy to generate invoker // 添加 proxy 参数，可以自定义代理实现, 默认为 javassist String proxy = url.getParameter(PROXY_KEY); if (StringUtils.isNotEmpty(proxy)) { registryURL = registryURL.addParameter(PROXY_KEY, proxy); } if (logger.isInfoEnabled()) { if (url.getParameter(REGISTER_KEY, true)) { logger.info(\"Register dubbo service \" + interfaceClass.getName() + \" url \" + url + \" to registry \" + registryURL.getAddress()); } else { logger.info(\"Export dubbo service \" + interfaceClass.getName() + \" to url \" + url); } } // 导出 url，这里很重要，把 url 添加到 registryUrl 的 export 参数中 doExportUrl(registryURL.putAttribute(EXPORT_KEY, url), true, registerType); } } else { if (logger.isInfoEnabled()) { logger.info(\"Export dubbo service \" + interfaceClass.getName() + \" to url \" + url); } // 导出url，不会把接口注册到注册中心 doExportUrl(url, true, registerType); } return url; } 源码位置: org.apache.dubbo.config.ServiceConfig#doExportUrl // 导出接口 private void doExportUrl(URL url, boolean withMetaData, RegisterTypeEnum registerType) { // 在 dubbo3 中，registerType 默认就是 AUTO_REGISTER_BY_DEPLOYER if (!url.getParameter(REGISTER_KEY, true)) { registerType = RegisterTypeEnum.MANUAL_REGISTER; } if (registerType == RegisterTypeEnum.NEVER_REGISTER || registerType == RegisterTypeEnum.MANUAL_REGISTER || registerType == RegisterTypeEnum.AUTO_REGISTER_BY_DEPLOYER) { url = url.addParameter(REGISTER_KEY, false); } // 包装 ref，生成 invoker Invoker\u003c?\u003e invoker = proxyFactory.getInvoker(ref, (Class) interfaceClass, url); if (withMetaData) { invoker = new DelegateProviderMetaDataInvoker(invoker, this); } // 通过 SPI 机制获取对应的实现类，这里的 url 是 registryUrl，实现类为 RegistryProtocol Exporter\u003c?\u003e exporter = protocolSPI.export(invoker); // 注册 exporter exporters.computeIfAbsent(registerType, k -\u003e new CopyOnWriteArrayList\u003c\u003e()).add(exporter); } ","date":"2023-11-25","objectID":"/ooooo-notes/05-%E5%AF%BC%E5%87%BA%E6%9C%8D%E5%8A%A1/:1:4","tags":["dubbo","source code","源码分析 dubbo 系列"],"title":"05 导出服务","uri":"/ooooo-notes/05-%E5%AF%BC%E5%87%BA%E6%9C%8D%E5%8A%A1/"},{"categories":null,"content":"RegistryProtocol#export 导出接口 源码位置: org.apache.dubbo.registry.integration.RegistryProtocol#export // 导出接口 @Override public \u003cT\u003e Exporter\u003cT\u003e export(final Invoker\u003cT\u003e originInvoker) throws RpcException { // 获取 registryUrl URL registryUrl = getRegistryUrl(originInvoker); // 获取 providerUrl, 之前这个url 放在 registryUrl 的 export 参数中 URL providerUrl = getProviderUrl(originInvoker); // Subscribe the override data // FIXME When the provider subscribes, it will affect the scene : a certain JVM exposes the service and call // the same service. Because the subscribed is cached key with the name of the service, it causes the // subscription information to cover. // 一些覆盖配置的监听器，这里包括 provider, service 两个维度的 final URL overrideSubscribeUrl = getSubscribedOverrideUrl(providerUrl); final OverrideListener overrideSubscribeListener = new OverrideListener(overrideSubscribeUrl, originInvoker); Map\u003cURL, Set\u003cNotifyListener\u003e\u003e overrideListeners = getProviderConfigurationListener(overrideSubscribeUrl).getOverrideListeners(); overrideListeners.computeIfAbsent(overrideSubscribeUrl, k -\u003e new ConcurrentHashSet\u003c\u003e()) .add(overrideSubscribeListener); providerUrl = overrideUrlWithConfig(providerUrl, overrideSubscribeListener); //export invoker // 导出接口，这个最重要 final ExporterChangeableWrapper\u003cT\u003e exporter = doLocalExport(originInvoker, providerUrl); // url to registry // 根据 SPI 机制获取对应的 registry 实现类, 比如 ServiceDiscoveryRegistry final Registry registry = getRegistry(registryUrl); // 获取注册的 url final URL registeredProviderUrl = getUrlToRegistry(providerUrl, registryUrl); // decide if we need to delay publish (provider itself and registry should both need to register) // 决定是否要注册 url，刚才是 AUTO_REGISTER_BY_DEPLOYER，所以不会注册 boolean register = providerUrl.getParameter(REGISTER_KEY, true) \u0026\u0026 registryUrl.getParameter(REGISTER_KEY, true); if (register) { register(registry, registeredProviderUrl); } // register stated url on provider model registerStatedUrl(registryUrl, registeredProviderUrl, register); // 设置一些参数 exporter.setRegisterUrl(registeredProviderUrl); exporter.setSubscribeUrl(overrideSubscribeUrl); exporter.setNotifyListener(overrideSubscribeListener); exporter.setRegistered(register); ... // 执行 RegistryProtocolListener 钩子函数 notifyExport(exporter); //Ensure that a new exporter instance is returned every time export return new DestroyableExporter\u003c\u003e(exporter); } 源码位置: org.apache.dubbo.registry.integration.RegistryProtocol#doLocalExport // 导出接口 private \u003cT\u003e ExporterChangeableWrapper\u003cT\u003e doLocalExport(final Invoker\u003cT\u003e originInvoker, URL providerUrl) { String providerUrlKey = getProviderUrlKey(originInvoker); String registryUrlKey = getRegistryUrlKey(originInvoker); Invoker\u003c?\u003e invokerDelegate = new InvokerDelegate\u003c\u003e(originInvoker, providerUrl); // 根据 SPI 机制获取对应的实现类，如 DubboProtocol, TripleProtocol, 在后面的章节会继续解析 ReferenceCountExporter\u003c?\u003e exporter = exporterFactory.createExporter(psfdsnroviderUrlKey, () -\u003e protocol.export(invokerDelegate)); // 记录导出的接口 return (ExporterChangeableWrapper\u003cT\u003e) bounds.computeIfAbsent(providerUrlKey, _k -\u003e new ConcurrentHashMap\u003c\u003e()) .computeIfAbsent(registryUrlKey, s -\u003e { // ExporterChangeableWrapper 这个类很重要，后续会调用 registry 方法来注册接口 return new ExporterChangeableWrapper\u003c\u003e( (ReferenceCountExporter\u003cT\u003e) exporter, originInvoker); }); } ","date":"2023-11-25","objectID":"/ooooo-notes/05-%E5%AF%BC%E5%87%BA%E6%9C%8D%E5%8A%A1/:1:5","tags":["dubbo","source code","源码分析 dubbo 系列"],"title":"05 导出服务","uri":"/ooooo-notes/05-%E5%AF%BC%E5%87%BA%E6%9C%8D%E5%8A%A1/"},{"categories":null,"content":"ServiceConfig#exported 源码位置: org.apache.dubbo.config.ServiceConfig#exported // 在导出接口后，会调用这个方法 protected void exported() { exported = true; List\u003cURL\u003e exportedURLs = this.getExportedUrls(); exportedURLs.forEach(url -\u003e { if (url.getParameters().containsKey(SERVICE_NAME_MAPPING_KEY)) { ServiceNameMapping serviceNameMapping = ServiceNameMapping.getDefaultExtension(getScopeModel()); ScheduledExecutorService scheduledExecutor = getScopeModel().getBeanFactory() .getBean(FrameworkExecutorRepository.class).getSharedScheduledExecutor(); // 对接口创建对应的 mappping，这样可以根据接口来获取是服务名，很重要 mapServiceName(url, serviceNameMapping, scheduledExecutor); } }); // 执行 ServiceListener 钩子函数 onExported(); } ","date":"2023-11-25","objectID":"/ooooo-notes/05-%E5%AF%BC%E5%87%BA%E6%9C%8D%E5%8A%A1/:1:6","tags":["dubbo","source code","源码分析 dubbo 系列"],"title":"05 导出服务","uri":"/ooooo-notes/05-%E5%AF%BC%E5%87%BA%E6%9C%8D%E5%8A%A1/"},{"categories":null,"content":"DefaultModuleDeployer#registerServices 注册服务 源码位置: org.apache.dubbo.config.deploy.DefaultModuleDeployer#registerServices // 注册服务 private void registerServices() { // 遍历所有的 service for (ServiceConfigBase sc : configManager.getServices()) { if (!Boolean.FALSE.equals(sc.isRegister())) { // 注册 service, 这个很重要 registerServiceInternal(sc); } } // 刷新服务实例, 这个很重要 applicationDeployer.refreshServiceInstance(); } 源码位置: org.apache.dubbo.config.deploy.DefaultModuleDeployer#registerServiceInternal // 注册 service private void registerServiceInternal(ServiceConfigBase sc) { ServiceConfig\u003c?\u003e serviceConfig = (ServiceConfig\u003c?\u003e) sc; // 刷新配置 if (!serviceConfig.isRefreshed()) { serviceConfig.refresh(); } if (!sc.isExported()) { return; } // 注册 sc.register(true); } 源码位置: org.apache.dubbo.config.ServiceConfig#register // 注册, byDeployer 参数为 true @Override public void register(boolean byDeployer) { if (!this.exported) { return; } synchronized (this) { if (!this.exported) { return; } // AUTO_REGISTER 类型的注册 for (Exporter\u003c?\u003e exporter : exporters.getOrDefault(RegisterTypeEnum.AUTO_REGISTER, Collections.emptyList())) { exporter.register(); } // AUTO_REGISTER_BY_DEPLOYER 类型的注册, dubbo3 默认走这里 if (byDeployer) { for (Exporter\u003c?\u003e exporter : exporters.getOrDefault(RegisterTypeEnum.AUTO_REGISTER_BY_DEPLOYER, Collections.emptyList())) { // exporter 实现类为 ExporterChangeableWrapper exporter.register(); } } } } ","date":"2023-11-25","objectID":"/ooooo-notes/05-%E5%AF%BC%E5%87%BA%E6%9C%8D%E5%8A%A1/:2:0","tags":["dubbo","source code","源码分析 dubbo 系列"],"title":"05 导出服务","uri":"/ooooo-notes/05-%E5%AF%BC%E5%87%BA%E6%9C%8D%E5%8A%A1/"},{"categories":null,"content":"ExporterChangeableWrapper#register 源码位置: org.apache.dubbo.registry.integration.RegistryProtocol.ExporterChangeableWrapper#register // ExporterChangeableWrapper 在 RegistryProtocol#doLocalExport 方法中会生成 @Override public void register() { if (registered.compareAndSet(false, true)) { URL registryUrl = getRegistryUrl(originInvoker); // 根据 SPI 机制获取 registry, 实现类为 ServiceDiscoveryRegistry（服务级别注册），ZookeeperRegistry（接口级别注册） Registry registry = getRegistry(registryUrl); // 注册 url，这个很重要，这里只分析 ServiceDiscoveryRegistry#register RegistryProtocol.register(registry, getRegisterUrl()); ProviderModel providerModel = frameworkModel.getServiceRepository() .lookupExportedService(getRegisterUrl().getServiceKey()); // 标记已注册 List\u003cProviderModel.RegisterStatedURL\u003e statedUrls = providerModel.getStatedUrl(); statedUrls.stream() .filter(u -\u003e u.getRegistryUrl().equals(registryUrl) \u0026\u0026 u.getProviderUrl().getProtocol().equals(getRegisterUrl().getProtocol())) .forEach(u -\u003e u.setRegistered(true)); logger.info(\"Registered dubbo service \" + getRegisterUrl().getServiceKey() + \" url \" + getRegisterUrl() + \" to registry \" + registryUrl); } } ","date":"2023-11-25","objectID":"/ooooo-notes/05-%E5%AF%BC%E5%87%BA%E6%9C%8D%E5%8A%A1/:2:1","tags":["dubbo","source code","源码分析 dubbo 系列"],"title":"05 导出服务","uri":"/ooooo-notes/05-%E5%AF%BC%E5%87%BA%E6%9C%8D%E5%8A%A1/"},{"categories":null,"content":"ServiceDiscoveryRegistry#register 服务级别注册 源码位置: org.apache.dubbo.registry.client.ServiceDiscoveryRegistry#register @Override public final void register(URL url) { if (!shouldRegister(url)) { // Should Not Register return; } doRegister(url); } @Override public void doRegister(URL url) { // fixme, add registry-cluster is not necessary anymore url = addRegistryClusterKey(url); // 注册 url serviceDiscovery.register(url); } 源码位置: org.apache.dubbo.registry.client.AbstractServiceDiscovery#register @Override public void register(URL url) { // 只是添加了 url，实际并没有发布 metadataInfo.addService(url); } ","date":"2023-11-25","objectID":"/ooooo-notes/05-%E5%AF%BC%E5%87%BA%E6%9C%8D%E5%8A%A1/:2:2","tags":["dubbo","source code","源码分析 dubbo 系列"],"title":"05 导出服务","uri":"/ooooo-notes/05-%E5%AF%BC%E5%87%BA%E6%9C%8D%E5%8A%A1/"},{"categories":null,"content":"DefaultApplicationDeployer#refreshServiceInstance 刷新服务实例 源码位置: org.apache.dubbo.config.deploy.DefaultApplicationDeployer#refreshServiceInstance @Override public void refreshServiceInstance() { if (registered) { try { // 刷新元数据和实例 ServiceInstanceMetadataUtils.refreshMetadataAndInstance(applicationModel); } catch (Exception e) { logger.error(CONFIG_REFRESH_INSTANCE_ERROR, \"\", \"\", \"Refresh instance and metadata error.\", e); } } } // 刷新元数据和实例 public static void refreshMetadataAndInstance(ApplicationModel applicationModel) { RegistryManager registryManager = applicationModel.getBeanFactory().getBean(RegistryManager.class); // update service instance revision // 对每一个 serviceDiscovery 都更新 registryManager.getServiceDiscoveries().forEach(ServiceDiscovery::update); } ","date":"2023-11-25","objectID":"/ooooo-notes/05-%E5%AF%BC%E5%87%BA%E6%9C%8D%E5%8A%A1/:3:0","tags":["dubbo","source code","源码分析 dubbo 系列"],"title":"05 导出服务","uri":"/ooooo-notes/05-%E5%AF%BC%E5%87%BA%E6%9C%8D%E5%8A%A1/"},{"categories":null,"content":"ServiceDiscovery#update 更新服务实例 源码位置: org.apache.dubbo.registry.client.AbstractServiceDiscovery#update @Override public synchronized void update() throws RuntimeException { if (isDestroy) { return; } // 注册实例, 会根据 metadataInfo 创建 serviceInstance if (this.serviceInstance == null) { register(); } if (!isValidInstance(this.serviceInstance)) { return; } ServiceInstance oldServiceInstance = this.serviceInstance; DefaultServiceInstance newServiceInstance = new DefaultServiceInstance((DefaultServiceInstance) oldServiceInstance); // 计算 revision 是否发生改变 boolean revisionUpdated = calOrUpdateInstanceRevision(newServiceInstance); if (revisionUpdated) { logger.info(String.format(\"Metadata of instance changed, updating instance with revision %s.\", newServiceInstance.getServiceMetadata().getRevision())); // 更新服务实例 doUpdate(oldServiceInstance, newServiceInstance); this.serviceInstance = newServiceInstance; } } 源码位置: org.apache.dubbo.registry.client.AbstractServiceDiscovery#doUpdate protected void doUpdate(ServiceInstance oldServiceInstance, ServiceInstance newServiceInstance) { // 注销旧的服务实例 this.doUnregister(oldServiceInstance); this.serviceInstance = newServiceInstance; if (!EMPTY_REVISION.equals(getExportedServicesRevision(newServiceInstance))) { // 报告服务元数据 reportMetadata(newServiceInstance.getServiceMetadata()); // 注册新的服务实例 this.doRegister(newServiceInstance); } } ","date":"2023-11-25","objectID":"/ooooo-notes/05-%E5%AF%BC%E5%87%BA%E6%9C%8D%E5%8A%A1/:3:1","tags":["dubbo","source code","源码分析 dubbo 系列"],"title":"05 导出服务","uri":"/ooooo-notes/05-%E5%AF%BC%E5%87%BA%E6%9C%8D%E5%8A%A1/"},{"categories":null,"content":"为什么学 在后端开发中，rpc 是我们经常用到的技术，而 dubbo 是 java 的一种流行的 rpc 框架。 ","date":"2023-11-24","objectID":"/ooooo-notes/%E4%BB%8E%E9%9B%B6%E5%AD%A6-dubbo/:1:0","tags":["dubbo","从零学技术系列"],"title":"从零学 dubbo","uri":"/ooooo-notes/%E4%BB%8E%E9%9B%B6%E5%AD%A6-dubbo/"},{"categories":null,"content":"怎么学 在 B 站中，找一个 dubbo 的入门视频来学习 学习github dubbo-samples 学习官方文档 阅读源码github dubbo ","date":"2023-11-24","objectID":"/ooooo-notes/%E4%BB%8E%E9%9B%B6%E5%AD%A6-dubbo/:2:0","tags":["dubbo","从零学技术系列"],"title":"从零学 dubbo","uri":"/ooooo-notes/%E4%BB%8E%E9%9B%B6%E5%AD%A6-dubbo/"},{"categories":null,"content":" 在 spring boot 中，只需要创建一个 bean 实现 filter 接口，spring boot 就会把这个 filter 加入到 servlet 容器中。 在实际使用中，常用的接口就是 OncePerRequestFilter 和 OrderedFilter, 所以来看看 spring boot 是如何适配 servlet 规范。 ","date":"2023-11-18","objectID":"/ooooo-notes/%E9%80%82%E9%85%8D-servlet-%E8%A7%84%E8%8C%83/:0:0","tags":["spring boot","source code","源码分析 spring boot 系列"],"title":"适配 servlet 规范","uri":"/ooooo-notes/%E9%80%82%E9%85%8D-servlet-%E8%A7%84%E8%8C%83/"},{"categories":null,"content":"创建 WebServer 源码位置: org.springframework.boot.web.servlet.context.ServletWebServerApplicationContext#onRefresh // 在 spring 容器刷新时，会调用此方法 @Override protected void onRefresh() { super.onRefresh(); try { // 创建 webServer createWebServer(); } catch (Throwable ex) { throw new ApplicationContextException(\"Unable to start web server\", ex); } } 源码位置: org.springframework.boot.web.servlet.context.ServletWebServerApplicationContext#createWebServer // 创建 webServer private void createWebServer() { WebServer webServer = this.webServer; ServletContext servletContext = getServletContext(); if (webServer == null \u0026\u0026 servletContext == null) { StartupStep createWebServer = this.getApplicationStartup().start(\"spring.boot.webserver.create\"); // 获取工厂类，比如有 tomcat，jetty 的实现, 这个省略了。 ServletWebServerFactory factory = getWebServerFactory(); createWebServer.tag(\"factory\", factory.getClass().toString()); // 创建 webServer，重点看这个 this.webServer = factory.getWebServer(getSelfInitializer()); createWebServer.end(); // 注册钩子 getBeanFactory().registerSingleton(\"webServerGracefulShutdown\", new WebServerGracefulShutdownLifecycle(this.webServer)); getBeanFactory().registerSingleton(\"webServerStartStop\", new WebServerStartStopLifecycle(this, this.webServer)); } ... } 源码位置: org.springframework.boot.web.servlet.context.ServletWebServerApplicationContext#getSelfInitializer // 获取 ServletContextInitializer private org.springframework.boot.web.servlet.ServletContextInitializer getSelfInitializer() { return this::selfInitialize; } private void selfInitialize(ServletContext servletContext) throws ServletException { prepareWebApplicationContext(servletContext); registerApplicationScope(servletContext); WebApplicationContextUtils.registerEnvironmentBeans(getBeanFactory(), servletContext); // 初始化 ServletContextInitializer, 里面就包括 filter，servlet，listener for (ServletContextInitializer beans : getServletContextInitializerBeans()) { beans.onStartup(servletContext); } } // ServletContextInitializerBeans 的构造方法很重要 protected Collection\u003cServletContextInitializer\u003e getServletContextInitializerBeans() { return new ServletContextInitializerBeans(getBeanFactory()); } ","date":"2023-11-18","objectID":"/ooooo-notes/%E9%80%82%E9%85%8D-servlet-%E8%A7%84%E8%8C%83/:1:0","tags":["spring boot","source code","源码分析 spring boot 系列"],"title":"适配 servlet 规范","uri":"/ooooo-notes/%E9%80%82%E9%85%8D-servlet-%E8%A7%84%E8%8C%83/"},{"categories":null,"content":"ServletContextInitializerBeans 适配器 源码位置: org.springframework.boot.web.servlet.ServletContextInitializerBeans#ServletContextInitializerBeans // ServletContextInitializerBeans 的构造函数 public ServletContextInitializerBeans(ListableBeanFactory beanFactory, Class\u003c? extends ServletContextInitializer\u003e... initializerTypes) { this.initializers = new LinkedMultiValueMap\u003c\u003e(); this.initializerTypes = (initializerTypes.length != 0) ? Arrays.asList(initializerTypes) : Collections.singletonList(ServletContextInitializer.class); // 适配 filter，servlet，listener，很重要 addServletContextInitializerBeans(beanFactory); // 适配 filter，servlet，很重要 addAdaptableBeans(beanFactory); // 排序 ServletContextInitializer List\u003cServletContextInitializer\u003e sortedInitializers = this.initializers.values().stream() .flatMap((value) -\u003e value.stream().sorted(AnnotationAwareOrderComparator.INSTANCE)) .collect(Collectors.toList()); this.sortedList = Collections.unmodifiableList(sortedInitializers); logMappings(this.initializers); } 源码位置: org.springframework.boot.web.servlet.ServletContextInitializerBeans#addServletContextInitializerBeans // 适配 filter，servlet，listener private void addServletContextInitializerBeans(ListableBeanFactory beanFactory) { for (Class\u003c? extends ServletContextInitializer\u003e initializerType : this.initializerTypes) { for (Entry\u003cString, ? extends ServletContextInitializer\u003e initializerBean : getOrderedBeansOfType(beanFactory, initializerType)) { addServletContextInitializerBean(initializerBean.getKey(), initializerBean.getValue(), beanFactory); } } } private void addServletContextInitializerBean(String beanName, ServletContextInitializer initializer, ListableBeanFactory beanFactory) { // 适配 servlet if (initializer instanceof ServletRegistrationBean) { Servlet source = ((ServletRegistrationBean\u003c?\u003e) initializer).getServlet(); addServletContextInitializerBean(Servlet.class, beanName, initializer, beanFactory, source); } // 适配 filter else if (initializer instanceof FilterRegistrationBean) { Filter source = ((FilterRegistrationBean\u003c?\u003e) initializer).getFilter(); addServletContextInitializerBean(Filter.class, beanName, initializer, beanFactory, source); } // 适配 filter else if (initializer instanceof DelegatingFilterProxyRegistrationBean) { String source = ((DelegatingFilterProxyRegistrationBean) initializer).getTargetBeanName(); addServletContextInitializerBean(Filter.class, beanName, initializer, beanFactory, source); } // 适配 listener else if (initializer instanceof ServletListenerRegistrationBean) { EventListener source = ((ServletListenerRegistrationBean\u003c?\u003e) initializer).getListener(); addServletContextInitializerBean(EventListener.class, beanName, initializer, beanFactory, source); } else { addServletContextInitializerBean(ServletContextInitializer.class, beanName, initializer, beanFactory, initializer); } } 源码位置: org.springframework.boot.web.servlet.ServletContextInitializerBeans#addAdaptableBeans // 适配 filter，servlet protected void addAdaptableBeans(ListableBeanFactory beanFactory) { MultipartConfigElement multipartConfig = getMultipartConfig(beanFactory); // 适配 servlet addAsRegistrationBean(beanFactory, Servlet.class, new ServletRegistrationBeanAdapter(multipartConfig)); // 适配 filter addAsRegistrationBean(beanFactory, Filter.class, new FilterRegistrationBeanAdapter()); for (Class\u003c?\u003e listenerType : ServletListenerRegistrationBean.getSupportedTypes()) { addAsRegistrationBean(beanFactory, EventListener.class, (Class\u003cEventListener\u003e) listenerType, new ServletListenerRegistrationBeanAdapter()); } } ","date":"2023-11-18","objectID":"/ooooo-notes/%E9%80%82%E9%85%8D-servlet-%E8%A7%84%E8%8C%83/:2:0","tags":["spring boot","source code","源码分析 spring boot 系列"],"title":"适配 servlet 规范","uri":"/ooooo-notes/%E9%80%82%E9%85%8D-servlet-%E8%A7%84%E8%8C%83/"},{"categories":null,"content":"字段更新为null的代码 // 实体类字段设置 @TableField(value = \"LOCK_EXP_TIME_\", updateStrategy = FieldStrategy.IGNORED) private Date lockExpirationTime; // mapper操作 JobEntity jobEntity = new JobEntity(); jobEntity.setId(1); jobEntity.setLockExpirationTime(null); JobEntityMapper.updateById(jobEntity); ","date":"2023-11-07","objectID":"/ooooo-notes/mybatis-plus-%E6%9B%B4%E6%96%B0%E5%AD%97%E6%AE%B5%E4%B8%BA-null-%E7%9A%84%E5%9D%91/:1:0","tags":["mybatis"],"title":"mybatis-plus 更新字段为 null 的坑","uri":"/ooooo-notes/mybatis-plus-%E6%9B%B4%E6%96%B0%E5%AD%97%E6%AE%B5%E4%B8%BA-null-%E7%9A%84%E5%9D%91/"},{"categories":null,"content":"问题 上面的操作可能会抛出下面的异常 ### Cause: org.apache.ibatis.type.TypeException: Could not set parameters for mapping: ParameterMapping{property='et.lockExpirationTime', mode=IN, javaType=class java.lang.Object, jdbcType=null, numericScale=null, resultMapId='null', jdbcTypeName='null', expression='null'}. Cause: org.apache.ibatis.type.TypeException: Error setting null for parameter #2 with JdbcType OTHER . Try setting a different JdbcType for this parameter or a different jdbcTypeForNull configuration property. ","date":"2023-11-07","objectID":"/ooooo-notes/mybatis-plus-%E6%9B%B4%E6%96%B0%E5%AD%97%E6%AE%B5%E4%B8%BA-null-%E7%9A%84%E5%9D%91/:2:0","tags":["mybatis"],"title":"mybatis-plus 更新字段为 null 的坑","uri":"/ooooo-notes/mybatis-plus-%E6%9B%B4%E6%96%B0%E5%AD%97%E6%AE%B5%E4%B8%BA-null-%E7%9A%84%E5%9D%91/"},{"categories":null,"content":"解决方法 // 添加 jdbcType @TableField(value = \"LOCK_EXP_TIME_\", updateStrategy = FieldStrategy.IGNORED, jdbcType = JdbcType.TIMESTAMP) private Date lockExpirationTime; ","date":"2023-11-07","objectID":"/ooooo-notes/mybatis-plus-%E6%9B%B4%E6%96%B0%E5%AD%97%E6%AE%B5%E4%B8%BA-null-%E7%9A%84%E5%9D%91/:3:0","tags":["mybatis"],"title":"mybatis-plus 更新字段为 null 的坑","uri":"/ooooo-notes/mybatis-plus-%E6%9B%B4%E6%96%B0%E5%AD%97%E6%AE%B5%E4%B8%BA-null-%E7%9A%84%E5%9D%91/"},{"categories":null,"content":"相应的源码 源码位置: com.baomidou.mybatisplus.core.MybatisParameterHandler#setParameters try { typeHandler.setParameter(ps, i + 1, value, jdbcType); } catch (TypeException | SQLException e) { throw new TypeException(\"Could not set parameters for mapping: \" + parameterMapping + \". Cause: \" + e, e); } ","date":"2023-11-07","objectID":"/ooooo-notes/mybatis-plus-%E6%9B%B4%E6%96%B0%E5%AD%97%E6%AE%B5%E4%B8%BA-null-%E7%9A%84%E5%9D%91/:4:0","tags":["mybatis"],"title":"mybatis-plus 更新字段为 null 的坑","uri":"/ooooo-notes/mybatis-plus-%E6%9B%B4%E6%96%B0%E5%AD%97%E6%AE%B5%E4%B8%BA-null-%E7%9A%84%E5%9D%91/"},{"categories":null,"content":" rocketmq 基于 5.1.4 版本 在 rocketmq 中，消息分为多个类型，比如普通消息、批量消息、延迟消息、事务消息等，这一节主要介绍普通消息的逻辑，后面的章节会继续介绍其他消息。 ","date":"2023-10-20","objectID":"/ooooo-notes/03-producer-%E5%8F%91%E9%80%81%E6%B6%88%E6%81%AF/:0:0","tags":["rocketmq","source code","源码分析 rocketmq 系列"],"title":"03 producer 发送消息","uri":"/ooooo-notes/03-producer-%E5%8F%91%E9%80%81%E6%B6%88%E6%81%AF/"},{"categories":null,"content":"producer 发送消息 源码位置: org.apache.rocketmq.client.producer.DefaultMQProducer#send // 发送消息 public SendResult send(Message msg) throws MQClientException, RemotingException, MQBrokerException, InterruptedException { msg.setTopic(withNamespace(msg.getTopic())); // autoBatch 默认 false if (this.getAutoBatch() \u0026\u0026 !(msg instanceof MessageBatch)) { // 通过累加器来实现批量消息，增大吞吐量 return sendByAccumulator(msg, null, null); } else { // 直接发送, 最终会调用 DefaultMQProducerImpl#sendDefaultImpl return sendDirect(msg, null, null); } } 源码位置: org.apache.rocketmq.client.impl.producer.DefaultMQProducerImpl#sendDefaultImpl // 发送消息 private SendResult sendDefaultImpl( Message msg, final CommunicationMode communicationMode, final SendCallback sendCallback, final long timeout ) throws MQClientException, RemotingException, MQBrokerException, InterruptedException { this.makeSureStateOK(); // 校验消息 Validators.checkMessage(msg, this.defaultMQProducer); final long invokeID = random.nextLong(); long beginTimestampFirst = System.currentTimeMillis(); long beginTimestampPrev = beginTimestampFirst; long endTimestamp = beginTimestampFirst; // 获取 topic，如果没有，会从 namesrv 中同步 topic TopicPublishInfo topicPublishInfo = this.tryToFindTopicPublishInfo(msg.getTopic()); if (topicPublishInfo != null \u0026\u0026 topicPublishInfo.ok()) { boolean callTimeout = false; MessageQueue mq = null; Exception exception = null; SendResult sendResult = null; // 同步调用，设置重试次数 int timesTotal = communicationMode == CommunicationMode.SYNC ? 1 + this.defaultMQProducer.getRetryTimesWhenSendFailed() : 1; int times = 0; String[] brokersSent = new String[timesTotal]; boolean resetIndex = false; // 如果失败，重试 for (; times \u003c timesTotal; times++) { String lastBrokerName = null == mq ? null : mq.getBrokerName(); if (times \u003e 0) { resetIndex = true; } // 根据策略来选择 queue, 实现方式有随机、可用性、延迟 MessageQueue mqSelected = this.selectOneMessageQueue(topicPublishInfo, lastBrokerName, resetIndex); if (mqSelected != null) { mq = mqSelected; // 记录每次选择的 queue brokersSent[times] = mq.getBrokerName(); try { ... // 发送消息, 很重要 sendResult = this.sendKernelImpl(msg, mq, communicationMode, sendCallback, topicPublishInfo, timeout - costTime); endTimestamp = System.currentTimeMillis(); // 更新延迟和可用性 this.updateFaultItem(mq.getBrokerName(), endTimestamp - beginTimestampPrev, false, true); // 根据通信模式来返回结果 switch (communicationMode) { case ASYNC: return null; case ONEWAY: return null; case SYNC: if (sendResult.getSendStatus() != SendStatus.SEND_OK) { if (this.defaultMQProducer.isRetryAnotherBrokerWhenNotStoreOK()) { continue; } } return sendResult; default: break; } } catch (MQClientException e) { endTimestamp = System.currentTimeMillis(); // 更新延迟和可用性 this.updateFaultItem(mq.getBrokerName(), endTimestamp - beginTimestampPrev, false, true); ... continue; } catch (RemotingException e) { endTimestamp = System.currentTimeMillis(); // 更新延迟和可用性 this.updateFaultItem(mq.getBrokerName(), endTimestamp - beginTimestampPrev, true, true); ... exception = e; continue; } catch (MQBrokerException e) { endTimestamp = System.currentTimeMillis(); // 更新延迟和可用性 this.updateFaultItem(mq.getBrokerName(), endTimestamp - beginTimestampPrev, true, false); ... exception = e; } catch (InterruptedException e) { endTimestamp = System.currentTimeMillis(); // 更新延迟和可用性 this.updateFaultItem(mq.getBrokerName(), endTimestamp - beginTimestampPrev, false, true); ... throw e; } } else { break; } } if (sendResult != null) { return sendResult; } ... // 抛出异常 throw mqClientException; } ... } 源码位置: org.apache.rocketmq.client.impl.producer.DefaultMQProducerImpl#sendKernelImpl // 发送消息, 很重要 private SendResult sendKernelImpl(final Message msg, final MessageQueue mq, final CommunicationMode communicationMode, final SendCallback sendCallback, final TopicPublishInfo topicPublishInfo, final long timeout) throws MQClientException, RemotingException, MQBrokerException, InterruptedException { long beginStartTime = System.currentTimeMillis(); // 获取 brokerName 和 brokerAddr String brokerName = this.mQClientFactory.getBrokerNameFr","date":"2023-10-20","objectID":"/ooooo-notes/03-producer-%E5%8F%91%E9%80%81%E6%B6%88%E6%81%AF/:1:0","tags":["rocketmq","source code","源码分析 rocketmq 系列"],"title":"03 producer 发送消息","uri":"/ooooo-notes/03-producer-%E5%8F%91%E9%80%81%E6%B6%88%E6%81%AF/"},{"categories":null,"content":"broker 接受消息 源码位置: org.apache.rocketmq.remoting.protocol.header.SendMessageRequestHeader#parseRequestHeader // 处理 RequestCode.SEND_MESSAGE_V2 public static SendMessageRequestHeader parseRequestHeader(RemotingCommand request) throws RemotingCommandException { SendMessageRequestHeaderV2 requestHeaderV2 = null; SendMessageRequestHeader requestHeader = null; switch (request.getCode()) { case RequestCode.SEND_BATCH_MESSAGE: case RequestCode.SEND_MESSAGE_V2: // 获取 requestHeaderV2 requestHeaderV2 = (SendMessageRequestHeaderV2) request .decodeCommandCustomHeader(SendMessageRequestHeaderV2.class); case RequestCode.SEND_MESSAGE: if (null == requestHeaderV2) { requestHeader = (SendMessageRequestHeader) request .decodeCommandCustomHeader(SendMessageRequestHeader.class); } else { // requestHeaderV2 转为 requestHeader requestHeader = SendMessageRequestHeaderV2.createSendMessageRequestHeaderV1(requestHeaderV2); } default: break; } return requestHeader; } 源码位置: org.apache.rocketmq.broker.processor.SendMessageProcessor#processRequest // public RemotingCommand processRequest(ChannelHandlerContext ctx, RemotingCommand request) throws RemotingCommandException { SendMessageContext sendMessageContext; switch (request.getCode()) { case RequestCode.CONSUMER_SEND_MSG_BACK: return this.consumerSendMsgBack(ctx, request); default: // 解析 requestHeader SendMessageRequestHeader requestHeader = parseRequestHeader(request); if (requestHeader == null) { return null; } TopicQueueMappingContext mappingContext = this.brokerController.getTopicQueueMappingManager().buildTopicQueueMappingContext(requestHeader, true); // 处理静态 topic RemotingCommand rewriteResult = this.brokerController.getTopicQueueMappingManager().rewriteRequestForStaticTopic(requestHeader, mappingContext); if (rewriteResult != null) { return rewriteResult; } ... RemotingCommand response; if (requestHeader.isBatch()) { // 处理批量消息 response = this.sendBatchMessage(ctx, request, sendMessageContext, requestHeader, mappingContext, (ctx1, response1) -\u003e executeSendMessageHookAfter(response1, ctx1)); } else { // 处理消息 response = this.sendMessage(ctx, request, sendMessageContext, requestHeader, mappingContext, (ctx12, response12) -\u003e executeSendMessageHookAfter(response12, ctx12)); } return response; } } 源码位置: org.apache.rocketmq.broker.processor.SendMessageProcessor#sendMessage // 处理消息 // 代码非常多，沉下心来看 public RemotingCommand sendMessage(final ChannelHandlerContext ctx, final RemotingCommand request, final SendMessageContext sendMessageContext, final SendMessageRequestHeader requestHeader, final TopicQueueMappingContext mappingContext, final SendMessageCallback sendMessageCallback) throws RemotingCommandException { // 设置 response, 添加属性 PROPERTY_MSG_REGION，PROPERTY_TRACE_SWITCH final RemotingCommand response = preSend(ctx, request, requestHeader); if (response.getCode() != -1) { return response; } final SendMessageResponseHeader responseHeader = (SendMessageResponseHeader) response.readCustomHeader(); final byte[] body = request.getBody(); int queueIdInt = requestHeader.getQueueId(); // 获取 topic 配置 TopicConfig topicConfig = this.brokerController.getTopicConfigManager().selectTopicConfig(requestHeader.getTopic()); // 随机选择一个 queue if (queueIdInt \u003c 0) { queueIdInt = randomQueueId(topicConfig.getWriteQueueNums()); } MessageExtBrokerInner msgInner = new MessageExtBrokerInner(); msgInner.setTopic(requestHeader.getTopic()); msgInner.setQueueId(queueIdInt); Map\u003cString, String\u003e oriProps = MessageDecoder.string2messageProperties(requestHeader.getProperties()); // 处理重试和死信消息 if (!handleRetryAndDLQ(requestHeader, response, request, msgInner, topicConfig, oriProps)) { return response; } msgInner.setBody(body); msgInner.setFlag(requestHeader.getFlag()); // 生成消息唯一ID String uniqKey = oriProps.get(MessageConst.PROPERTY_UNIQ_CLIENT_MESSAGE_ID_KEYIDX); if (uniqKey == null || uniqKey.length() \u003c= 0) { uniqKey = MessageClientIDSetter.createUniqID(); oriProps.put(MessageConst.PROPERTY_UNIQ_CLIENT_MESSAGE_ID_KEYIDX, uniqKey); } ... // 设置 tag ","date":"2023-10-20","objectID":"/ooooo-notes/03-producer-%E5%8F%91%E9%80%81%E6%B6%88%E6%81%AF/:2:0","tags":["rocketmq","source code","源码分析 rocketmq 系列"],"title":"03 producer 发送消息","uri":"/ooooo-notes/03-producer-%E5%8F%91%E9%80%81%E6%B6%88%E6%81%AF/"},{"categories":null,"content":"测试类 org.apache.rocketmq.test.client.consumer.topic.OneConsumerMulTopicIT#testSynSendMessage ","date":"2023-10-20","objectID":"/ooooo-notes/03-producer-%E5%8F%91%E9%80%81%E6%B6%88%E6%81%AF/:3:0","tags":["rocketmq","source code","源码分析 rocketmq 系列"],"title":"03 producer 发送消息","uri":"/ooooo-notes/03-producer-%E5%8F%91%E9%80%81%E6%B6%88%E6%81%AF/"},{"categories":null,"content":" rocketmq 基于 5.1.4 版本 ","date":"2023-10-19","objectID":"/ooooo-notes/02-broker-%E6%B3%A8%E5%86%8C-namesvr/:0:0","tags":["rocketmq","source code","源码分析 rocketmq 系列"],"title":"02 broker 注册 namesvr","uri":"/ooooo-notes/02-broker-%E6%B3%A8%E5%86%8C-namesvr/"},{"categories":null,"content":"broker 发起注册请求 源码位置: org.apache.rocketmq.broker.BrokerController#start // 启动定时任务，发起 broker 注册 public void start() throws Exception { ... if (!isIsolated \u0026\u0026 !this.messageStoreConfig.isEnableDLegerCommitLog() \u0026\u0026 !this.messageStoreConfig.isDuplicationEnable()) { changeSpecialServiceStatus(this.brokerConfig.getBrokerId() == MixAll.MASTER_ID); // 注册 broker this.registerBrokerAll(true, false, true); } // 定时任务 scheduledFutures.add(this.scheduledExecutorService.scheduleAtFixedRate(new AbstractBrokerRunnable(this.getBrokerIdentity()) { @Override public void run0() { try { if (System.currentTimeMillis() \u003c shouldStartTime) { BrokerController.LOG.info(\"Register to namesrv after {}\", shouldStartTime); return; } if (isIsolated) { BrokerController.LOG.info(\"Skip register for broker is isolated\"); return; } // 注册 broker BrokerController.this.registerBrokerAll(true, false, brokerConfig.isForceRegister()); } catch (Throwable e) { BrokerController.LOG.error(\"registerBrokerAll Exception\", e); } } }, 1000 * 10, Math.max(10000, Math.min(brokerConfig.getRegisterNameServerPeriod(), 60000)), TimeUnit.MILLISECONDS)); ... } 源码位置: org.apache.rocketmq.broker.BrokerController#registerBrokerAll // 注册 broker, 需要把 broker 的 topic 配置推送到 namesrv 中 public synchronized void registerBrokerAll(final boolean checkOrderConfig, boolean oneway, boolean forceRegister) { ConcurrentMap\u003cString, TopicConfig\u003e topicConfigMap = this.getTopicConfigManager().getTopicConfigTable(); ConcurrentHashMap\u003cString, TopicConfig\u003e topicConfigTable = new ConcurrentHashMap\u003c\u003e(); // 遍历 topic for (TopicConfig topicConfig : topicConfigMap.values()) { // 设置权限 if (!PermName.isWriteable(this.getBrokerConfig().getBrokerPermission()) || !PermName.isReadable(this.getBrokerConfig().getBrokerPermission())) { topicConfigTable.put(topicConfig.getTopicName(), new TopicConfig(topicConfig.getTopicName(), topicConfig.getReadQueueNums(), topicConfig.getWriteQueueNums(), topicConfig.getPerm() \u0026 getBrokerConfig().getBrokerPermission())); } else { topicConfigTable.put(topicConfig.getTopicName(), topicConfig); } // topic 很多，分几个请求 if (this.brokerConfig.isEnableSplitRegistration() \u0026\u0026 topicConfigTable.size() \u003e= this.brokerConfig.getSplitRegistrationSize()) { TopicConfigAndMappingSerializeWrapper topicConfigWrapper = this.getTopicConfigManager().buildSerializeWrapper(topicConfigTable); doRegisterBrokerAll(checkOrderConfig, oneway, topicConfigWrapper); topicConfigTable.clear(); } } // topic 的 TopicQueueMappingInfo, 暂时不用关心 Map\u003cString, TopicQueueMappingInfo\u003e topicQueueMappingInfoMap = this.getTopicQueueMappingManager().getTopicQueueMappingTable().entrySet().stream() .map(entry -\u003e new AbstractMap.SimpleImmutableEntry\u003c\u003e(entry.getKey(), TopicQueueMappingDetail.cloneAsMappingInfo(entry.getValue()))) .collect(Collectors.toMap(Map.Entry::getKey, Map.Entry::getValue)); TopicConfigAndMappingSerializeWrapper topicConfigWrapper = this.getTopicConfigManager(). buildSerializeWrapper(topicConfigTable, topicQueueMappingInfoMap); // 检查是否需要注册 if (this.brokerConfig.isEnableSplitRegistration() || forceRegister || needRegister(this.brokerConfig.getBrokerClusterName(), this.getBrokerAddr(), this.brokerConfig.getBrokerName(), this.brokerConfig.getBrokerId(), this.brokerConfig.getRegisterBrokerTimeoutMills(), this.brokerConfig.isInBrokerContainer())) { // 注册 broker 到所有的 namesrv doRegisterBrokerAll(checkOrderConfig, oneway, topicConfigWrapper); } } 源码位置: org.apache.rocketmq.broker.BrokerController#doRegisterBrokerAll // 注册 broker 到所有的 namesrv protected void doRegisterBrokerAll(boolean checkOrderConfig, boolean oneway, TopicConfigSerializeWrapper topicConfigWrapper) { if (shutdown) { BrokerController.LOG.info(\"BrokerController#doResterBrokerAll: broker has shutdown, no need to register any more.\"); return; } // 发起 RegisterBrokerRequestHeader 请求, RequestCode.REGISTER_BROKER List\u003cRegisterBrokerResult\u003e registerBrokerResultList = this.brokerOuterAPI.registerBrokerAll( this.brokerConfig.getBrokerClusterName(), this.getBrokerAddr(), this.brokerConfig.ge","date":"2023-10-19","objectID":"/ooooo-notes/02-broker-%E6%B3%A8%E5%86%8C-namesvr/:1:0","tags":["rocketmq","source code","源码分析 rocketmq 系列"],"title":"02 broker 注册 namesvr","uri":"/ooooo-notes/02-broker-%E6%B3%A8%E5%86%8C-namesvr/"},{"categories":null,"content":"namesrv 处理注册请求 源码位置: org.apache.rocketmq.namesrv.processor.DefaultRequestProcessor#processRequest public RemotingCommand processRequest(ChannelHandlerContext ctx, RemotingCommand request) throws RemotingCommandException { ... case RequestCode.REGISTER_BROKER: // 注册 broker return this.registerBroker(ctx, request); ... } 源码位置: org.apache.rocketmq.namesrv.processor.DefaultRequestProcessor#registerBroker // 注册 broker public RemotingCommand registerBroker(ChannelHandlerContext ctx, RemotingCommand request) throws RemotingCommandException { // 解析出 RegisterBrokerResponseHeader final RemotingCommand response = RemotingCommand.createResponseCommand(RegisterBrokerResponseHeader.class); final RegisterBrokerResponseHeader responseHeader = (RegisterBrokerResponseHeader) response.readCustomHeader(); final RegisterBrokerRequestHeader requestHeader = (RegisterBrokerRequestHeader) request.decodeCommandCustomHeader(RegisterBrokerRequestHeader.class); // 校验 crc32 if (!checksum(ctx, request, requestHeader)) { response.setCode(ResponseCode.SYSTEM_ERROR); response.setRemark(\"crc32 not match\"); return response; } TopicConfigSerializeWrapper topicConfigWrapper = null; List\u003cString\u003e filterServerList = null; // 获取 topic 配置 Version brokerVersion = MQVersion.value2Version(request.getVersion()); if (brokerVersion.ordinal() \u003e= MQVersion.Version.V3_0_11.ordinal()) { final RegisterBrokerBody registerBrokerBody = extractRegisterBrokerBodyFromRequest(request, requestHeader); topicConfigWrapper = registerBrokerBody.getTopicConfigSerializeWrapper(); filterServerList = registerBrokerBody.getFilterServerList(); } else { // RegisterBrokerBody of old version only contains TopicConfig. topicConfigWrapper = extractRegisterTopicConfigFromRequest(request); } // 注册 broker 和 topic 信息 RegisterBrokerResult result = this.namesrvController.getRouteInfoManager().registerBroker( requestHeader.getClusterName(), requestHeader.getBrokerAddr(), requestHeader.getBrokerName(), requestHeader.getBrokerId(), requestHeader.getHaServerAddr(), request.getExtFields().get(MixAll.ZONE_NAME), requestHeader.getHeartbeatTimeoutMillis(), requestHeader.getEnableActingMaster(), topicConfigWrapper, filterServerList, ctx.channel() ); if (result == null) { // Register single topic route info should be after the broker completes the first registration. response.setCode(ResponseCode.SYSTEM_ERROR); response.setRemark(\"register broker failed\"); return response; } // 返回 master 和 HaMaster 地址 responseHeader.setHaServerAddr(result.getHaServerAddr()); responseHeader.setMasterAddr(result.getMasterAddr()); ... return response; } 源码位置: org.apache.rocketmq.namesrv.routeinfo.RouteInfoManager#registerBroker // 注册 broker 和 topic 信息 // 此方法的代码比较多，但处理逻辑比较清晰 public RegisterBrokerResult registerBroker( final String clusterName, final String brokerAddr, final String brokerName, final long brokerId, final String haServerAddr, final String zoneName, final Long timeoutMillis, final Boolean enableActingMaster, final TopicConfigSerializeWrapper topicConfigWrapper, final List\u003cString\u003e filterServerList, final Channel channel) { RegisterBrokerResult result = new RegisterBrokerResult(); try { this.lock.writeLock().lockInterruptibly(); //init or update the cluster info // 更新 cluster 信息 Set\u003cString\u003e brokerNames = ConcurrentHashMapUtils.computeIfAbsent((ConcurrentHashMap\u003cString, Set\u003cString\u003e\u003e) this.clusterAddrTable, clusterName, k -\u003e new HashSet\u003c\u003e()); brokerNames.add(brokerName); boolean registerFirst = false; // 添加 broker 信息 BrokerData brokerData = this.brokerAddrTable.get(brokerName); if (null == brokerData) { registerFirst = true; brokerData = new BrokerData(clusterName, brokerName, new HashMap\u003c\u003e()); this.brokerAddrTable.put(brokerName, brokerData); } boolean isOldVersionBroker = enableActingMaster == null; brokerData.setEnableActingMaster(!isOldVersionBroker \u0026\u0026 enableActingMaster); brokerData.setZoneName(zoneName); Map\u003cLong, String\u003e brokerAddrsMap = brokerData.getBrokerAddrs(); boolean isMinBrokerIdChanged = false; long prevMinBrokerI","date":"2023-10-19","objectID":"/ooooo-notes/02-broker-%E6%B3%A8%E5%86%8C-namesvr/:2:0","tags":["rocketmq","source code","源码分析 rocketmq 系列"],"title":"02 broker 注册 namesvr","uri":"/ooooo-notes/02-broker-%E6%B3%A8%E5%86%8C-namesvr/"},{"categories":null,"content":"namesrv 检查失效的 broker 源码位置: org.apache.rocketmq.namesrv.NamesrvController#startScheduleService private void startScheduleService() { // 扫描失效的 broker this.scanExecutorService.scheduleAtFixedRate(NamesrvController.this.routeInfoManager::scanNotActiveBroker, 5, this.namesrvConfig.getScanNotActiveBrokerInterval(), TimeUnit.MILLISECONDS); ... } 源码位置: org.apache.rocketmq.namesrv.routeinfo.RouteInfoManager#scanNotActiveBroker // 扫描失效的 broker public void scanNotActiveBroker() { try { log.info(\"start scanNotActiveBroker\"); // 遍历 brokerLiveTable for (Entry\u003cBrokerAddrInfo, BrokerLiveInfo\u003e next : this.brokerLiveTable.entrySet()) { long last = next.getValue().getLastUpdateTimestamp(); long timeoutMillis = next.getValue().getHeartbeatTimeoutMillis(); // 检查时间 if ((last + timeoutMillis) \u003c System.currentTimeMillis()) { RemotingHelper.closeChannel(next.getValue().getChannel()); log.warn(\"The broker channel expired, {} {}ms\", next.getKey(), timeoutMillis); this.onChannelDestroy(next.getKey()); } } } catch (Exception e) { log.error(\"scanNotActiveBroker exception\", e); } } ","date":"2023-10-19","objectID":"/ooooo-notes/02-broker-%E6%B3%A8%E5%86%8C-namesvr/:3:0","tags":["rocketmq","source code","源码分析 rocketmq 系列"],"title":"02 broker 注册 namesvr","uri":"/ooooo-notes/02-broker-%E6%B3%A8%E5%86%8C-namesvr/"},{"categories":null,"content":"测试类 org.apache.rocketmq.test.client.consumer.topic.OneConsumerMulTopicIT#testSynSendMessage ","date":"2023-10-19","objectID":"/ooooo-notes/02-broker-%E6%B3%A8%E5%86%8C-namesvr/:4:0","tags":["rocketmq","source code","源码分析 rocketmq 系列"],"title":"02 broker 注册 namesvr","uri":"/ooooo-notes/02-broker-%E6%B3%A8%E5%86%8C-namesvr/"},{"categories":null,"content":" activiti 基于 8.0.0 版本 在这一节，详细介绍 BoundaryEvent, 这是工作流框架中很重要的节点，同时涉及到定时任务。 先来看看 BoundaryEvent 的 xml 定义 \u003cuserTask id=\"firstTask\" name=\"First Task\" /\u003e \u003c!-- 在到达 firstTask 节点时，会启动一个定时器 --\u003e \u003cboundaryEvent id=\"escalationTimer1\" cancelActivity=\"true\" attachedToRef=\"firstTask\"\u003e \u003ctimerEventDefinition\u003e \u003ctimeDuration\u003ePT2H\u003c/timeDuration\u003e \u003c/timerEventDefinition\u003e \u003c/boundaryEvent\u003e \u003c!-- 在定时器过期之后，会流转到 secondTask 节点 --\u003e \u003csequenceFlow id=\"flow3\" sourceRef=\"escalationTimer1\" targetRef=\"secondTask\" /\u003e \u003cuserTask id=\"secondTask\" name=\"Second Task\" /\u003e ","date":"2023-10-17","objectID":"/ooooo-notes/10-activiti-%E5%AE%9A%E6%97%B6%E4%BB%BB%E5%8A%A1/:0:0","tags":["activiti","source code","源码分析 activiti 系列"],"title":"10 activiti 定时任务","uri":"/ooooo-notes/10-activiti-%E5%AE%9A%E6%97%B6%E4%BB%BB%E5%8A%A1/"},{"categories":null,"content":"执行 BoundaryEvent 源码位置: org.activiti.engine.impl.agenda.ContinueProcessOperation#executeSynchronous // 每流转一个节点，就会执行下面的方法 protected void executeSynchronous(FlowNode flowNode) { commandContext.getHistoryManager().recordActivityStart(execution); .... // Execute any boundary events, sub process boundary events will be executed from the activity behavior if (!inCompensation \u0026\u0026 flowNode instanceof Activity) { // Only activities can have boundary events List\u003cBoundaryEvent\u003e boundaryEvents = ((Activity) flowNode).getBoundaryEvents(); if (CollectionUtil.isNotEmpty(boundaryEvents)) { // 执行 BoundaryEvent executeBoundaryEvents(boundaryEvents, execution); } } // Execute actual behavior ActivityBehavior activityBehavior = (ActivityBehavior) flowNode.getBehavior(); if (activityBehavior != null) { executeActivityBehavior(activityBehavior, flowNode); } else { logger.debug(\"No activityBehavior on activity '{}' with execution {}\", flowNode.getId(), execution.getId()); Context.getAgenda().planTakeOutgoingSequenceFlowsOperation(execution, true); } } 源码位置: org.activiti.engine.impl.agenda.ContinueProcessOperation#executeBoundaryEvents // 执行 BoundaryEvent protected void executeBoundaryEvents(Collection\u003cBoundaryEvent\u003e boundaryEvents, ExecutionEntity execution) { // The parent execution becomes a scope, and a child execution is created for each of the boundary events // 遍历 boundaryEvents for (BoundaryEvent boundaryEvent : boundaryEvents) { if (CollectionUtil.isEmpty(boundaryEvent.getEventDefinitions()) || (boundaryEvent.getEventDefinitions().get(0) instanceof CompensateEventDefinition)) { continue; } // A Child execution of the current execution is created to represent the boundary event being active // 创建子节点, 会新增一条 ACT_RU_EXECUTION 表的数据 ExecutionEntity childExecutionEntity = commandContext.getExecutionEntityManager().createChildExecution((ExecutionEntity) execution); childExecutionEntity.setParentId(execution.getId()); childExecutionEntity.setCurrentFlowElement(boundaryEvent); childExecutionEntity.setScope(false); // 获取 behavior ActivityBehavior boundaryEventBehavior = ((ActivityBehavior) boundaryEvent.getBehavior()); logger.debug(\"Executing boundary event activityBehavior {} with execution {}\", boundaryEventBehavior.getClass(), childExecutionEntity.getId()); // 执行 behavior boundaryEventBehavior.execute(childExecutionEntity); } } 源码位置: org.activiti.engine.impl.bpmn.behavior.BoundaryTimerEventActivityBehavior#execute // 执行 behavior public void execute(DelegateExecution execution) { ExecutionEntity executionEntity = (ExecutionEntity) execution; if (!(execution.getCurrentFlowElement() instanceof BoundaryEvent)) { throw new ActivitiException(\"Programmatic error: \" + this.getClass() + \" should not be used for anything else than a boundary event\"); } JobManager jobManager = Context.getCommandContext().getJobManager(); // 创建定时任务 TimerJobEntity timerJob = jobManager.createTimerJob(timerEventDefinition, interrupting, executionEntity, TriggerTimerEventJobHandler.TYPE, TimerEventHandler.createConfiguration(execution.getCurrentActivityId(), timerEventDefinition.getEndDate(), timerEventDefinition.getCalendarName())); if (timerJob != null) { // 调度定时任务, 会新增一条 ACT_RU_TIMER_JOB 表 jobManager.scheduleTimerJob(timerJob); } } public void scheduleTimerJob(TimerJobEntity timerJob) { ... // 会新增一条 ACT_RU_TIMER_JOB 表 processEngineConfiguration.getTimerJobEntityManager().insert(timerJob); .. } 从上面的代码可以看出，当遇到 BoundaryEvent 时，数据库就会插入两条数据，分别为 ACT_RU_EXECUTION 和 ACT_RU_TIMER_JOB。当前节点如果停住了，这些数据就会持久化到数据库中。 ","date":"2023-10-17","objectID":"/ooooo-notes/10-activiti-%E5%AE%9A%E6%97%B6%E4%BB%BB%E5%8A%A1/:1:0","tags":["activiti","source code","源码分析 activiti 系列"],"title":"10 activiti 定时任务","uri":"/ooooo-notes/10-activiti-%E5%AE%9A%E6%97%B6%E4%BB%BB%E5%8A%A1/"},{"categories":null,"content":"轮询 timerJob 源码位置: org.activiti.engine.impl.asyncexecutor.AcquireTimerJobsRunnable#run // 在工作流框架启动时,这个轮询任务就会执行 public synchronized void run() { log.info(\"{} starting to acquire async jobs due\"); Thread.currentThread().setName(\"activiti-acquire-timer-jobs\"); final CommandExecutor commandExecutor = asyncExecutor.getProcessEngineConfiguration().getCommandExecutor(); while (!isInterrupted) { try { // 获取到期的 timerJob final AcquiredTimerJobEntities acquiredJobs = commandExecutor.execute(new AcquireTimerJobsCmd(asyncExecutor)); commandExecutor.execute(new Command\u003cVoid\u003e() { @Override public Void execute(CommandContext commandContext) { for (TimerJobEntity job : acquiredJobs.getJobs()) { // 移动 timerJob 到 job 中，也就是 ACT_RU_TIMER_JOB 表数据到 ACT_RU_JOB 表 jobManager.moveTimerJobToExecutableJob(job); } return null; } }); } catch (ActivitiOptimisticLockingException optimisticLockingException) { // 发生这个异常，说明同时有另外一个服务获取相同的 timerJob if (log.isDebugEnabled()) { log.debug(\"Optimistic locking exception during timer job acquisition. If you have multiple timer executors running against the same database, \" + \"this exception means that this thread tried to acquire a timer job, which already was acquired by another timer executor acquisition thread.\" + \"This is expected behavior in a clustered environment. \" + \"You can ignore this message if you indeed have multiple timer executor acquisition threads running against the same database. \" + \"Exception message: {}\", optimisticLockingException.getMessage()); } } catch (Throwable e) { log.error(\"exception during timer job acquisition: {}\", e.getMessage(), e); millisToWait = asyncExecutor.getDefaultTimerJobAcquireWaitTimeInMillis(); } ...省略等待的代码 } log.info(\"{} stopped async job due acquisition\"); } 源码位置: org.activiti.engine.impl.cmd.AcquireTimerJobsCmd#execute // 获取到期的 timerJob public AcquiredTimerJobEntities execute(CommandContext commandContext) { AcquiredTimerJobEntities acquiredJobs = new AcquiredTimerJobEntities(); // 查找到期的 timerJob, mapperId = selectTimerJobsToExecute List\u003cTimerJobEntity\u003e timerJobs = commandContext.getTimerJobEntityManager() .findTimerJobsToExecute(new Page(0, asyncExecutor.getMaxAsyncJobsDuePerAcquisition())); for (TimerJobEntity job : timerJobs) { // 锁定 timerJob, 防止有其他服务获取 // 这里只是设置了属性，因为 TimerJobEntity 实现了 HasRevision 接口，会根据 reversion 来判断是否并发 lockJob(commandContext, job, asyncExecutor.getAsyncJobLockTimeInMillis()); acquiredJobs.addJob(job); } return acquiredJobs; } 源码位置: org.activiti.engine.impl.asyncexecutor.DefaultJobManager#moveTimerJobToExecutableJob // 移动 timerJob 到 job 中 public JobEntity moveTimerJobToExecutableJob(TimerJobEntity timerJob) { ... // 创建 job JobEntity executableJob = createExecutableJobFromOtherJob(timerJob); // 插入 job boolean insertSuccesful = processEngineConfiguration.getJobEntityManager().insertJobEntity(executableJob); if (insertSuccesful) { // 删除 timerJob processEngineConfiguration.getTimerJobEntityManager().delete(timerJob); // 触发 job 执行，最终会执行 asyncExecutor.executeAsyncJob(job) triggerExecutorIfNeeded(executableJob); return executableJob; } return null; } 从上面的代码可以看出，先会筛选到期的 timerJob, 然后移动到 job 中，然后再触发执行。 ","date":"2023-10-17","objectID":"/ooooo-notes/10-activiti-%E5%AE%9A%E6%97%B6%E4%BB%BB%E5%8A%A1/:2:0","tags":["activiti","source code","源码分析 activiti 系列"],"title":"10 activiti 定时任务","uri":"/ooooo-notes/10-activiti-%E5%AE%9A%E6%97%B6%E4%BB%BB%E5%8A%A1/"},{"categories":null,"content":"执行 job 源码位置: org.activiti.engine.impl.asyncexecutor.DefaultAsyncJobExecutor#executeAsyncJob // asyncExecutor.executeAsyncJob(job) public boolean executeAsyncJob(final Job job) { Runnable runnable = null; if (isActive) { // 创建 ExecuteAsyncRunnable runnable = createRunnableForJob(job); try { // 执行 ExecuteAsyncRunnable executorService.execute(runnable); } catch (RejectedExecutionException e) { CommandContext commandContext = Context.getCommandContext(); if (commandContext != null) { // 发生异常，解锁 job commandContext.getJobManager().unacquire(job); } else { processEngineConfiguration.getCommandExecutor().execute(new Command\u003cVoid\u003e() { public Void execute(CommandContext commandContext) { // 发生异常，解锁 job commandContext.getJobManager().unacquire(job); return null; } }); } // Job queue full, returning true so (if wanted) the acquiring can be throttled return false; } } else { temporaryJobQueue.add(job); } return true; } 源码位置: org.activiti.engine.impl.asyncexecutor.ExecuteAsyncRunnable#run // 执行 ExecuteAsyncRunnable public void run() { if (job == null) { job = processEngineConfiguration.getCommandExecutor().execute(new Command\u003cJobEntity\u003e() { @Override public JobEntity execute(CommandContext commandContext) { return commandContext.getJobEntityManager().findById(jobId); } }); } runInternal(); } protected void runInternal(){ // 锁定 job boolean lockNotNeededOrSuccess = lockJobIfNeeded(); if (lockNotNeededOrSuccess) { // 执行 job executeJob(); // 解锁 job unlockJobIfNeeded(); } } 源码位置: org.activiti.engine.impl.asyncexecutor.ExecuteAsyncRunnable#executeJob // 执行 job protected void executeJob() { try { // 执行 ExecuteAsyncJobCmd, 最终就会执行 commandContext.getJobManager().execute(job) processEngineConfiguration.getCommandExecutor().execute(new ExecuteAsyncJobCmd(jobId)); } catch (final ActivitiOptimisticLockingException e) { // 处理失败的 job, 最终会执行 JobRetryCmd，这个就不解析了 handleFailedJob(e); } catch (Throwable exception) { // 处理失败的 job, 最终会执行 JobRetryCmd，这个就不解析了 handleFailedJob(exception); // Finally, Throw the exception to indicate the ExecuteAsyncJobCmd failed String message = \"Job \" + jobId + \" failed\"; log.error(message, exception); } } 源码位置: org.activiti.engine.impl.asyncexecutor.DefaultJobManager#execute // commandContext.getJobManager().execute(job) @Override public void execute(Job job) { if (job instanceof JobEntity) { if (Job.JOB_TYPE_MESSAGE.equals(job.getJobType())) { executeMessageJob((JobEntity) job); } else if (Job.JOB_TYPE_TIMER.equals(job.getJobType())) { // 执行 timerJob executeTimerJob((JobEntity) job); } } else { throw new ActivitiException(\"Only jobs with type JobEntity are supported to be executed\"); } } 源码位置: org.activiti.engine.impl.asyncexecutor.DefaultJobManager#executeTimerJob // 执行 timerJob protected void executeTimerJob(JobEntity timerEntity) { TimerJobEntityManager timerJobEntityManager = processEngineConfiguration.getTimerJobEntityManager(); ... // timerJob 已经到期了，删除 timerJob, dueDate 属性是在 xml 文件中配置的 if (timerEntity.getDuedate() != null \u0026\u0026 !isValidTime(timerEntity, timerEntity.getDuedate(), variableScope)) { if (logger.isDebugEnabled()) { logger.debug(\"Timer {} fired. but the dueDate is after the endDate. Deleting timer.\", timerEntity.getId()); } processEngineConfiguration.getJobEntityManager().delete(timerEntity); return; } // 使用 jobHandler 来执行 job executeJobHandler(timerEntity); // 删除 job processEngineConfiguration.getJobEntityManager().delete(timerEntity); if (logger.isDebugEnabled()) { logger.debug(\"Timer {} fired. Deleting timer.\", timerEntity.getId()); } // repeat 属性不为空，继续创建 timerJob 来执行 if (timerEntity.getRepeat() != null) { TimerJobEntity newTimerJobEntity = timerJobEntityManager.createAndCalculateNextTimer(timerEntity, variableScope); if (newTimerJobEntity != null) { scheduleTimerJob(newTimerJobEntity); } } } 从上面的代码可以看出，当获取到一个 job 时，最终由 jobManager 负责执行 job。 ","date":"2023-10-17","objectID":"/ooooo-notes/10-activiti-%E5%AE%9A%E6%97%B6%E4%BB%BB%E5%8A%A1/:3:0","tags":["activiti","source code","源码分析 activiti 系列"],"title":"10 activiti 定时任务","uri":"/ooooo-notes/10-activiti-%E5%AE%9A%E6%97%B6%E4%BB%BB%E5%8A%A1/"},{"categories":null,"content":"测试类 定时任务的代码在工作流中是最复杂的，一定要多调试几遍。 org.activiti.engine.test.bpmn.event.timer.BoundaryTimerEventTest#testMultipleTimersOnUserTask ","date":"2023-10-17","objectID":"/ooooo-notes/10-activiti-%E5%AE%9A%E6%97%B6%E4%BB%BB%E5%8A%A1/:4:0","tags":["activiti","source code","源码分析 activiti 系列"],"title":"10 activiti 定时任务","uri":"/ooooo-notes/10-activiti-%E5%AE%9A%E6%97%B6%E4%BB%BB%E5%8A%A1/"},{"categories":null,"content":" activiti 基于 8.0.0 版本 工作流操作数据库，并不是直接执行 SQL 语句来完成的，而是通过操作缓存对象来实现的。 ","date":"2023-10-16","objectID":"/ooooo-notes/09-dbsqlsession/:0:0","tags":["activiti","source code","源码分析 activiti 系列"],"title":"09 DbSqlSession","uri":"/ooooo-notes/09-dbsqlsession/"},{"categories":null,"content":"Entity 类 源码位置: org.activiti.engine.impl.persistence.entity.Entity // 每个数据库实体对象都会实现这个接口 public interface Entity { String getId(); void setId(String id); boolean isInserted(); // 标记对象是新增的 void setInserted(boolean inserted); boolean isUpdated(); // 标记对象是更新的 void setUpdated(boolean updated); boolean isDeleted(); // 标记对象是删除的 void setDeleted(boolean deleted); /** * Returns a representation of the object, as would be stored in the database. * Used when deciding if updates have occurred to the object or not since it was last loaded. */ // 持久化状态，当对象的属性没有改动时，不需要更新到数据库 Object getPersistentState(); } ","date":"2023-10-16","objectID":"/ooooo-notes/09-dbsqlsession/:1:0","tags":["activiti","source code","源码分析 activiti 系列"],"title":"09 DbSqlSession","uri":"/ooooo-notes/09-dbsqlsession/"},{"categories":null,"content":"HasRevision 类 源码位置: org.activiti.engine.impl.db.HasRevision // 实现并发控制的实体需要实现这个接口 // 执行 update 语句，类似与 update reversion = ${revsionNext} where reversion = ${reversion} // 判断这个语句的影响条数，就可以知道是否有并发了 public interface HasRevision { void setRevision(int revision); int getRevision(); int getRevisionNext(); } ","date":"2023-10-16","objectID":"/ooooo-notes/09-dbsqlsession/:2:0","tags":["activiti","source code","源码分析 activiti 系列"],"title":"09 DbSqlSession","uri":"/ooooo-notes/09-dbsqlsession/"},{"categories":null,"content":"DbSqlSession 类 源码位置: org.activiti.engine.impl.db.DbSqlSession#insert // 插入实体 public void insert(Entity entity) { // 分配 id if (entity.getId() == null) { String id = dbSqlSessionFactory.getIdGenerator().getNextId(); entity.setId(id); } // 加入缓存 Class\u003c? extends Entity\u003e clazz = entity.getClass(); if (!insertedObjects.containsKey(clazz)) { insertedObjects.put(clazz, new LinkedHashMap\u003cString, Entity\u003e()); // order of insert is important, hence LinkedHashMap } insertedObjects.get(clazz).put(entity.getId(), entity); entityCache.put(entity, false); // False -\u003e entity is inserted, so always changed // 设置为新增 entity.setInserted(true); } 源码位置: org.activiti.engine.impl.db.DbSqlSession#update // 更新实体 public void update(Entity entity) { entityCache.put(entity, false); // false -\u003e we don't store state, meaning it will always be seen as changed // 设置为更新 entity.setUpdated(true); } 源码位置: org.activiti.engine.impl.db.DbSqlSession#delete // 删除实体 public void delete(Entity entity) { // 添加缓存 Class\u003c? extends Entity\u003e clazz = entity.getClass(); if (!deletedObjects.containsKey(clazz)) { deletedObjects.put(clazz, new LinkedHashMap\u003cString, Entity\u003e()); // order of insert is important, hence LinkedHashMap } deletedObjects.get(clazz).put(entity.getId(), entity); // 设置为删除 entity.setDeleted(true); } 源码位置: org.activiti.engine.impl.db.DbSqlSession#flush // 更新到数据库，此时事务还没有提交 public void flush() { // 有些更新对象可能标记删除了，所以需要删除 determineUpdatedObjects(); // Needs to be done before the removeUnnecessaryOperations, as removeUnnecessaryOperations will remove stuff from the cache // 有些新增对象可能标记删除了，所以需要删除 removeUnnecessaryOperations(); if (log.isDebugEnabled()) { debugFlush(); } // 执行 SQL 语句 flushInserts(); flushUpdates(); flushDeletes(); } 源码位置: org.activiti.engine.impl.db.DbSqlSession#commit // 提交事务 public void commit() { sqlSession.commit(); } ","date":"2023-10-16","objectID":"/ooooo-notes/09-dbsqlsession/:3:0","tags":["activiti","source code","源码分析 activiti 系列"],"title":"09 DbSqlSession","uri":"/ooooo-notes/09-dbsqlsession/"},{"categories":null,"content":" activiti 基于 8.0.0 版本 从之前的分析可以发现，工作流的每个操作都是一个 Command, 所以有必要看看内部的实现机制。 ","date":"2023-10-15","objectID":"/ooooo-notes/08-commandexecutor-%E6%89%A7%E8%A1%8C%E5%91%BD%E4%BB%A4/:0:0","tags":["activiti","source code","源码分析 activiti 系列"],"title":"08 CommandExecutor 执行命令","uri":"/ooooo-notes/08-commandexecutor-%E6%89%A7%E8%A1%8C%E5%91%BD%E4%BB%A4/"},{"categories":null,"content":"Command 类 源码位置: org.activiti.engine.impl.interceptor.Command // 接口非常简单，执行过程的参数都从 commandContext 中获取 public interface Command\u003cT\u003e { T execute(CommandContext commandContext); } ","date":"2023-10-15","objectID":"/ooooo-notes/08-commandexecutor-%E6%89%A7%E8%A1%8C%E5%91%BD%E4%BB%A4/:1:0","tags":["activiti","source code","源码分析 activiti 系列"],"title":"08 CommandExecutor 执行命令","uri":"/ooooo-notes/08-commandexecutor-%E6%89%A7%E8%A1%8C%E5%91%BD%E4%BB%A4/"},{"categories":null,"content":"CommandExecutor 类 源码位置: org.activiti.engine.impl.interceptor.CommandExecutor // 接口非常简单，用来执行一个 Command // 实现类为 CommandExecutorImpl public interface CommandExecutor { /** * @return the default {@link CommandConfig}, used if none is provided. */ CommandConfig getDefaultConfig(); /** * Execute a command with the specified {@link CommandConfig}. */ \u003cT\u003e T execute(CommandConfig config, Command\u003cT\u003e command); /** * Execute a command with the default {@link CommandConfig}. */ \u003cT\u003e T execute(Command\u003cT\u003e command); } ","date":"2023-10-15","objectID":"/ooooo-notes/08-commandexecutor-%E6%89%A7%E8%A1%8C%E5%91%BD%E4%BB%A4/:2:0","tags":["activiti","source code","源码分析 activiti 系列"],"title":"08 CommandExecutor 执行命令","uri":"/ooooo-notes/08-commandexecutor-%E6%89%A7%E8%A1%8C%E5%91%BD%E4%BB%A4/"},{"categories":null,"content":"CommandConfig 类 源码位置: org.activiti.engine.impl.interceptor.CommandConfig // 这个类非常重要，控制命令的事务级别和Context复用 public class CommandConfig { private boolean contextReusePossible; private TransactionPropagation propagation; } ","date":"2023-10-15","objectID":"/ooooo-notes/08-commandexecutor-%E6%89%A7%E8%A1%8C%E5%91%BD%E4%BB%A4/:3:0","tags":["activiti","source code","源码分析 activiti 系列"],"title":"08 CommandExecutor 执行命令","uri":"/ooooo-notes/08-commandexecutor-%E6%89%A7%E8%A1%8C%E5%91%BD%E4%BB%A4/"},{"categories":null,"content":"执行命令 源码位置: org.activiti.engine.impl.cfg.CommandExecutorImpl#execute // 执行命令 @Override public \u003cT\u003e T execute(CommandConfig config, Command\u003cT\u003e command) { // 开始执行第一个拦截器 return first.execute(config, command); } 运行 org.activiti.examples.bpmn.receivetask.ReceiveTaskTest#testWaitStateBehavior CommandInterceptor 顺序 从上图可以看出，有四个拦截器，LogInterceptor、CommandContextInterceptor、TransactionContextInterceptor、CommandInvoker。 ","date":"2023-10-15","objectID":"/ooooo-notes/08-commandexecutor-%E6%89%A7%E8%A1%8C%E5%91%BD%E4%BB%A4/:4:0","tags":["activiti","source code","源码分析 activiti 系列"],"title":"08 CommandExecutor 执行命令","uri":"/ooooo-notes/08-commandexecutor-%E6%89%A7%E8%A1%8C%E5%91%BD%E4%BB%A4/"},{"categories":null,"content":"CommandContextInterceptor 类 源码位置: org.activiti.engine.impl.interceptor.CommandContextInterceptor#execute // 这个拦截器用来创建 context public \u003cT\u003e T execute(CommandConfig config, Command\u003cT\u003e command) { CommandContext context = Context.getCommandContext(); boolean contextReused = false; // We need to check the exception, because the transaction can be in a // rollback state, and some other command is being fired to compensate (eg. decrementing job retries) // context 不复用, 需要创建新的 context if (!config.isContextReusePossible() || context == null || context.getException() != null) { context = commandContextFactory.createCommandContext(command); } else { log.debug(\"Valid context found. Reusing it for the current command '{}'\", command.getClass().getCanonicalName()); // 设置复用 contextReused = true; context.setReused(true); } try { // Push on stack // 放入栈中，可以通过 Context 来获取 Context.setCommandContext(context); Context.setProcessEngineConfiguration(processEngineConfiguration); // 执行下一个拦截器 return next.execute(config, command); } catch (Throwable e) { // 设置异常 // 需要注意的是，如果执行有异常，异常会保留在 context 中 // 当有多个 context 时，后面 context 的异常，不会传递到前面的 context context.exception(e); } finally { try { // 如果不复用，关闭 context if (!contextReused) { // 执行语句，但不会提交事务 context.close(); } } finally { // Pop from stack // 从栈中移除 Context.removeCommandContext(); Context.removeProcessEngineConfiguration(); Context.removeBpmnOverrideContext(); } } return null; } ","date":"2023-10-15","objectID":"/ooooo-notes/08-commandexecutor-%E6%89%A7%E8%A1%8C%E5%91%BD%E4%BB%A4/:5:0","tags":["activiti","source code","源码分析 activiti 系列"],"title":"08 CommandExecutor 执行命令","uri":"/ooooo-notes/08-commandexecutor-%E6%89%A7%E8%A1%8C%E5%91%BD%E4%BB%A4/"},{"categories":null,"content":"TransactionContextInterceptor 类 源码位置: org.activiti.engine.impl.interceptor.TransactionContextInterceptor#execute // 这个拦截器用来创建 transactionContext // 与 spring 集成时，会有另外一个拦截器 SpringTransactionInterceptor 来开启事务 public \u003cT\u003e T execute(CommandConfig config, Command\u003cT\u003e command) { CommandContext commandContext = Context.getCommandContext(); // Storing it in a variable, to reference later (it can change during command execution) boolean isReused = commandContext.isReused(); try { if (transactionContextFactory != null \u0026\u0026 !isReused) { // 创建 transactionContext TransactionContext transactionContext = transactionContextFactory.openTransactionContext(commandContext); Context.setTransactionContext(transactionContext); // 添加关闭监听器，在关闭时会提交事务，在异常时会回滚事务 commandContext.addCloseListener(new TransactionCommandContextCloseListener(transactionContext)); } // 执行下一个拦截器 return next.execute(config, command); } finally { if (transactionContextFactory != null \u0026\u0026 !isReused) { Context.removeTransactionContext(); } } } ","date":"2023-10-15","objectID":"/ooooo-notes/08-commandexecutor-%E6%89%A7%E8%A1%8C%E5%91%BD%E4%BB%A4/:6:0","tags":["activiti","source code","源码分析 activiti 系列"],"title":"08 CommandExecutor 执行命令","uri":"/ooooo-notes/08-commandexecutor-%E6%89%A7%E8%A1%8C%E5%91%BD%E4%BB%A4/"},{"categories":null,"content":"SpringTransactionInterceptor 类 源码位置: org.activiti.spring.SpringTransactionInterceptor#execute 拦截器顺序参考: org.activiti.engine.impl.cfg.ProcessEngineConfigurationImpl#getDefaultCommandInterceptors // 与 spring 集成时，这个拦截器会自动激活，负责开始事务 // 这个拦截器在 CommandContextInterceptor 之前 public \u003cT\u003e T execute(final CommandConfig config, final Command\u003cT\u003e command) { LOGGER.debug(\"Running command with propagation {}\", config.getTransactionPropagation()); TransactionTemplate transactionTemplate = new TransactionTemplate(transactionManager); // 设置事务传播行为 transactionTemplate.setPropagationBehavior(getPropagation(config)); // 开启事务 T result = transactionTemplate.execute(new TransactionCallback\u003cT\u003e() { public T doInTransaction(TransactionStatus status) { return next.execute(config, command); } }); return result; } ","date":"2023-10-15","objectID":"/ooooo-notes/08-commandexecutor-%E6%89%A7%E8%A1%8C%E5%91%BD%E4%BB%A4/:7:0","tags":["activiti","source code","源码分析 activiti 系列"],"title":"08 CommandExecutor 执行命令","uri":"/ooooo-notes/08-commandexecutor-%E6%89%A7%E8%A1%8C%E5%91%BD%E4%BB%A4/"},{"categories":null,"content":"CommandInvoker 类 源码位置: org.activiti.engine.impl.interceptor.CommandInvoker#execute // 这个拦截器负责执行命令 public \u003cT\u003e T execute(final CommandConfig config, final Command\u003cT\u003e command) { final CommandContext commandContext = Context.getCommandContext(); // Execute the command. // This will produce operations that will be put on the agenda. // 添加命令到 Agenda 中, 这里的设计很精妙，可以在一个命令中执行另外的命令 commandContext.getAgenda().planOperation(new Runnable() { @Override public void run() { commandContext.setResult(command.execute(commandContext)); } }); // Run loop for agenda // 取出命令来执行 executeOperations(commandContext); // At the end, call the execution tree change listeners. // TODO: optimization: only do this when the tree has actually changed (ie check dbSqlSession). if (commandContext.hasInvolvedExecutions()) { Context.getAgenda().planExecuteInactiveBehaviorsOperation(); executeOperations(commandContext); } // 获取结果 return (T) commandContext.getResult(); } // 取出命令来执行 protected void executeOperations(final CommandContext commandContext) { while (!commandContext.getAgenda().isEmpty()) { Runnable runnable = commandContext.getAgenda().getNextOperation(); executeOperation(runnable); } } ","date":"2023-10-15","objectID":"/ooooo-notes/08-commandexecutor-%E6%89%A7%E8%A1%8C%E5%91%BD%E4%BB%A4/:8:0","tags":["activiti","source code","源码分析 activiti 系列"],"title":"08 CommandExecutor 执行命令","uri":"/ooooo-notes/08-commandexecutor-%E6%89%A7%E8%A1%8C%E5%91%BD%E4%BB%A4/"},{"categories":null,"content":"测试类 org.activiti.examples.bpmn.receivetask.ReceiveTaskTest#testWaitStateBehavior ","date":"2023-10-15","objectID":"/ooooo-notes/08-commandexecutor-%E6%89%A7%E8%A1%8C%E5%91%BD%E4%BB%A4/:9:0","tags":["activiti","source code","源码分析 activiti 系列"],"title":"08 CommandExecutor 执行命令","uri":"/ooooo-notes/08-commandexecutor-%E6%89%A7%E8%A1%8C%E5%91%BD%E4%BB%A4/"},{"categories":null,"content":" rocketmq 基于 5.1.4 版本 ","date":"2023-10-14","objectID":"/ooooo-notes/01-%E6%90%AD%E5%BB%BA-rocketmq-%E6%BA%90%E7%A0%81%E8%B0%83%E8%AF%95%E7%8E%AF%E5%A2%83/:0:0","tags":["rocketmq","source code","源码分析 rocketmq 系列"],"title":"01 搭建 rocketmq 源码调试环境","uri":"/ooooo-notes/01-%E6%90%AD%E5%BB%BA-rocketmq-%E6%BA%90%E7%A0%81%E8%B0%83%E8%AF%95%E7%8E%AF%E5%A2%83/"},{"categories":null,"content":"启动 namesrv 在 org.apache.rocketmq.namesrv.NamesrvStartup 中，配置环境变量 ROCKETMQ_HOME，如下图。 启动 namesrv ","date":"2023-10-14","objectID":"/ooooo-notes/01-%E6%90%AD%E5%BB%BA-rocketmq-%E6%BA%90%E7%A0%81%E8%B0%83%E8%AF%95%E7%8E%AF%E5%A2%83/:1:0","tags":["rocketmq","source code","源码分析 rocketmq 系列"],"title":"01 搭建 rocketmq 源码调试环境","uri":"/ooooo-notes/01-%E6%90%AD%E5%BB%BA-rocketmq-%E6%BA%90%E7%A0%81%E8%B0%83%E8%AF%95%E7%8E%AF%E5%A2%83/"},{"categories":null,"content":"启动 broker 在 org.apache.rocketmq.broker.BrokerController 中，配置环境变量 ROCKETMQ_HOME 和启动参数，如下图。 # -n 指定 namesrv 的地址 # -c 指定 broker 配置文件 -n localhost:9876 -c C:\\Users\\ooooo\\Development\\Code\\Demo\\rocketmq\\rocketmq-home\\conf\\broker.conf 启动 broker ","date":"2023-10-14","objectID":"/ooooo-notes/01-%E6%90%AD%E5%BB%BA-rocketmq-%E6%BA%90%E7%A0%81%E8%B0%83%E8%AF%95%E7%8E%AF%E5%A2%83/:2:0","tags":["rocketmq","source code","源码分析 rocketmq 系列"],"title":"01 搭建 rocketmq 源码调试环境","uri":"/ooooo-notes/01-%E6%90%AD%E5%BB%BA-rocketmq-%E6%BA%90%E7%A0%81%E8%B0%83%E8%AF%95%E7%8E%AF%E5%A2%83/"},{"categories":null,"content":"测试类 在 rocketmq 中有很多的测试类，在看源码的时候，需要调试测试类，比如 org.apache.rocketmq.test.client.consumer.topic.OneConsumerMulTopicIT#testSynSendMessage ","date":"2023-10-14","objectID":"/ooooo-notes/01-%E6%90%AD%E5%BB%BA-rocketmq-%E6%BA%90%E7%A0%81%E8%B0%83%E8%AF%95%E7%8E%AF%E5%A2%83/:3:0","tags":["rocketmq","source code","源码分析 rocketmq 系列"],"title":"01 搭建 rocketmq 源码调试环境","uri":"/ooooo-notes/01-%E6%90%AD%E5%BB%BA-rocketmq-%E6%BA%90%E7%A0%81%E8%B0%83%E8%AF%95%E7%8E%AF%E5%A2%83/"},{"categories":null,"content":" activiti 基于 8.0.0 版本 通过在【agenda流转节点】章节，我们知道了每一个节点的行为由对应的 behavior 来决定，所以有必要看看常用的 behavior 实现。 ","date":"2023-10-14","objectID":"/ooooo-notes/07-%E5%B8%B8%E7%94%A8%E7%9A%84-activitybehavior/:0:0","tags":["activiti","source code","源码分析 activiti 系列"],"title":"07 常用的 ActivityBehavior","uri":"/ooooo-notes/07-%E5%B8%B8%E7%94%A8%E7%9A%84-activitybehavior/"},{"categories":null,"content":"StartEvent 对应的 behavior 类: NoneStartEventActivityBehavior 可以运行 org.activiti.examples.bpmn.receivetask.ReceiveTaskTest#testWaitStateBehavior 来调试。 // 没有实现 execute，直接使用父类的方法 public void execute(DelegateExecution execution) { // 离开节点 leave(execution); } ","date":"2023-10-14","objectID":"/ooooo-notes/07-%E5%B8%B8%E7%94%A8%E7%9A%84-activitybehavior/:1:0","tags":["activiti","source code","源码分析 activiti 系列"],"title":"07 常用的 ActivityBehavior","uri":"/ooooo-notes/07-%E5%B8%B8%E7%94%A8%E7%9A%84-activitybehavior/"},{"categories":null,"content":"ReceiveTask 对应的 behavior 类: ReceiveTaskActivityBehavior 可以运行 org.activiti.examples.bpmn.receivetask.ReceiveTaskTest#testWaitStateBehavior 来调试。 // 空实现，表示停在当前节点 public void execute(DelegateExecution execution) { // Do nothing: waitstate behavior } // 调用 org.activiti.engine.RuntimeService#trigger 方法来流转到下个节点 public void trigger(DelegateExecution execution, String signalName, Object data) { leave(execution); } ","date":"2023-10-14","objectID":"/ooooo-notes/07-%E5%B8%B8%E7%94%A8%E7%9A%84-activitybehavior/:2:0","tags":["activiti","source code","源码分析 activiti 系列"],"title":"07 常用的 ActivityBehavior","uri":"/ooooo-notes/07-%E5%B8%B8%E7%94%A8%E7%9A%84-activitybehavior/"},{"categories":null,"content":"ServiceTask 实际上有多个实现类，分为不同的类型，这里以 DefaultServiceTaskBehavior 为例. 可以运行模块 activiti-examples/activiti-api-basic-connector-example 来调试。 @Override public void execute(DelegateExecution execution) { // 获取 connector Connector connector = getConnector(getImplementation(execution)); // 执行 IntegrationContext integrationContext = connector.apply(integrationContextBuilder.from(execution)); variablesPropagator.propagate(execution, integrationContext.getOutBoundVariables()); // 离开节点 leave(execution); } ","date":"2023-10-14","objectID":"/ooooo-notes/07-%E5%B8%B8%E7%94%A8%E7%9A%84-activitybehavior/:3:0","tags":["activiti","source code","源码分析 activiti 系列"],"title":"07 常用的 ActivityBehavior","uri":"/ooooo-notes/07-%E5%B8%B8%E7%94%A8%E7%9A%84-activitybehavior/"},{"categories":null,"content":"UserTask 对应的 behavior 类: UserTaskActivityBehavior 可以运行 org.activiti.examples.bpmn.usertask.SkipExpressionUserTaskTest#test 来调试 // 这个方法的代码比较多，但是代码结构很清晰 public void execute(DelegateExecution execution) { CommandContext commandContext = Context.getCommandContext(); TaskEntityManager taskEntityManager = commandContext.getTaskEntityManager(); // 创建 TaskEntity，然后填充参数 TaskEntity task = taskEntityManager.create(); ExecutionEntity executionEntity = (ExecutionEntity) execution; task.setExecution(executionEntity); task.setTaskDefinitionKey(userTask.getId()); task.setBusinessKey(executionEntity.getProcessInstanceBusinessKey()); ... // 新增 ACT_RU_TASK 表数据 taskEntityManager.insert(task, executionEntity); ... if (skipUserTask) { // 删除 ACT_RU_TASK 表数据 taskEntityManager.deleteTask(task, null, false, false); // 离开节点 leave(execution); } } ","date":"2023-10-14","objectID":"/ooooo-notes/07-%E5%B8%B8%E7%94%A8%E7%9A%84-activitybehavior/:4:0","tags":["activiti","source code","源码分析 activiti 系列"],"title":"07 常用的 ActivityBehavior","uri":"/ooooo-notes/07-%E5%B8%B8%E7%94%A8%E7%9A%84-activitybehavior/"},{"categories":null,"content":"BoundaryTimerEvent 对应的 behavior 类: BoundaryTimerEventActivityBehavior 可以运行 org.activiti.examples.bpmn.event.timer.BoundaryTimerEventTest#testInterruptingTimerDuration 来调试 这个比较复杂，涉及到工作流的定时器，以后会继续解析 @Override public void execute(DelegateExecution execution) { ExecutionEntity executionEntity = (ExecutionEntity) execution; // 判断是否为边界事件 if (!(execution.getCurrentFlowElement() instanceof BoundaryEvent)) { throw new ActivitiException(\"Programmatic error: \" + this.getClass() + \" should not be used for anything else than a boundary event\"); } JobManager jobManager = Context.getCommandContext().getJobManager(); // 创建定时任务 TimerJobEntity timerJob = jobManager.createTimerJob(timerEventDefinition, interrupting, executionEntity, TriggerTimerEventJobHandler.TYPE, TimerEventHandler.createConfiguration(execution.getCurrentActivityId(), timerEventDefinition.getEndDate(), timerEventDefinition.getCalendarName())); if (timerJob != null) { // 调度定时任务, 插入到 ACT_RU_TIMER_JOB 表 jobManager.scheduleTimerJob(timerJob); } } ","date":"2023-10-14","objectID":"/ooooo-notes/07-%E5%B8%B8%E7%94%A8%E7%9A%84-activitybehavior/:5:0","tags":["activiti","source code","源码分析 activiti 系列"],"title":"07 常用的 ActivityBehavior","uri":"/ooooo-notes/07-%E5%B8%B8%E7%94%A8%E7%9A%84-activitybehavior/"},{"categories":null,"content":"EndEvent 对应的 behavior 类: NoneEndEventActivityBehavior 可以运行 org.activiti.examples.bpmn.receivetask.ReceiveTaskTest#testWaitStateBehavior 来调试。 public void execute(DelegateExecution execution) { // EndEvent 没有连线了，所以会结束流程, 会执行 Agenda#planEndExecutionOperation Context.getAgenda().planTakeOutgoingSequenceFlowsOperation((ExecutionEntity) execution, true); } ","date":"2023-10-14","objectID":"/ooooo-notes/07-%E5%B8%B8%E7%94%A8%E7%9A%84-activitybehavior/:6:0","tags":["activiti","source code","源码分析 activiti 系列"],"title":"07 常用的 ActivityBehavior","uri":"/ooooo-notes/07-%E5%B8%B8%E7%94%A8%E7%9A%84-activitybehavior/"},{"categories":null,"content":" activiti 基于 8.0.0 版本 Agenda 类是工作流框架中非常重要的类，它控制着节点怎么流转。这部分的代码比较复杂，建议多调试几遍。下面的代码实际上是一个闭环，从开始的代码，经过流转一个节点，又回到了开始的代码。 ","date":"2023-10-13","objectID":"/ooooo-notes/06-agenda-%E6%B5%81%E8%BD%AC%E8%8A%82%E7%82%B9/:0:0","tags":["activiti","source code","源码分析 activiti 系列"],"title":"06 Agenda 流转节点","uri":"/ooooo-notes/06-agenda-%E6%B5%81%E8%BD%AC%E8%8A%82%E7%82%B9/"},{"categories":null,"content":"流转节点 源码位置: org.activiti.engine.impl.agenda.DefaultActivitiEngineAgenda#planContinueProcessOperation // 流转 startEvent 节点，在启动流程之后，就会调用这个方法 @Override public void planContinueProcessOperation(ExecutionEntity execution) { // 最终会执行 ContinueProcessOperation#run 方法 planOperation(new ContinueProcessOperation(commandContext, execution)); } 源码位置: org.activiti.engine.impl.agenda.ContinueProcessOperation#run // 执行 ContinueProcessOperation#run 方法 @Override public void run() { // 获取当前节点 FlowElement currentFlowElement = getCurrentFlowElement(execution); if (currentFlowElement instanceof FlowNode) { // 处理节点 continueThroughFlowNode((FlowNode) currentFlowElement); } else if (currentFlowElement instanceof SequenceFlow) { // 处理连线 continueThroughSequenceFlow((SequenceFlow) currentFlowElement); } else { throw new ActivitiException(\"Programmatic error: no current flow element found or invalid type: \" + currentFlowElement + \". Halting.\"); } } 源码位置: org.activiti.engine.impl.agenda.ContinueProcessOperation#continueThroughFlowNode // 处理节点 protected void continueThroughFlowNode(FlowNode flowNode) { // Check if it's the initial flow element. If so, we must fire the execution listeners for the process too if (flowNode.getIncomingFlows() != null \u0026\u0026 flowNode.getIncomingFlows().size() == 0 \u0026\u0026 flowNode.getSubProcess() == null) { // 发布 StartExecution 事件 executeProcessStartExecutionListeners(); } ... if (isMultiInstance(flowNode)) { // the multi instance execution will look at async executeMultiInstanceSynchronous(flowNode); } else if (forceSynchronousOperation || !flowNode.isAsynchronous()) { // 同步执行，这里会等待流转节点完成, 重点分析这个，默认都是同步执行 executeSynchronous(flowNode); } else { // 异步执行, 不会等待 executeAsynchronous(flowNode); } } 源码位置: org.activiti.engine.impl.agenda.ContinueProcessOperation#executeSynchronous // 同步执行，这里会等待流转节点完成 protected void executeSynchronous(FlowNode flowNode) { // 会插入到历史节点表 ACT_HI_ACTINST commandContext.getHistoryManager().recordActivityStart(execution); // Execution listener: event 'start' // 执行监听器，默认为空 if (CollectionUtil.isNotEmpty(flowNode.getExecutionListeners())) { executeExecutionListeners(flowNode, ExecutionListener.EVENTNAME_START); } // Execute any boundary events, sub process boundary events will be executed from the activity behavior if (!inCompensation \u0026\u0026 flowNode instanceof Activity) { // Only activities can have boundary events List\u003cBoundaryEvent\u003e boundaryEvents = ((Activity) flowNode).getBoundaryEvents(); if (CollectionUtil.isNotEmpty(boundaryEvents)) { // 执行 BoundaryEvent，这个很重要，会在以后的章节解析 // 这里会新增一条 ACT_RU_EXECUTION 表的数据 executeBoundaryEvents(boundaryEvents, execution); } } // Execute actual behavior // 获取 behavior, 在【解析流程】章节说过的 ActivityBehavior activityBehavior = (ActivityBehavior) flowNode.getBehavior(); if (activityBehavior != null) { // 执行 behavior // 当前的 flowNode 是 StartEvent，所以 behavior 为 NoneStartEventActivityBehavior executeActivityBehavior(activityBehavior, flowNode); } else { logger.debug(\"No activityBehavior on activity '{}' with execution {}\", flowNode.getId(), execution.getId()); // behavior 为 null，会流转到下个节点 Context.getAgenda().planTakeOutgoingSequenceFlowsOperation(execution, true); } } 源码位置: org.activiti.engine.impl.bpmn.behavior.FlowNodeActivityBehavior#execute // NoneStartEventActivityBehavior 没有实现 execute 方法，所以会调用父类的方法 public void execute(DelegateExecution execution) { // 离开当前节点 leave(execution); } // 离开当前节点 public void leave(DelegateExecution execution) { // 执行 outgoing bpmnActivityBehavior.performDefaultOutgoingBehavior((ExecutionEntity) execution); } // 执行 outgoing protected void performOutgoingBehavior(ExecutionEntity execution, boolean checkConditions, boolean throwExceptionIfExecutionStuck) { // 执行 TakeOutgoingSequenceFlowsOperation#run 方法 getAgenda().planTakeOutgoingSequenceFlowsOperation(execution, true); } 源码位置: org.activiti.engine.impl.agenda.TakeOutgoingSequenceFlowsOperation#run // TakeOutgoingSequenceFlowsOperation @Override public void run() { // 当前节点元素，此时是 startEvent FlowElement currentFlowElement = getCurre","date":"2023-10-13","objectID":"/ooooo-notes/06-agenda-%E6%B5%81%E8%BD%AC%E8%8A%82%E7%82%B9/:1:0","tags":["activiti","source code","源码分析 activiti 系列"],"title":"06 Agenda 流转节点","uri":"/ooooo-notes/06-agenda-%E6%B5%81%E8%BD%AC%E8%8A%82%E7%82%B9/"},{"categories":null,"content":"测试类 org.activiti.examples.bpmn.receivetask.ReceiveTaskTest#testWaitStateBehavior ","date":"2023-10-13","objectID":"/ooooo-notes/06-agenda-%E6%B5%81%E8%BD%AC%E8%8A%82%E7%82%B9/:2:0","tags":["activiti","source code","源码分析 activiti 系列"],"title":"06 Agenda 流转节点","uri":"/ooooo-notes/06-agenda-%E6%B5%81%E8%BD%AC%E8%8A%82%E7%82%B9/"},{"categories":null,"content":" activiti 基于 8.0.0 版本 启动流程的方法有多个，这里以 startProcessInstanceByKey 为入口来分析 ","date":"2023-10-12","objectID":"/ooooo-notes/05-%E5%90%AF%E5%8A%A8%E6%B5%81%E7%A8%8B/:0:0","tags":["activiti","source code","源码分析 activiti 系列"],"title":"05 启动流程","uri":"/ooooo-notes/05-%E5%90%AF%E5%8A%A8%E6%B5%81%E7%A8%8B/"},{"categories":null,"content":"启动流程 源码位置: org.activiti.engine.impl.RuntimeServiceImpl#startProcessInstanceByKey // 启动流程 public ProcessInstance startProcessInstanceByKey(String processDefinitionKey) { // 执行 StartProcessInstanceCmd return commandExecutor.execute(new StartProcessInstanceCmd\u003cProcessInstance\u003e(processDefinitionKey, null, null, null)); } 源码位置: org.activiti.engine.impl.cmd.StartProcessInstanceCmd#execute // 执行 StartProcessInstanceCmd public ProcessInstance execute(CommandContext commandContext) { // 在部署流程时，会将流程定义加入到缓存 DeploymentManager deploymentCache = commandContext.getProcessEngineConfiguration().getDeploymentManager(); ProcessDefinitionRetriever processRetriever = new ProcessDefinitionRetriever(this.tenantId, deploymentCache); // 获取流程定义 ProcessDefinition processDefinition = processRetriever.getProcessDefinition(this.processDefinitionId, this.processDefinitionKey); processInstanceHelper = commandContext.getProcessEngineConfiguration().getProcessInstanceHelper(); // 创建和启动流程实例 ProcessInstance processInstance = createAndStartProcessInstance(processDefinition, businessKey, processInstanceName, variables, transientVariables); return processInstance; } 源码位置: org.activiti.engine.impl.util.ProcessInstanceHelper#createAndStartProcessInstance // 创建和启动流程实例 protected ProcessInstance createAndStartProcessInstance(ProcessDefinition processDefinition, String businessKey, String processInstanceName, Map\u003cString, Object\u003e variables, Map\u003cString, Object\u003e transientVariables, boolean startProcessInstance) { // 获取主流程 Process process = this.getActiveProcess(processDefinition); // 获取开始元素，就是 StartEvent 对象 FlowElement initialFlowElement = this.getInitialFlowElement(process, processDefinition.getId()); // 创建和启动流程 return createAndStartProcessInstanceWithInitialFlowElement(processDefinition, businessKey, processInstanceName, initialFlowElement, process, variables, transientVariables, startProcessInstance); } 源码位置: org.activiti.engine.impl.util.ProcessInstanceHelper#createAndStartProcessInstanceWithInitialFlowElement public ProcessInstance createAndStartProcessInstanceWithInitialFlowElement(ProcessDefinition processDefinition, String businessKey, String processInstanceName, FlowElement initialFlowElement, Process process, Map\u003cString, Object\u003e variables, Map\u003cString, Object\u003e transientVariables, boolean startProcessInstance) { // 创建流程实例 ExecutionEntity processInstance = createProcessInstanceWithInitialFlowElement(processDefinition, businessKey, processInstanceName, initialFlowElement, process); if (startProcessInstance) { CommandContext commandContext = Context.getCommandContext(); // 启动流程实例 startProcessInstance(processInstance, commandContext, variables, initialFlowElement, transientVariables); } // 返回 return processInstance; } ","date":"2023-10-12","objectID":"/ooooo-notes/05-%E5%90%AF%E5%8A%A8%E6%B5%81%E7%A8%8B/:1:0","tags":["activiti","source code","源码分析 activiti 系列"],"title":"05 启动流程","uri":"/ooooo-notes/05-%E5%90%AF%E5%8A%A8%E6%B5%81%E7%A8%8B/"},{"categories":null,"content":"创建流程实例 源码位置: org.activiti.engine.impl.util.ProcessInstanceHelper#createProcessInstanceWithInitialFlowElement // 创建流程实例 public ExecutionEntity createProcessInstanceWithInitialFlowElement(ProcessDefinition processDefinition, String businessKey, String processInstanceName, FlowElement initialFlowElement, Process process) { CommandContext commandContext = Context.getCommandContext(); // Create the process instance String initiatorVariableName = null; if (initialFlowElement instanceof StartEvent) { initiatorVariableName = ((StartEvent) initialFlowElement).getInitiator(); } // 创建父节点，这里会新增一条 ACT_RU_EXECUTION 表的数据 // scope 为 true，processInstanceId 就是自己的Id ExecutionEntity processInstance = commandContext.getExecutionEntityManager() .createProcessInstanceExecution(processDefinition, businessKey, processDefinition.getTenantId(), initiatorVariableName); ... // Create the first execution that will visit all the process definition elements // 创建子节点，这里会新增一条 ACT_RU_EXECUTION 表的数据 // scope 为 false, processInstanceId 是父节点的 Id ExecutionEntity execution = commandContext.getExecutionEntityManager().createChildExecution(processInstance); execution.setCurrentFlowElement(initialFlowElement); // 启动流程，一般就会创建两条数据在 ACT_RU_EXECUTION 表中，这个很重要 return processInstance; } ","date":"2023-10-12","objectID":"/ooooo-notes/05-%E5%90%AF%E5%8A%A8%E6%B5%81%E7%A8%8B/:2:0","tags":["activiti","source code","源码分析 activiti 系列"],"title":"05 启动流程","uri":"/ooooo-notes/05-%E5%90%AF%E5%8A%A8%E6%B5%81%E7%A8%8B/"},{"categories":null,"content":"启动流程实例 源码位置: org.activiti.engine.impl.util.ProcessInstanceHelper#startProcessInstance // 启动流程实例 public void startProcessInstance(ExecutionEntity processInstance, CommandContext commandContext, Map\u003cString, Object\u003e variables, FlowElement initialFlowElement, Map\u003cString, Object\u003e transientVariables) { Process process = ProcessDefinitionUtil.getProcess(processInstance.getProcessDefinitionId()); createProcessVariables(processInstance, variables, transientVariables, process); recordStartProcessInstance(commandContext, initialFlowElement, processInstance); ...省略了事件子流程的代码 // 在创建流程时，会将子节点加入到父节点的节点列表中, 这样就可以从流程实例中获取子节点 // There will always be one child execution created ExecutionEntity execution = processInstance.getExecutions().get(0); execution.setAppVersion(processInstance.getAppVersion()); // 流转节点，这个非常重要, 会在下一节继续分析 commandContext.getAgenda().planContinueProcessOperation(execution); } ","date":"2023-10-12","objectID":"/ooooo-notes/05-%E5%90%AF%E5%8A%A8%E6%B5%81%E7%A8%8B/:3:0","tags":["activiti","source code","源码分析 activiti 系列"],"title":"05 启动流程","uri":"/ooooo-notes/05-%E5%90%AF%E5%8A%A8%E6%B5%81%E7%A8%8B/"},{"categories":null,"content":"测试类 org.activiti.examples.bpmn.receivetask.ReceiveTaskTest#testWaitStateBehavior ","date":"2023-10-12","objectID":"/ooooo-notes/05-%E5%90%AF%E5%8A%A8%E6%B5%81%E7%A8%8B/:4:0","tags":["activiti","source code","源码分析 activiti 系列"],"title":"05 启动流程","uri":"/ooooo-notes/05-%E5%90%AF%E5%8A%A8%E6%B5%81%E7%A8%8B/"},{"categories":null,"content":"为什么学 想开发前端的人，就必须学习 vue，我之前也学习过，所以我来谈谈怎么学习 vue。 ","date":"2023-10-12","objectID":"/ooooo-notes/%E4%BB%8E%E9%9B%B6%E5%AD%A6-vue/:1:0","tags":["vue","从零学技术系列"],"title":"从零学 vue","uri":"/ooooo-notes/%E4%BB%8E%E9%9B%B6%E5%AD%A6-vue/"},{"categories":null,"content":"怎么学 如果你没有看过官方文档，说明你大概率是一个 vue 菜鸟。 vue 官方文档 vue router 官方文档 vuex 官方文档 vite 官方文档(进阶必备) pinia 官方文档(进阶必备) 书籍-Vue.js设计与实现(强烈推荐) 学习完上述知识，你可以熟练掌握 vue 了，你还需要看一些实际的代码, 推荐github-v3-admin-vite (star多一点，我个人没有看过) 开源框架，最重要的就是要熟悉源码，所以我建议你看 vue 的源码，然后就毕业了。 ","date":"2023-10-12","objectID":"/ooooo-notes/%E4%BB%8E%E9%9B%B6%E5%AD%A6-vue/:2:0","tags":["vue","从零学技术系列"],"title":"从零学 vue","uri":"/ooooo-notes/%E4%BB%8E%E9%9B%B6%E5%AD%A6-vue/"},{"categories":null,"content":" activiti 基于 8.0.0 版本 每次部署新的流程，必定会解析流程来检查文件是否正确，并将 xml 元素映射到 java 对象上。 ","date":"2023-10-11","objectID":"/ooooo-notes/04-%E8%A7%A3%E6%9E%90%E6%B5%81%E7%A8%8B/:0:0","tags":["activiti","source code","源码分析 activiti 系列"],"title":"04 解析流程","uri":"/ooooo-notes/04-%E8%A7%A3%E6%9E%90%E6%B5%81%E7%A8%8B/"},{"categories":null,"content":"解析流程 源码位置: org.activiti.engine.impl.bpmn.deployer.ParsedDeploymentBuilder#createBpmnParseFromResource // 在部署流程的过程中，就会调用该方法来解析流程 protected BpmnParse createBpmnParseFromResource(ResourceEntity resource) { String resourceName = resource.getName(); // 流程文件的字节流 ByteArrayInputStream inputStream = new ByteArrayInputStream(resource.getBytes()); // 创建解析对象，设置字节流 BpmnParse bpmnParse = bpmnParser.createParse() .sourceInputStream(inputStream) .setSourceSystemId(resourceName) .deployment(deployment) .name(resourceName); // 设置校验参数 if (deploymentSettings != null) { // Schema validation if needed if (deploymentSettings.containsKey(DeploymentSettings.IS_BPMN20_XSD_VALIDATION_ENABLED)) { bpmnParse.setValidateSchema((Boolean) deploymentSettings.get(DeploymentSettings.IS_BPMN20_XSD_VALIDATION_ENABLED)); } // Process validation if needed if (deploymentSettings.containsKey(DeploymentSettings.IS_PROCESS_VALIDATION_ENABLED)) { bpmnParse.setValidateProcess((Boolean) deploymentSettings.get(DeploymentSettings.IS_PROCESS_VALIDATION_ENABLED)); } } else { // On redeploy, we assume it is validated at the first deploy bpmnParse.setValidateSchema(false); bpmnParse.setValidateProcess(false); } // 执行解析流程 bpmnParse.execute(); return bpmnParse; } 源码位置: org.activiti.engine.impl.bpmn.parser.BpmnParse#execute // 执行解析流程 public BpmnParse execute() { try { ProcessEngineConfigurationImpl processEngineConfiguration = Context.getProcessEngineConfiguration(); BpmnXMLConverter converter = new BpmnXMLConverter(); boolean enableSafeBpmnXml = false; String encoding = null; if (processEngineConfiguration != null) { enableSafeBpmnXml = processEngineConfiguration.isEnableSafeBpmnXml(); encoding = processEngineConfiguration.getXmlEncoding(); } // 解析 xml 元素，转换为 java 对象，会调用 convertToBpmnModel 方法 bpmnModel = converter.convertToBpmnModel(streamSource, validateSchema, enableSafeBpmnXml, encoding); ... // Attach logic to the processes (eg. map ActivityBehaviors to bpmn model elements) // 应用解析器，这个很重要 applyParseHandlers(); // Finally, process the diagram interchange info processDI(); } catch (Exception e) { ... } return this; } 源码位置: org.activiti.bpmn.converter.BpmnXMLConverter#convertToBpmnModel // 解析 xml 元素，转换为 java 对象 public BpmnModel convertToBpmnModel(XMLStreamReader xtr) { BpmnModel model = new BpmnModel(); model.setStartEventFormTypes(startEventFormTypes); model.setUserTaskFormTypes(userTaskFormTypes); try { Process activeProcess = null; List\u003cSubProcess\u003e activeSubProcessList = new ArrayList\u003cSubProcess\u003e(); while (xtr.hasNext()) { try { xtr.next(); } catch (Exception e) { LOGGER.debug(\"Error reading XML document\", e); throw new XMLException(\"Error reading XML\", e); } ...省略了一堆的判断代码 } else if (ELEMENT_DI_EDGE.equals(xtr.getLocalName())) { bpmnEdgeParser.parse(xtr, model); } else { if (!activeSubProcessList.isEmpty() \u0026\u0026 ELEMENT_MULTIINSTANCE.equalsIgnoreCase(xtr.getLocalName())) { multiInstanceParser.parseChildElement(xtr, activeSubProcessList.get(activeSubProcessList.size() - 1), model); } else if (convertersToBpmnMap.containsKey(xtr.getLocalName())) { if (activeProcess != null) { // 获取XML转换器, 将 XML 元素转换为 java 对象 // 当你需要扩展流程文件时，这里的代码很有用 BaseBpmnXMLConverter converter = convertersToBpmnMap.get(xtr.getLocalName()); converter.convertToBpmnModel(xtr, model, activeProcess, activeSubProcessList); } } } } for (Process process : model.getProcesses()) { for (Pool pool : model.getPools()) { if (process.getId().equals(pool.getProcessRef())) { pool.setExecutable(process.isExecutable()); } } // 处理流程元素，设置节点之间的连线 processFlowElements(process.getFlowElements(), process); } } catch (XMLException e) { throw e; } catch (Exception e) { LOGGER.error(\"Error processing BPMN document\", e); throw new XMLException(\"Error processing BPMN document\", e); } return model; } 源码位置: org.activiti.engine.impl.bpmn.parser.BpmnParse#applyParseHandlers // 应用解析器，会对流程元素设置 behavior, 这个很重要 protected void applyParseHandlers() { sequenceFlows = new HashMap\u003cString, SequenceFlow\u003e(); // 遍历每个流程 for (Process process : bpmnModel.getProce","date":"2023-10-11","objectID":"/ooooo-notes/04-%E8%A7%A3%E6%9E%90%E6%B5%81%E7%A8%8B/:1:0","tags":["activiti","source code","源码分析 activiti 系列"],"title":"04 解析流程","uri":"/ooooo-notes/04-%E8%A7%A3%E6%9E%90%E6%B5%81%E7%A8%8B/"},{"categories":null,"content":"测试类 org.activiti.examples.bpmn.receivetask.ReceiveTaskTest#testWaitStateBehavior ","date":"2023-10-11","objectID":"/ooooo-notes/04-%E8%A7%A3%E6%9E%90%E6%B5%81%E7%A8%8B/:2:0","tags":["activiti","source code","源码分析 activiti 系列"],"title":"04 解析流程","uri":"/ooooo-notes/04-%E8%A7%A3%E6%9E%90%E6%B5%81%E7%A8%8B/"},{"categories":null,"content":" activiti 基于 8.0.0 版本 ","date":"2023-10-10","objectID":"/ooooo-notes/03-%E9%83%A8%E7%BD%B2%E6%B5%81%E7%A8%8B/:0:0","tags":["activiti","source code","源码分析 activiti 系列"],"title":"03 部署流程","uri":"/ooooo-notes/03-%E9%83%A8%E7%BD%B2%E6%B5%81%E7%A8%8B/"},{"categories":null,"content":"部署流程 源码位置: org.activiti.engine.impl.repository.DeploymentBuilderImpl#deploy // 部署流程 public Deployment deploy() { return repositoryService.deploy(this); } // org.activiti.engine.impl.RepositoryServiceImpl#deploy public Deployment deploy(DeploymentBuilderImpl deploymentBuilder) { // 执行 DeployCmd, 最终会执行 DeployCmd#execute 方法 return commandExecutor.execute(new DeployCmd\u003cDeployment\u003e(deploymentBuilder)); } 源码位置: org.activiti.engine.impl.cmd.DeployCmd#execute // 执行 DeployCmd#execute 方法 public Deployment execute(CommandContext commandContext) { // 执行部署 return executeDeploy(commandContext); } protected Deployment executeDeploy(CommandContext commandContext) { // DeploymentEntity 表示部署, 里面包含了流程文件 DeploymentEntity newDeployment = setUpNewDeploymentFromContext(commandContext); // 判断是否过滤重复的 // 每次部署流程，可能只有一部分的流程发生了改变，所以不需要部署所有的流程 if (deploymentBuilder.isDuplicateFilterEnabled()) { ... if (!existingDeployments.isEmpty()) { DeploymentEntity existingDeployment = (DeploymentEntity) existingDeployments.get(0); // 对比流程文件是否发生改动 if (deploymentsDiffer(newDeployment, existingDeployment)) { applyUpgradeLogic(newDeployment, existingDeployment); } else { LOGGER.info(\"An existing deployment of version {} matching the current one was found, no need to deploy again.\", existingDeployment.getVersion()); return existingDeployment; } } } // 持久化部署，会把流程文件插入到数据库中，也会返回 deploymentId 和 version persistDeploymentInDatabase(commandContext, newDeployment); ... LOGGER.info(\"Launching new deployment with version: \" + newDeployment.getVersion()); // 部署流程 commandContext.getProcessEngineConfiguration().getDeploymentManager().deploy(newDeployment, deploymentSettings); ... return newDeployment; } 源码位置: org.activiti.engine.impl.persistence.deploy.DeploymentManager#deploy // 部署流程 public void deploy(DeploymentEntity deployment, Map\u003cString, Object\u003e deploymentSettings) { // deployers 默认只有一个实现 BpmnDeployer for (Deployer deployer : deployers) { deployer.deploy(deployment, deploymentSettings); } } 源码位置: org.activiti.engine.impl.bpmn.deployer.BpmnDeployer#deploy // 部署流程 @Override public void deploy(DeploymentEntity deployment, Map\u003cString, Object\u003e deploymentSettings) { log.debug(\"Processing deployment {}\", deployment.getName()); // The ParsedDeployment represents the deployment, the process definitions, and the BPMN // resource, parse, and model associated with each process definition. // 这里会解析流程文件，很重要, 会在下一节继续解析 ParsedDeployment parsedDeployment = parsedDeploymentBuilderFactory .getBuilderForDeploymentAndSettings(deployment, deploymentSettings) .build(); // 校验 processDefinitionKey 是否重复 bpmnDeploymentHelper.verifyProcessDefinitionsDoNotShareKeys(parsedDeployment.getAllProcessDefinitions()); ... // 设置一些属性，然后会持久化到数据库中 if (deployment.isNew()) { Map\u003cProcessDefinitionEntity, ProcessDefinitionEntity\u003e mapOfNewProcessDefinitionToPreviousVersion = getPreviousVersionsOfProcessDefinitions(parsedDeployment); setProcessDefinitionVersionsAndIds(parsedDeployment, mapOfNewProcessDefinitionToPreviousVersion); setProcessDefinitionAppVersion(parsedDeployment); persistProcessDefinitionsAndAuthorizations(parsedDeployment); updateTimersAndEvents(parsedDeployment, mapOfNewProcessDefinitionToPreviousVersion); dispatchProcessDefinitionEntityInitializedEvent(parsedDeployment); } else { makeProcessDefinitionsConsistentWithPersistedVersions(parsedDeployment); } // 流程定义更新到缓存中 cachingAndArtifactsManager.updateCachingAndArtifacts(parsedDeployment); // 这里不是主要逻辑，可以不用关心 for (ProcessDefinitionEntity processDefinition : parsedDeployment.getAllProcessDefinitions()) { BpmnModel bpmnModel = parsedDeployment.getBpmnModelForProcessDefinition(processDefinition); createLocalizationValues(processDefinition.getId(), bpmnModel.getProcessById(processDefinition.getKey())); } } ","date":"2023-10-10","objectID":"/ooooo-notes/03-%E9%83%A8%E7%BD%B2%E6%B5%81%E7%A8%8B/:1:0","tags":["activiti","source code","源码分析 activiti 系列"],"title":"03 部署流程","uri":"/ooooo-notes/03-%E9%83%A8%E7%BD%B2%E6%B5%81%E7%A8%8B/"},{"categories":null,"content":"测试类 org.activiti.examples.processdefinitions.ProcessDefinitionsTest#testProcessDefinitionDescription ","date":"2023-10-10","objectID":"/ooooo-notes/03-%E9%83%A8%E7%BD%B2%E6%B5%81%E7%A8%8B/:2:0","tags":["activiti","source code","源码分析 activiti 系列"],"title":"03 部署流程","uri":"/ooooo-notes/03-%E9%83%A8%E7%BD%B2%E6%B5%81%E7%A8%8B/"},{"categories":null,"content":" activiti 基于 8.0.0 版本 在大多数情况下，activiti 都会与 spring boot 框架一起使用，所以这一节就来介绍 activiti 是如何集成 spring 的。 ","date":"2023-10-09","objectID":"/ooooo-notes/02-%E9%9B%86%E6%88%90-spring/:0:0","tags":["activiti","source code","源码分析 activiti 系列"],"title":"02 集成 spring","uri":"/ooooo-notes/02-%E9%9B%86%E6%88%90-spring/"},{"categories":null,"content":"activitiProperties 配置 配置类: org.activiti.spring.boot.ActivitiProperties java @ConfigurationProperties(\"spring.activiti\") public class ActivitiProperties { private boolean checkProcessDefinitions = true; // 开启定时器 private boolean asyncExecutorActivate = true; private String deploymentName = \"SpringAutoDeployment\"; // 邮件相关的配置 private String mailServerHost = \"localhost\"; private int mailServerPort = 1025; private String mailServerUserName; private String mailServerPassword; private String mailServerDefaultFrom; private boolean mailServerUseSsl; private boolean mailServerUseTls; // 数据库相关配置 private String databaseSchemaUpdate = \"true\"; private String databaseSchema; private boolean dbHistoryUsed = false; // 本地测试，建议设置为 full private HistoryLevel historyLevel = HistoryLevel.NONE; // 流程定义的路径 private String processDefinitionLocationPrefix = ResourcePatternResolver.CLASSPATH_ALL_URL_PREFIX + \"**/processes/\"; private List\u003cString\u003e processDefinitionLocationSuffixes = asList(\"**.bpmn20.xml\", \"**.bpmn\"); // 自定义的 mapper 文件 private List\u003cString\u003e customMybatisMappers; private List\u003cString\u003e customMybatisXMLMappers; private boolean useStrongUuids = true; private boolean copyVariablesToLocalForTasks = true; // 有不同的部署策略 private String deploymentMode = \"default\"; private boolean serializePOJOsInVariablesToJson = true; private String javaClassFieldForJackson = JsonTypeInfo.Id.CLASS.getDefaultPropertyName() } ","date":"2023-10-09","objectID":"/ooooo-notes/02-%E9%9B%86%E6%88%90-spring/:1:0","tags":["activiti","source code","源码分析 activiti 系列"],"title":"02 集成 spring","uri":"/ooooo-notes/02-%E9%9B%86%E6%88%90-spring/"},{"categories":null,"content":"springBoot 自动配置 源码位置: org.activiti.spring.boot.ProcessEngineAutoConfiguration // 主要构建了 SpringProcessEngineConfiguration, ProcessEngineConfigurationConfigurer, 还有一些事件监听器 // SpringProcessEngineConfiguration: 流程引擎的配置类，贯彻全局，非常重要 // ProcessEngineConfigurationConfigurer: 配置流程引擎，我们可以实现这个接口来自定义配置 // 它的父类 AbstractProcessEngineAutoConfiguration，里面定义了一些常用的 service 类，比如 RuntimeService @AutoConfiguration @AutoConfigureAfter(name = {\"org.springframework.boot.autoconfigure.jdbc.DataSourceAutoConfiguration\", \"org.springframework.boot.autoconfigure.task.TaskExecutionAutoConfiguration\"}) @EnableConfigurationProperties({ActivitiProperties.class, AsyncExecutorProperties.class}) public class ProcessEngineAutoConfiguration extends AbstractProcessEngineAutoConfiguration { ... @Bean @ConditionalOnMissingBean @DependsOnDatabaseInitialization public SpringProcessEngineConfiguration springProcessEngineConfiguration( DataSource dataSource, PlatformTransactionManager transactionManager, SpringAsyncExecutor springAsyncExecutor, ActivitiProperties activitiProperties, ResourceFinder resourceFinder, List\u003cResourceFinderDescriptor\u003e resourceFinderDescriptors, ApplicationUpgradeContextService applicationUpgradeContextService, @Autowired(required = false) List\u003cProcessEngineConfigurationConfigurer\u003e processEngineConfigurationConfigurers, @Autowired(required = false) List\u003cProcessEngineConfigurator\u003e processEngineConfigurators) throws IOException { SpringProcessEngineConfiguration conf = new SpringProcessEngineConfiguration(applicationUpgradeContextService); conf.setConfigurators(processEngineConfigurators); ... if (processEngineConfigurationConfigurers != null) { for (ProcessEngineConfigurationConfigurer processEngineConfigurationConfigurer : processEngineConfigurationConfigurers) { processEngineConfigurationConfigurer.configure(conf); } } springAsyncExecutor.applyConfig(conf); return conf; } @Bean @Order(Ordered.HIGHEST_PRECEDENCE) public ProcessEngineConfigurationConfigurer asyncExecutorPropertiesConfigurer(AsyncExecutorProperties properties) { return (configuration) -\u003e { configuration.setAsyncExecutorMessageQueueMode(properties.isMessageQueueMode()); ... configuration.setAsyncFailedJobWaitTime(properties.getRetryWaitTimeInMillis()); }; } } 源码位置: org.activiti.spring.boot.AbstractProcessEngineAutoConfiguration#processEngine // 根据流程引擎配置生成对应的 FactoryBean, 在初始化时，就会调用 FactoryBean#getObject 方法。 @Bean public ProcessEngineFactoryBean processEngine(SpringProcessEngineConfiguration configuration) { return super.springProcessEngineBean(configuration); } // FactoryBean#getObject public ProcessEngine getObject() throws Exception { // 配置表达式管理器, 可以从表达式执行 spring bean 的方法 configureExpressionManager(); // 配置事务管理，就是 spring 事务 configureExternallyManagedTransactions(); if (processEngineConfiguration.getBeans() == null) { processEngineConfiguration.setBeans(new SpringBeanFactoryProxyMap(applicationContext)); } // 构建流程引擎, 这里的 processEngineConfiguration 是 SpringProcessEngineConfiguration 类 this.processEngine = processEngineConfiguration.buildProcessEngine(); return this.processEngine; } 源码位置: org.activiti.engine.impl.cfg.SpringProcessEngineConfiguration#buildProcessEngine // 构建流程引擎 @Override public ProcessEngine buildProcessEngine() { // 调用父类的方法 super#buildProcessEngine ProcessEngine processEngine = super.buildProcessEngine(); ProcessEngines.setInitialized(true); // 自动部署流程文件 autoDeployResources(processEngine); return processEngine; } // super#buildProcessEngine @Override public ProcessEngine buildProcessEngine() { // 初始化，这里有很多组件需要初始化 init(); // 创建流程引擎 ProcessEngineImpl processEngine = new ProcessEngineImpl(this); postProcessEngineInitialisation(); return processEngine; } 源码位置: org.activiti.spring.SpringProcessEngineConfiguration#autoDeployResources // 自动部署流程文件 protected void autoDeployResources(ProcessEngine processEngine) { // 获取自动部署策略, 默认是 DefaultAutoDeploymentStrategy final AutoDeploymentStrategy strategy = getAutoDeploymentStrategy(deploymentMode); // 部署流程 strategy.deployResources(depl","date":"2023-10-09","objectID":"/ooooo-notes/02-%E9%9B%86%E6%88%90-spring/:2:0","tags":["activiti","source code","源码分析 activiti 系列"],"title":"02 集成 spring","uri":"/ooooo-notes/02-%E9%9B%86%E6%88%90-spring/"},{"categories":null,"content":"测试类 启动示例程序 activiti-examples/activiti-api-basic-full-example-bean ","date":"2023-10-09","objectID":"/ooooo-notes/02-%E9%9B%86%E6%88%90-spring/:3:0","tags":["activiti","source code","源码分析 activiti 系列"],"title":"02 集成 spring","uri":"/ooooo-notes/02-%E9%9B%86%E6%88%90-spring/"},{"categories":null,"content":" activiti 基于 8.0.0 版本 ","date":"2023-10-08","objectID":"/ooooo-notes/01-%E6%90%AD%E5%BB%BA-activiti-%E6%BA%90%E7%A0%81%E8%B0%83%E8%AF%95%E7%8E%AF%E5%A2%83/:0:0","tags":["activiti","source code","源码分析 activiti 系列"],"title":"01 搭建 activiti 源码调试环境","uri":"/ooooo-notes/01-%E6%90%AD%E5%BB%BA-activiti-%E6%BA%90%E7%A0%81%E8%B0%83%E8%AF%95%E7%8E%AF%E5%A2%83/"},{"categories":null,"content":"下载源码和编译 git clone git@github.com:Activiti/Activiti.git mvn clean package -DskipTests ","date":"2023-10-08","objectID":"/ooooo-notes/01-%E6%90%AD%E5%BB%BA-activiti-%E6%BA%90%E7%A0%81%E8%B0%83%E8%AF%95%E7%8E%AF%E5%A2%83/:1:0","tags":["activiti","source code","源码分析 activiti 系列"],"title":"01 搭建 activiti 源码调试环境","uri":"/ooooo-notes/01-%E6%90%AD%E5%BB%BA-activiti-%E6%BA%90%E7%A0%81%E8%B0%83%E8%AF%95%E7%8E%AF%E5%A2%83/"},{"categories":null,"content":"运行示例程序 在源码中有模块 activiti-examples/activiti-api-basic-full-example-bean，这是一个 spring boot 应用，是可以直接启动的，默认是以h2内存数据库来运行的，建议使用MySQL数据库，这样的话，可以更方便来观察数据库中的数据。 大多数情况下，推荐你使用测试类来调试代码。 在模块 activiti-core/activiti-engine 下，src/test/resources/activiti.cfg.xml 中可以配置数据库，也建议使用MySQL数据库。 如果你能成功运行 org.activiti.examples.bpmn.receivetask.ReceiveTaskTest#testWaitStateBehavior, 说明你的环境没有问题了。 ","date":"2023-10-08","objectID":"/ooooo-notes/01-%E6%90%AD%E5%BB%BA-activiti-%E6%BA%90%E7%A0%81%E8%B0%83%E8%AF%95%E7%8E%AF%E5%A2%83/:2:0","tags":["activiti","source code","源码分析 activiti 系列"],"title":"01 搭建 activiti 源码调试环境","uri":"/ooooo-notes/01-%E6%90%AD%E5%BB%BA-activiti-%E6%BA%90%E7%A0%81%E8%B0%83%E8%AF%95%E7%8E%AF%E5%A2%83/"},{"categories":null,"content":"为什么学 如果你要了解存储方面的知识，leveldb 你必须要懂。这是因为存储会涉及到LSM 树和B+ 树，而 leveldb 是LSM 树的实现。现在的单机存储引擎会用到 leveldb 或者 rocksdb。 ","date":"2023-10-08","objectID":"/ooooo-notes/%E4%BB%8E%E9%9B%B6%E5%AD%A6-leveldb/:1:0","tags":["leveldb","从零学技术系列"],"title":"从零学 leveldb","uri":"/ooooo-notes/%E4%BB%8E%E9%9B%B6%E5%AD%A6-leveldb/"},{"categories":null,"content":"怎么学 leveldb 的源码比较少，只有1w行，所以建议你看源码 参考资料 简书-leveldb从入门到原理详解 知乎-深入分析leveldb存储引擎 源码-github ","date":"2023-10-08","objectID":"/ooooo-notes/%E4%BB%8E%E9%9B%B6%E5%AD%A6-leveldb/:2:0","tags":["leveldb","从零学技术系列"],"title":"从零学 leveldb","uri":"/ooooo-notes/%E4%BB%8E%E9%9B%B6%E5%AD%A6-leveldb/"},{"categories":null,"content":" dubbo 基于 3.2.6 版本 在 dubbo 中支持配置中心，如果没有配置，则会检查注册中心能否当作配置中心。 ","date":"2023-09-28","objectID":"/ooooo-notes/04-%E9%85%8D%E7%BD%AE%E5%8A%A0%E8%BD%BD/:0:0","tags":["dubbo","source code","源码分析 dubbo 系列"],"title":"04 配置加载","uri":"/ooooo-notes/04-%E9%85%8D%E7%BD%AE%E5%8A%A0%E8%BD%BD/"},{"categories":null,"content":"启动配置中心 源码位置: org.apache.dubbo.config.deploy.DefaultApplicationDeployer#startConfigCenter // 启动配置中心 private void startConfigCenter() { // load application config // 加载配置 configManager.loadConfigsOfTypeFromProps(ApplicationConfig.class); // try set model name if (StringUtils.isBlank(applicationModel.getModelName())) { applicationModel.setModelName(applicationModel.tryGetApplicationName()); } // load config centers // 加载配置 configManager.loadConfigsOfTypeFromProps(ConfigCenterConfig.class); // 使用注册中心来作为配置中心, 默认情况下 zk 和 nacos 都是支持的 // 可以通过属性 RegistryConfig#useAsConfigCenter 来配置 useRegistryAsConfigCenterIfNecessary(); // check Config Center Collection\u003cConfigCenterConfig\u003e configCenters = configManager.getConfigCenters(); if (CollectionUtils.isEmpty(configCenters)) { // 没有配置中心，new 一个空的 ConfigCenterConfig configCenterConfig = new ConfigCenterConfig(); configCenterConfig.setScopeModel(applicationModel); configCenterConfig.refresh(); ConfigValidationUtils.validateConfigCenterConfig(configCenterConfig); if (configCenterConfig.isValid()) { configManager.addConfigCenter(configCenterConfig); configCenters = configManager.getConfigCenters(); } } else { // 遍历配置中心 for (ConfigCenterConfig configCenterConfig : configCenters) { // 配置中心执行 refresh configCenterConfig.refresh(); // 校验 ConfigValidationUtils.validateConfigCenterConfig(configCenterConfig); } } if (CollectionUtils.isNotEmpty(configCenters)) { CompositeDynamicConfiguration compositeDynamicConfiguration = new CompositeDynamicConfiguration(); // 遍历配置中心 for (ConfigCenterConfig configCenter : configCenters) { // Pass config from ConfigCenterBean to environment // 更新配置到 externalConfig，全局配置 environment.updateExternalConfigMap(configCenter.getExternalConfiguration()); // 更新配置到 appExternalConfig, 应用级别配置 environment.updateAppExternalConfigMap(configCenter.getAppExternalConfiguration()); // Fetch config from remote config center // 从配置中心拉取配置 compositeDynamicConfiguration.addConfiguration(prepareEnvironment(configCenter)); } // 设置动态配置 environment.setDynamicConfiguration(compositeDynamicConfiguration); } } ","date":"2023-09-28","objectID":"/ooooo-notes/04-%E9%85%8D%E7%BD%AE%E5%8A%A0%E8%BD%BD/:1:0","tags":["dubbo","source code","源码分析 dubbo 系列"],"title":"04 配置加载","uri":"/ooooo-notes/04-%E9%85%8D%E7%BD%AE%E5%8A%A0%E8%BD%BD/"},{"categories":null,"content":"加载配置 源码位置: org.apache.dubbo.config.context.AbstractConfigManager#loadConfigsOfTypeFromProps public \u003cT extends AbstractConfig\u003e List\u003cT\u003e loadConfigsOfTypeFromProps(Class\u003cT\u003e cls) { List\u003cT\u003e tmpConfigs = new ArrayList\u003c\u003e(); // dubbo.properties 文件配置 PropertiesConfiguration properties = environment.getPropertiesConfiguration(); // load multiple configs with id Set\u003cString\u003e configIds = this.getConfigIdsFromProps(cls); // 加载多配置，比如 key: dubbo.protocols.id-xxx.name configIds.forEach(id -\u003e { if (!this.getConfig(cls, id).isPresent()) { T config; try { config = createConfig(cls, scopeModel); config.setId(id); } catch (Exception e) { throw new IllegalStateException(\"create config instance failed, id: \" + id + \", type:\" + cls.getSimpleName()); } String key = null; boolean addDefaultNameConfig = false; try { // add default name config (same as id), e.g. dubbo.protocols.rest.port=1234 key = DUBBO + \".\" + AbstractConfig.getPluralTagName(cls) + \".\" + id + \".name\"; if (properties.getProperty(key) == null) { properties.setProperty(key, id); addDefaultNameConfig = true; } config.refresh(); this.addConfig(config); tmpConfigs.add(config); } catch (Exception e) { logger.error(COMMON_PROPERTY_TYPE_MISMATCH, \"\", \"\", \"load config failed, id: \" + id + \", type:\" + cls.getSimpleName(), e); throw new IllegalStateException(\"load config failed, id: \" + id + \", type:\" + cls.getSimpleName()); } finally { if (addDefaultNameConfig \u0026\u0026 key != null) { properties.remove(key); } } } }); // If none config of the type, try load single config // 加载单配置，也就是说单配置和多配置同时存在，多配置优先 if (this.getConfigs(cls).isEmpty()) { // load single config // configurationMaps 中包含多个配置，比如环境变量，dubbo.properies 文件配置，系统参数等 List\u003cMap\u003cString, String\u003e\u003e configurationMaps = environment.getConfigurationMaps(); if (ConfigurationUtils.hasSubProperties(configurationMaps, AbstractConfig.getTypePrefix(cls))) { T config; try { config = createConfig(cls, scopeModel); config.refresh(); } catch (Exception e) { throw new IllegalStateException(\"create default config instance failed, type:\" + cls.getSimpleName()); } this.addConfig(config); tmpConfigs.add(config); } } return tmpConfigs; } ","date":"2023-09-28","objectID":"/ooooo-notes/04-%E9%85%8D%E7%BD%AE%E5%8A%A0%E8%BD%BD/:2:0","tags":["dubbo","source code","源码分析 dubbo 系列"],"title":"04 配置加载","uri":"/ooooo-notes/04-%E9%85%8D%E7%BD%AE%E5%8A%A0%E8%BD%BD/"},{"categories":null,"content":"从配置中心拉取配置 源码位置: org.apache.dubbo.config.deploy.DefaultApplicationDeployer#prepareEnvironment // 从配置中心拉取配置 private DynamicConfiguration prepareEnvironment(ConfigCenterConfig configCenter) { if (configCenter.isValid()) { ... DynamicConfiguration dynamicConfiguration; try { // 获取动态配置 dynamicConfiguration = getDynamicConfiguration(configCenter.toUrl()); } catch (Exception e) { ... } ApplicationModel applicationModel = getApplicationModel(); // 配置文件就是 key if (StringUtils.isNotEmpty(configCenter.getConfigFile())) { // 获取配置内容, 全局级别的 String configContent = dynamicConfiguration.getProperties(configCenter.getConfigFile(), configCenter.getGroup()); if (StringUtils.isNotEmpty(configContent)) { logger.info(String.format(\"Got global remote configuration from config center with key-%s and group-%s: \\n %s\", configCenter.getConfigFile(), configCenter.getGroup(), configContent)); } String appGroup = getApplication().getName(); String appConfigContent = null; String appConfigFile = null; if (isNotEmpty(appGroup)) { appConfigFile = isNotEmpty(configCenter.getAppConfigFile()) ? configCenter.getAppConfigFile() : configCenter.getConfigFile(); // 获取配置内容, 应用级别的 appConfigContent = dynamicConfiguration.getProperties(appConfigFile, appGroup); if (StringUtils.isNotEmpty(appConfigContent)) { logger.info(String.format(\"Got application specific remote configuration from config center with key %s and group %s: \\n %s\", appConfigFile, appGroup, appConfigContent)); } } try { // 解析配置 Map\u003cString, String\u003e configMap = parseProperties(configContent); Map\u003cString, String\u003e appConfigMap = parseProperties(appConfigContent); // 更新配置 environment.updateExternalConfigMap(configMap); environment.updateAppExternalConfigMap(appConfigMap); ... } catch (IOException e) { throw new IllegalStateException(\"Failed to parse configurations from Config Center.\", e); } } return dynamicConfiguration; } return null; } ","date":"2023-09-28","objectID":"/ooooo-notes/04-%E9%85%8D%E7%BD%AE%E5%8A%A0%E8%BD%BD/:3:0","tags":["dubbo","source code","源码分析 dubbo 系列"],"title":"04 配置加载","uri":"/ooooo-notes/04-%E9%85%8D%E7%BD%AE%E5%8A%A0%E8%BD%BD/"},{"categories":null,"content":"Environment 初始化 源码位置: org.apache.dubbo.common.config.Environment#initialize @Override public void initialize() throws IllegalStateException { if (initialized.compareAndSet(false, true)) { // 属性配置，后面会说 this.propertiesConfiguration = new PropertiesConfiguration(scopeModel); // 系统配置 this.systemConfiguration = new SystemConfiguration(); // 环境变量 this.environmentConfiguration = new EnvironmentConfiguration(); // 配置中心的全局配置 this.externalConfiguration = new InmemoryConfiguration(\"ExternalConfig\"); // 配置中心的应用配置 this.appExternalConfiguration = new InmemoryConfiguration(\"AppExternalConfig\"); // 本地的应用配置 this.appConfiguration = new InmemoryConfiguration(\"AppConfig\"); loadMigrationRule(); } } // external config, such as config-center global/default config private InmemoryConfiguration externalConfiguration; // external app config, such as config-center app config private InmemoryConfiguration appExternalConfiguration; // local app config , such as Spring Environment/PropertySources/application.properties private InmemoryConfiguration appConfiguration; ","date":"2023-09-28","objectID":"/ooooo-notes/04-%E9%85%8D%E7%BD%AE%E5%8A%A0%E8%BD%BD/:4:0","tags":["dubbo","source code","源码分析 dubbo 系列"],"title":"04 配置加载","uri":"/ooooo-notes/04-%E9%85%8D%E7%BD%AE%E5%8A%A0%E8%BD%BD/"},{"categories":null,"content":"PropertiesConfiguration 初始化 源码位置: org.apache.dubbo.common.config.PropertiesConfiguration#refresh // PropertiesConfiguration 构造函数 public PropertiesConfiguration(ScopeModel scopeModel) { this.scopeModel = scopeModel; // 刷新配置 refresh(); } // 刷新配置 public void refresh() { properties = ConfigUtils.getProperties(scopeModel.getClassLoaders()); } // ConfigUtils#getProperties public static Properties getProperties(Set\u003cClassLoader\u003e classLoaders) { String path = System.getProperty(CommonConstants.DUBBO_PROPERTIES_KEY); if (StringUtils.isEmpty(path)) { path = System.getenv(CommonConstants.DUBBO_PROPERTIES_KEY); if (StringUtils.isEmpty(path)) { // dubbo.properties 文件 path = CommonConstants.DEFAULT_DUBBO_PROPERTIES; } } return ConfigUtils.loadProperties(classLoaders, path, false, true); } ","date":"2023-09-28","objectID":"/ooooo-notes/04-%E9%85%8D%E7%BD%AE%E5%8A%A0%E8%BD%BD/:5:0","tags":["dubbo","source code","源码分析 dubbo 系列"],"title":"04 配置加载","uri":"/ooooo-notes/04-%E9%85%8D%E7%BD%AE%E5%8A%A0%E8%BD%BD/"},{"categories":null,"content":"测试类 org.apache.dubbo.config.context.ConfigManagerTest#testLoadConfigsOfTypeFromProps org.apache.dubbo.configcenter.support.nacos.NacosDynamicConfigurationTest#testGetConfig: 需要启动一个 nacos 服务 org.apache.dubbo.configcenter.support.zookeeper.ZookeeperDynamicConfigurationTest#testGetConfig: 需要启动一个 zk 服务 ","date":"2023-09-28","objectID":"/ooooo-notes/04-%E9%85%8D%E7%BD%AE%E5%8A%A0%E8%BD%BD/:6:0","tags":["dubbo","source code","源码分析 dubbo 系列"],"title":"04 配置加载","uri":"/ooooo-notes/04-%E9%85%8D%E7%BD%AE%E5%8A%A0%E8%BD%BD/"},{"categories":null,"content":" dubbo 基于 3.2.6 版本 ","date":"2023-09-16","objectID":"/ooooo-notes/03-%E5%90%AF%E5%8A%A8%E6%B5%81%E7%A8%8B/:0:0","tags":["dubbo","source code","源码分析 dubbo 系列"],"title":"03 启动流程","uri":"/ooooo-notes/03-%E5%90%AF%E5%8A%A8%E6%B5%81%E7%A8%8B/"},{"categories":null,"content":"入口程序 // registry、protocol、reference、service 都会调用 configManager#addConfig，很重要。 DubboBootstrap bootstrap = DubboBootstrap.getInstance(); // 设置应用配置 bootstrap.application(new ApplicationConfig(\"dubbo-demo-api-consumer\")) // 注册中心 .registry(registryConfig) // 协议配置 .protocol(new ProtocolConfig(CommonConstants.DUBBO, -1)) // 服务引用 .reference(reference) // 服务暴露 .service(service) // 启动 dubbo .start(); 源码位置: org.apache.dubbo.config.context.AbstractConfigManager#addConfig public final \u003cT extends AbstractConfig\u003e T addConfig(AbstractConfig config) { if (config == null) { return null; } // ignore MethodConfig // 不支持 if (!isSupportConfigType(config.getClass())) { throw new IllegalArgumentException(\"Unsupported config type: \" + config); } if (config.getScopeModel() != scopeModel) { config.setScopeModel(scopeModel); } // 获取 tagName, 然后添加 Map\u003cString, AbstractConfig\u003e configsMap = configsCache.computeIfAbsent(getTagName(config.getClass()), type -\u003e new ConcurrentHashMap\u003c\u003e()); // fast check duplicated equivalent config before write lock if (!(config instanceof ReferenceConfigBase || config instanceof ServiceConfigBase)) { for (AbstractConfig value : configsMap.values()) { if (value.equals(config)) { return (T) value; } } } // lock by config type synchronized (configsMap) { return (T) addIfAbsent(config, configsMap); } } ","date":"2023-09-16","objectID":"/ooooo-notes/03-%E5%90%AF%E5%8A%A8%E6%B5%81%E7%A8%8B/:1:0","tags":["dubbo","source code","源码分析 dubbo 系列"],"title":"03 启动流程","uri":"/ooooo-notes/03-%E5%90%AF%E5%8A%A8%E6%B5%81%E7%A8%8B/"},{"categories":null,"content":"启动 dubbo 源码位置: org.apache.dubbo.config.bootstrap.DubboBootstrap#start public DubboBootstrap start(boolean wait) { Future future = applicationDeployer.start(); if (wait) { try { future.get(); } catch (Exception e) { throw new IllegalStateException(\"await dubbo application start finish failure\", e); } } return this; } 源码位置: org.apache.dubbo.config.deploy.DefaultApplicationDeployer#start @Override public Future start() { synchronized (startLock) { // 判断状态 if (isStopping() || isStopped() || isFailed()) { throw new IllegalStateException(getIdentifier() + \" is stopping or stopped, can not start again\"); } try { // maybe call start again after add new module, check if any new module // 有待启动的模块 boolean hasPendingModule = hasPendingModule(); // 正在启动 if (isStarting()) { // currently, is starting, maybe both start by module and application // if it has new modules, start them if (hasPendingModule) { // 启动模块 startModules(); } // if it is starting, reuse previous startFuture return startFuture; } // if is started and no new module, just return if (isStarted() \u0026\u0026 !hasPendingModule) { return CompletableFuture.completedFuture(false); } // pending -\u003e starting : first start app // started -\u003e starting : re-start app // 改变状态为正在启动, 执行回调函数 DeployListener onStarting(); // 初始化 initialize(); // 启动 doStart(); } catch (Throwable e) { onFailed(getIdentifier() + \" start failure\", e); throw e; } return startFuture; } } 源码位置: org.apache.dubbo.config.deploy.DefaultApplicationDeployer#initialize // 初始化 @Override public void initialize() { if (initialized) { return; } // Ensure that the initialization is completed when concurrent calls synchronized (startLock) { if (initialized) { return; } // 执行 DeployListener#onInitialize 钩子函数 onInitialize(); // register shutdown hook // 注册 shutdown 钩子 registerShutdownHook(); // 启动配置中心，之后会用一章来说 startConfigCenter(); // 加载应用配置 loadApplicationConfigs(); // 初始化 modoule deployer initModuleDeployers(); initMetricsReporter(); initMetricsService(); // @since 2.7.8 // 启动元数据中心 startMetadataCenter(); // 变更状态 initialized = true; if (logger.isInfoEnabled()) { logger.info(getIdentifier() + \" has been initialized!\"); } } } 源码位置: org.apache.dubbo.config.deploy.DefaultApplicationDeployer#doStart // 启动 private void doStart() { // 启动模块 startModules(); } // 启动模块 private void startModules() { // ensure init and start internal module first // 启动内部模块 prepareInternalModule(); // filter and start pending modules, ignore new module during starting, throw exception of module start // 启动外部模块 for (ModuleModel moduleModel : applicationModel.getModuleModels()) { if (moduleModel.getDeployer().isPending()) { moduleModel.getDeployer().start(); } } } 源码位置: org.apache.dubbo.config.deploy.DefaultModuleDeployer#start // 启动模块 @Override public Future start() throws IllegalStateException { // initialize，maybe deadlock applicationDeployer lock \u0026 moduleDeployer lock // 初始化, 上面已经说过了 applicationDeployer.initialize(); // 启动，加锁 return startSync(); } private synchronized Future startSync() throws IllegalStateException { // 判断状态 if (isStopping() || isStopped() || isFailed()) { throw new IllegalStateException(getIdentifier() + \" is stopping or stopped, can not start again\"); } try { if (isStarting() || isStarted()) { return startFuture; } // 变更状态，启动中 onModuleStarting(); // 初始化，加载配置 initialize(); // export services // 暴露服务，很重要 exportServices(); // prepare application instance // exclude internal module to avoid wait itself if (moduleModel != moduleModel.getApplicationModel().getInternalModule()) { applicationDeployer.prepareInternalModule(); } // refer services // 引用服务，很重要 referServices(); // if no async export/refer services, just set started // 下面的逻辑分为同步和异步处理, 逻辑都是一样的 if (asyncExportingFutures.isEmpty() \u0026\u0026 asyncReferringFutures.isEmpty()) { // publish module started event // 变更状态为已启动，暴露 metadataService, 这个很重要 onModuleStarted(); // register services to registry // 注册服务，刷新元数据 registerServices(); // check reference config // 检查 checkReferences(); // complete module start futu","date":"2023-09-16","objectID":"/ooooo-notes/03-%E5%90%AF%E5%8A%A8%E6%B5%81%E7%A8%8B/:2:0","tags":["dubbo","source code","源码分析 dubbo 系列"],"title":"03 启动流程","uri":"/ooooo-notes/03-%E5%90%AF%E5%8A%A8%E6%B5%81%E7%A8%8B/"},{"categories":null,"content":" nacos 基于 2.2.4 版本 这里的 client 是指 nacos SDK，也就是模块 nacos-client. ","date":"2023-09-16","objectID":"/ooooo-notes/21-client-%E8%AE%A2%E9%98%85%E9%85%8D%E7%BD%AE/:0:0","tags":["nacos","source code","源码分析 nacos 系列"],"title":"21 client 订阅配置","uri":"/ooooo-notes/21-client-%E8%AE%A2%E9%98%85%E9%85%8D%E7%BD%AE/"},{"categories":null,"content":"添加订阅者 源码位置: com.alibaba.nacos.client.config.NacosConfigService#addListener // 添加监听器 @Override public void addListener(String dataId, String group, Listener listener) throws NacosException { worker.addTenantListeners(dataId, group, Arrays.asList(listener)); } // 添加监听器 public void addTenantListeners(String dataId, String group, List\u003c? extends Listener\u003e listeners) throws NacosException { group = blank2defaultGroup(group); String tenant = agent.getTenant(); // 添加到 CacheData 里面，对同一个 dataId, group, tenant 可能有多个 listener CacheData cache = addCacheDataIfAbsent(dataId, group, tenant); synchronized (cache) { for (Listener listener : listeners) { cache.addListener(listener); } // 不删除 cache.setDiscard(false); // 和服务器不一致 cache.setConsistentWithServer(false); // 通知监听配置 agent.notifyListenConfig(); } } ","date":"2023-09-16","objectID":"/ooooo-notes/21-client-%E8%AE%A2%E9%98%85%E9%85%8D%E7%BD%AE/:1:0","tags":["nacos","source code","源码分析 nacos 系列"],"title":"21 client 订阅配置","uri":"/ooooo-notes/21-client-%E8%AE%A2%E9%98%85%E9%85%8D%E7%BD%AE/"},{"categories":null,"content":"通知监听配置 源码位置: com.alibaba.nacos.client.config.impl.ClientWorker.ConfigRpcTransportClient#notifyListenConfig // 通知监听配置 @Override public void notifyListenConfig() { // 向队列 listenExecutebell 添加一个通知 listenExecutebell.offer(bellItem); } // 客户端启动会调用这个方法 @Override public void startInternal() { executor.schedule(() -\u003e { while (!executor.isShutdown() \u0026\u0026 !executor.isTerminated()) { try { // 获取通知, 最大间隔时间为 5 秒 listenExecutebell.poll(5L, TimeUnit.SECONDS); if (executor.isShutdown() || executor.isTerminated()) { continue; } // 执行配置监听 executeConfigListen(); } catch (Throwable e) { LOGGER.error(\"[rpc listen execute] [rpc listen] exception\", e); try { Thread.sleep(50L); } catch (InterruptedException interruptedException) { //ignore } notifyListenConfig(); } } }, 0L, TimeUnit.MILLISECONDS); } 源码位置: `` // 执行配置监听 @Override public void executeConfigListen() { Map\u003cString, List\u003cCacheData\u003e\u003e listenCachesMap = new HashMap\u003c\u003e(16); Map\u003cString, List\u003cCacheData\u003e\u003e removeListenCachesMap = new HashMap\u003c\u003e(16); long now = System.currentTimeMillis(); // 每隔一段时间都需要全同步配置 boolean needAllSync = now - lastAllSyncTime \u003e= ALL_SYNC_INTERNAL; // 遍历 cacheMap, 这个 map 都是要监听的配置 for (CacheData cache : cacheMap.get().values()) { synchronized (cache) { //check local listeners consistent. // 判断是否和服务端一致，不一致，需要刷新配置 if (cache.isConsistentWithServer()) { // 检查配置 md5 值, 不一致就推送给订阅者 cache.checkListenerMd5(); if (!needAllSync) { continue; } } // 不是删除的配置， if (!cache.isDiscard()) { //get listen config if (!cache.isUseLocalConfigInfo()) { List\u003cCacheData\u003e cacheDatas = listenCachesMap.get(String.valueOf(cache.getTaskId())); if (cacheDatas == null) { cacheDatas = new LinkedList\u003c\u003e(); listenCachesMap.put(String.valueOf(cache.getTaskId()), cacheDatas); } // 添加要监听的配置 cacheDatas.add(cache); } } else if (cache.isDiscard() \u0026\u0026 CollectionUtils.isEmpty(cache.getListeners())) { // 是删除的配置，并且订阅者是空 if (!cache.isUseLocalConfigInfo()) { List\u003cCacheData\u003e cacheDatas = removeListenCachesMap.get(String.valueOf(cache.getTaskId())); if (cacheDatas == null) { cacheDatas = new LinkedList\u003c\u003e(); removeListenCachesMap.put(String.valueOf(cache.getTaskId()), cacheDatas); } // 添加要删除的订阅者 cacheDatas.add(cache); } } } } //execute check listen ,return true if has change keys. // 拉取配置，检查是否改变 boolean hasChangedKeys = checkListenCache(listenCachesMap); //execute check remove listen. // 删除监听 checkRemoveListenCache(removeListenCachesMap); // 记录全同步的时间 if (needAllSync) { lastAllSyncTime = now; } //If has changed keys,notify re sync md5. // 有配置改变，重新再运行一次, 推送配置给订阅者 if (hasChangedKeys) { notifyListenConfig(); } } 源码位置: com.alibaba.nacos.client.config.impl.ClientWorker.ConfigRpcTransportClient#checkListenCache // 拉取配置，检查是否改变 private boolean checkListenCache(Map\u003cString, List\u003cCacheData\u003e\u003e listenCachesMap) { final AtomicBoolean hasChangedKeys = new AtomicBoolean(false); if (!listenCachesMap.isEmpty()) { List\u003cFuture\u003e listenFutures = new ArrayList\u003c\u003e(); // 遍历 listenCachesMap, 每一个 taskId, 有一个线程负责拉取配置 for (Map.Entry\u003cString, List\u003cCacheData\u003e\u003e entry : listenCachesMap.entrySet()) { String taskId = entry.getKey(); ExecutorService executorService = ensureSyncExecutor(taskId); Future future = executorService.submit(() -\u003e { List\u003cCacheData\u003e listenCaches = entry.getValue(); //reset notify change flag. // 重置 for (CacheData cacheData : listenCaches) { cacheData.getReceiveNotifyChanged().set(false); } // 构建 ConfigBatchListenRequest 请求，里面有 md5 值 ConfigBatchListenRequest configChangeListenRequest = buildConfigRequest(listenCaches); configChangeListenRequest.setListen(true); try { RpcClient rpcClient = ensureRpcClient(taskId); // 请求服务端，如果 md5 值不一样，就会返回 ConfigChangeBatchListenResponse listenResponse = (ConfigChangeBatchListenResponse) requestProxy( rpcClient, configChangeListenRequest); if (listenResponse != null \u0026\u0026 listenResponse.isSuccess()) { // 表示是否拉取过配置 Set\u003cString\u003e changeKeys = new HashSet\u003cString\u003e(); List\u003cConfigChangeBatchListenResponse.ConfigContext\u003e changedConfigs = listenResponse.getChangedConfigs(); //handle changed keys,notify listener if (!CollectionUtils.is","date":"2023-09-16","objectID":"/ooooo-notes/21-client-%E8%AE%A2%E9%98%85%E9%85%8D%E7%BD%AE/:2:0","tags":["nacos","source code","源码分析 nacos 系列"],"title":"21 client 订阅配置","uri":"/ooooo-notes/21-client-%E8%AE%A2%E9%98%85%E9%85%8D%E7%BD%AE/"},{"categories":null,"content":"刷新配置，通知订阅者 源码位置: com.alibaba.nacos.client.config.impl.ClientWorker#refreshContentAndCheck // 刷新配置，通知订阅者 private void refreshContentAndCheck(CacheData cacheData, boolean notify) { try { // 获取配置 ConfigResponse response = getServerConfig(cacheData.dataId, cacheData.group, cacheData.tenant, 3000L, notify); cacheData.setEncryptedDataKey(response.getEncryptedDataKey()); cacheData.setContent(response.getContent()); if (null != response.getConfigType()) { cacheData.setType(response.getConfigType()); } if (notify) { LOGGER.info(\"[{}] [data-received] dataId={}, group={}, tenant={}, md5={}, content={}, type={}\", agent.getName(), cacheData.dataId, cacheData.group, cacheData.tenant, cacheData.getMd5(), ContentUtils.truncateContent(response.getContent()), response.getConfigType()); } // 检查 md5 值 cacheData.checkListenerMd5(); } catch (Exception e) { LOGGER.error(\"refresh content and check md5 fail ,dataId={},group={},tenant={} \", cacheData.dataId, cacheData.group, cacheData.tenant, e); } } // 检查 md5 值 void checkListenerMd5() { for (ManagerListenerWrap wrap : listeners) { // 不一致，通知订阅者 if (!md5.equals(wrap.lastCallMd5)) { safeNotifyListener(dataId, group, content, type, md5, encryptedDataKey, wrap); } } } ","date":"2023-09-16","objectID":"/ooooo-notes/21-client-%E8%AE%A2%E9%98%85%E9%85%8D%E7%BD%AE/:3:0","tags":["nacos","source code","源码分析 nacos 系列"],"title":"21 client 订阅配置","uri":"/ooooo-notes/21-client-%E8%AE%A2%E9%98%85%E9%85%8D%E7%BD%AE/"},{"categories":null,"content":"rpcClient 初始化 源码位置: com.alibaba.nacos.client.config.impl.ClientWorker.ConfigRpcTransportClient#ensureRpcClient private RpcClient ensureRpcClient(String taskId) throws NacosException { synchronized (ClientWorker.this) { Map\u003cString, String\u003e labels = getLabels(); Map\u003cString, String\u003e newLabels = new HashMap\u003c\u003e(labels); newLabels.put(\"taskId\", taskId); RpcClient rpcClient = RpcClientFactory.createClient(uuid + \"_config-\" + taskId, getConnectionType(), newLabels, RpcClientTlsConfig.properties(this.properties)); if (rpcClient.isWaitInitiated()) { // 初始化 rpcClient initRpcClientHandler(rpcClient); rpcClient.setTenant(getTenant()); rpcClient.clientAbilities(initAbilities()); // 启动 rpcClient.start(); } return rpcClient; } } 源码位置: com.alibaba.nacos.client.config.impl.ClientWorker.ConfigRpcTransportClient#initRpcClientHandler private void initRpcClientHandler(final RpcClient rpcClientInner) { /* * Register Config Change /Config ReSync Handler */ // 注册配置通知的 requestHandler rpcClientInner.registerServerRequestHandler((request) -\u003e { if (request instanceof ConfigChangeNotifyRequest) { ConfigChangeNotifyRequest configChangeNotifyRequest = (ConfigChangeNotifyRequest) request; LOGGER.info(\"[{}] [server-push] config changed. dataId={}, group={},tenant={}\", rpcClientInner.getName(), configChangeNotifyRequest.getDataId(), configChangeNotifyRequest.getGroup(), configChangeNotifyRequest.getTenant()); String groupKey = GroupKey.getKeyTenant(configChangeNotifyRequest.getDataId(), configChangeNotifyRequest.getGroup(), configChangeNotifyRequest.getTenant()); CacheData cacheData = cacheMap.get().get(groupKey); if (cacheData != null) { synchronized (cacheData) { cacheData.getReceiveNotifyChanged().set(true); cacheData.setConsistentWithServer(false); notifyListenConfig(); } } return new ConfigChangeNotifyResponse(); } return null; }); // ClientConfigMetricRequest rpcClientInner.registerServerRequestHandler((request) -\u003e { if (request instanceof ClientConfigMetricRequest) { ClientConfigMetricResponse response = new ClientConfigMetricResponse(); response.setMetrics(getMetrics(((ClientConfigMetricRequest) request).getMetricsKeys())); return response; } return null; }); // 连接事件 rpcClientInner.registerConnectionListener(new ConnectionEventListener() { @Override public void onConnected() { LOGGER.info(\"[{}] Connected,notify listen context...\", rpcClientInner.getName()); notifyListenConfig(); } @Override public void onDisConnect() { String taskId = rpcClientInner.getLabels().get(\"taskId\"); LOGGER.info(\"[{}] DisConnected,clear listen context...\", rpcClientInner.getName()); Collection\u003cCacheData\u003e values = cacheMap.get().values(); for (CacheData cacheData : values) { if (StringUtils.isNotBlank(taskId)) { if (Integer.valueOf(taskId).equals(cacheData.getTaskId())) { cacheData.setConsistentWithServer(false); } } else { cacheData.setConsistentWithServer(false); } } } }); // 挑选下一个地址 rpcClientInner.serverListFactory(new ServerListFactory() { @Override public String genNextServer() { return ConfigRpcTransportClient.super.serverListManager.getNextServerAddr(); } @Override public String getCurrentServer() { return ConfigRpcTransportClient.super.serverListManager.getCurrentServerAddr(); } @Override public List\u003cString\u003e getServerList() { return ConfigRpcTransportClient.super.serverListManager.getServerUrls(); } }); // 地址变动的监听器 subscriber = new Subscriber() { @Override public void onEvent(Event event) { rpcClientInner.onServerListChange(); } @Override public Class\u003c? extends Event\u003e subscribeType() { return ServerlistChangeEvent.class; } }; NotifyCenter.registerSubscriber(subscriber); } ","date":"2023-09-16","objectID":"/ooooo-notes/21-client-%E8%AE%A2%E9%98%85%E9%85%8D%E7%BD%AE/:4:0","tags":["nacos","source code","源码分析 nacos 系列"],"title":"21 client 订阅配置","uri":"/ooooo-notes/21-client-%E8%AE%A2%E9%98%85%E9%85%8D%E7%BD%AE/"},{"categories":null,"content":" dubbo 基于 3.2.6 版本 在 dubbo 中，ExtensionLoader 是很重要的类，实现了 dubbo 的扩展机制，主要有三个方法，getExtension、getActivateExtension、getAdaptiveExtension。 ","date":"2023-09-15","objectID":"/ooooo-notes/02-extensionloader/:0:0","tags":["dubbo","source code","源码分析 dubbo 系列"],"title":"02 ExtensionLoader","uri":"/ooooo-notes/02-extensionloader/"},{"categories":null,"content":"getExtension 方法 源码位置: org.apache.dubbo.common.extension.ExtensionLoader#getExtension // 根据 name 来获取扩展 public T getExtension(String name) { T extension = getExtension(name, true); if (extension == null) { throw new IllegalArgumentException(\"Not find extension: \" + name); } return extension; } // 根据 name 来获取扩展 public T getExtension(String name, boolean wrap) { checkDestroyed(); if (StringUtils.isEmpty(name)) { throw new IllegalArgumentException(\"Extension name == null\"); } // 获取默认扩展 if (\"true\".equals(name)) { return getDefaultExtension(); } String cacheKey = name; if (!wrap) { cacheKey += \"_origin\"; } final Holder\u003cObject\u003e holder = getOrCreateHolder(cacheKey); Object instance = holder.get(); if (instance == null) { synchronized (holder) { instance = holder.get(); if (instance == null) { // 创建 extension instance = createExtension(name, wrap); holder.set(instance); } } } return (T) instance; } 源码位置: org.apache.dubbo.common.extension.ExtensionLoader#createExtension // 创建 extension private T createExtension(String name, boolean wrap) { // 获取扩展的 class, 读取 META-INF/dubbo/internal, META-INF/services, META-INF/dubbo 目录下 Class\u003c?\u003e clazz = getExtensionClasses().get(name); if (clazz == null || unacceptableExceptions.contains(name)) { throw findException(name); } try { // 从缓存中获取对象 T instance = (T) extensionInstances.get(clazz); if (instance == null) { // 创建 extension 实例 extensionInstances.putIfAbsent(clazz, createExtensionInstance(clazz)); instance = (T) extensionInstances.get(clazz); instance = postProcessBeforeInitialization(instance, name); // 注入属性 injectExtension(instance); instance = postProcessAfterInitialization(instance, name); } if (wrap) { List\u003cClass\u003c?\u003e\u003e wrapperClassesList = new ArrayList\u003c\u003e(); if (cachedWrapperClasses != null) { wrapperClassesList.addAll(cachedWrapperClasses); wrapperClassesList.sort(WrapperComparator.COMPARATOR); Collections.reverse(wrapperClassesList); } if (CollectionUtils.isNotEmpty(wrapperClassesList)) { // 遍历包装类 for (Class\u003c?\u003e wrapperClass : wrapperClassesList) { Wrapper wrapper = wrapperClass.getAnnotation(Wrapper.class); boolean match = (wrapper == null) || ((ArrayUtils.isEmpty( wrapper.matches()) || ArrayUtils.contains(wrapper.matches(), name)) \u0026\u0026 !ArrayUtils.contains(wrapper.mismatches(), name)); if (match) { // 注入类 instance = injectExtension( (T) wrapperClass.getConstructor(type).newInstance(instance)); instance = postProcessAfterInitialization(instance, name); } } } } // Warning: After an instance of Lifecycle is wrapped by cachedWrapperClasses, it may not still be Lifecycle instance, this application may not invoke the lifecycle.initialize hook. // 初始化 extension initExtension(instance); return instance; } catch (Throwable t) { throw new IllegalStateException( \"Extension instance (name: \" + name + \", class: \" + type + \") couldn't be instantiated: \" + t.getMessage(), t); } } 源码位置: org.apache.dubbo.common.extension.ExtensionLoader#getExtensionClasses // 获取扩展的 class, 读取 META-INF/dubbo/internal, META-INF/services, META-INF/dubbo 目录下 private Map\u003cString, Class\u003c?\u003e\u003e getExtensionClasses() { // 单例延迟加载 Map\u003cString, Class\u003c?\u003e\u003e classes = cachedClasses.get(); if (classes == null) { synchronized (cachedClasses) { classes = cachedClasses.get(); if (classes == null) { try { // 加载 extension class classes = loadExtensionClasses(); } catch (InterruptedException e) { logger.error(COMMON_ERROR_LOAD_EXTENSION, \"\", \"\", \"Exception occurred when loading extension class (interface: \" + type + \")\", e); throw new IllegalStateException( \"Exception occurred when loading extension class (interface: \" + type + \")\", e); } cachedClasses.set(classes); } } } return classes; } 源码位置: org.apache.dubbo.common.extension.ExtensionLoader#loadExtensionClasses // 加载 extension class private Map\u003cString, Class\u003c?\u003e\u003e loadExtensionClasses() throws InterruptedException { checkDestroyed(); cacheDefaultExtensionName(); Map\u003cString, Class\u003c?\u003e\u003e extensionClasses = new HashMap\u003c\u003e(); // 遍历 strategies for (LoadingStrategy strategy : strategies) { // 加载目录中类, 最终会调用 loadCla","date":"2023-09-15","objectID":"/ooooo-notes/02-extensionloader/:1:0","tags":["dubbo","source code","源码分析 dubbo 系列"],"title":"02 ExtensionLoader","uri":"/ooooo-notes/02-extensionloader/"},{"categories":null,"content":"getActivateExtension 方法 源码位置: org.apache.dubbo.common.extension.ExtensionLoader#getActivateExtension // 获取 Activate Extension public List\u003cT\u003e getActivateExtension(URL url, String key, String group) { String value = url.getParameter(key); // 获取 Activate return getActivateExtension(url, StringUtils.isEmpty(value) ? null : COMMA_SPLIT_PATTERN.split(value), group); } 源码位置: org.apache.dubbo.common.extension.ExtensionLoader#getActivateExtension // 获取 Activate public List\u003cT\u003e getActivateExtension(URL url, String[] values, String group) { checkDestroyed(); // solve the bug of using @SPI's wrapper method to report a null pointer exception. Map\u003cClass\u003c?\u003e, T\u003e activateExtensionsMap = new TreeMap\u003c\u003e(activateComparator); List\u003cString\u003e names = values == null ? new ArrayList\u003c\u003e(0) : Arrays.stream(values).map(StringUtils::trim).collect(Collectors.toList()); Set\u003cString\u003e namesSet = new HashSet\u003c\u003e(names); // 判断是否移除默认的 if (!namesSet.contains(REMOVE_VALUE_PREFIX + DEFAULT_KEY)) { if (cachedActivateGroups.size() == 0) { synchronized (cachedActivateGroups) { // cache all extensions if (cachedActivateGroups.size() == 0) { // 确保 extension 已经加载，这个方法上面已经分析过了 getExtensionClasses(); // 遍历所有的 activate class for (Map.Entry\u003cString, Object\u003e entry : cachedActivates.entrySet()) { String name = entry.getKey(); Object activate = entry.getValue(); String[] activateGroup, activateValue; // 获取 @Activate 的 group 和 value if (activate instanceof Activate) { activateGroup = ((Activate) activate).group(); activateValue = ((Activate) activate).value(); } else if (activate instanceof com.alibaba.dubbo.common.extension.Activate) { activateGroup = ((com.alibaba.dubbo.common.extension.Activate) activate).group(); activateValue = ((com.alibaba.dubbo.common.extension.Activate) activate).value(); } else { continue; } cachedActivateGroups.put(name, new HashSet\u003c\u003e(Arrays.asList(activateGroup))); String[][] keyPairs = new String[activateValue.length][]; for (int i = 0; i \u003c activateValue.length; i++) { if (activateValue[i].contains(\":\")) { keyPairs[i] = new String[2]; String[] arr = activateValue[i].split(\":\"); keyPairs[i][0] = arr[0]; keyPairs[i][1] = arr[1]; } else { keyPairs[i] = new String[1]; keyPairs[i][0] = activateValue[i]; } } // 加入到缓存中 cachedActivateValues.put(name, keyPairs); } } } } // traverse all cached extensions cachedActivateGroups.forEach((name, activateGroup) -\u003e { // 判断 group 和 name 是否匹配 if (isMatchGroup(group, activateGroup) \u0026\u0026 !namesSet.contains( name) \u0026\u0026 !namesSet.contains(REMOVE_VALUE_PREFIX + name) \u0026\u0026 isActive( cachedActivateValues.get(name), url)) { // 保存默认的 activate activateExtensionsMap.put(getExtensionClass(name), getExtension(name)); } }); } // 有默认的配置, 组合 activate if (namesSet.contains(DEFAULT_KEY)) { // will affect order // `ext1,default,ext2` means ext1 will happens before all of the default extensions while ext2 will after them ArrayList\u003cT\u003e extensionsResult = new ArrayList\u003c\u003e( activateExtensionsMap.size() + names.size()); for (String name : names) { if (name.startsWith(REMOVE_VALUE_PREFIX) || namesSet.contains( REMOVE_VALUE_PREFIX + name)) { continue; } if (DEFAULT_KEY.equals(name)) { extensionsResult.addAll(activateExtensionsMap.values()); continue; } if (containsExtension(name)) { extensionsResult.add(getExtension(name)); } } return extensionsResult; } else { // add extensions, will be sorted by its order for (String name : names) { if (name.startsWith(REMOVE_VALUE_PREFIX) || namesSet.contains( REMOVE_VALUE_PREFIX + name)) { continue; } if (DEFAULT_KEY.equals(name)) { continue; } if (containsExtension(name)) { activateExtensionsMap.put(getExtensionClass(name), getExtension(name)); } } return new ArrayList\u003c\u003e(activateExtensionsMap.values()); } } ","date":"2023-09-15","objectID":"/ooooo-notes/02-extensionloader/:2:0","tags":["dubbo","source code","源码分析 dubbo 系列"],"title":"02 ExtensionLoader","uri":"/ooooo-notes/02-extensionloader/"},{"categories":null,"content":"getAdaptiveExtension 方法 源码位置: `` public T getAdaptiveExtension() { checkDestroyed(); // 延迟获取 Object instance = cachedAdaptiveInstance.get(); if (instance == null) { if (createAdaptiveInstanceError != null) { throw new IllegalStateException( \"Failed to create adaptive instance: \" + createAdaptiveInstanceError.toString(), createAdaptiveInstanceError); } synchronized (cachedAdaptiveInstance) { instance = cachedAdaptiveInstance.get(); if (instance == null) { try { // 创建 Adaptive instance = createAdaptiveExtension(); cachedAdaptiveInstance.set(instance); } catch (Throwable t) { createAdaptiveInstanceError = t; throw new IllegalStateException( \"Failed to create adaptive instance: \" + t.toString(), t); } } } } return (T) instance; } 源码位置: org.apache.dubbo.common.extension.ExtensionLoader#createAdaptiveExtension // 创建 Adaptive private T createAdaptiveExtension() { try { // 获取 AdaptiveExtensionClass, 如果不存在，就会动态创建 T instance = (T) getAdaptiveExtensionClass().newInstance(); instance = postProcessBeforeInitialization(instance, null); injectExtension(instance); instance = postProcessAfterInitialization(instance, null); initExtension(instance); return instance; } catch (Exception e) { throw new IllegalStateException( \"Can't create adaptive extension \" + type + \", cause: \" + e.getMessage(), e); } } 源码位置: org.apache.dubbo.common.extension.ExtensionLoader#createAdaptiveExtensionClass // 动态创建 private Class\u003c?\u003e createAdaptiveExtensionClass() { // Adaptive Classes' ClassLoader should be the same with Real SPI interface classes' ClassLoader ClassLoader classLoader = type.getClassLoader(); try { if (NativeUtils.isNative()) { return classLoader.loadClass(type.getName() + \"$Adaptive\"); } } catch (Throwable ignore) { } // 生成代码 String code = new AdaptiveClassCodeGenerator(type, cachedDefaultName).generate(); org.apache.dubbo.common.compiler.Compiler compiler = extensionDirector.getExtensionLoader( org.apache.dubbo.common.compiler.Compiler.class).getAdaptiveExtension(); // 编译代码，然后加载 return compiler.compile(type, code, classLoader); } ","date":"2023-09-15","objectID":"/ooooo-notes/02-extensionloader/:3:0","tags":["dubbo","source code","源码分析 dubbo 系列"],"title":"02 ExtensionLoader","uri":"/ooooo-notes/02-extensionloader/"},{"categories":null,"content":"测试类 org.apache.dubbo.common.extension.ExtensionLoaderTest#test_getExtension org.apache.dubbo.common.extension.ExtensionLoaderTest#test_getExtension_WithWrapper org.apache.dubbo.common.extension.ExtensionLoaderTest#test_getActivateExtension_WithWrapper1 org.apache.dubbo.common.extension.ExtensionLoader_Adaptive_Test#test_getAdaptiveExtension_customizeAdaptiveKey org.apache.dubbo.common.extension.ExtensionLoader_Adaptive_Test#test_getAdaptiveExtension_protocolKey ","date":"2023-09-15","objectID":"/ooooo-notes/02-extensionloader/:4:0","tags":["dubbo","source code","源码分析 dubbo 系列"],"title":"02 ExtensionLoader","uri":"/ooooo-notes/02-extensionloader/"},{"categories":null,"content":" nacos 基于 2.2.4 版本 这里的 client 是指 nacos SDK，也就是模块 nacos-client. ","date":"2023-09-15","objectID":"/ooooo-notes/20-client-%E8%8E%B7%E5%8F%96%E9%85%8D%E7%BD%AE/:0:0","tags":["nacos","source code","源码分析 nacos 系列"],"title":"20 client 获取配置","uri":"/ooooo-notes/20-client-%E8%8E%B7%E5%8F%96%E9%85%8D%E7%BD%AE/"},{"categories":null,"content":"获取配置 源码位置: com.alibaba.nacos.client.config.NacosConfigService#getConfig // 获取配置 @Override public String getConfig(String dataId, String group, long timeoutMs) throws NacosException { return getConfigInner(namespace, dataId, group, timeoutMs); } 源码位置: com.alibaba.nacos.client.config.NacosConfigService#getConfigInner // 获取配置 private String getConfigInner(String tenant, String dataId, String group, long timeoutMs) throws NacosException { group = blank2defaultGroup(group); // 检查参数 ParamUtils.checkKeyParam(dataId, group); ConfigResponse cr = new ConfigResponse(); cr.setDataId(dataId); cr.setTenant(tenant); cr.setGroup(group); // We first try to use local failover content if exists. // A config content for failover is not created by client program automatically, // but is maintained by user. // This is designed for certain scenario like client emergency reboot, // changing config needed in the same time, while nacos server is down. // 是否从本地获取, 默认不是 String content = LocalConfigInfoProcessor.getFailover(worker.getAgentName(), dataId, group, tenant); if (content != null) { LOGGER.warn(\"[{}] [get-config] get failover ok, dataId={}, group={}, tenant={}, config={}\", worker.getAgentName(), dataId, group, tenant, ContentUtils.truncateContent(content)); cr.setContent(content); String encryptedDataKey = LocalEncryptedDataKeyProcessor .getEncryptDataKeyFailover(agent.getName(), dataId, group, tenant); cr.setEncryptedDataKey(encryptedDataKey); configFilterChainManager.doFilter(null, cr); content = cr.getContent(); return content; } try { // 获取配置，发送 ConfigQueryRequest 请求，会被 ConfigQueryRequestHandler 处理 ConfigResponse response = worker.getServerConfig(dataId, group, tenant, timeoutMs, false); cr.setContent(response.getContent()); cr.setEncryptedDataKey(response.getEncryptedDataKey()); // 加解密 configFilterChainManager.doFilter(null, cr); content = cr.getContent(); return content; } catch (NacosException ioe) { if (NacosException.NO_RIGHT == ioe.getErrCode()) { throw ioe; } LOGGER.warn(\"[{}] [get-config] get from server error, dataId={}, group={}, tenant={}, msg={}\", worker.getAgentName(), dataId, group, tenant, ioe.toString()); } // 获取本地快照, 返回 content = LocalConfigInfoProcessor.getSnapshot(worker.getAgentName(), dataId, group, tenant); if (content != null) { LOGGER.warn(\"[{}] [get-config] get snapshot ok, dataId={}, group={}, tenant={}, config={}\", worker.getAgentName(), dataId, group, tenant, ContentUtils.truncateContent(content)); } cr.setContent(content); String encryptedDataKey = LocalEncryptedDataKeyProcessor .getEncryptDataKeySnapshot(agent.getName(), dataId, group, tenant); cr.setEncryptedDataKey(encryptedDataKey); configFilterChainManager.doFilter(null, cr); content = cr.getContent(); return content; } ","date":"2023-09-15","objectID":"/ooooo-notes/20-client-%E8%8E%B7%E5%8F%96%E9%85%8D%E7%BD%AE/:1:0","tags":["nacos","source code","源码分析 nacos 系列"],"title":"20 client 获取配置","uri":"/ooooo-notes/20-client-%E8%8E%B7%E5%8F%96%E9%85%8D%E7%BD%AE/"},{"categories":null,"content":" dubbo 基于 3.2.6 版本 源码分析，主要介绍服务级别的注册和发现，所以我们需要设置下参数来启用。 选取 dubbo-demo-api 模块作为示例。 ","date":"2023-09-14","objectID":"/ooooo-notes/01-%E6%90%AD%E5%BB%BA-dubbo-%E6%BA%90%E7%A0%81%E8%B0%83%E8%AF%95%E7%8E%AF%E5%A2%83/:0:0","tags":["dubbo","source code","源码分析 dubbo 系列"],"title":"01 搭建 dubbo 源码调试环境","uri":"/ooooo-notes/01-%E6%90%AD%E5%BB%BA-dubbo-%E6%BA%90%E7%A0%81%E8%B0%83%E8%AF%95%E7%8E%AF%E5%A2%83/"},{"categories":null,"content":"provider 示例程序 public class Application { private static final String REGISTRY_URL = \"zookeeper://127.0.0.1:2181\"; public static void main(String[] args) { startWithBootstrap(); } private static void startWithBootstrap() { ServiceConfig\u003cDemoServiceImpl\u003e service = new ServiceConfig\u003c\u003e(); service.setInterface(DemoService.class); service.setRef(new DemoServiceImpl()); RegistryConfig registryConfig = new RegistryConfig(REGISTRY_URL); // 注册类型为 service registryConfig.setParameters(Collections.singletonMap(RegistryConstants.REGISTRY_TYPE_KEY, RegistryConstants.SERVICE_REGISTRY_TYPE)); DubboBootstrap bootstrap = DubboBootstrap.getInstance(); bootstrap.application(new ApplicationConfig(\"dubbo-demo-api-provider\")) .registry(registryConfig) .protocol(new ProtocolConfig(CommonConstants.DUBBO, -1)) .service(service) .start() .await(); } } ","date":"2023-09-14","objectID":"/ooooo-notes/01-%E6%90%AD%E5%BB%BA-dubbo-%E6%BA%90%E7%A0%81%E8%B0%83%E8%AF%95%E7%8E%AF%E5%A2%83/:1:0","tags":["dubbo","source code","源码分析 dubbo 系列"],"title":"01 搭建 dubbo 源码调试环境","uri":"/ooooo-notes/01-%E6%90%AD%E5%BB%BA-dubbo-%E6%BA%90%E7%A0%81%E8%B0%83%E8%AF%95%E7%8E%AF%E5%A2%83/"},{"categories":null,"content":"consumer 示例程序 public class Application { private static final String REGISTRY_URL = \"zookeeper://127.0.0.1:2181\"; public static void main(String[] args) { runWithBootstrap(); } private static void runWithBootstrap() { ReferenceConfig\u003cDemoService\u003e reference = new ReferenceConfig\u003c\u003e(); reference.setInterface(DemoService.class); reference.setGeneric(\"true\"); RegistryConfig registryConfig = new RegistryConfig(REGISTRY_URL); // 注册类型为 service registryConfig.setParameters(Collections.singletonMap(RegistryConstants.REGISTRY_TYPE_KEY, RegistryConstants.SERVICE_REGISTRY_TYPE)); DubboBootstrap bootstrap = DubboBootstrap.getInstance(); bootstrap.application(new ApplicationConfig(\"dubbo-demo-api-consumer\")) .registry(registryConfig) .protocol(new ProtocolConfig(CommonConstants.DUBBO, -1)) .reference(reference) .start(); DemoService demoService = bootstrap.getCache().get(reference); String message = demoService.sayHello(\"dubbo\"); System.out.println(message); // generic invoke GenericService genericService = (GenericService) demoService; Object genericInvokeResult = genericService.$invoke(\"sayHello\", new String[]{String.class.getName()}, new Object[]{\"dubbo generic invoke\"}); System.out.println(genericInvokeResult.toString()); } } ","date":"2023-09-14","objectID":"/ooooo-notes/01-%E6%90%AD%E5%BB%BA-dubbo-%E6%BA%90%E7%A0%81%E8%B0%83%E8%AF%95%E7%8E%AF%E5%A2%83/:2:0","tags":["dubbo","source code","源码分析 dubbo 系列"],"title":"01 搭建 dubbo 源码调试环境","uri":"/ooooo-notes/01-%E6%90%AD%E5%BB%BA-dubbo-%E6%BA%90%E7%A0%81%E8%B0%83%E8%AF%95%E7%8E%AF%E5%A2%83/"},{"categories":null,"content":" nacos 基于 2.2.4 版本 在 nacos 中，订阅配置分为 http长轮询 和 grpc 两种方式。 ","date":"2023-09-14","objectID":"/ooooo-notes/19-%E8%AE%A2%E9%98%85%E9%85%8D%E7%BD%AE/:0:0","tags":["nacos","source code","源码分析 nacos 系列"],"title":"19 订阅配置","uri":"/ooooo-notes/19-%E8%AE%A2%E9%98%85%E9%85%8D%E7%BD%AE/"},{"categories":null,"content":"http 长轮询 源码位置: com.alibaba.nacos.config.server.controller.ConfigController#listener // http 长轮询 @PostMapping(\"/listener\") @Secured(action = ActionTypes.READ, signType = SignType.CONFIG) public void listener(HttpServletRequest request, HttpServletResponse response) throws ServletException, IOException { // 启用 servlet 异步 request.setAttribute(\"org.apache.catalina.ASYNC_SUPPORTED\", true); // 获取要监听的配置 String probeModify = request.getParameter(\"Listening-Configs\"); if (StringUtils.isBlank(probeModify)) { LOGGER.warn(\"invalid probeModify is blank\"); throw new IllegalArgumentException(\"invalid probeModify\"); } probeModify = URLDecoder.decode(probeModify, Constants.ENCODE); Map\u003cString, String\u003e clientMd5Map; try { // 解析出 md5 值，key 是 groupKey, value 是 md5 值 clientMd5Map = MD5Util.getClientMd5Map(probeModify); } catch (Throwable e) { throw new IllegalArgumentException(\"invalid probeModify\"); } // do long-polling // 长轮询 inner.doPollingConfig(request, response, clientMd5Map, probeModify.length()); } 源码位置: com.alibaba.nacos.config.server.controller.ConfigServletInner#doPollingConfig // 长轮询 public String doPollingConfig(HttpServletRequest request, HttpServletResponse response, Map\u003cString, String\u003e clientMd5Map, int probeRequestSize) throws IOException { // Long polling. // 是否支持长轮询, 判断请求头 Long-Pulling-Timeout 是否存在 if (LongPollingService.isSupportLongPolling(request)) { // 添加长轮询 longPollingService.addLongPollingClient(request, response, clientMd5Map, probeRequestSize); return HttpServletResponse.SC_OK + \"\"; } // Compatible with short polling logic. // 下面的逻辑都是兼容短轮询的，也就是立马对比 md5 值是否发生改变 // 如果不一样，返回改变的 groupKey，下面的逻辑就不看了 List\u003cString\u003e changedGroups = MD5Util.compareMd5(request, response, clientMd5Map); // Compatible with short polling result. String oldResult = MD5Util.compareMd5OldResult(changedGroups); String newResult = MD5Util.compareMd5ResultString(changedGroups); String version = request.getHeader(Constants.CLIENT_VERSION_HEADER); if (version == null) { version = \"2.0.0\"; } int versionNum = Protocol.getVersionNumber(version); // Before 2.0.4 version, return value is put into header. if (versionNum \u003c START_LONG_POLLING_VERSION_NUM) { response.addHeader(Constants.PROBE_MODIFY_RESPONSE, oldResult); response.addHeader(Constants.PROBE_MODIFY_RESPONSE_NEW, newResult); } else { request.setAttribute(\"content\", newResult); } // Disable cache. response.setHeader(\"Pragma\", \"no-cache\"); response.setDateHeader(\"Expires\", 0); response.setHeader(\"Cache-Control\", \"no-cache,no-store\"); response.setStatus(HttpServletResponse.SC_OK); return HttpServletResponse.SC_OK + \"\"; } 源码位置: com.alibaba.nacos.config.server.service.LongPollingService#addLongPollingClient // 添加长轮询 public void addLongPollingClient(HttpServletRequest req, HttpServletResponse rsp, Map\u003cString, String\u003e clientMd5Map, int probeRequestSize) { // str 就是长轮询的超时时间，由客户端传入 String str = req.getHeader(LongPollingService.LONG_POLLING_HEADER); String noHangUpFlag = req.getHeader(LongPollingService.LONG_POLLING_NO_HANG_UP_HEADER); int delayTime = SwitchService.getSwitchInteger(SwitchService.FIXED_DELAY_TIME, 500); // Add delay time for LoadBalance, and one response is returned 500 ms in advance to avoid client timeout. long timeout = -1L; // 固定长轮询的时间 if (isFixedPolling()) { timeout = Math.max(10000, getFixedPollingInterval()); // Do nothing but set fix polling timeout. } else { // 计算长轮询时间 timeout = Math.max(10000, Long.parseLong(str) - delayTime); long start = System.currentTimeMillis(); List\u003cString\u003e changedGroups = MD5Util.compareMd5(req, rsp, clientMd5Map); // 如果 md5 值有变化，立即返回结果 if (changedGroups.size() \u003e 0) { generateResponse(req, rsp, changedGroups); LogUtil.CLIENT_LOG.info(\"{}|{}|{}|{}|{}|{}|{}\", System.currentTimeMillis() - start, \"instant\", RequestUtil.getRemoteIp(req), \"polling\", clientMd5Map.size(), probeRequestSize, changedGroups.size()); return; } else if (noHangUpFlag != null \u0026\u0026 noHangUpFlag.equalsIgnoreCase(TRUE_STR)) { // 不挂起请求 LogUtil.CLIENT_LOG.info(\"{}|{}|{}|{}|{}|{}|{}\", System.currentTimeMill","date":"2023-09-14","objectID":"/ooooo-notes/19-%E8%AE%A2%E9%98%85%E9%85%8D%E7%BD%AE/:1:0","tags":["nacos","source code","源码分析 nacos 系列"],"title":"19 订阅配置","uri":"/ooooo-notes/19-%E8%AE%A2%E9%98%85%E9%85%8D%E7%BD%AE/"},{"categories":null,"content":"grpc 订阅 源码位置: com.alibaba.nacos.config.server.remote.ConfigChangeBatchListenRequestHandler#handle // grpc 监听配置 public ConfigChangeBatchListenResponse handle(ConfigBatchListenRequest configChangeListenRequest, RequestMeta meta) throws NacosException { String connectionId = StringPool.get(meta.getConnectionId()); String tag = configChangeListenRequest.getHeader(Constants.VIPSERVER_TAG); ConfigChangeBatchListenResponse configChangeBatchListenResponse = new ConfigChangeBatchListenResponse(); for (ConfigBatchListenRequest.ConfigListenContext listenContext : configChangeListenRequest .getConfigListenContexts()) { String groupKey = GroupKey2 .getKey(listenContext.getDataId(), listenContext.getGroup(), listenContext.getTenant()); groupKey = StringPool.get(groupKey); String md5 = StringPool.get(listenContext.getMd5()); // 监听配置 if (configChangeListenRequest.isListen()) { // 添加监听 configChangeListenContext.addListen(groupKey, md5, connectionId); boolean isUptoDate = ConfigCacheService.isUptodate(groupKey, md5, meta.getClientIp(), tag); // 如果有变动，直接返回配置 if (!isUptoDate) { configChangeBatchListenResponse.addChangeConfig(listenContext.getDataId(), listenContext.getGroup(), listenContext.getTenant()); } } else { // 取消监听 configChangeListenContext.removeListen(groupKey, connectionId); } } return configChangeBatchListenResponse; } 源码位置: com.alibaba.nacos.config.server.remote.ConfigChangeListenContext#addListen // 添加监听 public synchronized void addListen(String groupKey, String md5, String connectionId) { // 1.add groupKeyContext Set\u003cString\u003e listenClients = groupKeyContext.get(groupKey); if (listenClients == null) { groupKeyContext.putIfAbsent(groupKey, new HashSet\u003c\u003e()); listenClients = groupKeyContext.get(groupKey); } listenClients.add(connectionId); // 2.add connectionIdContext HashMap\u003cString, String\u003e groupKeys = connectionIdContext.get(connectionId); if (groupKeys == null) { connectionIdContext.putIfAbsent(connectionId, new HashMap\u003c\u003e(16)); groupKeys = connectionIdContext.get(connectionId); } groupKeys.put(groupKey, md5); } 源码位置: com.alibaba.nacos.config.server.remote.ConfigChangeListenContext#removeListen // 取消监听 public synchronized void removeListen(String groupKey, String connectionId) { //1. remove groupKeyContext Set\u003cString\u003e connectionIds = groupKeyContext.get(groupKey); if (connectionIds != null) { connectionIds.remove(connectionId); if (connectionIds.isEmpty()) { groupKeyContext.remove(groupKey); } } //2.remove connectionIdContext HashMap\u003cString, String\u003e groupKeys = connectionIdContext.get(connectionId); if (groupKeys != null) { groupKeys.remove(groupKey); } } 源码位置: com.alibaba.nacos.config.server.remote.RpcConfigChangeNotifier#configDataChanged // 接受 LocalDataChangeEvent 事件，会执行这个方法 public void configDataChanged(String groupKey, String dataId, String group, String tenant, boolean isBeta, List\u003cString\u003e betaIps, String tag) { Set\u003cString\u003e listeners = configChangeListenContext.getListeners(groupKey); if (CollectionUtils.isEmpty(listeners)) { return; } int notifyClientCount = 0; for (final String client : listeners) { // 获取连接 Connection connection = connectionManager.getConnection(client); if (connection == null) { continue; } ConnectionMeta metaInfo = connection.getMetaInfo(); //beta ips check. String clientIp = metaInfo.getClientIp(); String clientTag = metaInfo.getTag(); // 判断 beat 配置 if (isBeta \u0026\u0026 betaIps != null \u0026\u0026 !betaIps.contains(clientIp)) { continue; } //tag check // 判断 tag 配置 if (StringUtils.isNotBlank(tag) \u0026\u0026 !tag.equals(clientTag)) { continue; } ConfigChangeNotifyRequest notifyRequest = ConfigChangeNotifyRequest.build(dataId, group, tenant); // 推送配置的 groupKey RpcPushTask rpcPushRetryTask = new RpcPushTask(notifyRequest, 50, client, clientIp, metaInfo.getAppName()); push(rpcPushRetryTask); notifyClientCount++; } Loggers.REMOTE_PUSH.info(\"push [{}] clients ,groupKey=[{}]\", notifyClientCount, groupKey); } ","date":"2023-09-14","objectID":"/ooooo-notes/19-%E8%AE%A2%E9%98%85%E9%85%8D%E7%BD%AE/:2:0","tags":["nacos","source code","源码分析 nacos 系列"],"title":"19 订阅配置","uri":"/ooooo-notes/19-%E8%AE%A2%E9%98%85%E9%85%8D%E7%BD%AE/"},{"categories":null,"content":"测试类 com.alibaba.nacos.test.config.ConfigLongPollReturnChanges_CITCase#testAdd ","date":"2023-09-14","objectID":"/ooooo-notes/19-%E8%AE%A2%E9%98%85%E9%85%8D%E7%BD%AE/:3:0","tags":["nacos","source code","源码分析 nacos 系列"],"title":"19 订阅配置","uri":"/ooooo-notes/19-%E8%AE%A2%E9%98%85%E9%85%8D%E7%BD%AE/"},{"categories":null,"content":" nacos 基于 2.2.4 版本 在 nacos 中，删除配置分为 http 和 grpc 两种方式，分别为 ConfigControllerV2#deleteConfig 和 ConfigRemoveRequestHandler。这两个方法的处理逻辑都是一样的，所以我就选择 http 的方式来分析代码。 ","date":"2023-09-13","objectID":"/ooooo-notes/18-%E5%88%A0%E9%99%A4%E9%85%8D%E7%BD%AE/:0:0","tags":["nacos","source code","源码分析 nacos 系列"],"title":"18 删除配置","uri":"/ooooo-notes/18-%E5%88%A0%E9%99%A4%E9%85%8D%E7%BD%AE/"},{"categories":null,"content":"删除配置 源码位置: com.alibaba.nacos.config.server.controller.v2.ConfigControllerV2#deleteConfig // 接受 http 请求 public Result\u003cBoolean\u003e deleteConfig(HttpServletRequest request, @RequestParam(\"dataId\") String dataId, @RequestParam(\"group\") String group, @RequestParam(value = \"namespaceId\", required = false, defaultValue = StringUtils.EMPTY) String namespaceId, @RequestParam(value = \"tag\", required = false) String tag) throws NacosException { //fix issue #9783 namespaceId = NamespaceUtil.processNamespaceParameter(namespaceId); // check namespaceId // 检查参数 ParamUtils.checkTenantV2(namespaceId); ParamUtils.checkParam(dataId, group, \"datumId\", \"rm\"); ParamUtils.checkParamV2(tag); String clientIp = RequestUtil.getRemoteIp(request); String srcUser = RequestUtil.getSrcUserName(request); // 删除配置 return Result.success(configOperationService.deleteConfig(dataId, group, namespaceId, tag, clientIp, srcUser)); } 源码位置: com.alibaba.nacos.config.server.service.ConfigOperationService#deleteConfig // 删除配置 public Boolean deleteConfig(String dataId, String group, String namespaceId, String tag, String clientIp, String srcUser) { // 这个方法中没有删除 beta 配置，是因为有单独的接口来操作，接口为 ConfigController#stopBeta if (StringUtils.isBlank(tag)) { // 删除 configInfo configInfoPersistService.removeConfigInfo(dataId, group, namespaceId, clientIp, srcUser); } else { // 删除 configInfoTag configInfoTagPersistService.removeConfigInfoTag(dataId, group, namespaceId, tag, clientIp, srcUser); } final Timestamp time = TimeUtils.getCurrentTime(); // 记录日志 ConfigTraceService.logPersistenceEvent(dataId, group, namespaceId, null, time.getTime(), clientIp, ConfigTraceService.PERSISTENCE_EVENT_REMOVE, null); // 发布 ConfigDataChangeEvent 事件，这个事件在发布配置章节中分析过了 ConfigChangePublisher .notifyConfigChange(new ConfigDataChangeEvent(false, dataId, group, namespaceId, tag, time.getTime())); return true; } ","date":"2023-09-13","objectID":"/ooooo-notes/18-%E5%88%A0%E9%99%A4%E9%85%8D%E7%BD%AE/:1:0","tags":["nacos","source code","源码分析 nacos 系列"],"title":"18 删除配置","uri":"/ooooo-notes/18-%E5%88%A0%E9%99%A4%E9%85%8D%E7%BD%AE/"},{"categories":null,"content":"测试类 com.alibaba.nacos.test.config.ConfigAPI_V2_CITCase#test ","date":"2023-09-13","objectID":"/ooooo-notes/18-%E5%88%A0%E9%99%A4%E9%85%8D%E7%BD%AE/:2:0","tags":["nacos","source code","源码分析 nacos 系列"],"title":"18 删除配置","uri":"/ooooo-notes/18-%E5%88%A0%E9%99%A4%E9%85%8D%E7%BD%AE/"},{"categories":null,"content":"build.gradle 文件 plugins { id(\"com.linecorp.thrift-gradle-plugin\") version \"0.5.0\" } dependencies { api('org.apache.thrift:libthrift:0.19.0') api('org.springframework.boot:spring-boot-starter-logging') api('cn.hutool:hutool-all') testImplementation('org.springframework.boot:spring-boot-starter-test') } ","date":"2023-09-13","objectID":"/ooooo-notes/%E7%AE%80%E5%8D%95%E5%B0%81%E8%A3%85-thrift-%E5%8D%8F%E8%AE%AE/:1:0","tags":["thrift"],"title":"简单封装 thrift 协议","uri":"/ooooo-notes/%E7%AE%80%E5%8D%95%E5%B0%81%E8%A3%85-thrift-%E5%8D%8F%E8%AE%AE/"},{"categories":null,"content":"example.thrift 文件 文件放在路径: src/main/thrift, 运行 gradle 命令 compileThrift /** * The first thing to know about are types. The available types in Thrift are: * * bool Boolean, one byte * i8 (byte) Signed 8-bit integer * i16 Signed 16-bit integer * i32 Signed 32-bit integer * i64 Signed 64-bit integer * double 64-bit floating point value * string String * binary Blob (byte array) * map\u003ct1,t2\u003e Map from one type to another * list\u003ct1\u003e Ordered list of one type * set\u003ct1\u003e Set of unique elements of one type * * Did you also notice that Thrift supports C style comments? */ /** * Thrift files can namespace, package, or prefix their output in various * target languages. */ namespace cl example namespace cpp example namespace d example namespace dart example namespace java example namespace php example namespace perl example namespace haxe example namespace netstd example service Calculator { i32 add(1:i32 num1, 2:i32 num2), } ","date":"2023-09-13","objectID":"/ooooo-notes/%E7%AE%80%E5%8D%95%E5%B0%81%E8%A3%85-thrift-%E5%8D%8F%E8%AE%AE/:2:0","tags":["thrift"],"title":"简单封装 thrift 协议","uri":"/ooooo-notes/%E7%AE%80%E5%8D%95%E5%B0%81%E8%A3%85-thrift-%E5%8D%8F%E8%AE%AE/"},{"categories":null,"content":"thrift server 代码 @Slf4j public class ThriftServer implements Closeable { private static final String CLASS_NAME_SUFFIX_IFACE = \"$Iface\"; private static final String CLASS_NAME_SUFFIX_PROCESSOR = \"$Processor\"; private final TMultiplexedProcessor multiplexedProcessor = new TMultiplexedProcessor(); private final AtomicBoolean started = new AtomicBoolean(false); private int port; private TNonblockingServerSocket serverSocket; public ThriftServer(int port) { this.port = port; } public void startServer() { if (!started.compareAndSet(false, true)) { return; } try { serverSocket = new TNonblockingServerSocket(port); } catch (TTransportException e) { log.error(\"start thrift server\", e); throw new RuntimeException(e); } TThreadedSelectorServer.Args args = new TThreadedSelectorServer.Args(serverSocket).processor(multiplexedProcessor); TServer server = new TThreadedSelectorServer(args); new Thread(() -\u003e { log.info(\"start thrift server on port {}\",port); server.serve(); log.info(\"stop thrift server on port {}\", port); }, \"thrift-server\").start(); } public void addService(Object service) { Class\u003c?\u003e interfaceClass = findInterfaceClass(service); addProcessor(interfaceClass, service); } private synchronized void addProcessor(Class\u003c?\u003e interfaceClass, Object service) { String processorClassName = interfaceClass.getName().replace(CLASS_NAME_SUFFIX_IFACE, CLASS_NAME_SUFFIX_PROCESSOR); Class\u003c?\u003e processorClass; try { processorClass = Class.forName(processorClassName, true, interfaceClass.getClassLoader()); } catch (ClassNotFoundException e) { throw new RuntimeException(e); } TProcessor processor = (TProcessor) ReflectUtil.newInstance(processorClass, service); log.info(\"add thrift interface {}\", interfaceClass); multiplexedProcessor.registerProcessor(interfaceClass.getName(), processor); } private static Class\u003c?\u003e findInterfaceClass(Object service) { Assert.notNull(service); Class\u003c?\u003e clazz = service.getClass(); Class\u003c?\u003e interfaceClazz = null; for (Class\u003c?\u003e c : clazz.getInterfaces()) { if (c.getName().contains(CLASS_NAME_SUFFIX_IFACE)) { interfaceClazz = c; break; } } if (interfaceClazz == null) { throw new IllegalArgumentException(\"service is not thrift implement object\"); } return interfaceClazz; } @Override public void close() throws IOException { if (serverSocket != null) { serverSocket.close(); } } } ","date":"2023-09-13","objectID":"/ooooo-notes/%E7%AE%80%E5%8D%95%E5%B0%81%E8%A3%85-thrift-%E5%8D%8F%E8%AE%AE/:3:0","tags":["thrift"],"title":"简单封装 thrift 协议","uri":"/ooooo-notes/%E7%AE%80%E5%8D%95%E5%B0%81%E8%A3%85-thrift-%E5%8D%8F%E8%AE%AE/"},{"categories":null,"content":"thrift client 代码 @Slf4j public class ThriftClient implements Closeable { private static final String CLASS_NAME_SUFFIX_IFACE = \"$Iface\"; private static final String CLASS_NAME_SUFFIX_CLIENT = \"$Client\"; private TTransport transport; private TProtocol protocol; private final AtomicBoolean started = new AtomicBoolean(false); private String host; private int port; public ThriftClient(String host, int port) { this.host = host; this.port = port; } public void startClient() { if (!started.compareAndSet(false, true)) { return; } try { transport = new TSocket(host, port); transport.open(); TTransport framedTransport = new TFramedTransport(transport, Integer.MAX_VALUE); protocol = new TBinaryProtocol(framedTransport); } catch (TTransportException e) { log.info(\"connect thrift error\", e); throw new RuntimeException(e); } } public \u003cT extends TServiceClient\u003e T getClient(String interfaceClassName) { log.info(\"add thrift interface {}\", interfaceClassName); String clientClassName = interfaceClassName.replace(CLASS_NAME_SUFFIX_IFACE, CLASS_NAME_SUFFIX_CLIENT); Class\u003cT\u003e clientClass = ClassUtil.loadClass(clientClassName); TMultiplexedProtocol multiplexedProtocol = new TMultiplexedProtocol(protocol, interfaceClassName); return ReflectUtil.newInstance(clientClass, multiplexedProtocol); } public void close() { if (transport != null) { transport.close(); } } } ","date":"2023-09-13","objectID":"/ooooo-notes/%E7%AE%80%E5%8D%95%E5%B0%81%E8%A3%85-thrift-%E5%8D%8F%E8%AE%AE/:4:0","tags":["thrift"],"title":"简单封装 thrift 协议","uri":"/ooooo-notes/%E7%AE%80%E5%8D%95%E5%B0%81%E8%A3%85-thrift-%E5%8D%8F%E8%AE%AE/"},{"categories":null,"content":"测试类相关代码 实现类: CalculatorHandler public class CalculatorHandler implements Calculator.Iface { @Override public int add(int num1, int num2) throws TException { System.out.println(\"num1: \" + num1 + \", num2: \" + num2); return num1 + num2; } } 测试类: MultiThriftExampleTest public class MultiThriftExampleTest { @Test void test1() throws Exception { @Cleanup ThriftServer thriftServer = new ThriftServer(9090); thriftServer.addService(new CalculatorHandler()); thriftServer.startServer(); @Cleanup ThriftClient thriftClient = new ThriftClient(\"localhost\", 9090); thriftClient.startClient(); Calculator.Client client = thriftClient.getClient(Calculator.Iface.class.getName()); int sum = client.add(1, 2); assertEquals(3, sum); } @Test void test2() throws TException { startServer(); startClient(); } private void startClient() throws TException { try { TimeUnit.SECONDS.sleep(3); } catch (InterruptedException e) { throw new RuntimeException(e); } TTransport transport = new TSocket(\"localhost\", 9090); transport.open(); TTransport framedTransport = new TFramedTransport(transport, Integer.MAX_VALUE); TProtocol protocol = new TBinaryProtocol(framedTransport); TMultiplexedProtocol multiplexedProtocol = new TMultiplexedProtocol(protocol, Calculator.Iface.class.getName()); Calculator.Client client = new Calculator.Client(multiplexedProtocol); int add = client.add(1, 2); System.out.println(\"add: \" + add); transport.close(); } private void startServer() { CalculatorHandler handler = new CalculatorHandler(); Calculator.Processor\u003cCalculatorHandler\u003e processor = new Calculator.Processor\u003c\u003e(handler); TNonblockingServerSocket serverSocket; try { serverSocket = new TNonblockingServerSocket(9090); } catch (TTransportException e) { throw new RuntimeException(e); } TMultiplexedProcessor multiplexedProcessor = new TMultiplexedProcessor(); multiplexedProcessor.registerProcessor(Calculator.Iface.class.getName(), processor); TServer server = new TThreadedSelectorServer(new TThreadedSelectorServer.Args(serverSocket).processor(multiplexedProcessor)); new Thread(() -\u003e { System.out.println(\"Starting the simple server...\"); server.serve(); }).start(); } } ","date":"2023-09-13","objectID":"/ooooo-notes/%E7%AE%80%E5%8D%95%E5%B0%81%E8%A3%85-thrift-%E5%8D%8F%E8%AE%AE/:5:0","tags":["thrift"],"title":"简单封装 thrift 协议","uri":"/ooooo-notes/%E7%AE%80%E5%8D%95%E5%B0%81%E8%A3%85-thrift-%E5%8D%8F%E8%AE%AE/"},{"categories":null,"content":" nacos 基于 2.2.4 版本 在 nacos 中，获取配置分为 http 和 grpc 两种方式，分别为 ConfigControllerV2#getConfig 和 ConfigQueryRequestHandler。这两个方法的处理逻辑都是一样的，所以我就选择 http 的方式来分析代码。 ","date":"2023-09-12","objectID":"/ooooo-notes/17-%E8%8E%B7%E5%8F%96%E9%85%8D%E7%BD%AE/:0:0","tags":["nacos","source code","源码分析 nacos 系列"],"title":"17 获取配置","uri":"/ooooo-notes/17-%E8%8E%B7%E5%8F%96%E9%85%8D%E7%BD%AE/"},{"categories":null,"content":"获取配置 源码位置: com.alibaba.nacos.config.server.controller.v2.ConfigControllerV2#getConfig // 接受 http 请求 public void getConfig(HttpServletRequest request, HttpServletResponse response, @RequestParam(\"dataId\") String dataId, @RequestParam(\"group\") String group, @RequestParam(value = \"namespaceId\", required = false, defaultValue = StringUtils.EMPTY) String namespaceId, @RequestParam(value = \"tag\", required = false) String tag) throws NacosException, IOException, ServletException { // check namespaceId // 检查参数 ParamUtils.checkTenantV2(namespaceId); namespaceId = NamespaceUtil.processNamespaceParameter(namespaceId); // check params ParamUtils.checkParam(dataId, group, \"datumId\", \"content\"); ParamUtils.checkParamV2(tag); final String clientIp = RequestUtil.getRemoteIp(request); String isNotify = request.getHeader(\"notify\"); // 获取配置 inner.doGetConfig(request, response, dataId, group, namespaceId, tag, isNotify, clientIp, true); } 源码位置: com.alibaba.nacos.config.server.controller.ConfigServletInner#doGetConfig // 获取配置 public String doGetConfig(HttpServletRequest request, HttpServletResponse response, String dataId, String group, String tenant, String tag, String isNotify, String clientIp, boolean isV2) throws IOException, ServletException { ... final String groupKey = GroupKey2.getKey(dataId, group, tenant); String autoTag = request.getHeader(\"Vipserver-Tag\"); String requestIpApp = RequestUtil.getAppName(request); // 获取读锁 int lockResult = tryConfigReadLock(groupKey); final String requestIp = RequestUtil.getRemoteIp(request); boolean isBeta = false; boolean isSli = false; // 获取锁成功 if (lockResult \u003e 0) { // LockResult \u003e 0 means cacheItem is not null and other thread can`t delete this cacheItem FileInputStream fis = null; try { String md5 = Constants.NULL; long lastModified = 0L; CacheItem cacheItem = ConfigCacheService.getContentCache(groupKey); // 判断配置是否为 beta if (cacheItem.isBeta() \u0026\u0026 cacheItem.getIps4Beta().contains(clientIp)) { isBeta = true; } ... File file = null; ConfigInfoBase configInfoBase = null; PrintWriter out; // 下面的逻辑分为 beta 和 tag，分别读取不同的表或者文件 if (isBeta) { md5 = cacheItem.getMd54Beta(); lastModified = cacheItem.getLastModifiedTs4Beta(); if (PropertyUtil.isDirectRead()) { configInfoBase = configInfoBetaPersistService.findConfigInfo4Beta(dataId, group, tenant); } else { file = DiskUtil.targetBetaFile(dataId, group, tenant); } response.setHeader(\"isBeta\", \"true\"); } else { if (StringUtils.isBlank(tag)) { if (isUseTag(cacheItem, autoTag)) { if (cacheItem.tagMd5 != null) { md5 = cacheItem.tagMd5.get(autoTag); } if (cacheItem.tagLastModifiedTs != null) { lastModified = cacheItem.tagLastModifiedTs.get(autoTag); } // 从 configInfoTag 中读取 if (PropertyUtil.isDirectRead()) { configInfoBase = configInfoTagPersistService.findConfigInfo4Tag(dataId, group, tenant, autoTag); } else { file = DiskUtil.targetTagFile(dataId, group, tenant, autoTag); } response.setHeader(com.alibaba.nacos.api.common.Constants.VIPSERVER_TAG, URLEncoder.encode(autoTag, StandardCharsets.UTF_8.displayName())); } else { md5 = cacheItem.getMd5(); lastModified = cacheItem.getLastModifiedTs(); // 从 configInfo 中读取 if (PropertyUtil.isDirectRead()) { configInfoBase = configInfoPersistService.findConfigInfo(dataId, group, tenant); } else { file = DiskUtil.targetFile(dataId, group, tenant); } if (configInfoBase == null \u0026\u0026 fileNotExist(file)) { // FIXME CacheItem // No longer exists. It is impossible to simply calculate the push delayed. Here, simply record it as - 1. ConfigTraceService.logPullEvent(dataId, group, tenant, requestIpApp, -1, ConfigTraceService.PULL_EVENT_NOTFOUND, -1, requestIp, notify); // pullLog.info(\"[client-get] clientIp={}, {}, // no data\", // new Object[]{clientIp, groupKey}); // 没有配置，返回404 return get404Result(response, isV2); } isSli = true; } } else { if (cacheItem.tagMd5 != null) { md5 = cacheItem.tagMd5.get(tag); } if (cacheItem.tagLastModifiedTs != null) { Long lm = cacheItem.tagLastModifiedTs.get(tag); if (lm != null) { lastModified = lm; } } // 从 con","date":"2023-09-12","objectID":"/ooooo-notes/17-%E8%8E%B7%E5%8F%96%E9%85%8D%E7%BD%AE/:1:0","tags":["nacos","source code","源码分析 nacos 系列"],"title":"17 获取配置","uri":"/ooooo-notes/17-%E8%8E%B7%E5%8F%96%E9%85%8D%E7%BD%AE/"},{"categories":null,"content":"测试类 com.alibaba.nacos.test.config.ConfigAPI_V2_CITCase#test ","date":"2023-09-12","objectID":"/ooooo-notes/17-%E8%8E%B7%E5%8F%96%E9%85%8D%E7%BD%AE/:2:0","tags":["nacos","source code","源码分析 nacos 系列"],"title":"17 获取配置","uri":"/ooooo-notes/17-%E8%8E%B7%E5%8F%96%E9%85%8D%E7%BD%AE/"},{"categories":null,"content":" nacos 基于 2.2.4 版本 在 nacos 中，发布配置分为 http 和 grpc 两种方式，分别为 ConfigControllerV2#publishConfig 和 ConfigPublishRequestHandler。这两个方法的处理逻辑都是一样的，所以我就选择 http 的方式来分析代码。 ","date":"2023-09-11","objectID":"/ooooo-notes/16-%E5%8F%91%E5%B8%83%E9%85%8D%E7%BD%AE/:0:0","tags":["nacos","source code","源码分析 nacos 系列"],"title":"16 发布配置","uri":"/ooooo-notes/16-%E5%8F%91%E5%B8%83%E9%85%8D%E7%BD%AE/"},{"categories":null,"content":"发布配置 源码位置: com.alibaba.nacos.config.server.controller.v2.ConfigControllerV2#publishConfig // 接受 http 请求，发布配置 public Result\u003cBoolean\u003e publishConfig(ConfigForm configForm, HttpServletRequest request) throws NacosException { // check required field configForm.validate(); // encrypted Pair\u003cString, String\u003e pair = EncryptionHandler.encryptHandler(configForm.getDataId(), configForm.getContent()); configForm.setContent(pair.getSecond()); //fix issue #9783 configForm.setNamespaceId(NamespaceUtil.processNamespaceParameter(configForm.getNamespaceId())); // check param ParamUtils.checkTenantV2(configForm.getNamespaceId()); ParamUtils.checkParam(configForm.getDataId(), configForm.getGroup(), \"datumId\", configForm.getContent()); ParamUtils.checkParamV2(configForm.getTag()); if (StringUtils.isBlank(configForm.getSrcUser())) { configForm.setSrcUser(RequestUtil.getSrcUserName(request)); } if (!ConfigType.isValidType(configForm.getType())) { configForm.setType(ConfigType.getDefaultType().getType()); } ConfigRequestInfo configRequestInfo = new ConfigRequestInfo(); configRequestInfo.setSrcIp(RequestUtil.getRemoteIp(request)); configRequestInfo.setRequestIpApp(RequestUtil.getAppName(request)); configRequestInfo.setBetaIps(request.getHeader(\"betaIps\")); String encryptedDataKey = pair.getFirst(); // configOperationService 发布配置 return Result.success(configOperationService.publishConfig(configForm, configRequestInfo, encryptedDataKey)); } 源码位置: com.alibaba.nacos.config.server.service.ConfigOperationService#publishConfig // configOperationService 发布配置 public Boolean publishConfig(ConfigForm configForm, ConfigRequestInfo configRequestInfo, String encryptedDataKey) throws NacosException { // 组装参数 Map\u003cString, Object\u003e configAdvanceInfo = getConfigAdvanceInfo(configForm); // 检查参数 ParamUtils.checkParam(configAdvanceInfo); // dataId 黑名单校验 if (AggrWhitelist.isAggrDataId(configForm.getDataId())) { LOGGER.warn(\"[aggr-conflict] {} attempt to publish single data, {}, {}\", configRequestInfo.getSrcIp(), configForm.getDataId(), configForm.getGroup()); throw new NacosApiException(HttpStatus.FORBIDDEN.value(), ErrorCode.INVALID_DATA_ID, \"dataId:\" + configForm.getDataId() + \" is aggr\"); } final Timestamp time = TimeUtils.getCurrentTime(); // 封装为 configInfo 对象 ConfigInfo configInfo = new ConfigInfo(configForm.getDataId(), configForm.getGroup(), configForm.getNamespaceId(), configForm.getAppName(), configForm.getContent()); configInfo.setType(configForm.getType()); configInfo.setEncryptedDataKey(encryptedDataKey); if (StringUtils.isBlank(configRequestInfo.getBetaIps())) { if (StringUtils.isBlank(configForm.getTag())) { // 没有 beta 和 tag，插入或者更新 config_info 表，这个也是最常用的 configInfoPersistService.insertOrUpdate(configRequestInfo.getSrcIp(), configForm.getSrcUser(), configInfo, time, configAdvanceInfo, false); // 发布 ConfigDataChangeEvent 事件 ConfigChangePublisher.notifyConfigChange( new ConfigDataChangeEvent(false, configForm.getDataId(), configForm.getGroup(), configForm.getNamespaceId(), time.getTime())); } else { // 有 tag，插入或者更新 config_info_tag 表，注意控制台没有用到这个 configInfoTagPersistService.insertOrUpdateTag(configInfo, configForm.getTag(), configRequestInfo.getSrcIp(), configForm.getSrcUser(), time, false); // 发布 ConfigDataChangeEvent 事件 ConfigChangePublisher.notifyConfigChange( new ConfigDataChangeEvent(false, configForm.getDataId(), configForm.getGroup(), configForm.getNamespaceId(), configForm.getTag(), time.getTime())); } } else { // beta publish // 有 beta, 插入或者更新 config_info_beta 表 configInfoBetaPersistService.insertOrUpdateBeta(configInfo, configRequestInfo.getBetaIps(), configRequestInfo.getSrcIp(), configForm.getSrcUser(), time, false); // 发布 ConfigDataChangeEvent 事件 ConfigChangePublisher.notifyConfigChange( new ConfigDataChangeEvent(true, configForm.getDataId(), configForm.getGroup(), configForm.getNamespaceId(), time.getTime())); } // 记录日志 ConfigTraceService.logPersistenceEvent(configForm.getDataId(), configForm.getGroup(), configForm.getNamespaceId(), configRequestInfo.ge","date":"2023-09-11","objectID":"/ooooo-notes/16-%E5%8F%91%E5%B8%83%E9%85%8D%E7%BD%AE/:1:0","tags":["nacos","source code","源码分析 nacos 系列"],"title":"16 发布配置","uri":"/ooooo-notes/16-%E5%8F%91%E5%B8%83%E9%85%8D%E7%BD%AE/"},{"categories":null,"content":"处理 ConfigDataChangeEvent 事件 源码位置: com.alibaba.nacos.config.server.service.notify.AsyncNotifyService#AsyncNotifyService // 处理 ConfigDataChangeEvent 事件 @Autowired public AsyncNotifyService(ServerMemberManager memberManager) { this.memberManager = memberManager; // Register ConfigDataChangeEvent to NotifyCenter. // 分配事件的缓冲大小 NotifyCenter.registerToPublisher(ConfigDataChangeEvent.class, NotifyCenter.ringBufferSize); // Register A Subscriber to subscribe ConfigDataChangeEvent. // 注册 ConfigDataChangeEvent 的订阅者 NotifyCenter.registerSubscriber(new Subscriber() { @Override public void onEvent(Event event) { // Generate ConfigDataChangeEvent concurrently if (event instanceof ConfigDataChangeEvent) { ConfigDataChangeEvent evt = (ConfigDataChangeEvent) event; long dumpTs = evt.lastModifiedTs; String dataId = evt.dataId; String group = evt.group; String tenant = evt.tenant; String tag = evt.tag; MetricsMonitor.incrementConfigChangeCount(tenant, group, dataId); Collection\u003cMember\u003e ipList = memberManager.allMembers(); // In fact, any type of queue here can be Queue\u003cNotifySingleRpcTask\u003e rpcQueue = new LinkedList\u003c\u003e(); // 遍历集群成员列表, 添加到队列中 // A 服务配置发生改变之后，必须推送给 B 服务，因为 B 服务可能有订阅者来监听这个配置。 for (Member member : ipList) { // grpc report data change only rpcQueue.add( new NotifySingleRpcTask(dataId, group, tenant, tag, dumpTs, evt.isBeta, member)); } if (!rpcQueue.isEmpty()) { // 线程池调度 AsyncRpcTask ConfigExecutor.executeAsyncNotify(new AsyncRpcTask(rpcQueue)); } } } @Override public Class\u003c? extends Event\u003e subscribeType() { return ConfigDataChangeEvent.class; } }); } 源码位置: com.alibaba.nacos.config.server.service.notify.AsyncNotifyService.AsyncRpcTask // 线程池调度 AsyncRpcTask class AsyncRpcTask implements Runnable { ... @Override public void run() { while (!queue.isEmpty()) { // 从队列中取出 task NotifySingleRpcTask task = queue.poll(); ConfigChangeClusterSyncRequest syncRequest = new ConfigChangeClusterSyncRequest(); syncRequest.setDataId(task.getDataId()); syncRequest.setGroup(task.getGroup()); syncRequest.setBeta(task.isBeta); syncRequest.setLastModified(task.getLastModified()); syncRequest.setTag(task.tag); syncRequest.setTenant(task.getTenant()); Member member = task.member; // 是当前服务 if (memberManager.getSelf().equals(member)) { // 处理逻辑分为是不是 beta，调用的方法都是 dumpService#dump if (syncRequest.isBeta()) { dumpService.dump(syncRequest.getDataId(), syncRequest.getGroup(), syncRequest.getTenant(), syncRequest.getLastModified(), NetUtils.localIP(), true); } else { dumpService.dump(syncRequest.getDataId(), syncRequest.getGroup(), syncRequest.getTenant(), syncRequest.getTag(), syncRequest.getLastModified(), NetUtils.localIP()); } continue; } // 地址是有效的 if (memberManager.hasMember(member.getAddress())) { // start the health check and there are ips that are not monitored, put them directly in the notification queue, otherwise notify boolean unHealthNeedDelay = memberManager.isUnHealth(member.getAddress()); if (unHealthNeedDelay) { // target ip is unhealthy, then put it in the notification list ConfigTraceService.logNotifyEvent(task.getDataId(), task.getGroup(), task.getTenant(), null, task.getLastModified(), InetUtils.getSelfIP(), ConfigTraceService.NOTIFY_EVENT_UNHEALTH, 0, member.getAddress()); // get delay time and set fail count to the task // 如果地址是不健康的，延时来执行 task asyncTaskExecute(task); } else { // grpc report data change only try { // 地址是健康的，用 grpc 来发送 ConfigChangeClusterSyncRequest 请求， // 会被 ConfigChangeClusterSyncRequestHandler 来处理 configClusterRpcClientProxy .syncConfigChange(member, syncRequest, new AsyncRpcNotifyCallBack(task)); } catch (Exception e) { MetricsMonitor.getConfigNotifyException().increment(); // 发送异常，延时来重试 asyncTaskExecute(task); } } } else { //No nothig if member has offline. } } } } 源码位置: com.alibaba.nacos.config.server.remote.ConfigChangeClusterSyncRequestHandler#handle // 处理 ConfigChangeClusterSyncRequest 请求 @Override public ConfigChangeClusterSyncResponse handle(ConfigChangeClusterSyncRequest configChangeSyncRequest, RequestMeta meta) throws N","date":"2023-09-11","objectID":"/ooooo-notes/16-%E5%8F%91%E5%B8%83%E9%85%8D%E7%BD%AE/:2:0","tags":["nacos","source code","源码分析 nacos 系列"],"title":"16 发布配置","uri":"/ooooo-notes/16-%E5%8F%91%E5%B8%83%E9%85%8D%E7%BD%AE/"},{"categories":null,"content":"dumpService#dump 方法 源码位置: com.alibaba.nacos.config.server.service.dump.DumpService#dump // dumpService#dump 方法 public void dump(String dataId, String group, String tenant, long lastModified, String handleIp, boolean isBeta) { String groupKey = GroupKey2.getKey(dataId, group, tenant); String taskKey = String.join(\"+\", dataId, group, tenant, String.valueOf(isBeta)); // 添加了 DumpTask，会被 DumpProcessor 来处理 dumpTaskMgr.addTask(taskKey, new DumpTask(groupKey, lastModified, handleIp, isBeta)); DUMP_LOG.info(\"[dump-task] add task. groupKey={}, taskKey={}\", groupKey, taskKey); } 源码位置: com.alibaba.nacos.config.server.service.dump.processor.DumpProcessor#process // 处理 DumpTask @Override public boolean process(NacosTask task) { DumpTask dumpTask = (DumpTask) task; String[] pair = GroupKey2.parseKey(dumpTask.getGroupKey()); String dataId = pair[0]; String group = pair[1]; String tenant = pair[2]; long lastModified = dumpTask.getLastModified(); String handleIp = dumpTask.getHandleIp(); boolean isBeta = dumpTask.isBeta(); String tag = dumpTask.getTag(); ConfigDumpEvent.ConfigDumpEventBuilder build = ConfigDumpEvent.builder().namespaceId(tenant).dataId(dataId) .group(group).isBeta(isBeta).tag(tag).lastModifiedTs(lastModified).handleIp(handleIp); // 下面的逻辑又分为 beta, tag if (isBeta) { // if publish beta, then dump config, update beta cache ConfigInfo4Beta cf = configInfoBetaPersistService.findConfigInfo4Beta(dataId, group, tenant); // cf 为 null，说明之前删除了 build.remove(Objects.isNull(cf)); build.betaIps(Objects.isNull(cf) ? null : cf.getBetaIps()); build.content(Objects.isNull(cf) ? null : cf.getContent()); build.encryptedDataKey(Objects.isNull(cf) ? null : cf.getEncryptedDataKey()); return DumpConfigHandler.configDump(build.build()); } if (StringUtils.isBlank(tag)) { ConfigInfo cf = configInfoPersistService.findConfigInfo(dataId, group, tenant); build.remove(Objects.isNull(cf)); build.content(Objects.isNull(cf) ? null : cf.getContent()); build.type(Objects.isNull(cf) ? null : cf.getType()); build.encryptedDataKey(Objects.isNull(cf) ? null : cf.getEncryptedDataKey()); } else { ConfigInfo4Tag cf = configInfoTagPersistService.findConfigInfo4Tag(dataId, group, tenant, tag); build.remove(Objects.isNull(cf)); build.content(Objects.isNull(cf) ? null : cf.getContent()); } // 调用 DumpConfigHandler#configDump 方法 return DumpConfigHandler.configDump(build.build()); } 源码位置: com.alibaba.nacos.config.server.service.dump.DumpConfigHandler#configDump // DumpConfigHandler#configDump 方法 public static boolean configDump(ConfigDumpEvent event) { ... // 处理逻辑又分为 beat， tag if (event.isBeta()) { boolean result; if (event.isRemove()) { // 删除配置 result = ConfigCacheService.removeBeta(dataId, group, namespaceId); if (result) { // 记录日志 ConfigTraceService.logDumpEvent(dataId, group, namespaceId, null, lastModified, event.getHandleIp(), ConfigTraceService.DUMP_EVENT_REMOVE_OK, System.currentTimeMillis() - lastModified, 0); } return result; } else { // 更新配置 result = ConfigCacheService .dumpBeta(dataId, group, namespaceId, content, lastModified, event.getBetaIps(), encryptedDataKey); if (result) { // 记录日志 ConfigTraceService.logDumpEvent(dataId, group, namespaceId, null, lastModified, event.getHandleIp(), ConfigTraceService.DUMP_EVENT_OK, System.currentTimeMillis() - lastModified, content.length()); } } return result; } if (StringUtils.isBlank(event.getTag())) { // dataId 黑名单加载 if (dataId.equals(AggrWhitelist.AGGRIDS_METADATA)) { AggrWhitelist.load(content); } // clientIp 白名单 if (dataId.equals(ClientIpWhiteList.CLIENT_IP_WHITELIST_METADATA)) { ClientIpWhiteList.load(content); } // switchMeta 元数据 if (dataId.equals(SwitchService.SWITCH_META_DATAID)) { SwitchService.load(content); } boolean result; if (!event.isRemove()) { // 更新配置 result = ConfigCacheService .dump(dataId, group, namespaceId, content, lastModified, type, encryptedDataKey); ... } else { // 删除配置 result = ConfigCacheService.remove(dataId, group, namespaceId); ... } return result; } else { // boolean result; if (!event.isRemove","date":"2023-09-11","objectID":"/ooooo-notes/16-%E5%8F%91%E5%B8%83%E9%85%8D%E7%BD%AE/:3:0","tags":["nacos","source code","源码分析 nacos 系列"],"title":"16 发布配置","uri":"/ooooo-notes/16-%E5%8F%91%E5%B8%83%E9%85%8D%E7%BD%AE/"},{"categories":null,"content":"处理 LocalDataChangeEvent 事件 源码位置: `` // 处理 LocalDataChangeEvent 事件 @Override public void onEvent(LocalDataChangeEvent event) { ... // 处理事件 configDataChanged(groupKey, dataId, group, tenant, isBeta, betaIps, tag); } // 处理事件 public void configDataChanged(String groupKey, String dataId, String group, String tenant, boolean isBeta, List\u003cString\u003e betaIps, String tag) { Set\u003cString\u003e listeners = configChangeListenContext.getListeners(groupKey); if (CollectionUtils.isEmpty(listeners)) { return; } int notifyClientCount = 0; // 遍历所有订阅者 for (final String client : listeners) { // 获取 Connection Connection connection = connectionManager.getConnection(client); if (connection == null) { continue; } ConnectionMeta metaInfo = connection.getMetaInfo(); //beta ips check. String clientIp = metaInfo.getClientIp(); String clientTag = metaInfo.getTag(); // 判断是否要 beta 推送 if (isBeta \u0026\u0026 betaIps != null \u0026\u0026 !betaIps.contains(clientIp)) { continue; } //tag check // 判断是否要 tag 推送 if (StringUtils.isNotBlank(tag) \u0026\u0026 !tag.equals(clientTag)) { continue; } ConfigChangeNotifyRequest notifyRequest = ConfigChangeNotifyRequest.build(dataId, group, tenant); // 封装为 RpcPushTask，错误次数为 50 RpcPushTask rpcPushRetryTask = new RpcPushTask(notifyRequest, 50, client, clientIp, metaInfo.getAppName()); // 异步推送配置数据 push(rpcPushRetryTask); notifyClientCount++; } Loggers.REMOTE_PUSH.info(\"push [{}] clients ,groupKey=[{}]\", notifyClientCount, groupKey); } // 异步推送配置数据 private void push(RpcPushTask retryTask) { ConfigChangeNotifyRequest notifyRequest = retryTask.notifyRequest; if (retryTask.isOverTimes()) { // 错误次数达到上限 Loggers.REMOTE_PUSH .warn(\"push callback retry fail over times .dataId={},group={},tenant={},clientId={},will unregister client.\", notifyRequest.getDataId(), notifyRequest.getGroup(), notifyRequest.getTenant(), retryTask.connectionId); // 注销连接 connectionManager.unregister(retryTask.connectionId); } else if (connectionManager.getConnection(retryTask.connectionId) != null) { // first time:delay 0s; second time:delay 2s; third time:delay 4s // 线程池延迟调度 ConfigExecutor.getClientConfigNotifierServiceExecutor() .schedule(retryTask, retryTask.tryTimes * 2, TimeUnit.SECONDS); } else { // client is already offline, ignore task. } } 源码位置: com.alibaba.nacos.config.server.remote.RpcConfigChangeNotifier.RpcPushTask#run // RpcPushTask 执行 @Override public void run() { tryTimes++; TpsCheckRequest tpsCheckRequest = new TpsCheckRequest(); tpsCheckRequest.setPointName(POINT_CONFIG_PUSH); // 检查 tps，默认没有实现 if (!tpsControlManager.check(tpsCheckRequest).isSuccess()) { push(this); } else { // 发送 notifyRequest 请求给客户端 rpcPushService.pushWithCallback(connectionId, notifyRequest, new AbstractPushCallBack(3000L) { @Override public void onSuccess() { TpsCheckRequest tpsCheckRequest = new TpsCheckRequest(); tpsCheckRequest.setPointName(POINT_CONFIG_PUSH_SUCCESS); tpsControlManager.check(tpsCheckRequest); } @Override public void onFail(Throwable e) { TpsCheckRequest tpsCheckRequest = new TpsCheckRequest(); tpsCheckRequest.setPointName(POINT_CONFIG_PUSH_FAIL); tpsControlManager.check(tpsCheckRequest); Loggers.REMOTE_PUSH.warn(\"Push fail\", e); push(RpcPushTask.this); } }, ConfigExecutor.getClientConfigNotifierServiceExecutor()); } } ","date":"2023-09-11","objectID":"/ooooo-notes/16-%E5%8F%91%E5%B8%83%E9%85%8D%E7%BD%AE/:4:0","tags":["nacos","source code","源码分析 nacos 系列"],"title":"16 发布配置","uri":"/ooooo-notes/16-%E5%8F%91%E5%B8%83%E9%85%8D%E7%BD%AE/"},{"categories":null,"content":"测试类 com.alibaba.nacos.test.config.ConfigAPI_V2_CITCase#test ","date":"2023-09-11","objectID":"/ooooo-notes/16-%E5%8F%91%E5%B8%83%E9%85%8D%E7%BD%AE/:5:0","tags":["nacos","source code","源码分析 nacos 系列"],"title":"16 发布配置","uri":"/ooooo-notes/16-%E5%8F%91%E5%B8%83%E9%85%8D%E7%BD%AE/"},{"categories":null,"content":" nacos 基于 2.2.4 版本 在 nacos 中，手动创建 service，更新 service，删除 service，更新 instance，都是通过 raft 协议来实现的，所以来简单介绍下。 ","date":"2023-09-09","objectID":"/ooooo-notes/15-%E5%90%8C%E6%AD%A5%E6%9C%8D%E5%8A%A1%E5%92%8C%E5%AE%9E%E4%BE%8B%E5%85%83%E6%95%B0%E6%8D%AE%E4%BF%A1%E6%81%AF/:0:0","tags":["nacos","source code","源码分析 nacos 系列"],"title":"15 同步服务和实例元数据信息","uri":"/ooooo-notes/15-%E5%90%8C%E6%AD%A5%E6%9C%8D%E5%8A%A1%E5%92%8C%E5%AE%9E%E4%BE%8B%E5%85%83%E6%95%B0%E6%8D%AE%E4%BF%A1%E6%81%AF/"},{"categories":null,"content":"service 元数据同步 源码位置: com.alibaba.nacos.naming.core.ServiceOperatorV2Impl // 下面这些方法最终都会调用 metadataOperateService 的 updateServiceMetadata 或者 deleteServiceMetadata // 创建 service public void create(Service service, ServiceMetadata metadata) throws NacosException { // 检查 service if (ServiceManager.getInstance().containSingleton(service)) { throw new NacosApiException(NacosException.INVALID_PARAM, ErrorCode.SERVICE_ALREADY_EXIST, String.format(\"specified service %s already exists!\", service.getGroupedServiceName())); } // raft 协议更新服务元数据 metadataOperateService.updateServiceMetadata(service, metadata); } // 更新 service @Override public void update(Service service, ServiceMetadata metadata) throws NacosException { // 检查 service if (!ServiceManager.getInstance().containSingleton(service)) { throw new NacosApiException(NacosException.INVALID_PARAM, ErrorCode.SERVICE_NOT_EXIST, String.format(\"service %s not found!\", service.getGroupedServiceName())); } // raft 协议更新服务元数据 metadataOperateService.updateServiceMetadata(service, metadata); } // 删除 service @Override public void delete(String namespaceId, String serviceName) throws NacosException { Service service = getServiceFromGroupedServiceName(namespaceId, serviceName, true); // raft 协议删除服务元数据 delete(service); } 源码位置: com.alibaba.nacos.naming.core.v2.metadata.NamingMetadataOperateService#updateServiceMetadata // 更新服务元数据 public void updateServiceMetadata(Service service, ServiceMetadata serviceMetadata) { MetadataOperation\u003cServiceMetadata\u003e operation = buildMetadataOperation(service); operation.setMetadata(serviceMetadata); // 构建 WriteRequest, 这里的 group 是 naming_service_metadata WriteRequest operationLog = WriteRequest.newBuilder().setGroup(Constants.SERVICE_METADATA) .setOperation(DataOperation.CHANGE.name()).setData(ByteString.copyFrom(serializer.serialize(operation))) .build(); // 提交元数据 submitMetadataOperation(operationLog); } // 删除服务元数据 public void delete(Service service) throws NacosException { // 检查 service if (!ServiceManager.getInstance().containSingleton(service)) { throw new NacosApiException(NacosException.INVALID_PARAM, ErrorCode.SERVICE_NOT_EXIST, String.format(\"service %s not found!\", service.getGroupedServiceName())); } // 删除 service，必须先注销所有的 instance if (!serviceStorage.getPushData(service).getHosts().isEmpty()) { throw new NacosApiException(NacosException.INVALID_PARAM, ErrorCode.SERVICE_DELETE_FAILURE, \"Service \" + service.getGroupedServiceName() + \" is not empty, can't be delete. Please unregister instance first\"); } // 删除服务元数据 metadataOperateService.deleteServiceMetadata(service); } 源码位置: com.alibaba.nacos.naming.core.v2.metadata.NamingMetadataOperateService#submitMetadataOperation // 提交元数据, 使用 raft 协议, 最后会被 ServiceMetadataProcessor#onApply 处理 private void submitMetadataOperation(WriteRequest operationLog) { try { Response response = cpProtocol.write(operationLog); if (!response.getSuccess()) { throw new NacosRuntimeException(NacosException.SERVER_ERROR, \"do metadata operation failed \" + response.getErrMsg()); } } catch (Exception e) { throw new NacosRuntimeException(NacosException.SERVER_ERROR, \"do metadata operation failed\", e); } } 源码位置: com.alibaba.nacos.naming.core.v2.metadata.ServiceMetadataProcessor#onApply @Override public Response onApply(WriteRequest request) { readLock.lock(); try { MetadataOperation\u003cServiceMetadata\u003e op = serializer.deserialize(request.getData().toByteArray(), processType); switch (DataOperation.valueOf(request.getOperation())) { case ADD: addClusterMetadataToService(op); break; case CHANGE: updateServiceMetadata(op); break; case DELETE: deleteServiceMetadata(op); break; default: return Response.newBuilder().setSuccess(false) .setErrMsg(\"Unsupported operation \" + request.getOperation()).build(); } return Response.newBuilder().setSuccess(true).build(); } catch (Exception e) { Loggers.RAFT.error(\"onApply {} service metadata operation failed. \", request.getOperation(), e); String errorMessage = null == e.getMessage() ? e.getClass().getName() : e.getMe","date":"2023-09-09","objectID":"/ooooo-notes/15-%E5%90%8C%E6%AD%A5%E6%9C%8D%E5%8A%A1%E5%92%8C%E5%AE%9E%E4%BE%8B%E5%85%83%E6%95%B0%E6%8D%AE%E4%BF%A1%E6%81%AF/:1:0","tags":["nacos","source code","源码分析 nacos 系列"],"title":"15 同步服务和实例元数据信息","uri":"/ooooo-notes/15-%E5%90%8C%E6%AD%A5%E6%9C%8D%E5%8A%A1%E5%92%8C%E5%AE%9E%E4%BE%8B%E5%85%83%E6%95%B0%E6%8D%AE%E4%BF%A1%E6%81%AF/"},{"categories":null,"content":"instance 元数据同步 源码位置: com.alibaba.nacos.naming.core.InstanceOperatorClientImpl#updateInstance // 更新实例元数据 @Override public void updateInstance(String namespaceId, String serviceName, Instance instance) throws NacosException { NamingUtils.checkInstanceIsLegal(instance); Service service = getService(namespaceId, serviceName, instance.isEphemeral()); // 检查 service if (!ServiceManager.getInstance().containSingleton(service)) { throw new NacosApiException(NacosException.INVALID_PARAM, ErrorCode.INSTANCE_ERROR, \"service not found, namespace: \" + namespaceId + \", service: \" + service); } String metadataId = InstancePublishInfo .genMetadataId(instance.getIp(), instance.getPort(), instance.getClusterName()); // 更新实例元数据 metadataOperateService.updateInstanceMetadata(service, metadataId, buildMetadata(instance)); } 源码位置: com.alibaba.nacos.naming.core.v2.metadata.NamingMetadataOperateService#updateInstanceMetadata // 更新实例元数据 public void updateInstanceMetadata(Service service, String metadataId, InstanceMetadata instanceMetadata) { MetadataOperation\u003cInstanceMetadata\u003e operation = buildMetadataOperation(service); operation.setTag(metadataId); operation.setMetadata(instanceMetadata); // 构造 WriteRequest 请求，注意这里的 group 为 naming_instance_metadata WriteRequest operationLog = WriteRequest.newBuilder().setGroup(Constants.INSTANCE_METADATA) .setOperation(DataOperation.CHANGE.name()).setData(ByteString.copyFrom(serializer.serialize(operation))) .build(); // 提交元数据请求 submitMetadataOperation(operationLog); } 源码位置: com.alibaba.nacos.naming.core.v2.metadata.NamingMetadataOperateService#submitMetadataOperation // 提交元数据请求 private void submitMetadataOperation(WriteRequest operationLog) { try { // raft 提交请求，会被 InstanceMetadataProcessor#onApply 来处理 Response response = cpProtocol.write(operationLog); if (!response.getSuccess()) { throw new NacosRuntimeException(NacosException.SERVER_ERROR, \"do metadata operation failed \" + response.getErrMsg()); } } catch (Exception e) { throw new NacosRuntimeException(NacosException.SERVER_ERROR, \"do metadata operation failed\", e); } } 源码位置: com.alibaba.nacos.naming.core.v2.metadata.InstanceMetadataProcessor#onApply @Override public Response onApply(WriteRequest request) { readLock.lock(); try { MetadataOperation\u003cInstanceMetadata\u003e op = serializer.deserialize(request.getData().toByteArray(), processType); switch (DataOperation.valueOf(request.getOperation())) { case ADD: case CHANGE: updateInstanceMetadata(op); break; case DELETE: deleteInstanceMetadata(op); break; default: return Response.newBuilder().setSuccess(false) .setErrMsg(\"Unsupported operation \" + request.getOperation()).build(); } return Response.newBuilder().setSuccess(true).build(); } catch (Exception e) { Loggers.RAFT.error(\"onApply {} instance metadata operation failed. \", request.getOperation(), e); String errorMessage = null == e.getMessage() ? e.getClass().getName() : e.getMessage(); return Response.newBuilder().setSuccess(false).setErrMsg(errorMessage).build(); } finally { readLock.unlock(); } } ","date":"2023-09-09","objectID":"/ooooo-notes/15-%E5%90%8C%E6%AD%A5%E6%9C%8D%E5%8A%A1%E5%92%8C%E5%AE%9E%E4%BE%8B%E5%85%83%E6%95%B0%E6%8D%AE%E4%BF%A1%E6%81%AF/:2:0","tags":["nacos","source code","源码分析 nacos 系列"],"title":"15 同步服务和实例元数据信息","uri":"/ooooo-notes/15-%E5%90%8C%E6%AD%A5%E6%9C%8D%E5%8A%A1%E5%92%8C%E5%AE%9E%E4%BE%8B%E5%85%83%E6%95%B0%E6%8D%AE%E4%BF%A1%E6%81%AF/"},{"categories":null,"content":"测试类 com.alibaba.nacos.test.naming.CPInstancesAPI_ITCase#createService ","date":"2023-09-09","objectID":"/ooooo-notes/15-%E5%90%8C%E6%AD%A5%E6%9C%8D%E5%8A%A1%E5%92%8C%E5%AE%9E%E4%BE%8B%E5%85%83%E6%95%B0%E6%8D%AE%E4%BF%A1%E6%81%AF/:3:0","tags":["nacos","source code","源码分析 nacos 系列"],"title":"15 同步服务和实例元数据信息","uri":"/ooooo-notes/15-%E5%90%8C%E6%AD%A5%E6%9C%8D%E5%8A%A1%E5%92%8C%E5%AE%9E%E4%BE%8B%E5%85%83%E6%95%B0%E6%8D%AE%E4%BF%A1%E6%81%AF/"},{"categories":null,"content":"raft 节点在机器ip变动之后，可能出现选主不成功的问题。 ","date":"2023-09-08","objectID":"/ooooo-notes/raft-%E5%8D%8F%E8%AE%AE%E9%87%8D%E6%96%B0%E8%AE%BE%E7%BD%AE-ip/:0:0","tags":["raft"],"title":"raft 协议重新设置 ip","uri":"/ooooo-notes/raft-%E5%8D%8F%E8%AE%AE%E9%87%8D%E6%96%B0%E8%AE%BE%E7%BD%AE-ip/"},{"categories":null,"content":"解决方法 下面是 nacos 的 JRaft 解决方法 @Test public void test() throws IOException { String[] groupIds = {\"naming_instance_metadata\", \"naming_persistent_service_v2\", \"naming_service_metadata\"}; String logPath = \"/Users/ooooo/nacos/data/protocol/raft/%s/log\"; // 遍历 groupId for (String groupId : groupIds) { // logStorage LogStorage logStorage = new RocksDBLogStorage(String.format(logPath, groupId), new RaftOptions()); // logStorageOptions LogStorageOptions logStorageOptions = new LogStorageOptions(); logStorageOptions.setConfigurationManager(new ConfigurationManager()); logStorageOptions.setLogEntryCodecFactory(LogEntryV2CodecFactory.getInstance()); logStorageOptions.setGroupId(groupId); // 初始化 boolean init = logStorage.init(logStorageOptions); if (init) { // 获取最后一个 index long lastLogIndex = logStorage.getLastLogIndex(); System.out.println(lastLogIndex); // 新增配置 LogEntry logEntry = new LogEntry(); logEntry.setType(EnumOutter.EntryType.ENTRY_TYPE_CONFIGURATION); logEntry.setPeers(Collections.singletonList(PeerId.parsePeer(\"127.0.0.1:7848\"))); // 添加到日志中 boolean b = logStorage.appendEntry(logEntry); System.out.println(b); } } } ","date":"2023-09-08","objectID":"/ooooo-notes/raft-%E5%8D%8F%E8%AE%AE%E9%87%8D%E6%96%B0%E8%AE%BE%E7%BD%AE-ip/:1:0","tags":["raft"],"title":"raft 协议重新设置 ip","uri":"/ooooo-notes/raft-%E5%8D%8F%E8%AE%AE%E9%87%8D%E6%96%B0%E8%AE%BE%E7%BD%AE-ip/"},{"categories":null,"content":" nacos 基于 2.2.4 版本 ","date":"2023-09-06","objectID":"/ooooo-notes/14-cp-%E5%8D%8F%E8%AE%AE-raft/:0:0","tags":["nacos","source code","源码分析 nacos 系列"],"title":"14 CP 协议 Raft","uri":"/ooooo-notes/14-cp-%E5%8D%8F%E8%AE%AE-raft/"},{"categories":null,"content":"raft 协议的初始化 源码位置: com.alibaba.nacos.core.distributed.raft.JRaftProtocol#init @Override public void init(RaftConfig config) { // 判断是否已经初始化 if (initialized.compareAndSet(false, true)) { this.raftConfig = config; // 这里是一个空方法 NotifyCenter.registerToSharePublisher(RaftEvent.class); // raftServer 初始化 this.raftServer.init(this.raftConfig); // raftServer 启动 this.raftServer.start(); // There is only one consumer to ensure that the internal consumption // is sequential and there is no concurrent competition // 监听 RaftEvent 事件 NotifyCenter.registerSubscriber(new Subscriber\u003cRaftEvent\u003e() { @Override public void onEvent(RaftEvent event) { Loggers.RAFT.info(\"This Raft event changes : {}\", event); final String groupId = event.getGroupId(); Map\u003cString, Map\u003cString, Object\u003e\u003e value = new HashMap\u003c\u003e(); Map\u003cString, Object\u003e properties = new HashMap\u003c\u003e(); final String leader = event.getLeader(); final Long term = event.getTerm(); final List\u003cString\u003e raftClusterInfo = event.getRaftClusterInfo(); final String errMsg = event.getErrMsg(); // Leader information needs to be selectively updated. If it is valid data, // the information in the protocol metadata is updated. MapUtil.putIfValNoEmpty(properties, MetadataKey.LEADER_META_DATA, leader); MapUtil.putIfValNoNull(properties, MetadataKey.TERM_META_DATA, term); MapUtil.putIfValNoEmpty(properties, MetadataKey.RAFT_GROUP_MEMBER, raftClusterInfo); MapUtil.putIfValNoEmpty(properties, MetadataKey.ERR_MSG, errMsg); value.put(groupId, properties); // 保存元数据 metaData.load(value); // The metadata information is injected into the metadata information of the node // 会发布 MembersChangeEvent 事件 injectProtocolMetaData(metaData); } @Override public Class\u003c? extends Event\u003e subscribeType() { return RaftEvent.class; } }); } } ","date":"2023-09-06","objectID":"/ooooo-notes/14-cp-%E5%8D%8F%E8%AE%AE-raft/:1:0","tags":["nacos","source code","源码分析 nacos 系列"],"title":"14 CP 协议 Raft","uri":"/ooooo-notes/14-cp-%E5%8D%8F%E8%AE%AE-raft/"},{"categories":null,"content":"raftServer 的初始化 源码位置: com.alibaba.nacos.core.distributed.raft.JRaftServer#init void init(RaftConfig config) { this.raftConfig = config; this.serializer = SerializeFactory.getDefault(); Loggers.RAFT.info(\"Initializes the Raft protocol, raft-config info : {}\", config); // 初始化 raft 线程池 RaftExecutor.init(config); // 解析配置 final String self = config.getSelfMember(); String[] info = InternetAddressUtil.splitIPPortStr(self); selfIp = info[0]; selfPort = Integer.parseInt(info[1]); localPeerId = PeerId.parsePeer(self); nodeOptions = new NodeOptions(); // Set the election timeout time. The default is 5 seconds. // 选举的超时时间 int electionTimeout = Math.max(ConvertUtils.toInt(config.getVal(RaftSysConstants.RAFT_ELECTION_TIMEOUT_MS), RaftSysConstants.DEFAULT_ELECTION_TIMEOUT), RaftSysConstants.DEFAULT_ELECTION_TIMEOUT); // 请求超时时间 rpcRequestTimeoutMs = ConvertUtils.toInt(raftConfig.getVal(RaftSysConstants.RAFT_RPC_REQUEST_TIMEOUT_MS), RaftSysConstants.DEFAULT_RAFT_RPC_REQUEST_TIMEOUT_MS); // 共享定时器 nodeOptions.setSharedElectionTimer(true); nodeOptions.setSharedVoteTimer(true); nodeOptions.setSharedStepDownTimer(true); nodeOptions.setSharedSnapshotTimer(true); nodeOptions.setElectionTimeoutMs(electionTimeout); // 配置 raft RaftOptions raftOptions = RaftOptionsBuilder.initRaftOptions(raftConfig); nodeOptions.setRaftOptions(raftOptions); // open jraft node metrics record function nodeOptions.setEnableMetrics(true); CliOptions cliOptions = new CliOptions(); // 初始化 raft 的 cliService this.cliService = RaftServiceFactory.createAndInitCliService(cliOptions); // cliService 的通信类, 从这个类中可以拿到 rpcClient this.cliClientService = (CliClientServiceImpl) ((CliServiceImpl) this.cliService).getCliClientService(); } ","date":"2023-09-06","objectID":"/ooooo-notes/14-cp-%E5%8D%8F%E8%AE%AE-raft/:2:0","tags":["nacos","source code","源码分析 nacos 系列"],"title":"14 CP 协议 Raft","uri":"/ooooo-notes/14-cp-%E5%8D%8F%E8%AE%AE-raft/"},{"categories":null,"content":"raftServer 的启动 源码位置: com.alibaba.nacos.core.distributed.raft.JRaftServer#start synchronized void start() { // 判断是否已经启动 if (!isStarted) { Loggers.RAFT.info(\"========= The raft protocol is starting... =========\"); try { // init raft group node // 初始化 raft 的节点列表 com.alipay.sofa.jraft.NodeManager raftNodeManager = com.alipay.sofa.jraft.NodeManager.getInstance(); for (String address : raftConfig.getMembers()) { PeerId peerId = PeerId.parsePeer(address); conf.addPeer(peerId); raftNodeManager.addAddress(peerId.getEndpoint()); } // 设置节点配置 nodeOptions.setInitialConf(conf); // 创建 rpcServer 和自定义请求处理器, 这个 server 在多个 raft group 中是共享的 rpcServer = JRaftUtils.initRpcServer(this, localPeerId); // rpcServer 初始化 if (!this.rpcServer.init(null)) { Loggers.RAFT.error(\"Fail to init [BaseRpcServer].\"); throw new RuntimeException(\"Fail to init [BaseRpcServer].\"); } // Initialize multi raft group service framework isStarted = true; // 创建 raftGroup createMultiRaftGroup(processors); Loggers.RAFT.info(\"========= The raft protocol start finished... =========\"); } catch (Exception e) { Loggers.RAFT.error(\"raft protocol start failure, cause: \", e); throw new JRaftException(e); } } } 源码位置: com.alibaba.nacos.core.distributed.raft.utils.JRaftUtils#initRpcServer // 创建 rpcServer 和自定义请求处理器 public static RpcServer initRpcServer(JRaftServer server, PeerId peerId) { GrpcRaftRpcFactory raftRpcFactory = (GrpcRaftRpcFactory) RpcFactoryHelper.rpcFactory(); // 注册 protobuf 序列化类 raftRpcFactory.registerProtobufSerializer(Log.class.getName(), Log.getDefaultInstance()); raftRpcFactory.registerProtobufSerializer(GetRequest.class.getName(), GetRequest.getDefaultInstance()); raftRpcFactory.registerProtobufSerializer(WriteRequest.class.getName(), WriteRequest.getDefaultInstance()); raftRpcFactory.registerProtobufSerializer(ReadRequest.class.getName(), ReadRequest.getDefaultInstance()); raftRpcFactory.registerProtobufSerializer(Response.class.getName(), Response.getDefaultInstance()); // 注册响应类 MarshallerRegistry registry = raftRpcFactory.getMarshallerRegistry(); registry.registerResponseInstance(Log.class.getName(), Response.getDefaultInstance()); registry.registerResponseInstance(GetRequest.class.getName(), Response.getDefaultInstance()); registry.registerResponseInstance(WriteRequest.class.getName(), Response.getDefaultInstance()); registry.registerResponseInstance(ReadRequest.class.getName(), Response.getDefaultInstance()); final RpcServer rpcServer = raftRpcFactory.createRpcServer(peerId.getEndpoint()); // 添加 raft 的 requestProcessor RaftRpcServerFactory.addRaftRequestProcessors(rpcServer, RaftExecutor.getRaftCoreExecutor(), RaftExecutor.getRaftCliServiceExecutor()); // 注册自定义的 requestProcessor rpcServer.registerProcessor(new NacosWriteRequestProcessor(server, SerializeFactory.getDefault())); rpcServer.registerProcessor(new NacosReadRequestProcessor(server, SerializeFactory.getDefault())); return rpcServer; } 源码位置: com.alibaba.nacos.core.distributed.raft.JRaftServer#createMultiRaftGroup // 创建 raftGroup // 从这个方法可以看出 // 每个 groupName 都会对应一个 processor，一个 NacosStateMachine，一个 Node。 synchronized void createMultiRaftGroup(Collection\u003cRequestProcessor4CP\u003e processors) { // There is no reason why the LogProcessor cannot be processed because of the synchronization if (!this.isStarted) { // 添加 processor this.processors.addAll(processors); return; } // raft 日志存储路径 final String parentPath = Paths.get(EnvUtil.getNacosHome(), \"data/protocol/raft\").toString(); // 遍历 processors for (RequestProcessor4CP processor : processors) { final String groupName = processor.group(); // 判断 group 是否重复 if (multiRaftGroup.containsKey(groupName)) { throw new DuplicateRaftGroupException(groupName); } // Ensure that each Raft Group has its own configuration and NodeOptions Configuration configuration = conf.copy(); NodeOptions copy = nodeOptions.copy(); // 初始化目录 JRaftUtils.initDirectory(parentPath, groupName, copy); // Here, the LogProcessor is passed into StateMachine, and when the StateMachine // triggers onApply, ","date":"2023-09-06","objectID":"/ooooo-notes/14-cp-%E5%8D%8F%E8%AE%AE-raft/:3:0","tags":["nacos","source code","源码分析 nacos 系列"],"title":"14 CP 协议 Raft","uri":"/ooooo-notes/14-cp-%E5%8D%8F%E8%AE%AE-raft/"},{"categories":null,"content":"raft 节点变更 源码位置: com.alibaba.nacos.core.distributed.raft.JRaftServer#registerSelfToCluster // 增加节点 void registerSelfToCluster(String groupId, PeerId selfIp, Configuration conf) { while (!isShutdown) { try { // 获取 groupId 对应的成员列表 List\u003cPeerId\u003e peerIds = cliService.getPeers(groupId, conf); if (peerIds.contains(selfIp)) { return; } // 添加自己的 ip 到集群中 Status status = cliService.addPeer(groupId, conf, selfIp); if (status.isOk()) { return; } Loggers.RAFT.warn(\"Failed to join the cluster, retry...\"); } catch (Exception e) { Loggers.RAFT.error(\"Failed to join the cluster, retry...\", e); } ThreadUtils.sleep(1_000L); } } 源码位置: com.alibaba.nacos.consistency.ConsistencyProtocol#memberChange // 删除节点 @Override public void memberChange(Set\u003cString\u003e addresses) { // 这里会重试 5 次 for (int i = 0; i \u003c 5; i++) { // 删除节点 if (this.raftServer.peerChange(jRaftMaintainService, addresses)) { return; } ThreadUtils.sleep(100L); } Loggers.RAFT.warn(\"peer removal failed\"); } boolean peerChange(JRaftMaintainService maintainService, Set\u003cString\u003e newPeers) { // This is only dealing with node deletion, the Raft protocol, where the node adds itself to the cluster when it starts up Set\u003cString\u003e oldPeers = new HashSet\u003c\u003e(this.raftConfig.getMembers()); this.raftConfig.setMembers(localPeerId.toString(), newPeers); oldPeers.removeAll(newPeers); // 检查节点是否有删除，为空，表示节点不变或者有新的节点加入 if (oldPeers.isEmpty()) { return true; } Set\u003cString\u003e waitRemove = oldPeers; AtomicInteger successCnt = new AtomicInteger(0); // 遍历 multiRaftGroup 来删除 multiRaftGroup.forEach(new BiConsumer\u003cString, RaftGroupTuple\u003e() { @Override public void accept(String group, RaftGroupTuple tuple) { Map\u003cString, String\u003e params = new HashMap\u003c\u003e(); params.put(JRaftConstants.GROUP_ID, group); params.put(JRaftConstants.COMMAND_NAME, JRaftConstants.REMOVE_PEERS); params.put(JRaftConstants.COMMAND_VALUE, StringUtils.join(waitRemove, StringUtils.COMMA)); // 执行删除命令, REMOVE_PEERS RestResult\u003cString\u003e result = maintainService.execute(params); if (result.ok()) { successCnt.incrementAndGet(); } else { Loggers.RAFT.error(\"Node removal failed : {}\", result); } } }); return successCnt.get() == multiRaftGroup.size(); } // 源码位置：com.alibaba.nacos.core.distributed.raft.utils.JRaftOps#REMOVE_PEERS REMOVE_PEERS(JRaftConstants.REMOVE_PEERS) { @Override public RestResult\u003cString\u003e execute(CliService cliService, String groupId, Node node, Map\u003cString, String\u003e args) { final Configuration conf = node.getOptions().getInitialConf(); final String peers = args.get(JRaftConstants.COMMAND_VALUE); // 遍历节点 for (String s : peers.split(\",\")) { List\u003cPeerId\u003e peerIds = cliService.getPeers(groupId, conf); final PeerId waitRemove = PeerId.parsePeer(s); // 不包含，则不需要删除 if (!peerIds.contains(waitRemove)) { continue; } // 删除节点 Status status = cliService.removePeer(groupId, conf, waitRemove); if (!status.isOk()) { return RestResultUtils.failed(status.getErrorMsg()); } } return RestResultUtils.success(); } }, ","date":"2023-09-06","objectID":"/ooooo-notes/14-cp-%E5%8D%8F%E8%AE%AE-raft/:4:0","tags":["nacos","source code","源码分析 nacos 系列"],"title":"14 CP 协议 Raft","uri":"/ooooo-notes/14-cp-%E5%8D%8F%E8%AE%AE-raft/"},{"categories":null,"content":"raft 的请求处理过程 这里以 writeRequest 为例，来说明 raft 是如何处理请求的。 源码位置: com.alibaba.nacos.naming.core.v2.service.impl.PersistentClientOperationServiceImpl#registerInstance // 客户端发起注册持久化实例 @Override public void registerInstance(Service service, Instance instance, String clientId) { Service singleton = ServiceManager.getInstance().getSingleton(service); if (singleton.isEphemeral()) { throw new NacosRuntimeException(NacosException.INVALID_PARAM, String.format(\"Current service %s is ephemeral service, can't register persistent instance.\", singleton.getGroupedServiceName())); } final InstanceStoreRequest request = new InstanceStoreRequest(); request.setService(service); request.setInstance(instance); request.setClientId(clientId); // 这里设置了 raft group, 等下会用到 final WriteRequest writeRequest = WriteRequest.newBuilder().setGroup(group()) .setData(ByteString.copyFrom(serializer.serialize(request))).setOperation(DataOperation.ADD.name()) .build(); try { // raftProtocol 来写请求 protocol.write(writeRequest); Loggers.RAFT.info(\"Client registered. service={}, clientId={}, instance={}\", service, instance, clientId); } catch (Exception e) { throw new NacosRuntimeException(NacosException.SERVER_ERROR, e); } } 源码位置: com.alibaba.nacos.core.distributed.raft.JRaftProtocol#write // raftProtocol 来写请求 @Override public Response write(WriteRequest request) throws Exception { // 异步请求 CompletableFuture\u003cResponse\u003e future = writeAsync(request); // Here you wait for 10 seconds, as long as possible, for the request to complete return future.get(10_000L, TimeUnit.MILLISECONDS); } @Override public CompletableFuture\u003cResponse\u003e writeAsync(WriteRequest request) { // raftServer 提交请求 return raftServer.commit(request.getGroup(), request, new CompletableFuture\u003c\u003e()); } 源码位置: com.alibaba.nacos.core.distributed.raft.JRaftServer#commit // raftServer 提交请求 public CompletableFuture\u003cResponse\u003e commit(final String group, final Message data, final CompletableFuture\u003cResponse\u003e future) { LoggerUtils.printIfDebugEnabled(Loggers.RAFT, \"data requested this time : {}\", data); // 通过 group 找到对应的 raft 配置 final RaftGroupTuple tuple = findTupleByGroup(group); if (tuple == null) { future.completeExceptionally(new IllegalArgumentException(\"No corresponding Raft Group found : \" + group)); return future; } // 包装 future 回调函数 FailoverClosureImpl closure = new FailoverClosureImpl(future); final Node node = tuple.node; if (node.isLeader()) { // The leader node directly applies this request // 如果是 leader，直接 apply 请求 applyOperation(node, data, closure); } else { // Forward to Leader for request processing // 如果不是 leader，把请求转发给 leader，后面 leader 继续 apply 请求 invokeToLeader(group, data, rpcRequestTimeoutMs, closure); } return future; } 源码位置: com.alibaba.nacos.core.distributed.raft.JRaftServer#applyOperation // leader 节点 apply 请求 public void applyOperation(Node node, Message data, FailoverClosure closure) { final Task task = new Task(); // 设置回调 task.setDone(new NacosClosure(data, status -\u003e { // 把响应设置给 closure, closure 就是 FailoverClosureImpl NacosClosure.NacosStatus nacosStatus = (NacosClosure.NacosStatus) status; closure.setThrowable(nacosStatus.getThrowable()); closure.setResponse(nacosStatus.getResponse()); closure.run(nacosStatus); })); // add request type field at the head of task data. // 封装请求，WriteRequest 或者 ReadRequest byte[] requestTypeFieldBytes = new byte[2]; requestTypeFieldBytes[0] = ProtoMessageUtil.REQUEST_TYPE_FIELD_TAG; if (data instanceof ReadRequest) { requestTypeFieldBytes[1] = ProtoMessageUtil.REQUEST_TYPE_READ; } else { requestTypeFieldBytes[1] = ProtoMessageUtil.REQUEST_TYPE_WRITE; } byte[] dataBytes = data.toByteArray(); // 设置数据 task.setData((ByteBuffer) ByteBuffer.allocate(requestTypeFieldBytes.length + dataBytes.length) .put(requestTypeFieldBytes).put(dataBytes).position(0)); // apply 请求，写入主节点日志，复制日志到从节点，超过半数节点成功，然后执行状态机 NacosStateMachine node.apply(task); } 源码位置: com.alibaba.nacos.core.distributed.raft.NacosStateMachine#onApply // 执行状态机 @Override public void onApply(Iterator iter) { int index = 0; ","date":"2023-09-06","objectID":"/ooooo-notes/14-cp-%E5%8D%8F%E8%AE%AE-raft/:5:0","tags":["nacos","source code","源码分析 nacos 系列"],"title":"14 CP 协议 Raft","uri":"/ooooo-notes/14-cp-%E5%8D%8F%E8%AE%AE-raft/"},{"categories":null,"content":"转发请求给 leader 源码位置: com.alibaba.nacos.core.distributed.raft.JRaftServer#invokeToLeader // 转发请求给 leader private void invokeToLeader(final String group, final Message request, final int timeoutMillis, FailoverClosure closure) { try { final Endpoint leaderIp = Optional.ofNullable(getLeader(group)) .orElseThrow(() -\u003e new NoLeaderException(group)).getEndpoint(); // 调用 cliClientService 来转发请求 cliClientService.getRpcClient().invokeAsync(leaderIp, request, new InvokeCallback() { @Override public void complete(Object o, Throwable ex) { if (Objects.nonNull(ex)) { closure.setThrowable(ex); closure.run(new Status(RaftError.UNKNOWN, ex.getMessage())); return; } if (!((Response)o).getSuccess()) { closure.setThrowable(new IllegalStateException(((Response) o).getErrMsg())); closure.run(new Status(RaftError.UNKNOWN, ((Response) o).getErrMsg())); return; } closure.setResponse((Response) o); closure.run(Status.OK()); } @Override public Executor executor() { return RaftExecutor.getRaftCliServiceExecutor(); } }, timeoutMillis); } catch (Exception e) { closure.setThrowable(e); closure.run(new Status(RaftError.UNKNOWN, e.toString())); } } 源码位置: com.alibaba.nacos.core.distributed.raft.processor.NacosWriteRequestProcessor // 接受 WriteRequest public class NacosWriteRequestProcessor extends AbstractProcessor implements RpcProcessor\u003cWriteRequest\u003e { private static final String INTEREST_NAME = WriteRequest.class.getName(); private final JRaftServer server; public NacosWriteRequestProcessor(JRaftServer server, Serializer serializer) { super(serializer); this.server = server; } @Override public void handleRequest(RpcContext rpcCtx, WriteRequest request) { // 处理请求 handleRequest(server, request.getGroup(), rpcCtx, request); } @Override public String interest() { return INTEREST_NAME; } } // 处理请求 protected void handleRequest(final JRaftServer server, final String group, final RpcContext rpcCtx, Message message) { try { ... // 如果是 leader 节点，就处理请求，否则返回错误 if (tuple.getNode().isLeader()) { execute(server, rpcCtx, message, tuple); } else { rpcCtx.sendResponse( Response.newBuilder().setSuccess(false).setErrMsg(\"Could not find leader : \" + group).build()); } } catch (Throwable e) { Loggers.RAFT.error(\"handleRequest has error : \", e); rpcCtx.sendResponse(Response.newBuilder().setSuccess(false).setErrMsg(e.toString()).build()); } } // 执行请求 protected void execute(JRaftServer server, final RpcContext asyncCtx, final Message message, final JRaftServer.RaftGroupTuple tuple) { ... // apply 请求，这个和上面的逻辑一样，不继续分析了 server.applyOperation(tuple.getNode(), message, closure); } ","date":"2023-09-06","objectID":"/ooooo-notes/14-cp-%E5%8D%8F%E8%AE%AE-raft/:6:0","tags":["nacos","source code","源码分析 nacos 系列"],"title":"14 CP 协议 Raft","uri":"/ooooo-notes/14-cp-%E5%8D%8F%E8%AE%AE-raft/"},{"categories":null,"content":"raft 处理 ReadRequest 源码位置: com.alibaba.nacos.core.distributed.raft.JRaftServer#get // raft 处理 ReadRequest CompletableFuture\u003cResponse\u003e get(final ReadRequest request) { final String group = request.getGroup(); CompletableFuture\u003cResponse\u003e future = new CompletableFuture\u003c\u003e(); // 检查 group 是否存在 final RaftGroupTuple tuple = findTupleByGroup(group); if (Objects.isNull(tuple)) { future.completeExceptionally(new NoSuchRaftGroupException(group)); return future; } final Node node = tuple.node; final RequestProcessor processor = tuple.processor; try { // raft 协议的一致性读，如果返回成功，可以确保数据是一致的，直接本地处理就可以 node.readIndex(BytesUtil.EMPTY_BYTES, new ReadIndexClosure() { @Override public void run(Status status, long index, byte[] reqCtx) { if (status.isOk()) { try { Response response = processor.onRequest(request); future.complete(response); } catch (Throwable t) { MetricsMonitor.raftReadIndexFailed(); future.completeExceptionally(new ConsistencyException( \"The conformance protocol is temporarily unavailable for reading\", t)); } return; } // 返回错误，从 leader 中读取数据 MetricsMonitor.raftReadIndexFailed(); Loggers.RAFT.error(\"ReadIndex has error : {}, go to Leader read.\", status.getErrorMsg()); MetricsMonitor.raftReadFromLeader(); readFromLeader(request, future); } }); return future; } catch (Throwable e) { MetricsMonitor.raftReadFromLeader(); Loggers.RAFT.warn(\"Raft linear read failed, go to Leader read logic : {}\", e.toString()); // run raft read readFromLeader(request, future); return future; } } // 从 leader 中读取数据 public void readFromLeader(final ReadRequest request, final CompletableFuture\u003cResponse\u003e future) { commit(request.getGroup(), request, future); } // 提交请求, 这个方法上面已经解析过了 public CompletableFuture\u003cResponse\u003e commit(final String group, final Message data, final CompletableFuture\u003cResponse\u003e future) { LoggerUtils.printIfDebugEnabled(Loggers.RAFT, \"data requested this time : {}\", data); final RaftGroupTuple tuple = findTupleByGroup(group); if (tuple == null) { future.completeExceptionally(new IllegalArgumentException(\"No corresponding Raft Group found : \" + group)); return future; } FailoverClosureImpl closure = new FailoverClosureImpl(future); final Node node = tuple.node; if (node.isLeader()) { // The leader node directly applies this request applyOperation(node, data, closure); } else { // Forward to Leader for request processing invokeToLeader(group, data, rpcRequestTimeoutMs, closure); } return future; } ","date":"2023-09-06","objectID":"/ooooo-notes/14-cp-%E5%8D%8F%E8%AE%AE-raft/:7:0","tags":["nacos","source code","源码分析 nacos 系列"],"title":"14 CP 协议 Raft","uri":"/ooooo-notes/14-cp-%E5%8D%8F%E8%AE%AE-raft/"},{"categories":null,"content":"nacos 中的 raft 用法 com.alibaba.nacos.naming.core.v2.service.impl.PersistentClientOperationServiceImpl: 客户端注册实例 com.alibaba.nacos.naming.consistency.persistent.impl.PersistentServiceProcessor: naming 模块，配置管理 com.alibaba.nacos.config.server.service.repository.embedded.DistributedDatabaseOperateImpl: 内嵌数据库 ","date":"2023-09-06","objectID":"/ooooo-notes/14-cp-%E5%8D%8F%E8%AE%AE-raft/:8:0","tags":["nacos","source code","源码分析 nacos 系列"],"title":"14 CP 协议 Raft","uri":"/ooooo-notes/14-cp-%E5%8D%8F%E8%AE%AE-raft/"},{"categories":null,"content":"测试类 com.alibaba.nacos.test.naming.CPInstancesAPI_ITCase#registerInstance_ephemeral_false ","date":"2023-09-06","objectID":"/ooooo-notes/14-cp-%E5%8D%8F%E8%AE%AE-raft/:9:0","tags":["nacos","source code","源码分析 nacos 系列"],"title":"14 CP 协议 Raft","uri":"/ooooo-notes/14-cp-%E5%8D%8F%E8%AE%AE-raft/"},{"categories":null,"content":" nacos 基于 2.2.4 版本 在 nacos 中，集群成员分为静态加载和动态加载，静态加载就是读取 cluster.conf 文件，动态加载就是从一个接口中获取。 ","date":"2023-09-05","objectID":"/ooooo-notes/13-%E9%9B%86%E7%BE%A4%E6%88%90%E5%91%98%E7%AE%A1%E7%90%86/:0:0","tags":["nacos","source code","源码分析 nacos 系列"],"title":"13 集群成员管理","uri":"/ooooo-notes/13-%E9%9B%86%E7%BE%A4%E6%88%90%E5%91%98%E7%AE%A1%E7%90%86/"},{"categories":null,"content":"集群成员加载的入口 源码位置: com.alibaba.nacos.core.cluster.ServerMemberManager#initAndStartLookup private void initAndStartLookup() throws NacosException { // 查找对应的 lookup, lookup 的实现有 file, standalone, address this.lookup = LookupFactory.createLookUp(this); // 是否使用 address lookup isUseAddressServer = this.lookup.useAddressServer(); // 启动 lookup this.lookup.start(); } 源码位置: com.alibaba.nacos.core.cluster.lookup.LookupFactory#createLookUp // 查找对应的 lookup public static MemberLookup createLookUp(ServerMemberManager memberManager) throws NacosException { // 不是 standalone 模式 if (!EnvUtil.getStandaloneMode()) { // lookupType 默认为空 String lookupType = EnvUtil.getProperty(LOOKUP_MODE_TYPE); // 根据 lookupType 来选择 LookupType type = chooseLookup(lookupType); LOOK_UP = find(type); currentLookupType = type; } else { // standalone lookup LOOK_UP = new StandaloneMemberLookup(); } // 注入 memberManager LOOK_UP.injectMemberManager(memberManager); Loggers.CLUSTER.info(\"Current addressing mode selection : {}\", LOOK_UP.getClass().getSimpleName()); return LOOK_UP; } 源码位置: com.alibaba.nacos.core.cluster.lookup.LookupFactory#chooseLookup // 根据 lookupType 来选择 private static LookupType chooseLookup(String lookupType) { // lookupType 不为空，则返回 if (StringUtils.isNotBlank(lookupType)) { LookupType type = LookupType.sourceOf(lookupType); if (Objects.nonNull(type)) { return type; } } // 判断 cluster.conf 是否存在，如果存在就是 file lookup File file = new File(EnvUtil.getClusterConfFilePath()); if (file.exists() || StringUtils.isNotBlank(EnvUtil.getMemberList())) { return LookupType.FILE_CONFIG; } // 默认返回 address lookup return LookupType.ADDRESS_SERVER; } ","date":"2023-09-05","objectID":"/ooooo-notes/13-%E9%9B%86%E7%BE%A4%E6%88%90%E5%91%98%E7%AE%A1%E7%90%86/:1:0","tags":["nacos","source code","源码分析 nacos 系列"],"title":"13 集群成员管理","uri":"/ooooo-notes/13-%E9%9B%86%E7%BE%A4%E6%88%90%E5%91%98%E7%AE%A1%E7%90%86/"},{"categories":null,"content":"standalone lookup 源码位置: com.alibaba.nacos.core.cluster.lookup.StandaloneMemberLookup // 这个类，就是添加了自己的 url public class StandaloneMemberLookup extends AbstractMemberLookup { @Override public void doStart() { String url = EnvUtil.getLocalAddress(); // 发布 MembersChangeEvent 事件 afterLookup(MemberUtil.readServerConf(Collections.singletonList(url))); } @Override protected void doDestroy() throws NacosException { } @Override public boolean useAddressServer() { return false; } } ","date":"2023-09-05","objectID":"/ooooo-notes/13-%E9%9B%86%E7%BE%A4%E6%88%90%E5%91%98%E7%AE%A1%E7%90%86/:2:0","tags":["nacos","source code","源码分析 nacos 系列"],"title":"13 集群成员管理","uri":"/ooooo-notes/13-%E9%9B%86%E7%BE%A4%E6%88%90%E5%91%98%E7%AE%A1%E7%90%86/"},{"categories":null,"content":"file lookup 源码位置: com.alibaba.nacos.core.cluster.lookup.FileConfigMemberLookup#doStart @Override public void doStart() throws NacosException { // 读取文件, 然后发布 MembersChangeEvent 事件 readClusterConfFromDisk(); // Use the inotify mechanism to monitor file changes and automatically // trigger the reading of cluster.conf try { // 动态监听配置文件 WatchFileCenter.registerWatcher(EnvUtil.getConfPath(), watcher); } catch (Throwable e) { Loggers.CLUSTER.error(\"An exception occurred in the launch file monitor : {}\", e.getMessage()); } } // 监听器，会监听 cluster.conf 文件，如果变动，就重新读取文件 private FileWatcher watcher = new FileWatcher() { @Override public void onChange(FileChangeEvent event) { readClusterConfFromDisk(); } @Override public boolean interest(String context) { return StringUtils.contains(context, DEFAULT_SEARCH_SEQ); } }; ","date":"2023-09-05","objectID":"/ooooo-notes/13-%E9%9B%86%E7%BE%A4%E6%88%90%E5%91%98%E7%AE%A1%E7%90%86/:3:0","tags":["nacos","source code","源码分析 nacos 系列"],"title":"13 集群成员管理","uri":"/ooooo-notes/13-%E9%9B%86%E7%BE%A4%E6%88%90%E5%91%98%E7%AE%A1%E7%90%86/"},{"categories":null,"content":"address lookup 源码位置: com.alibaba.nacos.core.cluster.lookup.AddressServerMemberLookup#doStart @Override public void doStart() throws NacosException { // 最大的是失败个数 this.maxFailCount = Integer.parseInt(EnvUtil.getProperty(HEALTH_CHECK_FAIL_COUNT_PROPERTY, DEFAULT_HEALTH_CHECK_FAIL_COUNT)); // 初始化发现地址 initAddressSys(); // 运行，获取地址 run(); } 源码位置: com.alibaba.nacos.core.cluster.lookup.AddressServerMemberLookup#initAddressSys // 初始化发现地址 // 获取域名，端口，然后拼装为地址 private void initAddressSys() { String envDomainName = System.getenv(ADDRESS_SERVER_DOMAIN_ENV); if (StringUtils.isBlank(envDomainName)) { domainName = EnvUtil.getProperty(ADDRESS_SERVER_DOMAIN_PROPERTY, DEFAULT_SERVER_DOMAIN); } else { domainName = envDomainName; } String envAddressPort = System.getenv(ADDRESS_SERVER_PORT_ENV); if (StringUtils.isBlank(envAddressPort)) { addressPort = EnvUtil.getProperty(ADDRESS_SERVER_PORT_PROPERTY, DEFAULT_SERVER_POINT); } else { addressPort = envAddressPort; } String envAddressUrl = System.getenv(ADDRESS_SERVER_URL_ENV); if (StringUtils.isBlank(envAddressUrl)) { addressUrl = EnvUtil.getProperty(ADDRESS_SERVER_URL_PROPERTY, EnvUtil.getContextPath() + \"/\" + \"serverlist\"); } else { addressUrl = envAddressUrl; } addressServerUrl = HTTP_PREFIX + domainName + \":\" + addressPort + addressUrl; envIdUrl = HTTP_PREFIX + domainName + \":\" + addressPort + \"/env\"; Loggers.CORE.info(\"ServerListService address-server port:\" + addressPort); Loggers.CORE.info(\"ADDRESS_SERVER_URL:\" + addressServerUrl); } 源码位置: com.alibaba.nacos.core.cluster.lookup.AddressServerMemberLookup#run // 运行，获取地址 private void run() throws NacosException { // With the address server, you need to perform a synchronous member node pull at startup // Repeat three times, successfully jump out boolean success = false; Throwable ex = null; int maxRetry = EnvUtil.getProperty(ADDRESS_SERVER_RETRY_PROPERTY, Integer.class, DEFAULT_SERVER_RETRY_TIME); // 重试次数 for (int i = 0; i \u003c maxRetry; i++) { try { // 获取集群成员地址, 发布 MembersChangeEvent 事件 syncFromAddressUrl(); success = true; break; } catch (Throwable e) { ex = e; Loggers.CLUSTER.error(\"[serverlist] exception, error : {}\", ExceptionUtil.getAllExceptionMsg(ex)); } } if (!success) { throw new NacosException(NacosException.SERVER_ERROR, ex); } // 定时调用 GlobalExecutor.scheduleByCommon(new AddressServerSyncTask(), DEFAULT_SYNC_TASK_DELAY_MS); } ","date":"2023-09-05","objectID":"/ooooo-notes/13-%E9%9B%86%E7%BE%A4%E6%88%90%E5%91%98%E7%AE%A1%E7%90%86/:4:0","tags":["nacos","source code","源码分析 nacos 系列"],"title":"13 集群成员管理","uri":"/ooooo-notes/13-%E9%9B%86%E7%BE%A4%E6%88%90%E5%91%98%E7%AE%A1%E7%90%86/"},{"categories":null,"content":"测试类 com.alibaba.nacos.test.core.cluster.MemberLookup_ITCase#test_a_lookup_file_config com.alibaba.nacos.test.core.cluster.MemberLookup_ITCase#test_c_lookup_address_server ","date":"2023-09-05","objectID":"/ooooo-notes/13-%E9%9B%86%E7%BE%A4%E6%88%90%E5%91%98%E7%AE%A1%E7%90%86/:5:0","tags":["nacos","source code","源码分析 nacos 系列"],"title":"13 集群成员管理","uri":"/ooooo-notes/13-%E9%9B%86%E7%BE%A4%E6%88%90%E5%91%98%E7%AE%A1%E7%90%86/"},{"categories":null,"content":" nacos 基于 2.2.4 版本 nacos 对于临时实例注册，采用的是 AP 协议，我们看看是怎么设计的。 ","date":"2023-08-29","objectID":"/ooooo-notes/12-ap-%E5%8D%8F%E8%AE%AE-distro/:0:0","tags":["nacos","source code","源码分析 nacos 系列"],"title":"12 AP 协议 Distro","uri":"/ooooo-notes/12-ap-%E5%8D%8F%E8%AE%AE-distro/"},{"categories":null,"content":"DistroProtocol 初始化 源码位置: com.alibaba.nacos.core.distributed.distro.DistroProtocol#DistroProtocol // DistroProtocol 构造函数 public DistroProtocol(ServerMemberManager memberManager, DistroComponentHolder distroComponentHolder, DistroTaskEngineHolder distroTaskEngineHolder) { this.memberManager = memberManager; this.distroComponentHolder = distroComponentHolder; this.distroTaskEngineHolder = distroTaskEngineHolder; // 开始定时任务 startDistroTask(); } // 开始定时任务 private void startDistroTask() { // standalone 表示单个节点，不用开启定时任务 if (EnvUtil.getStandaloneMode()) { isInitialized = true; return; } // 定时任务，同步当前节点的数据给其他节点, 最终会执行 DistroVerifyTimedTask startVerifyTask(); // 定时任务，从其他节点加载数据，最终会执行 DistroLoadDataTask startLoadTask(); } ","date":"2023-08-29","objectID":"/ooooo-notes/12-ap-%E5%8D%8F%E8%AE%AE-distro/:1:0","tags":["nacos","source code","源码分析 nacos 系列"],"title":"12 AP 协议 Distro","uri":"/ooooo-notes/12-ap-%E5%8D%8F%E8%AE%AE-distro/"},{"categories":null,"content":"DistroVerifyTimedTask 同步节点数据来续约 DistroVerifyTimedTask 源码位置: com.alibaba.nacos.core.distributed.distro.task.verify.DistroVerifyTimedTask // 同步当前节点的数据给其他节点, 这个类很重要 // 在 distro 协议中，每个节点只会处理部分数据, 数据的版本要通过定时任务来发送给其他节点进行续约， // 否则 client 下一次请求到其他节点，因为数据没有定时续约，会导致这个数据会过期删除. public class DistroVerifyTimedTask implements Runnable { ... @Override public void run() { try { // 获取其他节点列表 List\u003cMember\u003e targetServer = serverMemberManager.allMembersWithoutSelf(); if (Loggers.DISTRO.isDebugEnabled()) { Loggers.DISTRO.debug(\"server list is: {}\", targetServer); } // 根据 type 来同步数据 for (String each : distroComponentHolder.getDataStorageTypes()) { verifyForDataStorage(each, targetServer); } } catch (Exception e) { Loggers.DISTRO.error(\"[DISTRO-FAILED] verify task failed.\", e); } } private void verifyForDataStorage(String type, List\u003cMember\u003e targetServer) { // DistroDataStorage 数据存储，目前只有一个实现类 DistroClientDataProcessor DistroDataStorage dataStorage = distroComponentHolder.findDataStorage(type); if (!dataStorage.isFinishInitial()) { Loggers.DISTRO.warn(\"data storage {} has not finished initial step, do not send verify data\", dataStorage.getClass().getSimpleName()); return; } // 获取当前节点的数据，很重要 List\u003cDistroData\u003e verifyData = dataStorage.getVerifyData(); if (null == verifyData || verifyData.isEmpty()) { return; } // 同步给其他节点 for (Member member : targetServer) { DistroTransportAgent agent = distroComponentHolder.findTransportAgent(type); if (null == agent) { continue; } // 同步数据, 执行 DistroVerifyExecuteTask executeTaskExecuteEngine.addTask(member.getAddress() + type, new DistroVerifyExecuteTask(agent, verifyData, member.getAddress(), type)); } } } 源码位置: com.alibaba.nacos.naming.consistency.ephemeral.distro.v2.DistroClientDataProcessor#getVerifyData // 获取当前节点的数据 public List\u003cDistroData\u003e getVerifyData() { List\u003cDistroData\u003e result = null; for (String each : clientManager.allClientId()) { Client client = clientManager.getClient(each); if (null == client || !client.isEphemeral()) { continue; } // 是当前节点的数据 if (clientManager.isResponsibleClient(client)) { // clientId 和 reversion 来校验数据，并进行续约 DistroClientVerifyInfo verifyData = new DistroClientVerifyInfo(client.getClientId(), client.getRevision()); DistroKey distroKey = new DistroKey(client.getClientId(), TYPE); DistroData data = new DistroData(distroKey, ApplicationUtils.getBean(Serializer.class).serialize(verifyData)); data.setType(DataOperation.VERIFY); if (result == null) { result = new LinkedList\u003c\u003e(); } result.add(data); } } return result; } 源码位置: com.alibaba.nacos.core.distributed.distro.task.verify.DistroVerifyExecuteTask#run // 同步数据 // 数据会包装为 DistroDataRequest, 会被 DistroDataRequestHandler 处理 @Override public void run() { for (DistroData each : verifyData) { try { if (transportAgent.supportCallbackTransport()) { doSyncVerifyDataWithCallback(each); } else { doSyncVerifyData(each); } } catch (Exception e) { Loggers.DISTRO .error(\"[DISTRO-FAILED] verify data for type {} to {} failed.\", resourceType, targetServer, e); } } } 源码位置: com.alibaba.nacos.naming.remote.rpc.handler.DistroDataRequestHandler#handle // DistroDataRequestHandler 处理 DistroDataRequest // 这些方法都是委托 DistroProtocol 类来完成具体的调用 @Override public DistroDataResponse handle(DistroDataRequest request, RequestMeta meta) throws NacosException { try { switch (request.getDataOperation()) { case VERIFY: return handleVerify(request.getDistroData(), meta); case SNAPSHOT: return handleSnapshot(); case ADD: case CHANGE: case DELETE: return handleSyncData(request.getDistroData()); case QUERY: return handleQueryData(request.getDistroData()); default: return new DistroDataResponse(); } } catch (Exception e) { Loggers.DISTRO.error(\"[DISTRO-FAILED] distro handle with exception\", e); DistroDataResponse result = new DistroDataResponse(); result.setErrorCode(ResponseCode.FAIL.getCode()); result.setMessage(\"handle distro request with exception\"); return result; } } // 处理 VERIFY 请求 private DistroDataResponse handleVerify(DistroData distroData, RequestMeta meta) { DistroDataResponse re","date":"2023-08-29","objectID":"/ooooo-notes/12-ap-%E5%8D%8F%E8%AE%AE-distro/:2:0","tags":["nacos","source code","源码分析 nacos 系列"],"title":"12 AP 协议 Distro","uri":"/ooooo-notes/12-ap-%E5%8D%8F%E8%AE%AE-distro/"},{"categories":null,"content":"DistroLoadDataTask 加载数据 源码位置: com.alibaba.nacos.core.distributed.distro.task.load.DistroLoadDataTask#run @Override public void run() { try { load(); if (!checkCompleted()) { GlobalExecutor.submitLoadDataTask(this, distroConfig.getLoadDataRetryDelayMillis()); } else { loadCallback.onSuccess(); Loggers.DISTRO.info(\"[DISTRO-INIT] load snapshot data success\"); } } catch (Exception e) { loadCallback.onFailed(e); Loggers.DISTRO.error(\"[DISTRO-INIT] load snapshot data failed. \", e); } } // 加载节点数据 private void load() throws Exception { // 检查节点列表 while (memberManager.allMembersWithoutSelf().isEmpty()) { Loggers.DISTRO.info(\"[DISTRO-INIT] waiting server list init...\"); TimeUnit.SECONDS.sleep(1); } while (distroComponentHolder.getDataStorageTypes().isEmpty()) { Loggers.DISTRO.info(\"[DISTRO-INIT] waiting distro data storage register...\"); TimeUnit.SECONDS.sleep(1); } // 从远端加载快照数据, 用于服务快速启动 for (String each : distroComponentHolder.getDataStorageTypes()) { if (!loadCompletedMap.containsKey(each) || !loadCompletedMap.get(each)) { loadCompletedMap.put(each, loadAllDataSnapshotFromRemote(each)); } } } ","date":"2023-08-29","objectID":"/ooooo-notes/12-ap-%E5%8D%8F%E8%AE%AE-distro/:3:0","tags":["nacos","source code","源码分析 nacos 系列"],"title":"12 AP 协议 Distro","uri":"/ooooo-notes/12-ap-%E5%8D%8F%E8%AE%AE-distro/"},{"categories":null,"content":"DistroFilter 拦截请求 源码位置: com.alibaba.nacos.naming.web.DistroFilter#doFilter // DistroFilter 会拦截所有的请求 @Override public void doFilter(ServletRequest servletRequest, ServletResponse servletResponse, FilterChain filterChain) throws IOException, ServletException { ReuseHttpServletRequest req = new ReuseHttpServletRequest((HttpServletRequest) servletRequest); HttpServletResponse resp = (HttpServletResponse) servletResponse; String urlString = req.getRequestURI(); if (StringUtils.isNotBlank(req.getQueryString())) { urlString += \"?\" + req.getQueryString(); } try { // 获取请求对应的方法 Method method = controllerMethodsCache.getMethod(req); String path = new URI(req.getRequestURI()).getPath(); if (method == null) { throw new NoSuchMethodException(req.getMethod() + \" \" + path); } // 方法是否有 @CanDistro 注解，没有就直接放行，不处理 if (!method.isAnnotationPresent(CanDistro.class)) { filterChain.doFilter(req, resp); return; } // 获取请求参数中的 ip 和 port String distroTag = distroTagGenerator.getResponsibleTag(req); // 当前节点是否响应该请求，如果是，直接放行，这个很重要, 后面继续解析 if (distroMapper.responsible(distroTag)) { filterChain.doFilter(req, resp); return; } // proxy request to other server if necessary: String userAgent = req.getHeader(HttpHeaderConsts.USER_AGENT_HEADER); // 判断必须是 client 的请求，不能是 server 之间的请求 if (StringUtils.isNotBlank(userAgent) \u0026\u0026 userAgent.contains(UtilsAndCommons.NACOS_SERVER_HEADER)) { // This request is sent from peer server, should not be redirected again: Loggers.SRV_LOG.error(\"receive invalid redirect request from peer {}\", req.getRemoteAddr()); resp.sendError(HttpServletResponse.SC_BAD_REQUEST, \"receive invalid redirect request from peer \" + req.getRemoteAddr()); return; } // 获取转发节点, 根据 ip:port 的 hash 值对 serverList.size() 取余来计算 final String targetServer = distroMapper.mapSrv(distroTag); List\u003cString\u003e headerList = new ArrayList\u003c\u003e(16); Enumeration\u003cString\u003e headers = req.getHeaderNames(); while (headers.hasMoreElements()) { String headerName = headers.nextElement(); headerList.add(headerName); headerList.add(req.getHeader(headerName)); } final String body = IoUtils.toString(req.getInputStream(), StandardCharsets.UTF_8.name()); final Map\u003cString, String\u003e paramsValue = HttpClient.translateParameterMap(req.getParameterMap()); // 用 HttpClient 来转发请求到对应的节点上 RestResult\u003cString\u003e result = HttpClient .request(HTTP_PREFIX + targetServer + req.getRequestURI(), headerList, paramsValue, body, PROXY_CONNECT_TIMEOUT, PROXY_READ_TIMEOUT, StandardCharsets.UTF_8.name(), req.getMethod()); String data = result.ok() ? result.getData() : result.getMessage(); try { // 响应客户端请求 WebUtils.response(resp, data, result.getCode()); } catch (Exception ignore) { Loggers.SRV_LOG.warn(\"[DISTRO-FILTER] request failed: \" + distroMapper.mapSrv(distroTag) + urlString); } } catch (AccessControlException e) { resp.sendError(HttpServletResponse.SC_FORBIDDEN, \"access denied: \" + ExceptionUtil.getAllExceptionMsg(e)); } catch (NoSuchMethodException e) { resp.sendError(HttpServletResponse.SC_NOT_IMPLEMENTED, \"no such api:\" + req.getMethod() + \":\" + req.getRequestURI()); } catch (Exception e) { Loggers.SRV_LOG.warn(\"[DISTRO-FILTER] Server failed: \", e); resp.sendError(HttpServletResponse.SC_INTERNAL_SERVER_ERROR, \"Server failed, \" + ExceptionUtil.getAllExceptionMsg(e)); } } 源码位置: com.alibaba.nacos.naming.core.DistroMapper#responsible // 当前节点是否响应该请求 public boolean responsible(String responsibleTag) { final List\u003cString\u003e servers = healthyList; if (!switchDomain.isDistroEnabled() || EnvUtil.getStandaloneMode()) { return true; } if (CollectionUtils.isEmpty(servers)) { // means distro config is not ready yet return false; } // 当前节点地址找不到，不转发请求 String localAddress = EnvUtil.getLocalAddress(); int index = servers.indexOf(localAddress); int lastIndex = servers.lastIndexOf(localAddress); if (lastIndex \u003c 0 || index \u003c 0) { return true; } // 获取 hash 值，然后取余 int target = distroHash(responsibleTag) % servers.size(); return target \u003e= index \u0026\u0026 target \u003c= lastIndex; } ","date":"2023-08-29","objectID":"/ooooo-notes/12-ap-%E5%8D%8F%E8%AE%AE-distro/:4:0","tags":["nacos","source code","源码分析 nacos 系列"],"title":"12 AP 协议 Distro","uri":"/ooooo-notes/12-ap-%E5%8D%8F%E8%AE%AE-distro/"},{"categories":null,"content":"测试类 com.alibaba.nacos.test.naming.CPInstancesAPI_ITCase#registerInstance_ephemeral_true ","date":"2023-08-29","objectID":"/ooooo-notes/12-ap-%E5%8D%8F%E8%AE%AE-distro/:5:0","tags":["nacos","source code","源码分析 nacos 系列"],"title":"12 AP 协议 Distro","uri":"/ooooo-notes/12-ap-%E5%8D%8F%E8%AE%AE-distro/"},{"categories":null,"content":" nacos 基于 2.2.4 版本 nacos 基于 grpc 的长连接来实现 client 和 server 的通信。 在有多个 server 端时，最初开始 client 的连接会均匀分布在 server 端，当重新上线 server 时，这时候 client 的连接会偏移到其他 server 端，这样会造成 server 端请求负载不均匀。 connection ","date":"2023-08-28","objectID":"/ooooo-notes/11-%E8%BF%9E%E6%8E%A5%E7%AE%A1%E7%90%86/:0:0","tags":["nacos","source code","源码分析 nacos 系列"],"title":"11 连接管理","uri":"/ooooo-notes/11-%E8%BF%9E%E6%8E%A5%E7%AE%A1%E7%90%86/"},{"categories":null,"content":"client 发起连接 源码位置: com.alibaba.nacos.common.remote.client.grpc.GrpcClient#connectToServer // GrpcClient 发送 ConnectionSetupRequest 请求，建立连接 @Override public Connection connectToServer(ServerInfo serverInfo) { try { ... int port = serverInfo.getServerPort() + rpcPortOffset(); ManagedChannel managedChannel = createNewManagedChannel(serverInfo.getServerIp(), port); RequestGrpc.RequestFutureStub newChannelStubTemp = createNewChannelStub(managedChannel); if (newChannelStubTemp != null) { // 检查连接 Response response = serverCheck(serverInfo.getServerIp(), port, newChannelStubTemp); if (response == null || !(response instanceof ServerCheckResponse)) { shuntDownChannel(managedChannel); return null; } BiRequestStreamGrpc.BiRequestStreamStub biRequestStreamStub = BiRequestStreamGrpc.newStub( newChannelStubTemp.getChannel()); GrpcConnection grpcConn = new GrpcConnection(serverInfo, grpcExecutor); grpcConn.setConnectionId(((ServerCheckResponse) response).getConnectionId()); //create stream request and bind connection event to this connection. StreamObserver\u003cPayload\u003e payloadStreamObserver = bindRequestStream(biRequestStreamStub, grpcConn); // stream observer to send response to server grpcConn.setPayloadStreamObserver(payloadStreamObserver); grpcConn.setGrpcFutureServiceStub(newChannelStubTemp); grpcConn.setChannel(managedChannel); //send a setup request. // 发送 ConnectionSetupRequest 请求，建立连接 ConnectionSetupRequest conSetupRequest = new ConnectionSetupRequest(); ... grpcConn.sendRequest(conSetupRequest); //wait to register connection setup Thread.sleep(100L); return grpcConn; } return null; } catch (Exception e) { LOGGER.error(\"[{}]Fail to connect to server!,error={}\", GrpcClient.this.getName(), e); } return null; } ","date":"2023-08-28","objectID":"/ooooo-notes/11-%E8%BF%9E%E6%8E%A5%E7%AE%A1%E7%90%86/:1:0","tags":["nacos","source code","源码分析 nacos 系列"],"title":"11 连接管理","uri":"/ooooo-notes/11-%E8%BF%9E%E6%8E%A5%E7%AE%A1%E7%90%86/"},{"categories":null,"content":"server 接受连接 源码位置: com.alibaba.nacos.core.remote.grpc.GrpcBiStreamRequestAcceptor#requestBiStream // GrpcBiStreamRequestAcceptor 处理 client 请求 @Override public void onNext(Payload payload) { ... // 处理 ConnectionSetupRequest 请求 if (parseObj instanceof ConnectionSetupRequest) { ConnectionSetupRequest setUpRequest = (ConnectionSetupRequest) parseObj; Map\u003cString, String\u003e labels = setUpRequest.getLabels(); String appName = \"-\"; if (labels != null \u0026\u0026 labels.containsKey(Constants.APPNAME)) { appName = labels.get(Constants.APPNAME); } ConnectionMeta metaInfo = new ConnectionMeta(connectionId, payload.getMetadata().getClientIp(), remoteIp, remotePort, localPort, ConnectionType.GRPC.getType(), setUpRequest.getClientVersion(), appName, setUpRequest.getLabels()); metaInfo.setTenant(setUpRequest.getTenant()); Connection connection = new GrpcConnection(metaInfo, responseObserver, GrpcServerConstants.CONTEXT_KEY_CHANNEL.get()); connection.setAbilities(setUpRequest.getAbilities()); boolean rejectSdkOnStarting = metaInfo.isSdkSource() \u0026\u0026 !ApplicationUtils.isStarted(); // 注册 connectionId 和 connection if (rejectSdkOnStarting || !connectionManager.register(connectionId, connection)) { //Not register to the connection manager if current server is over limit or server is starting. try { Loggers.REMOTE_DIGEST.warn(\"[{}]Connection register fail,reason:{}\", connectionId, rejectSdkOnStarting ? \" server is not started\" : \" server is over limited.\"); connection.request(new ConnectResetRequest(), 3000L); connection.close(); } catch (Exception e) { //Do nothing. if (connectionManager.traced(clientIp)) { Loggers.REMOTE_DIGEST .warn(\"[{}]Send connect reset request error,error={}\", connectionId, e); } } } ... } } 源码位置: com.alibaba.nacos.core.remote.ConnectionManager#register // 注册 connectionId 和 connection public synchronized boolean register(String connectionId, Connection connection) { // 判断是否连接 if (connection.isConnected()) { String clientIp = connection.getMetaInfo().clientIp; if (connections.containsKey(connectionId)) { return true; } if (checkLimit(connection)) { return false; } if (traced(clientIp)) { connection.setTraced(true); } // 添加 connection connections.put(connectionId, connection); if (!connectionForClientIp.containsKey(clientIp)) { connectionForClientIp.put(clientIp, new AtomicInteger(0)); } // 计算 clientIp 的连接数，这个数值可以供我们判断 是否需要 reloadClient (后面会介绍这个 http 请求) connectionForClientIp.get(clientIp).getAndIncrement(); // connection 回调函数 clientConnectionEventListenerRegistry.notifyClientConnected(connection); LOGGER.info(\"new connection registered successfully, connectionId = {},connection={} \", connectionId, connection); return true; } return false; } 源码位置: com.alibaba.nacos.core.remote.ClientConnectionEventListenerRegistry#notifyClientConnected // ClientConnectionEventListenerRegistry 通过 registerClientConnectionEventListener 方法来注册 // ClientConnectionEventListener 的实现类有 ConnectionBasedClientManager 和 RpcAckCallbackInitorOrCleaner, 它们都在父类中注册了 // connection 回调函数 public void notifyClientConnected(final Connection connection) { for (ClientConnectionEventListener clientConnectionEventListener : clientConnectionEventListeners) { try { clientConnectionEventListener.clientConnected(connection); } catch (Throwable throwable) { Loggers.REMOTE .info(\"[NotifyClientConnected] failed for listener {}\", clientConnectionEventListener.getName(), throwable); } } } // connection 回调函数 public void notifyClientDisConnected(final Connection connection) { for (ClientConnectionEventListener clientConnectionEventListener : clientConnectionEventListeners) { try { clientConnectionEventListener.clientDisConnected(connection); } catch (Throwable throwable) { Loggers.REMOTE.info(\"[NotifyClientDisConnected] failed for listener {}\", clientConnectionEventListener.getName(), throwable); } } } // 注册 listener public void registerClientConnectionEventListener(ClientConnectionEventListener listener) { Loggers.REMOTE.info(\"[ClientConnectionEventListenerRegistry] registry listener - \" + liste","date":"2023-08-28","objectID":"/ooooo-notes/11-%E8%BF%9E%E6%8E%A5%E7%AE%A1%E7%90%86/:2:0","tags":["nacos","source code","源码分析 nacos 系列"],"title":"11 连接管理","uri":"/ooooo-notes/11-%E8%BF%9E%E6%8E%A5%E7%AE%A1%E7%90%86/"},{"categories":null,"content":"reloadClient 重置连接 源码位置: com.alibaba.nacos.core.controller.ServerLoaderController#reloadSingle @Secured(resource = Commons.NACOS_CORE_CONTEXT_V2 + \"/loader\", action = ActionTypes.WRITE) @GetMapping(\"/reloadClient\") public ResponseEntity\u003cString\u003e reloadSingle(@RequestParam String connectionId, @RequestParam(value = \"redirectAddress\", required = false) String redirectAddress) { // 发送 ConnectResetRequest 请求，重置客户端 connectionManager.loadSingle(connectionId, redirectAddress); return ResponseEntity.ok().body(\"success\"); } 源码位置: com.alibaba.nacos.core.remote.ConnectionManager#loadSingle // 发送 ConnectResetRequest 请求，重置客户端 public void loadSingle(String connectionId, String redirectAddress) { Connection connection = getConnection(connectionId); if (connection != null) { // isSdkSource 表示是 nacos 客户端 if (connection.getMetaInfo().isSdkSource()) { ConnectResetRequest connectResetRequest = new ConnectResetRequest(); if (StringUtils.isNotBlank(redirectAddress) \u0026\u0026 redirectAddress.contains(Constants.COLON)) { String[] split = redirectAddress.split(Constants.COLON); connectResetRequest.setServerIp(split[0]); connectResetRequest.setServerPort(split[1]); } try { // 发送 connectResetRequest 请求给客户端，会被 ConnectResetRequestHandler 处理 connection.request(connectResetRequest, 3000L); } catch (ConnectionAlreadyClosedException e) { // 发送异常，说明这个连接已经断开了，所以注销 connectionId unregister(connectionId); } catch (Exception e) { LOGGER.error(\"error occurs when expel connection, connectionId: {} \", connectionId, e); } } } } 源码位置: com.alibaba.nacos.common.remote.client.RpcClient.ConnectResetRequestHandler // ConnectResetRequestHandler 处理 ConnectResetRequest 请求 // 在 RpcClient 的 start 方法中添加了 ServerRequestHandler class ConnectResetRequestHandler implements ServerRequestHandler { @Override public Response requestReply(Request request) { if (request instanceof ConnectResetRequest) { try { synchronized (RpcClient.this) { if (isRunning()) { ConnectResetRequest connectResetRequest = (ConnectResetRequest) request; if (StringUtils.isNotBlank(connectResetRequest.getServerIp())) { ServerInfo serverInfo = resolveServerInfo( connectResetRequest.getServerIp() + Constants.COLON + connectResetRequest.getServerPort()); // 指定 serverInfo 变换 sever switchServerAsync(serverInfo, false); } else { // 变换 sever switchServerAsync(); } } } } catch (Exception e) { LoggerUtils.printIfErrorEnabled(LOGGER, \"[{}] Switch server error, {}\", rpcClientConfig.name(), e); } return new ConnectResetResponse(); } return null; } } ","date":"2023-08-28","objectID":"/ooooo-notes/11-%E8%BF%9E%E6%8E%A5%E7%AE%A1%E7%90%86/:3:0","tags":["nacos","source code","源码分析 nacos 系列"],"title":"11 连接管理","uri":"/ooooo-notes/11-%E8%BF%9E%E6%8E%A5%E7%AE%A1%E7%90%86/"},{"categories":null,"content":"server 断开连接检查 源码位置: com.alibaba.nacos.core.remote.grpc.AddressTransportFilter#transportTerminated // AddressTransportFilter 在 BaseGrpcServer 的 startServer 方法中注册 @Override public void transportTerminated(Attributes transportAttrs) { // 获取 connectionId String connectionId = null; try { connectionId = transportAttrs.get(ATTR_TRANS_KEY_CONN_ID); } catch (Exception e) { // Ignore } if (StringUtils.isNotBlank(connectionId)) { Loggers.REMOTE_DIGEST .info(\"Connection transportTerminated,connectionId = {} \", connectionId); // 注销 connectionId, 回调 clientConnectionEventListener 接口 connectionManager.unregister(connectionId); } } ","date":"2023-08-28","objectID":"/ooooo-notes/11-%E8%BF%9E%E6%8E%A5%E7%AE%A1%E7%90%86/:4:0","tags":["nacos","source code","源码分析 nacos 系列"],"title":"11 连接管理","uri":"/ooooo-notes/11-%E8%BF%9E%E6%8E%A5%E7%AE%A1%E7%90%86/"},{"categories":null,"content":"测试类 com.alibaba.nacos.core.remote.ConnectionManagerTest#testLoadSingle ","date":"2023-08-28","objectID":"/ooooo-notes/11-%E8%BF%9E%E6%8E%A5%E7%AE%A1%E7%90%86/:5:0","tags":["nacos","source code","源码分析 nacos 系列"],"title":"11 连接管理","uri":"/ooooo-notes/11-%E8%BF%9E%E6%8E%A5%E7%AE%A1%E7%90%86/"},{"categories":null,"content":" nacos 基于 2.2.4 版本 这里的 client 是指 nacos SDK，也就是模块 nacos-client. ","date":"2023-08-27","objectID":"/ooooo-notes/10-client-%E8%AE%A2%E9%98%85%E6%9C%8D%E5%8A%A1/:0:0","tags":["nacos","source code","源码分析 nacos 系列"],"title":"10 client 订阅服务","uri":"/ooooo-notes/10-client-%E8%AE%A2%E9%98%85%E6%9C%8D%E5%8A%A1/"},{"categories":null,"content":"订阅服务的主流程 源码位置: com.alibaba.nacos.client.naming.NacosNamingService#subscribe // NacosNamingService 订阅服务 @Override public void subscribe(String serviceName, String groupName, List\u003cString\u003e clusters, EventListener listener) throws NacosException { if (null == listener) { return; } String clusterString = StringUtils.join(clusters, \",\"); // 监听服务改变的回调函数，changeNotifier 订阅了 InstancesChangeEvent 事件 changeNotifier.registerListener(groupName, serviceName, clusterString, listener); // clientProxy 的实现类为 NamingClientProxyDelegate clientProxy.subscribe(serviceName, groupName, clusterString); } 源码位置: com.alibaba.nacos.client.naming.remote.NamingClientProxyDelegate#subscribe // NamingClientProxyDelegate 订阅服务 @Override public ServiceInfo subscribe(String serviceName, String groupName, String clusters) throws NacosException { NAMING_LOGGER.info(\"[SUBSCRIBE-SERVICE] service:{}, group:{}, clusters:{} \", serviceName, groupName, clusters); String serviceNameWithGroup = NamingUtils.getGroupedName(serviceName, groupName); String serviceKey = ServiceInfo.getKey(serviceNameWithGroup, clusters); // 注册 UpdateTask, 发送 http 请求来全量更新, 这个后面说 serviceInfoUpdateService.scheduleUpdateIfAbsent(serviceName, groupName, clusters); ServiceInfo result = serviceInfoHolder.getServiceInfoMap().get(serviceKey); if (null == result || !isSubscribed(serviceName, groupName, clusters)) { // grpc 订阅服务, 返回 serviceInfo result = grpcClientProxy.subscribe(serviceName, groupName, clusters); } // 处理 serviceInfo, 发布 InstancesChangeEvent 事件 serviceInfoHolder.processServiceInfo(result); return result; } 源码位置: com.alibaba.nacos.client.naming.remote.gprc.NamingGrpcClientProxy#subscribe // grpc 订阅服务 @Override public ServiceInfo subscribe(String serviceName, String groupName, String clusters) throws NacosException { if (NAMING_LOGGER.isDebugEnabled()) { NAMING_LOGGER.debug(\"[GRPC-SUBSCRIBE] service:{}, group:{}, cluster:{} \", serviceName, groupName, clusters); } // 标记服务要订阅，在 redoService 的定时任务中重新订阅 redoService.cacheSubscriberForRedo(serviceName, groupName, clusters); // 订阅服务 return doSubscribe(serviceName, groupName, clusters); } // redoService.cacheSubscriberForRedo public void cacheSubscriberForRedo(String serviceName, String groupName, String cluster) { String key = ServiceInfo.getKey(NamingUtils.getGroupedName(serviceName, groupName), cluster); SubscriberRedoData redoData = SubscriberRedoData.build(serviceName, groupName, cluster); synchronized (subscribes) { // 标记订阅 subscribes.put(key, redoData); } } // 订阅服务 public ServiceInfo doSubscribe(String serviceName, String groupName, String clusters) throws NacosException { SubscribeServiceRequest request = new SubscribeServiceRequest(namespaceId, groupName, serviceName, clusters, true); // 发送 SubscribeServiceRequest 请求，会被 SubscribeServiceRequestHandler 处理 SubscribeServiceResponse response = requestToServer(request, SubscribeServiceResponse.class); // 标记服务已订阅 redoService.subscriberRegistered(serviceName, groupName, clusters); return response.getServiceInfo(); } 源码位置: com.alibaba.nacos.naming.remote.rpc.handler.SubscribeServiceRequestHandler#handle // SubscribeServiceRequestHandler 处理请求 @Override @Secured(action = ActionTypes.READ) public SubscribeServiceResponse handle(SubscribeServiceRequest request, RequestMeta meta) throws NacosException { String namespaceId = request.getNamespace(); String serviceName = request.getServiceName(); String groupName = request.getGroupName(); String app = request.getHeader(\"app\", \"unknown\"); String groupedServiceName = NamingUtils.getGroupedName(serviceName, groupName); Service service = Service.newService(namespaceId, groupName, serviceName, true); // 把订阅的信息包装为 Subscriber 对象 Subscriber subscriber = new Subscriber(meta.getClientIp(), meta.getClientVersion(), app, meta.getClientIp(), namespaceId, groupedServiceName, 0, request.getClusters()); // 第一次订阅要返回对应的 serviceInfo ServiceInfo serviceInfo = ServiceUtil.selectInstancesWithHealthyProtection(serviceStorage.getData(service), metadataManager.getServiceMetadata(servic","date":"2023-08-27","objectID":"/ooooo-notes/10-client-%E8%AE%A2%E9%98%85%E6%9C%8D%E5%8A%A1/:1:0","tags":["nacos","source code","源码分析 nacos 系列"],"title":"10 client 订阅服务","uri":"/ooooo-notes/10-client-%E8%AE%A2%E9%98%85%E6%9C%8D%E5%8A%A1/"},{"categories":null,"content":"grpc 订阅处理 源码位置: com.alibaba.nacos.client.naming.remote.gprc.NamingGrpcClientProxy#start // start 方法在 NamingGrpcClientProxy 构造函数中调用 private void start(ServerListFactory serverListFactory, ServiceInfoHolder serviceInfoHolder) throws NacosException { // serverListFactory 来选择服务 rpcClient.serverListFactory(serverListFactory); // 监听 connectionEvent 事件 rpcClient.registerConnectionListener(redoService); // client 处理 server 请求, 重点看 NamingPushRequestHandler rpcClient.registerServerRequestHandler(new NamingPushRequestHandler(serviceInfoHolder)); rpcClient.start(); // 注册事件订阅 NotifyCenter.registerSubscriber(this); } 源码位置: com.alibaba.nacos.client.naming.remote.gprc.NamingPushRequestHandler#requestReply // client 处理 server 请求 @Override public Response requestReply(Request request) { if (request instanceof NotifySubscriberRequest) { // 服务实例变动了，服务端推送 serviceInfo NotifySubscriberRequest notifyRequest = (NotifySubscriberRequest) request; // 处理 serviceInfo, 发布 InstancesChangeEvent 事件 serviceInfoHolder.processServiceInfo(notifyRequest.getServiceInfo()); return new NotifySubscriberResponse(); } return null; } 源码位置: com.alibaba.nacos.client.naming.event.InstancesChangeNotifier#onEvent // InstancesChangeNotifier 监听 InstancesChangeEvent 事件 @Override public void onEvent(InstancesChangeEvent event) { String key = ServiceInfo .getKey(NamingUtils.getGroupedName(event.getServiceName(), event.getGroupName()), event.getClusters()); ConcurrentHashSet\u003cEventListener\u003e eventListeners = listenerMap.get(key); if (CollectionUtils.isEmpty(eventListeners)) { return; } for (final EventListener listener : eventListeners) { // 遍历回调函数 final com.alibaba.nacos.api.naming.listener.Event namingEvent = transferToNamingEvent(event); if (listener instanceof AbstractEventListener \u0026\u0026 ((AbstractEventListener) listener).getExecutor() != null) { ((AbstractEventListener) listener).getExecutor().execute(() -\u003e listener.onEvent(namingEvent)); } else { listener.onEvent(namingEvent); } } } ","date":"2023-08-27","objectID":"/ooooo-notes/10-client-%E8%AE%A2%E9%98%85%E6%9C%8D%E5%8A%A1/:2:0","tags":["nacos","source code","源码分析 nacos 系列"],"title":"10 client 订阅服务","uri":"/ooooo-notes/10-client-%E8%AE%A2%E9%98%85%E6%9C%8D%E5%8A%A1/"},{"categories":null,"content":"UpdateTask 全量更新 源码位置: com.alibaba.nacos.client.naming.core.ServiceInfoUpdateService.UpdateTask#run // UpdateTask 定时拉取全量的 instances @Override public void run() { long delayTime = DEFAULT_DELAY; try { // 判断是否订阅服务 if (!changeNotifier.isSubscribed(groupName, serviceName, clusters) \u0026\u0026 !futureMap.containsKey( serviceKey)) { NAMING_LOGGER.info(\"update task is stopped, service:{}, clusters:{}\", groupedServiceName, clusters); isCancel = true; return; } ServiceInfo serviceObj = serviceInfoHolder.getServiceInfoMap().get(serviceKey); // 第一次拉取 if (serviceObj == null) { serviceObj = namingClientProxy.queryInstancesOfService(serviceName, groupName, clusters, 0, false); serviceInfoHolder.processServiceInfo(serviceObj); lastRefTime = serviceObj.getLastRefTime(); return; } // 判断过期时间，然后再拉取 if (serviceObj.getLastRefTime() \u003c= lastRefTime) { serviceObj = namingClientProxy.queryInstancesOfService(serviceName, groupName, clusters, 0, false); serviceInfoHolder.processServiceInfo(serviceObj); } lastRefTime = serviceObj.getLastRefTime(); if (CollectionUtils.isEmpty(serviceObj.getHosts())) { incFailCount(); return; } // TODO multiple time can be configured. // 更新延时时间 delayTime = serviceObj.getCacheMillis() * DEFAULT_UPDATE_CACHE_TIME_MULTIPLE; resetFailCount(); } catch (NacosException e) { handleNacosException(e); } catch (Throwable e) { handleUnknownException(e); } finally { if (!isCancel) { // 下一次拉取任务 executor.schedule(this, Math.min(delayTime \u003c\u003c failCount, DEFAULT_DELAY * 60), TimeUnit.MILLISECONDS); } } } ","date":"2023-08-27","objectID":"/ooooo-notes/10-client-%E8%AE%A2%E9%98%85%E6%9C%8D%E5%8A%A1/:3:0","tags":["nacos","source code","源码分析 nacos 系列"],"title":"10 client 订阅服务","uri":"/ooooo-notes/10-client-%E8%AE%A2%E9%98%85%E6%9C%8D%E5%8A%A1/"},{"categories":null,"content":"测试类 com.alibaba.nacos.test.naming.SubscribeCluster_ITCase#subscribeAdd ","date":"2023-08-27","objectID":"/ooooo-notes/10-client-%E8%AE%A2%E9%98%85%E6%9C%8D%E5%8A%A1/:4:0","tags":["nacos","source code","源码分析 nacos 系列"],"title":"10 client 订阅服务","uri":"/ooooo-notes/10-client-%E8%AE%A2%E9%98%85%E6%9C%8D%E5%8A%A1/"},{"categories":null,"content":" nacos 基于 2.2.4 版本 这里的 client 是指 nacos SDK，也就是模块 nacos-client. ","date":"2023-08-26","objectID":"/ooooo-notes/09-client-%E6%B3%A8%E9%94%80%E5%AE%9E%E4%BE%8B/:0:0","tags":["nacos","source code","源码分析 nacos 系列"],"title":"09 client 注销实例","uri":"/ooooo-notes/09-client-%E6%B3%A8%E9%94%80%E5%AE%9E%E4%BE%8B/"},{"categories":null,"content":"注销实例的主流程 源码位置: com.alibaba.nacos.client.naming.NacosNamingService#deregisterInstance // 入口类: NacosNamingService @Override public void deregisterInstance(String serviceName, String groupName, Instance instance) throws NacosException { // clientProxy 的实现类为 NamingClientProxyDelegate clientProxy.deregisterService(serviceName, groupName, instance); } 源码位置: com.alibaba.nacos.client.naming.remote.NamingClientProxyDelegate#deregisterService // NamingClientProxyDelegate 注销实例 // 临时实例, grpcClientProxy // 持久化实例, httpClientProxy @Override public void deregisterService(String serviceName, String groupName, Instance instance) throws NacosException { getExecuteClientProxy(instance).deregisterService(serviceName, groupName, instance); } // 根据是否为临时实例来获取 clientProxy private NamingClientProxy getExecuteClientProxy(Instance instance) { return instance.isEphemeral() ? grpcClientProxy : httpClientProxy; } ","date":"2023-08-26","objectID":"/ooooo-notes/09-client-%E6%B3%A8%E9%94%80%E5%AE%9E%E4%BE%8B/:1:0","tags":["nacos","source code","源码分析 nacos 系列"],"title":"09 client 注销实例","uri":"/ooooo-notes/09-client-%E6%B3%A8%E9%94%80%E5%AE%9E%E4%BE%8B/"},{"categories":null,"content":"注销临时实例 源码位置: com.alibaba.nacos.client.naming.remote.gprc.NamingGrpcClientProxy#deregisterService @Override public void deregisterService(String serviceName, String groupName, Instance instance) throws NacosException { NAMING_LOGGER .info(\"[DEREGISTER-SERVICE] {} deregistering service {} with instance: {}\", namespaceId, serviceName, instance); // 标记 instance 要注销，可以在 redoService 定时任务重试 redoService.instanceDeregister(serviceName, groupName); // 注销实例 doDeregisterService(serviceName, groupName, instance); } // redoService.instanceDeregister public void instanceDeregister(String serviceName, String groupName) { String key = NamingUtils.getGroupedName(serviceName, groupName); synchronized (registeredInstances) { InstanceRedoData redoData = registeredInstances.get(key); if (null != redoData) { // 设置注销中 redoData.setUnregistering(true); // 设置最终状态 redoData.setExpectedRegistered(false); } } } // 注销实例 public void doDeregisterService(String serviceName, String groupName, Instance instance) throws NacosException { InstanceRequest request = new InstanceRequest(namespaceId, serviceName, groupName, NamingRemoteConstants.DE_REGISTER_INSTANCE, instance); // 发送 InstanceRequest 请求，会被 InstanceRequestHandler 处理 requestToServer(request, Response.class); // 标记 instance 已经注销 redoService.instanceDeregistered(serviceName, groupName); } 源码位置: com.alibaba.nacos.naming.remote.rpc.handler.InstanceRequestHandler#handle // InstanceRequestHandler 处理请求 @Override @Secured(action = ActionTypes.WRITE) public InstanceResponse handle(InstanceRequest request, RequestMeta meta) throws NacosException { Service service = Service .newService(request.getNamespace(), request.getGroupName(), request.getServiceName(), true); switch (request.getType()) { ... case NamingRemoteConstants.DE_REGISTER_INSTANCE: return deregisterInstance(service, request, meta); } } // 注销实例 private InstanceResponse deregisterInstance(Service service, InstanceRequest request, RequestMeta meta) { // 这个逻辑在【注销实例】章节中分析过了 clientOperationService.deregisterInstance(service, request.getInstance(), meta.getConnectionId()); NotifyCenter.publishEvent(new DeregisterInstanceTraceEvent(System.currentTimeMillis(), meta.getClientIp(), true, DeregisterInstanceReason.REQUEST, service.getNamespace(), service.getGroup(), service.getName(), request.getInstance().getIp(), request.getInstance().getPort())); return new InstanceResponse(NamingRemoteConstants.DE_REGISTER_INSTANCE); } ","date":"2023-08-26","objectID":"/ooooo-notes/09-client-%E6%B3%A8%E9%94%80%E5%AE%9E%E4%BE%8B/:2:0","tags":["nacos","source code","源码分析 nacos 系列"],"title":"09 client 注销实例","uri":"/ooooo-notes/09-client-%E6%B3%A8%E9%94%80%E5%AE%9E%E4%BE%8B/"},{"categories":null,"content":"注销持久化实例 源码位置: com.alibaba.nacos.client.naming.remote.http.NamingHttpClientProxy#deregisterService // 发送一个 http 请求 @Override public void deregisterService(String serviceName, String groupName, Instance instance) throws NacosException { NAMING_LOGGER .info(\"[DEREGISTER-SERVICE] {} deregistering service {} with instance: {}\", namespaceId, serviceName, instance); if (instance.isEphemeral()) { return; } final Map\u003cString, String\u003e params = new HashMap\u003c\u003e(16); params.put(CommonParams.NAMESPACE_ID, namespaceId); params.put(CommonParams.SERVICE_NAME, NamingUtils.getGroupedName(serviceName, groupName)); params.put(CommonParams.CLUSTER_NAME, instance.getClusterName()); params.put(IP_PARAM, instance.getIp()); params.put(PORT_PARAM, String.valueOf(instance.getPort())); params.put(EPHEMERAL_PARAM, String.valueOf(instance.isEphemeral())); reqApi(UtilAndComs.nacosUrlInstance, params, HttpMethod.DELETE); } ","date":"2023-08-26","objectID":"/ooooo-notes/09-client-%E6%B3%A8%E9%94%80%E5%AE%9E%E4%BE%8B/:3:0","tags":["nacos","source code","源码分析 nacos 系列"],"title":"09 client 注销实例","uri":"/ooooo-notes/09-client-%E6%B3%A8%E9%94%80%E5%AE%9E%E4%BE%8B/"},{"categories":null,"content":"测试类 com.alibaba.nacos.test.naming.CPInstancesAPI_ITCase#registerInstance_ephemeral_true ","date":"2023-08-26","objectID":"/ooooo-notes/09-client-%E6%B3%A8%E9%94%80%E5%AE%9E%E4%BE%8B/:4:0","tags":["nacos","source code","源码分析 nacos 系列"],"title":"09 client 注销实例","uri":"/ooooo-notes/09-client-%E6%B3%A8%E9%94%80%E5%AE%9E%E4%BE%8B/"},{"categories":null,"content":" nacos 基于 2.2.4 版本 这里的 client 是指 nacos SDK，也就是模块 nacos-client. ","date":"2023-08-25","objectID":"/ooooo-notes/08-client-%E6%B3%A8%E5%86%8C%E5%AE%9E%E4%BE%8B/:0:0","tags":["nacos","source code","源码分析 nacos 系列"],"title":"08 client 注册实例","uri":"/ooooo-notes/08-client-%E6%B3%A8%E5%86%8C%E5%AE%9E%E4%BE%8B/"},{"categories":null,"content":"注册实例的主流程 源码位置: com.alibaba.nacos.client.naming.NacosNamingService // NacosNamingService 注册实例，最后由 NamingClientProxyDelegate 来注册。 @Override public void registerInstance(String serviceName, String groupName, Instance instance) throws NacosException { // 检查参数 NamingUtils.checkInstanceIsLegal(instance); // 注册实例，clientProxy 实现类为 NamingClientProxyDelegate，具体分为 httpClientProxy 和 grpcClientProxy clientProxy.registerService(serviceName, groupName, instance); } 源码位置: com.alibaba.nacos.client.naming.remote.NamingClientProxyDelegate#registerService // 根据是否为临时实例选择对应的实现来注册实例 @Override public void registerService(String serviceName, String groupName, Instance instance) throws NacosException { getExecuteClientProxy(instance).registerService(serviceName, groupName, instance); } // 临时实例就是 grpcClientProxy, 因为 grpc 可以通过长连接来维持，连接断开，说明临时实例注销了 // 持久化实例就是 httpClientProxy, 因为持久化实例只需要一次请求就可以了，后续不需要维持 private NamingClientProxy getExecuteClientProxy(Instance instance) { return instance.isEphemeral() ? grpcClientProxy : httpClientProxy; } ","date":"2023-08-25","objectID":"/ooooo-notes/08-client-%E6%B3%A8%E5%86%8C%E5%AE%9E%E4%BE%8B/:1:0","tags":["nacos","source code","源码分析 nacos 系列"],"title":"08 client 注册实例","uri":"/ooooo-notes/08-client-%E6%B3%A8%E5%86%8C%E5%AE%9E%E4%BE%8B/"},{"categories":null,"content":"注册临时实例 源码位置: com.alibaba.nacos.client.naming.remote.gprc.NamingGrpcClientProxy#registerService @Override public void registerService(String serviceName, String groupName, Instance instance) throws NacosException { NAMING_LOGGER.info(\"[REGISTER-SERVICE] {} registering service {} with instance {}\", namespaceId, serviceName, instance); // 标记这个 instance 要注册，在连接断开之后通过 redoService 的定时任务重新注册 redoService.cacheInstanceForRedo(serviceName, groupName, instance); // 注册实例 doRegisterService(serviceName, groupName, instance); } // redoService.cacheInstanceForRedo public void cacheInstanceForRedo(String serviceName, String groupName, Instance instance) { String key = NamingUtils.getGroupedName(serviceName, groupName); InstanceRedoData redoData = InstanceRedoData.build(serviceName, groupName, instance); synchronized (registeredInstances) { registeredInstances.put(key, redoData); } } // 注册实例 public void doRegisterService(String serviceName, String groupName, Instance instance) throws NacosException { InstanceRequest request = new InstanceRequest(namespaceId, serviceName, groupName, NamingRemoteConstants.REGISTER_INSTANCE, instance); // 发送 InstanceRequest 请求，会被 InstanceRequestHandler 处理 requestToServer(request, Response.class); // 标记 instance 已经注册过 redoService.instanceRegistered(serviceName, groupName); } 源码位置: com.alibaba.nacos.naming.remote.rpc.handler.InstanceRequestHandler#handle // InstanceRequestHandler 处理请求 @Override @Secured(action = ActionTypes.WRITE) public InstanceResponse handle(InstanceRequest request, RequestMeta meta) throws NacosException { Service service = Service .newService(request.getNamespace(), request.getGroupName(), request.getServiceName(), true); switch (request.getType()) { case NamingRemoteConstants.REGISTER_INSTANCE: return registerInstance(service, request, meta); ... } } private InstanceResponse registerInstance(Service service, InstanceRequest request, RequestMeta meta) throws NacosException { // 注册实例，这个逻辑在【注册实例】章节分析过了 clientOperationService.registerInstance(service, request.getInstance(), meta.getConnectionId()); NotifyCenter.publishEvent(new RegisterInstanceTraceEvent(System.currentTimeMillis(), meta.getClientIp(), true, service.getNamespace(), service.getGroup(), service.getName(), request.getInstance().getIp(), request.getInstance().getPort())); return new InstanceResponse(NamingRemoteConstants.REGISTER_INSTANCE); } ","date":"2023-08-25","objectID":"/ooooo-notes/08-client-%E6%B3%A8%E5%86%8C%E5%AE%9E%E4%BE%8B/:2:0","tags":["nacos","source code","源码分析 nacos 系列"],"title":"08 client 注册实例","uri":"/ooooo-notes/08-client-%E6%B3%A8%E5%86%8C%E5%AE%9E%E4%BE%8B/"},{"categories":null,"content":"注册持久化实例 源码位置: com.alibaba.nacos.client.naming.remote.http.NamingHttpClientProxy#registerService // NamingHttpClientProxy 注册实例 // 发送 http 请求, 会被 com.alibaba.nacos.naming.controllers.InstanceController#register 处理, 这个逻辑在【注册实例】章节分析过了 public void registerService(String serviceName, String groupName, Instance instance) throws NacosException { NAMING_LOGGER.info(\"[REGISTER-SERVICE] {} registering service {} with instance: {}\", namespaceId, serviceName, instance); String groupedServiceName = NamingUtils.getGroupedName(serviceName, groupName); if (instance.isEphemeral()) { throw new UnsupportedOperationException( \"Do not support register ephemeral instances by HTTP, please use gRPC replaced.\"); } final Map\u003cString, String\u003e params = new HashMap\u003c\u003e(32); params.put(CommonParams.NAMESPACE_ID, namespaceId); params.put(CommonParams.SERVICE_NAME, groupedServiceName); params.put(CommonParams.GROUP_NAME, groupName); params.put(CommonParams.CLUSTER_NAME, instance.getClusterName()); params.put(IP_PARAM, instance.getIp()); params.put(PORT_PARAM, String.valueOf(instance.getPort())); params.put(WEIGHT_PARAM, String.valueOf(instance.getWeight())); params.put(REGISTER_ENABLE_PARAM, String.valueOf(instance.isEnabled())); params.put(HEALTHY_PARAM, String.valueOf(instance.isHealthy())); params.put(EPHEMERAL_PARAM, String.valueOf(instance.isEphemeral())); params.put(META_PARAM, JacksonUtils.toJson(instance.getMetadata())); reqApi(UtilAndComs.nacosUrlInstance, params, HttpMethod.POST); } ","date":"2023-08-25","objectID":"/ooooo-notes/08-client-%E6%B3%A8%E5%86%8C%E5%AE%9E%E4%BE%8B/:3:0","tags":["nacos","source code","源码分析 nacos 系列"],"title":"08 client 注册实例","uri":"/ooooo-notes/08-client-%E6%B3%A8%E5%86%8C%E5%AE%9E%E4%BE%8B/"},{"categories":null,"content":"redoService 定时任务 源码位置: com.alibaba.nacos.client.naming.remote.gprc.redo.NamingGrpcRedoService // NamingGrpcRedoService 构造函数 public NamingGrpcRedoService(NamingGrpcClientProxy clientProxy) { this.redoExecutor = new ScheduledThreadPoolExecutor(REDO_THREAD, new NameThreadFactory(REDO_THREAD_NAME)); // 定时调度 RedoScheduledTask this.redoExecutor.scheduleWithFixedDelay(new RedoScheduledTask(clientProxy, this), DEFAULT_REDO_DELAY, DEFAULT_REDO_DELAY, TimeUnit.MILLISECONDS); } 源码位置: com.alibaba.nacos.client.naming.remote.gprc.redo.RedoScheduledTask#run // RedoScheduledTask 定时任务 @Override public void run() { // 判断是否已连接，通过 NamingGrpcRedoService#onConnected 来改变状态 if (!redoService.isConnected()) { LogUtils.NAMING_LOGGER.warn(\"Grpc Connection is disconnect, skip current redo task\"); return; } // 因为 grpc 连接可能会断，所以需要重新注册实例和订阅服务 try { // 重新注册实例 redoForInstances(); // 重新订阅服务 redoForSubscribes(); } catch (Exception e) { LogUtils.NAMING_LOGGER.warn(\"Redo task run with unexpected exception: \", e); } } ","date":"2023-08-25","objectID":"/ooooo-notes/08-client-%E6%B3%A8%E5%86%8C%E5%AE%9E%E4%BE%8B/:4:0","tags":["nacos","source code","源码分析 nacos 系列"],"title":"08 client 注册实例","uri":"/ooooo-notes/08-client-%E6%B3%A8%E5%86%8C%E5%AE%9E%E4%BE%8B/"},{"categories":null,"content":"测试类 com.alibaba.nacos.test.naming.CPInstancesAPI_ITCase#registerInstance_ephemeral_true ","date":"2023-08-25","objectID":"/ooooo-notes/08-client-%E6%B3%A8%E5%86%8C%E5%AE%9E%E4%BE%8B/:5:0","tags":["nacos","source code","源码分析 nacos 系列"],"title":"08 client 注册实例","uri":"/ooooo-notes/08-client-%E6%B3%A8%E5%86%8C%E5%AE%9E%E4%BE%8B/"},{"categories":null,"content":" nacos 基于 2.2.4 版本 nacos 的 grpc client 使用的是生成的代码，位置在 com.alibaba.nacos.api.grpc.auto ","date":"2023-08-24","objectID":"/ooooo-notes/07-grpc-client-%E8%AE%BE%E8%AE%A1/:0:0","tags":["nacos","source code","源码分析 nacos 系列"],"title":"07 grpc client 设计","uri":"/ooooo-notes/07-grpc-client-%E8%AE%BE%E8%AE%A1/"},{"categories":null,"content":"client 的启动 源码位置: com.alibaba.nacos.common.remote.client.RpcClient#start // RpcClient 是父类，完成了基本功能，比如重试、连接事件 public final void start() throws NacosException { // 初始化状态变为启动中状态 boolean success = rpcClientStatus.compareAndSet(RpcClientStatus.INITIALIZED, RpcClientStatus.STARTING); if (!success) { return; } // 初始化客户端事件线程池 clientEventExecutor = new ScheduledThreadPoolExecutor(2, r -\u003e { ... }); // connection event consumer. // 连接事件 clientEventExecutor.submit(() -\u003e { while (!clientEventExecutor.isTerminated() \u0026\u0026 !clientEventExecutor.isShutdown()) { ConnectionEvent take; try { take = eventLinkedBlockingQueue.take(); if (take.isConnected()) { notifyConnected(); } else if (take.isDisConnected()) { notifyDisConnected(); } } catch (Throwable e) { // Do nothing } } }); // 重连事件 clientEventExecutor.submit(() -\u003e { while (true) { try { if (isShutdown()) { break; } ReconnectContext reconnectContext = reconnectionSignal.poll(rpcClientConfig.connectionKeepAlive(), TimeUnit.MILLISECONDS); if (reconnectContext == null) { // check alive time. // 进行健康检查，发送 HealthCheckRequest 请求 if (System.currentTimeMillis() - lastActiveTimeStamp \u003e= rpcClientConfig.connectionKeepAlive()) { boolean isHealthy = healthCheck(); if (!isHealthy) { // 判断当前连接 if (currentConnection == null) { continue; } LoggerUtils.printIfInfoEnabled(LOGGER, \"[{}] Server healthy check fail, currentConnection = {}\", rpcClientConfig.name(), currentConnection.getConnectionId()); RpcClientStatus rpcClientStatus = RpcClient.this.rpcClientStatus.get(); // 已经关闭了，无需检查了 if (RpcClientStatus.SHUTDOWN.equals(rpcClientStatus)) { break; } boolean statusFLowSuccess = RpcClient.this.rpcClientStatus.compareAndSet( rpcClientStatus, RpcClientStatus.UNHEALTHY); if (statusFLowSuccess) { reconnectContext = new ReconnectContext(null, false); } else { continue; } } else { lastActiveTimeStamp = System.currentTimeMillis(); continue; } } else { continue; } } // 检查服务是否已经删除 if (reconnectContext.serverInfo != null) { // clear recommend server if server is not in server list. boolean serverExist = false; for (String server : getServerListFactory().getServerList()) { ServerInfo serverInfo = resolveServerInfo(server); if (serverInfo.getServerIp().equals(reconnectContext.serverInfo.getServerIp())) { serverExist = true; reconnectContext.serverInfo.serverPort = serverInfo.serverPort; break; } } if (!serverExist) { LoggerUtils.printIfInfoEnabled(LOGGER, \"[{}] Recommend server is not in server list, ignore recommend server {}\", rpcClientConfig.name(), reconnectContext.serverInfo.getAddress()); // 赋值为 null，会挑选下一个服务来进行重连 reconnectContext.serverInfo = null; } } // 进行重连 reconnect(reconnectContext.serverInfo, reconnectContext.onRequestFail); } catch (Throwable throwable) { // Do nothing } } }); // connect to server, try to connect to server sync retryTimes times, async starting if failed. Connection connectToServer = null; rpcClientStatus.set(RpcClientStatus.STARTING); // 重试 int startUpRetryTimes = rpcClientConfig.retryTimes(); while (startUpRetryTimes \u003e 0 \u0026\u0026 connectToServer == null) { try { startUpRetryTimes--; // 获取下一个 serverInfo，因为 nacos 的地址可以配置多个，或者配置一个 http 地址来动态获取 ServerInfo serverInfo = nextRpcServer(); LoggerUtils.printIfInfoEnabled(LOGGER, \"[{}] Try to connect to server on start up, server: {}\", rpcClientConfig.name(), serverInfo); // 连接服务，由子类来实现，接下来继续看 connectToServer = connectToServer(serverInfo); } catch (Throwable e) { LoggerUtils.printIfWarnEnabled(LOGGER, \"[{}] Fail to connect to server on start up, error message = {}, start up retry times left: {}\", rpcClientConfig.name(), e.getMessage(), startUpRetryTimes, e); } } // 向 eventLinkedBlockingQueue 队列中添加 ConnectionEvent, 产生连接事件 if (connectToServer != null) { LoggerUtils.printIfInfoEnabled(LOGGER, \"[{}] Success to connect to server [{}] on start up, connectionId = {}\", rpcClientConfig.name(), connectToServer.serverInfo.getAddress(), connectToServer.getConnectionId()); // 设置当前连接 this.currentConnection = connectToServer; rpcClientStatus.set(RpcClientStatus.RUNNING); eventLinke","date":"2023-08-24","objectID":"/ooooo-notes/07-grpc-client-%E8%AE%BE%E8%AE%A1/:1:0","tags":["nacos","source code","源码分析 nacos 系列"],"title":"07 grpc client 设计","uri":"/ooooo-notes/07-grpc-client-%E8%AE%BE%E8%AE%A1/"},{"categories":null,"content":"连接 grpc server 源码位置: com.alibaba.nacos.common.remote.client.grpc.GrpcClient#connectToServer // 子类实现连接 grpc server // 1. 发送 ServerCheckRequest 请求来检查服务, 会返回 connectionId // 2. 发送 ConnectionSetupRequest 请求来注册 connection // 3. 最后包装为 GrpcConnection @Override public Connection connectToServer(ServerInfo serverInfo) { try { if (grpcExecutor == null) { this.grpcExecutor = createGrpcExecutor(serverInfo.getServerIp()); } // 计算端口偏移，对于 sdkClient 来说，就是 8848 + 1000 = 9848 // 如果用 nginx 来做代理，9848 端口也需要代理 int port = serverInfo.getServerPort() + rpcPortOffset(); ManagedChannel managedChannel = createNewManagedChannel(serverInfo.getServerIp(), port); // 单一请求 RequestGrpc.RequestFutureStub newChannelStubTemp = createNewChannelStub(managedChannel); if (newChannelStubTemp != null) { // 发送 ServerCheckRequest 请求，会被 GrpcRequestAcceptor#request 处理 Response response = serverCheck(serverInfo.getServerIp(), port, newChannelStubTemp); if (response == null || !(response instanceof ServerCheckResponse)) { shuntDownChannel(managedChannel); return null; } // 流式请求 BiRequestStreamGrpc.BiRequestStreamStub biRequestStreamStub = BiRequestStreamGrpc.newStub( newChannelStubTemp.getChannel()); GrpcConnection grpcConn = new GrpcConnection(serverInfo, grpcExecutor); grpcConn.setConnectionId(((ServerCheckResponse) response).getConnectionId()); //create stream request and bind connection event to this connection. // client 流式处理请求 StreamObserver\u003cPayload\u003e payloadStreamObserver = bindRequestStream(biRequestStreamStub, grpcConn); // stream observer to send response to server grpcConn.setPayloadStreamObserver(payloadStreamObserver); grpcConn.setGrpcFutureServiceStub(newChannelStubTemp); grpcConn.setChannel(managedChannel); //send a setup request. // 发送 ConnectionSetupRequest 请求，会被 GrpcBiStreamRequestAcceptor#requestBiStream 处理，注册 connection ConnectionSetupRequest conSetupRequest = new ConnectionSetupRequest(); conSetupRequest.setClientVersion(VersionUtils.getFullClientVersion()); conSetupRequest.setLabels(super.getLabels()); conSetupRequest.setAbilities(super.clientAbilities); conSetupRequest.setTenant(super.getTenant()); grpcConn.sendRequest(conSetupRequest); //wait to register connection setup Thread.sleep(100L); return grpcConn; } return null; } catch (Exception e) { LOGGER.error(\"[{}]Fail to connect to server!,error={}\", GrpcClient.this.getName(), e); } return null; } 源码位置: com.alibaba.nacos.common.remote.client.grpc.GrpcClient#bindRequestStream // client 流式处理请求 // onNext: 处理请求 // onError 和 onCompleted 来变换 grpc server private StreamObserver\u003cPayload\u003e bindRequestStream(final BiRequestStreamGrpc.BiRequestStreamStub streamStub, final GrpcConnection grpcConn) { return streamStub.requestBiStream(new StreamObserver\u003cPayload\u003e() { @Override public void onNext(Payload payload) { LoggerUtils.printIfDebugEnabled(LOGGER, \"[{}]Stream server request receive, original info: {}\", grpcConn.getConnectionId(), payload.toString()); try { Object parseBody = GrpcUtils.parse(payload); final Request request = (Request) parseBody; if (request != null) { try { // 处理服务端请求 Response response = handleServerRequest(request); if (response != null) { response.setRequestId(request.getRequestId()); // 响应 sendResponse(response); } else { LOGGER.warn(\"[{}]Fail to process server request, ackId-\u003e{}\", grpcConn.getConnectionId(), request.getRequestId()); } } catch (Exception e) { LoggerUtils.printIfErrorEnabled(LOGGER, \"[{}]Handle server request exception: {}\", grpcConn.getConnectionId(), payload.toString(), e.getMessage()); Response errResponse = ErrorResponse.build(NacosException.CLIENT_ERROR, \"Handle server request error\"); errResponse.setRequestId(request.getRequestId()); sendResponse(errResponse); } } } catch (Exception e) { LoggerUtils.printIfErrorEnabled(LOGGER, \"[{}]Error to process server push response: {}\", grpcConn.getConnectionId(), payload.getBody().getValue().toStringUtf8()); } } @Override public void onError(Throwable throwable) { boolean isRunning = isRunning(); boolean isAbandon = grpcConn.isAbandon","date":"2023-08-24","objectID":"/ooooo-notes/07-grpc-client-%E8%AE%BE%E8%AE%A1/:2:0","tags":["nacos","source code","源码分析 nacos 系列"],"title":"07 grpc client 设计","uri":"/ooooo-notes/07-grpc-client-%E8%AE%BE%E8%AE%A1/"},{"categories":null,"content":"测试类 com.alibaba.nacos.test.naming.CPInstancesAPI_ITCase#registerInstance_ephemeral_true ","date":"2023-08-24","objectID":"/ooooo-notes/07-grpc-client-%E8%AE%BE%E8%AE%A1/:3:0","tags":["nacos","source code","源码分析 nacos 系列"],"title":"07 grpc client 设计","uri":"/ooooo-notes/07-grpc-client-%E8%AE%BE%E8%AE%A1/"},{"categories":null,"content":" nacos 基于 2.2.4 版本 nacos 在 2.0 版本中引入了 grpc，用来处理http连接数过多的问题，所以有必要看看nacos 是怎么使用 grpc的，这样方便我们理清整个请求流程。 在 nacos 中只定义了两个通用的请求模型，一个是 request-response, 另外一个就是基于双向流的 request-response. ","date":"2023-08-23","objectID":"/ooooo-notes/06-grpc-server-%E8%AE%BE%E8%AE%A1/:0:0","tags":["nacos","source code","源码分析 nacos 系列"],"title":"06 grpc server 设计","uri":"/ooooo-notes/06-grpc-server-%E8%AE%BE%E8%AE%A1/"},{"categories":null,"content":"grpc server 的启动 所有的 grpc 服务都基于这个 BaseGrpcServer ，子类通过不同配置来定制服务，比如端口、超时时间 源码位置: com.alibaba.nacos.core.remote.grpc.BaseGrpcServer // 在父类中调用 start 方法，来执行 startServer 方法 @Override public void startServer() throws Exception { final MutableHandlerRegistry handlerRegistry = new MutableHandlerRegistry(); // 添加 rpc 请求 addServices(handlerRegistry, new GrpcConnectionInterceptor()); NettyServerBuilder builder = NettyServerBuilder.forPort(getServicePort()).executor(getRpcExecutor()); // 配置 tls if (grpcServerConfig.getEnableTls()) { if (grpcServerConfig.getCompatibility()) { builder.protocolNegotiator(new OptionalTlsProtocolNegotiator(getSslContextBuilder())); } else { builder.sslContext(getSslContextBuilder()); } } // 配置 grpc server 的参数 server = builder.maxInboundMessageSize(getMaxInboundMessageSize()).fallbackHandlerRegistry(handlerRegistry) .compressorRegistry(CompressorRegistry.getDefaultInstance()) .decompressorRegistry(DecompressorRegistry.getDefaultInstance()) // 连接的过滤器，设置了一些属性，比如 ATTR_TRANS_KEY_CONN_ID .addTransportFilter(new AddressTransportFilter(connectionManager)) .keepAliveTime(getKeepAliveTime(), TimeUnit.MILLISECONDS) .keepAliveTimeout(getKeepAliveTimeout(), TimeUnit.MILLISECONDS) .permitKeepAliveTime(getPermitKeepAliveTime(), TimeUnit.MILLISECONDS) .build(); // 启动 grpc server server.start(); } // 添加 rpc 请求 // 这里添加了两个通用的请求模型，payload -\u003e payload , stream payload \u003c-\u003e stream payload // 所有的请求都会由 grpcCommonRequestAcceptor 和 grpcBiStreamRequestAcceptor 来处理，接下来看看是怎么处理请求的 private void addServices(MutableHandlerRegistry handlerRegistry, ServerInterceptor... serverInterceptor) { // unary common call register. final MethodDescriptor\u003cPayload, Payload\u003e unaryPayloadMethod = MethodDescriptor.\u003cPayload, Payload\u003enewBuilder() .setType(MethodDescriptor.MethodType.UNARY) .setFullMethodName(MethodDescriptor.generateFullMethodName(GrpcServerConstants.REQUEST_SERVICE_NAME, GrpcServerConstants.REQUEST_METHOD_NAME)) .setRequestMarshaller(ProtoUtils.marshaller(Payload.getDefaultInstance())) .setResponseMarshaller(ProtoUtils.marshaller(Payload.getDefaultInstance())).build(); // 定义 payload -\u003e payload final ServerCallHandler\u003cPayload, Payload\u003e payloadHandler = ServerCalls .asyncUnaryCall((request, responseObserver) -\u003e grpcCommonRequestAcceptor.request(request, responseObserver)); final ServerServiceDefinition serviceDefOfUnaryPayload = ServerServiceDefinition.builder( GrpcServerConstants.REQUEST_SERVICE_NAME) .addMethod(unaryPayloadMethod, payloadHandler).build(); handlerRegistry.addService(ServerInterceptors.intercept(serviceDefOfUnaryPayload, serverInterceptor)); // bi stream register. // 定义 stream payload \u003c-\u003e stream payload final ServerCallHandler\u003cPayload, Payload\u003e biStreamHandler = ServerCalls.asyncBidiStreamingCall( (responseObserver) -\u003e grpcBiStreamRequestAcceptor.requestBiStream(responseObserver)); final MethodDescriptor\u003cPayload, Payload\u003e biStreamMethod = MethodDescriptor.\u003cPayload, Payload\u003enewBuilder() .setType(MethodDescriptor.MethodType.BIDI_STREAMING).setFullMethodName(MethodDescriptor .generateFullMethodName(GrpcServerConstants.REQUEST_BI_STREAM_SERVICE_NAME, GrpcServerConstants.REQUEST_BI_STREAM_METHOD_NAME)) .setRequestMarshaller(ProtoUtils.marshaller(Payload.newBuilder().build())) .setResponseMarshaller(ProtoUtils.marshaller(Payload.getDefaultInstance())).build(); final ServerServiceDefinition serviceDefOfBiStream = ServerServiceDefinition .builder(GrpcServerConstants.REQUEST_BI_STREAM_SERVICE_NAME).addMethod(biStreamMethod, biStreamHandler).build(); handlerRegistry.addService(ServerInterceptors.intercept(serviceDefOfBiStream, serverInterceptor)); } ","date":"2023-08-23","objectID":"/ooooo-notes/06-grpc-server-%E8%AE%BE%E8%AE%A1/:1:0","tags":["nacos","source code","源码分析 nacos 系列"],"title":"06 grpc server 设计","uri":"/ooooo-notes/06-grpc-server-%E8%AE%BE%E8%AE%A1/"},{"categories":null,"content":"GrpcRequestAcceptor 处理单一请求 源码位置: com.alibaba.nacos.core.remote.grpc.GrpcRequestAcceptor // GrpcRequestAcceptor 处理请求 // 请求的逻辑比较清楚，最终由 RequestHandler 来处理请求 @Override public void request(Payload grpcRequest, StreamObserver\u003cPayload\u003e responseObserver) { // trace 请求 traceIfNecessary(grpcRequest, true); String type = grpcRequest.getMetadata().getType(); //server is on starting. // server 正在启动中， 返回错误 if (!ApplicationUtils.isStarted()) { Payload payloadResponse = GrpcUtils.convert( ErrorResponse.build(NacosException.INVALID_SERVER_STATUS, \"Server is starting,please try later.\")); traceIfNecessary(payloadResponse, false); responseObserver.onNext(payloadResponse); responseObserver.onCompleted(); return; } // server check. // 检查请求处理，在客户端启动时，会发送 ServerCheckRequest 请求 if (ServerCheckRequest.class.getSimpleName().equals(type)) { Payload serverCheckResponseP = GrpcUtils.convert(new ServerCheckResponse(GrpcServerConstants.CONTEXT_KEY_CONN_ID.get())); traceIfNecessary(serverCheckResponseP, false); responseObserver.onNext(serverCheckResponseP); responseObserver.onCompleted(); return; } // 根据 type 来获取 handler，这里最重要 RequestHandler requestHandler = requestHandlerRegistry.getByRequestType(type); //no handler found. // 没有找到对应的 handler，返回错误 if (requestHandler == null) { Loggers.REMOTE_DIGEST.warn(String.format(\"[%s] No handler for request type : %s :\", \"grpc\", type)); Payload payloadResponse = GrpcUtils .convert(ErrorResponse.build(NacosException.NO_HANDLER, \"RequestHandler Not Found\")); traceIfNecessary(payloadResponse, false); responseObserver.onNext(payloadResponse); responseObserver.onCompleted(); return; } //check connection status. // 检查连接状态, 在客户端启动时，会发送 ConnectionSetupRequest 请求来创建 Connection String connectionId = GrpcServerConstants.CONTEXT_KEY_CONN_ID.get(); boolean requestValid = connectionManager.checkValid(connectionId); if (!requestValid) { Loggers.REMOTE_DIGEST .warn(\"[{}] Invalid connection Id ,connection [{}] is un registered ,\", \"grpc\", connectionId); Payload payloadResponse = GrpcUtils .convert(ErrorResponse.build(NacosException.UN_REGISTER, \"Connection is unregistered.\")); traceIfNecessary(payloadResponse, false); responseObserver.onNext(payloadResponse); responseObserver.onCompleted(); return; } Object parseObj = null; try { // 根据 grpcRequest 中的 type 来进行 json 反序列化 parseObj = GrpcUtils.parse(grpcRequest); } catch (Exception e) { Loggers.REMOTE_DIGEST .warn(\"[{}] Invalid request receive from connection [{}] ,error={}\", \"grpc\", connectionId, e); Payload payloadResponse = GrpcUtils.convert(ErrorResponse.build(NacosException.BAD_GATEWAY, e.getMessage())); traceIfNecessary(payloadResponse, false); responseObserver.onNext(payloadResponse); responseObserver.onCompleted(); return; } // 无效的请求，返回错误 if (parseObj == null) { Loggers.REMOTE_DIGEST.warn(\"[{}] Invalid request receive ,parse request is null\", connectionId); Payload payloadResponse = GrpcUtils .convert(ErrorResponse.build(NacosException.BAD_GATEWAY, \"Invalid request\")); traceIfNecessary(payloadResponse, false); responseObserver.onNext(payloadResponse); responseObserver.onCompleted(); return; } // 只能是 request 对象 if (!(parseObj instanceof Request)) { Loggers.REMOTE_DIGEST .warn(\"[{}] Invalid request receive ,parsed payload is not a request,parseObj={}\", connectionId, parseObj); Payload payloadResponse = GrpcUtils .convert(ErrorResponse.build(NacosException.BAD_GATEWAY, \"Invalid request\")); traceIfNecessary(payloadResponse, false); responseObserver.onNext(payloadResponse); responseObserver.onCompleted(); return; } Request request = (Request) parseObj; try { // 获取对应的 connection，设置 requestMeta Connection connection = connectionManager.getConnection(GrpcServerConstants.CONTEXT_KEY_CONN_ID.get()); RequestMeta requestMeta = new RequestMeta(); requestMeta.setClientIp(connection.getMetaInfo().getClientIp()); requestMeta.setConnectionId(GrpcServerConstants.CONTEXT_KEY_CONN_ID.get()); requestMeta.setClientVersion(connection.getMetaInfo().getVersion()); requestMeta.setLabels(connection.getMetaInf","date":"2023-08-23","objectID":"/ooooo-notes/06-grpc-server-%E8%AE%BE%E8%AE%A1/:2:0","tags":["nacos","source code","源码分析 nacos 系列"],"title":"06 grpc server 设计","uri":"/ooooo-notes/06-grpc-server-%E8%AE%BE%E8%AE%A1/"},{"categories":null,"content":"GrpcBiStreamRequestAcceptor 处理流式请求 源码位置: com.alibaba.nacos.core.remote.grpc.GrpcBiStreamRequestAcceptor // GrpcBiStreamRequestAcceptor 处理流式请求 // 处理逻辑比较清楚，主要分为 onNext(正常请求)、onError(错误请求)、onCompleted(关闭请求) // 在流式处理中，没有 requestHandler 来处理请求， // 这是因为流式请求主要是 服务端发送数据给客户端，客户端接受后发送 ack response @Override public StreamObserver\u003cPayload\u003e requestBiStream(StreamObserver\u003cPayload\u003e responseObserver) { StreamObserver\u003cPayload\u003e streamObserver = new StreamObserver\u003cPayload\u003e() { final String connectionId = GrpcServerConstants.CONTEXT_KEY_CONN_ID.get(); final Integer localPort = GrpcServerConstants.CONTEXT_KEY_CONN_LOCAL_PORT.get(); final int remotePort = GrpcServerConstants.CONTEXT_KEY_CONN_REMOTE_PORT.get(); String remoteIp = GrpcServerConstants.CONTEXT_KEY_CONN_REMOTE_IP.get(); String clientIp = \"\"; @Override public void onNext(Payload payload) { // 获取客户端的 ip clientIp = payload.getMetadata().getClientIp(); traceDetailIfNecessary(payload); Object parseObj; try { // 反序列化对象 parseObj = GrpcUtils.parse(payload); } catch (Throwable throwable) { Loggers.REMOTE_DIGEST .warn(\"[{}]Grpc request bi stream,payload parse error={}\", connectionId, throwable); return; } // 请求对象为 null，不处理 if (parseObj == null) { Loggers.REMOTE_DIGEST .warn(\"[{}]Grpc request bi stream,payload parse null ,body={},meta={}\", connectionId, payload.getBody().getValue().toStringUtf8(), payload.getMetadata()); return; } // 处理 ConnectionSetupRequest 请求，客户端启动时会发送这个请求 if (parseObj instanceof ConnectionSetupRequest) { ConnectionSetupRequest setUpRequest = (ConnectionSetupRequest) parseObj; Map\u003cString, String\u003e labels = setUpRequest.getLabels(); String appName = \"-\"; if (labels != null \u0026\u0026 labels.containsKey(Constants.APPNAME)) { appName = labels.get(Constants.APPNAME); } ConnectionMeta metaInfo = new ConnectionMeta(connectionId, payload.getMetadata().getClientIp(), remoteIp, remotePort, localPort, ConnectionType.GRPC.getType(), setUpRequest.getClientVersion(), appName, setUpRequest.getLabels()); metaInfo.setTenant(setUpRequest.getTenant()); // 新建 connection Connection connection = new GrpcConnection(metaInfo, responseObserver, GrpcServerConstants.CONTEXT_KEY_CHANNEL.get()); connection.setAbilities(setUpRequest.getAbilities()); boolean rejectSdkOnStarting = metaInfo.isSdkSource() \u0026\u0026 !ApplicationUtils.isStarted(); // 注册 connection 对象, 如果不成功，则关闭连接 if (rejectSdkOnStarting || !connectionManager.register(connectionId, connection)) { //Not register to the connection manager if current server is over limit or server is starting. try { Loggers.REMOTE_DIGEST.warn(\"[{}]Connection register fail,reason:{}\", connectionId, rejectSdkOnStarting ? \" server is not started\" : \" server is over limited.\"); connection.request(new ConnectResetRequest(), 3000L); connection.close(); } catch (Exception e) { //Do nothing. if (connectionManager.traced(clientIp)) { Loggers.REMOTE_DIGEST .warn(\"[{}]Send connect reset request error,error={}\", connectionId, e); } } } } else if (parseObj instanceof Response) { // 处理 response 请求，请求可能需要 ack Response response = (Response) parseObj; if (connectionManager.traced(clientIp)) { Loggers.REMOTE_DIGEST .warn(\"[{}]Receive response of server request ,response={}\", connectionId, response); } // ack 通知 RpcAckCallbackSynchronizer.ackNotify(connectionId, response); connectionManager.refreshActiveTime(connectionId); } else { Loggers.REMOTE_DIGEST .warn(\"[{}]Grpc request bi stream,unknown payload receive ,parseObj={}\", connectionId, parseObj); } } @Override public void onError(Throwable t) { if (connectionManager.traced(clientIp)) { Loggers.REMOTE_DIGEST.warn(\"[{}]Bi stream on error,error={}\", connectionId, t); } if (responseObserver instanceof ServerCallStreamObserver) { ServerCallStreamObserver serverCallStreamObserver = ((ServerCallStreamObserver) responseObserver); if (serverCallStreamObserver.isCancelled()) { //client close the stream. } else { try { serverCallStreamObserver.onCompleted(); } catch (Throwable throwable) { //ignore } } } } @Override public void onCompleted() { if (connectionMa","date":"2023-08-23","objectID":"/ooooo-notes/06-grpc-server-%E8%AE%BE%E8%AE%A1/:3:0","tags":["nacos","source code","源码分析 nacos 系列"],"title":"06 grpc server 设计","uri":"/ooooo-notes/06-grpc-server-%E8%AE%BE%E8%AE%A1/"},{"categories":null,"content":"测试类 com.alibaba.nacos.core.remote.grpc.GrpcServerTest#testGrpcSdkServer com.alibaba.nacos.core.remote.grpc.GrpcServerTest#testGrpcClusterServer ","date":"2023-08-23","objectID":"/ooooo-notes/06-grpc-server-%E8%AE%BE%E8%AE%A1/:4:0","tags":["nacos","source code","源码分析 nacos 系列"],"title":"06 grpc server 设计","uri":"/ooooo-notes/06-grpc-server-%E8%AE%BE%E8%AE%A1/"},{"categories":null,"content":" nacos 基于 2.2.4 版本 ","date":"2023-08-22","objectID":"/ooooo-notes/05-%E4%BB%BB%E5%8A%A1%E6%89%A7%E8%A1%8C%E5%99%A8%E7%9A%84%E8%AE%BE%E8%AE%A1/:0:0","tags":["nacos","source code","源码分析 nacos 系列"],"title":"05 任务执行器的设计","uri":"/ooooo-notes/05-%E4%BB%BB%E5%8A%A1%E6%89%A7%E8%A1%8C%E5%99%A8%E7%9A%84%E8%AE%BE%E8%AE%A1/"},{"categories":null,"content":"任务执行器的设计 在 nacos 中，所有的任务都实现了 NacosTask 接口，所有的任务执行器都实现了 NacosTaskExecuteEngine 接口。 接下来看看这两个接口的设计 // NacosTask public interface NacosTask { /** * Judge Whether this nacos task should do. * * @return true means the nacos task should be done, otherwise false */ // 对于立即执行的任务，默认 return true. // 对于延时执行的任务，判断 System.currentTimeMillis() - this.lastProcessTime \u003e= this.taskInterval. boolean shouldProcess(); } // NacosTaskExecuteEngine // 从接口中可以看出 // 1. addTask, 一个 key 对应一个 task // 2. addProcessor, 一个 key 对应一个 processor, 有默认的 processor public interface NacosTaskExecuteEngine\u003cT extends NacosTask\u003e extends Closeable { /** * Get Task size in execute engine. * * @return size of task */ int size(); /** * Whether the execute engine is empty. * * @return true if the execute engine has no task to do, otherwise false */ boolean isEmpty(); /** * Add task processor {@link NacosTaskProcessor} for execute engine. * * @param key key of task * @param taskProcessor task processor */ void addProcessor(Object key, NacosTaskProcessor taskProcessor); /** * Remove task processor {@link NacosTaskProcessor} form execute engine for key. * * @param key key of task */ void removeProcessor(Object key); /** * Try to get {@link NacosTaskProcessor} by key, if non-exist, will return default processor. * * @param key key of task * @return task processor for task key or default processor if task processor for task key non-exist */ NacosTaskProcessor getProcessor(Object key); /** * Get all processor key. * * @return collection of processors */ Collection\u003cObject\u003e getAllProcessorKey(); /** * Set default task processor. If do not find task processor by task key, use this default processor to process * task. * * @param defaultTaskProcessor default task processor */ void setDefaultTaskProcessor(NacosTaskProcessor defaultTaskProcessor); /** * Add task into execute pool. * * @param key key of task * @param task task */ void addTask(Object key, T task); /** * Remove task. * * @param key key of task * @return nacos task */ T removeTask(Object key); /** * Get all task keys. * * @return collection of task keys. */ Collection\u003cObject\u003e getAllTaskKeys(); } ","date":"2023-08-22","objectID":"/ooooo-notes/05-%E4%BB%BB%E5%8A%A1%E6%89%A7%E8%A1%8C%E5%99%A8%E7%9A%84%E8%AE%BE%E8%AE%A1/:1:0","tags":["nacos","source code","源码分析 nacos 系列"],"title":"05 任务执行器的设计","uri":"/ooooo-notes/05-%E4%BB%BB%E5%8A%A1%E6%89%A7%E8%A1%8C%E5%99%A8%E7%9A%84%E8%AE%BE%E8%AE%A1/"},{"categories":null,"content":"具体实现类(举例) PushDelayTask 和 PushDelayTaskExecuteEngine 源码位置: com.alibaba.nacos.naming.push.v2.task.PushDelayTask // PushDelayTask 的构造方法 // delay: 延时的事件 // targetClient: 推送的 clientId public PushDelayTask(Service service, long delay, String targetClient) { this.service = service; this.pushToAll = false; this.targetClients = new HashSet\u003c\u003e(1); this.targetClients.add(targetClient); setTaskInterval(delay); // 设置上一次处理时间，用来判断是否过期 setLastProcessTime(System.currentTimeMillis()); } // 每一个延时任务都会有 merge 方法, 用来合并相同的 task, 这样可以更高效的处理任务 // 比如因为客户端重试，发起了两个一样的 task，经过 merge 之后，处理一个就行。 @Override public void merge(AbstractDelayTask task) { if (!(task instanceof PushDelayTask)) { return; } PushDelayTask oldTask = (PushDelayTask) task; if (isPushToAll() || oldTask.isPushToAll()) { pushToAll = true; targetClients = null; } else { targetClients.addAll(oldTask.getTargetClients()); } setLastProcessTime(Math.min(getLastProcessTime(), task.getLastProcessTime())); Loggers.PUSH.info(\"[PUSH] Task merge for {}\", service); } // shouldProcess 方法在父类上面, 判断当前任务都是过期 @Override public boolean shouldProcess() { return (System.currentTimeMillis() - this.lastProcessTime \u003e= this.taskInterval); } 源码位置: com.alibaba.nacos.naming.push.v2.task.PushDelayTaskExecuteEngine#PushDelayTaskExecuteEngine // PushDelayTaskExecuteEngine 的初始化 public PushDelayTaskExecuteEngine(ClientManager clientManager, ClientServiceIndexesManager indexesManager, ServiceStorage serviceStorage, NamingMetadataManager metadataManager, PushExecutor pushExecutor, SwitchDomain switchDomain) { ... // 设置默认的processor, 用来处理任务, 这里没有特殊的 processor setDefaultTaskProcessor(new PushDelayTaskProcessor(this)); } // 在父类 NacosDelayTaskExecuteEngine 中用单一的线程池来启动，然后处理任务 public NacosDelayTaskExecuteEngine(String name, int initCapacity, Logger logger, long processInterval) { ... tasks = new ConcurrentHashMap\u003c\u003e(initCapacity); // 线程池 processingExecutor = ExecutorFactory.newSingleScheduledExecutorService(new NameThreadFactory(name)); // 最后调用自己的方法来处理任务 processingExecutor .scheduleWithFixedDelay(new ProcessRunnable(), processInterval, processInterval, TimeUnit.MILLISECONDS); } // 处理任务 private class ProcessRunnable implements Runnable { @Override public void run() { try { processTasks(); } catch (Throwable e) { getEngineLog().error(e.toString(), e); } } } // 在父类 NacosDelayTaskExecuteEngine 中 // com.alibaba.nacos.common.task.engine.NacosDelayTaskExecuteEngine#processTasks protected void processTasks() { // 获取所有的 key，因为 addTask 方法，所以每一个 task 都会关联到一个 key Collection\u003cObject\u003e keys = getAllTaskKeys(); for (Object taskKey : keys) { // 判断 task 是否到期，每个 task 创建时都是指定 taskInterval AbstractDelayTask task = removeTask(taskKey); if (null == task) { continue; } // 获取相应的 processor 来处理，一般来说就是默认的 processor, 比如 PushDelayTaskProcessor, 下面会说这个类 NacosTaskProcessor processor = getProcessor(taskKey); if (null == processor) { getEngineLog().error(\"processor not found for task, so discarded. \" + task); continue; } try { // 处理 task，如果处理失败，重新添加 task // ReAdd task if process failed if (!processor.process(task)) { retryFailedTask(taskKey, task); } } catch (Throwable e) { getEngineLog().error(\"Nacos task execute error \", e); retryFailedTask(taskKey, task); } } } // 重新添加 task private void retryFailedTask(Object key, AbstractDelayTask task) { task.setLastProcessTime(System.currentTimeMillis()); addTask(key, task); } 源码位置: com.alibaba.nacos.naming.push.v2.task.PushDelayTaskExecuteEngine.PushDelayTaskProcessor // PushDelayTaskProcessor 处理 PushDelayTask, 重新包装为 PushExecuteTask, 然后放入线程池中运行 private static class PushDelayTaskProcessor implements NacosTaskProcessor { private final PushDelayTaskExecuteEngine executeEngine; public PushDelayTaskProcessor(PushDelayTaskExecuteEngine executeEngine) { this.executeEngine = executeEngine; } @Override public boolean process(NacosTask task) { PushDelayTask pushDelayTask = (PushDelayTask) task; Service service = pushDelayTask.getService(); NamingExecuteTaskDispatcher.getInstance() .dispatchAndExecuteTask(service, new PushExecuteTask(servi","date":"2023-08-22","objectID":"/ooooo-notes/05-%E4%BB%BB%E5%8A%A1%E6%89%A7%E8%A1%8C%E5%99%A8%E7%9A%84%E8%AE%BE%E8%AE%A1/:2:0","tags":["nacos","source code","源码分析 nacos 系列"],"title":"05 任务执行器的设计","uri":"/ooooo-notes/05-%E4%BB%BB%E5%8A%A1%E6%89%A7%E8%A1%8C%E5%99%A8%E7%9A%84%E8%AE%BE%E8%AE%A1/"},{"categories":null,"content":"测试类 com.alibaba.nacos.common.task.engine.NacosDelayTaskExecuteEngineTest ","date":"2023-08-22","objectID":"/ooooo-notes/05-%E4%BB%BB%E5%8A%A1%E6%89%A7%E8%A1%8C%E5%99%A8%E7%9A%84%E8%AE%BE%E8%AE%A1/:3:0","tags":["nacos","source code","源码分析 nacos 系列"],"title":"05 任务执行器的设计","uri":"/ooooo-notes/05-%E4%BB%BB%E5%8A%A1%E6%89%A7%E8%A1%8C%E5%99%A8%E7%9A%84%E8%AE%BE%E8%AE%A1/"},{"categories":null,"content":"时间轮的用法 // 默认间隔时间为 100 毫秒 Timer timer = new HashedWheelTimer(); Timeout timeout = timer.newTimeout(new TimerTask() { @Override public void run(Timeout timeout) throws Exception { System.out.println(\"run\"); } }, 10, TimeUnit.SECONDS); timer.stop(); ","date":"2023-08-22","objectID":"/ooooo-notes/%E6%97%B6%E9%97%B4%E8%BD%AE%E7%AE%97%E6%B3%95/:1:0","tags":["netty","source code","源码分析 netty 系列"],"title":"时间轮算法","uri":"/ooooo-notes/%E6%97%B6%E9%97%B4%E8%BD%AE%E7%AE%97%E6%B3%95/"},{"categories":null,"content":"时间轮的原理 有一个环形数组，每个格子都是一个队列，每隔 interval 时间，指针就会移动到下一格，然后把队列中的任务拿出来判断是否到期并执行。 timer ","date":"2023-08-22","objectID":"/ooooo-notes/%E6%97%B6%E9%97%B4%E8%BD%AE%E7%AE%97%E6%B3%95/:2:0","tags":["netty","source code","源码分析 netty 系列"],"title":"时间轮算法","uri":"/ooooo-notes/%E6%97%B6%E9%97%B4%E8%BD%AE%E7%AE%97%E6%B3%95/"},{"categories":null,"content":"netty 的时间轮 源码位置: io.netty.util.HashedWheelTimer#HashedWheelTimer // 先来看看构造函数 // threadFactory: 可以用来指定线程的名称 // tickDuration, unit : 间隔时间 // ticksPerWheel: 环形数组的大小, 也就是时间轮的大小 // leakDetection: 检查资源，默认开启 // maxPendingTimeouts: 定时任务上限个数 // taskExecutor: 任务执行器，默认是同步执行，可以自定义来异步执行 public HashedWheelTimer( ThreadFactory threadFactory, long tickDuration, TimeUnit unit, int ticksPerWheel, boolean leakDetection, long maxPendingTimeouts, Executor taskExecutor) { checkNotNull(threadFactory, \"threadFactory\"); checkNotNull(unit, \"unit\"); checkPositive(tickDuration, \"tickDuration\"); checkPositive(ticksPerWheel, \"ticksPerWheel\"); this.taskExecutor = checkNotNull(taskExecutor, \"taskExecutor\"); // Normalize ticksPerWheel to power of two and initialize the wheel. // 创建时间轮, 其大小是 2 的倍数，最接近于 ticksPerWheel wheel = createWheel(ticksPerWheel); mask = wheel.length - 1; // Convert tickDuration to nanos. long duration = unit.toNanos(tickDuration); // Prevent overflow. if (duration \u003e= Long.MAX_VALUE / wheel.length) { throw new IllegalArgumentException(String.format( \"tickDuration: %d (expected: 0 \u003c tickDuration in nanos \u003c %d\", tickDuration, Long.MAX_VALUE / wheel.length)); } if (duration \u003c MILLISECOND_NANOS) { logger.warn(\"Configured tickDuration {} smaller than {}, using 1ms.\", tickDuration, MILLISECOND_NANOS); this.tickDuration = MILLISECOND_NANOS; } else { this.tickDuration = duration; } // 创建 work 线程，来进行 sleep workerThread = threadFactory.newThread(worker); leak = leakDetection || !workerThread.isDaemon() ? leakDetector.track(this) : null; this.maxPendingTimeouts = maxPendingTimeouts; // 时间轮不能开启太多，因为每个时间轮都要 work 线程来轮询 if (INSTANCE_COUNTER.incrementAndGet() \u003e INSTANCE_COUNT_LIMIT \u0026\u0026 WARNED_TOO_MANY_INSTANCES.compareAndSet(false, true)) { reportTooManyInstances(); } } // 创建时间轮, 实际就是一个数组，然后通过 i++ % size 来达到环形数组 private static HashedWheelBucket[] createWheel(int ticksPerWheel) { //ticksPerWheel may not be greater than 2^30 checkInRange(ticksPerWheel, 1, 1073741824, \"ticksPerWheel\"); // ticksPerWheel 是 2 的倍数 ticksPerWheel = normalizeTicksPerWheel(ticksPerWheel); HashedWheelBucket[] wheel = new HashedWheelBucket[ticksPerWheel]; for (int i = 0; i \u003c wheel.length; i ++) { wheel[i] = new HashedWheelBucket(); } return wheel; } 源码位置: io.netty.util.HashedWheelTimer#newTimeout // 添加一个定时任务 @Override public Timeout newTimeout(TimerTask task, long delay, TimeUnit unit) { checkNotNull(task, \"task\"); checkNotNull(unit, \"unit\"); long pendingTimeoutsCount = pendingTimeouts.incrementAndGet(); // 判断定时任务的个数 if (maxPendingTimeouts \u003e 0 \u0026\u0026 pendingTimeoutsCount \u003e maxPendingTimeouts) { pendingTimeouts.decrementAndGet(); throw new RejectedExecutionException(\"Number of pending timeouts (\" + pendingTimeoutsCount + \") is greater than or equal to maximum allowed pending \" + \"timeouts (\" + maxPendingTimeouts + \")\"); } // 启动 work 线程，这里是懒加载，一定是有定时任务添加，才会启动 start(); // Add the timeout to the timeout queue which will be processed on the next tick. // During processing all the queued HashedWheelTimeouts will be added to the correct HashedWheelBucket. // deadline 表示要等待的间隔时间 long deadline = System.nanoTime() + unit.toNanos(delay) - startTime; // Guard against overflow. if (delay \u003e 0 \u0026\u0026 deadline \u003c 0) { deadline = Long.MAX_VALUE; } // 创建 timeout, 然后添加到 timeouts 队列中，之后 work 线程会从队列中获取 HashedWheelTimeout timeout = new HashedWheelTimeout(this, task, deadline); timeouts.add(timeout); return timeout; } 源码位置: io.netty.util.HashedWheelTimer#start // 启动 work 线程 public void start() { // 判断状态 switch (WORKER_STATE_UPDATER.get(this)) { case WORKER_STATE_INIT: // 可能多线程启动，所以 cas 判断 if (WORKER_STATE_UPDATER.compareAndSet(this, WORKER_STATE_INIT, WORKER_STATE_STARTED)) { // 启动 work 线程，执行 run 方法 workerThread.start(); } break; case WORKER_STATE_STARTED: break; case WORKER_STATE_SHUTDOWN: throw new IllegalStateException(\"cannot be started once stopped\"); default: throw new Error(\"Invalid WorkerState\"); } // Wait until the startTime is initialized by the worker. while (startTime == 0) { try { startTimeInitialized.aw","date":"2023-08-22","objectID":"/ooooo-notes/%E6%97%B6%E9%97%B4%E8%BD%AE%E7%AE%97%E6%B3%95/:3:0","tags":["netty","source code","源码分析 netty 系列"],"title":"时间轮算法","uri":"/ooooo-notes/%E6%97%B6%E9%97%B4%E8%BD%AE%E7%AE%97%E6%B3%95/"},{"categories":null,"content":" nacos 基于 2.2.4 版本 nacos 订阅服务主要分为 http+udp 和 grpc 这两种方式，这两者的内部调用方法都是一样的，这里主要分析 http+udp 的方式。 ","date":"2023-08-21","objectID":"/ooooo-notes/04-%E8%AE%A2%E9%98%85%E6%9C%8D%E5%8A%A1/:0:0","tags":["nacos","source code","源码分析 nacos 系列"],"title":"04 订阅服务","uri":"/ooooo-notes/04-%E8%AE%A2%E9%98%85%E6%9C%8D%E5%8A%A1/"},{"categories":null,"content":"订阅服务的 curl curl --location 'localhost:8848/nacos/v2/ns/instance/list?serviceName=test' ","date":"2023-08-21","objectID":"/ooooo-notes/04-%E8%AE%A2%E9%98%85%E6%9C%8D%E5%8A%A1/:1:0","tags":["nacos","source code","源码分析 nacos 系列"],"title":"04 订阅服务","uri":"/ooooo-notes/04-%E8%AE%A2%E9%98%85%E6%9C%8D%E5%8A%A1/"},{"categories":null,"content":"订阅服务的主流程 源码位置: com.alibaba.nacos.naming.controllers.v2.InstanceControllerV2#list // 处理请求 public Result\u003cServiceInfo\u003e list(@RequestParam(value = \"namespaceId\", defaultValue = Constants.DEFAULT_NAMESPACE_ID) String namespaceId, @RequestParam(value = \"groupName\", defaultValue = Constants.DEFAULT_GROUP) String groupName, @RequestParam(\"serviceName\") String serviceName, @RequestParam(value = \"clusterName\", defaultValue = StringUtils.EMPTY) String clusterName, @RequestParam(value = \"ip\", defaultValue = StringUtils.EMPTY) String ip, @RequestParam(value = \"port\", defaultValue = \"0\") Integer port, @RequestParam(value = \"healthyOnly\", defaultValue = \"false\") Boolean healthyOnly, @RequestParam(value = \"app\", defaultValue = StringUtils.EMPTY) String app, @RequestHeader(value = HttpHeaderConsts.USER_AGENT_HEADER, required = false) String userAgent, @RequestHeader(value = HttpHeaderConsts.CLIENT_VERSION_HEADER, required = false) String clientVersion) { if (StringUtils.isEmpty(userAgent)) { userAgent = StringUtils.defaultIfEmpty(clientVersion, StringUtils.EMPTY); } String compositeServiceName = NamingUtils.getGroupedName(serviceName, groupName); // 根据 ip 和 port 来进行 udp 推送 Subscriber subscriber = new Subscriber(ip + \":\" + port, userAgent, app, ip, namespaceId, compositeServiceName, port, clusterName); // 获取所有的实例 return Result.success(instanceServiceV2.listInstance(namespaceId, compositeServiceName, subscriber, clusterName, healthyOnly)); } 源码位置: com.alibaba.nacos.naming.core.InstanceOperatorClientImpl#listInstance // 获取所有的实例 @Override public ServiceInfo listInstance(String namespaceId, String serviceName, Subscriber subscriber, String cluster, boolean healthOnly) { Service service = getService(namespaceId, serviceName, true); // For adapt 1.X subscribe logic if (subscriber.getPort() \u003e 0 \u0026\u0026 pushService.canEnablePush(subscriber.getAgent())) { // clientId = address + ID_DELIMITER + ephemeral, 这个很重要，用来判断是不是 udp push String clientId = IpPortBasedClient.getClientId(subscriber.getAddrStr(), true); // 根据 udp 的 ip 和 port 来创建 client createIpPortClientIfAbsent(clientId); // 添加订阅, 实现类只有一个，就是 EphemeralClientOperationServiceImpl，接下来分析这个 clientOperationService.subscribeService(service, subscriber, clientId); } ServiceInfo serviceInfo = serviceStorage.getData(service); ServiceMetadata serviceMetadata = metadataManager.getServiceMetadata(service).orElse(null); // 根据条件来筛选最终的 instances ServiceInfo result = ServiceUtil .selectInstancesWithHealthyProtection(serviceInfo, serviceMetadata, cluster, healthOnly, true, subscriber.getIp()); // adapt for v1.x sdk result.setName(NamingUtils.getGroupedName(result.getName(), result.getGroupName())); return result; } 源码位置: com.alibaba.nacos.naming.core.v2.service.impl.EphemeralClientOperationServiceImpl#subscribeService @Override public void subscribeService(Service service, Subscriber subscriber, String clientId) { // 获取单例的 service Service singleton = ServiceManager.getInstance().getSingletonIfExist(service).orElse(service); Client client = clientManager.getClient(clientId); if (!clientIsLegal(client, clientId)) { return; } // client 添加 service 和 subscriber client.addServiceSubscriber(singleton, subscriber); // 设置更新时间，以防被过期定时任务清理了 client.setLastUpdatedTime(); // 发布 ClientSubscribeServiceEvent NotifyCenter.publishEvent(new ClientOperationEvent.ClientSubscribeServiceEvent(singleton, clientId)); } 源码位置: com.alibaba.nacos.naming.core.v2.index.ClientServiceIndexesManager#handleClientOperation // ClientServiceIndexesManager 监听 ClientSubscribeServiceEvent 事件 private void handleClientOperation(ClientOperationEvent event) { Service service = event.getService(); String clientId = event.getClientId(); if (event instanceof ClientOperationEvent.ClientRegisterServiceEvent) { addPublisherIndexes(service, clientId); } else if (event instanceof ClientOperationEvent.ClientDeregisterServiceEvent) { removePublisherIndexes(service, clientId); } else if (event instanceof ClientOperationEvent.ClientSubscribeServiceEvent) { // 添加 service 对应的 client","date":"2023-08-21","objectID":"/ooooo-notes/04-%E8%AE%A2%E9%98%85%E6%9C%8D%E5%8A%A1/:2:0","tags":["nacos","source code","源码分析 nacos 系列"],"title":"04 订阅服务","uri":"/ooooo-notes/04-%E8%AE%A2%E9%98%85%E6%9C%8D%E5%8A%A1/"},{"categories":null,"content":"延时任务推送 每个 PushDelayTask 到期之后，都会被 PushDelayTaskProcessor 来处理，重新包装为 PushExecuteTask. 源码位置: `` private static class PushDelayTaskProcessor implements NacosTaskProcessor { private final PushDelayTaskExecuteEngine executeEngine; public PushDelayTaskProcessor(PushDelayTaskExecuteEngine executeEngine) { this.executeEngine = executeEngine; } @Override public boolean process(NacosTask task) { PushDelayTask pushDelayTask = (PushDelayTask) task; Service service = pushDelayTask.getService(); // 丢任务到线程池来执行 NamingExecuteTaskDispatcher.getInstance() .dispatchAndExecuteTask(service, new PushExecuteTask(service, executeEngine, pushDelayTask)); return true; } } 源码位置: com.alibaba.nacos.naming.push.v2.task.PushExecuteTask#run // 执行 push 任务 public void run() { try { PushDataWrapper wrapper = generatePushData(); ClientManager clientManager = delayTaskEngine.getClientManager(); // 获取所有推送的 clientId for (String each : getTargetClientIds()) { Client client = clientManager.getClient(each); if (null == client) { // means this client has disconnect continue; } // 获取 service 对应的 subscriber Subscriber subscriber = client.getSubscriber(service); // skip if null if (subscriber == null) { continue; } // 执行具体的 push, 接下来看看是如何获取对应的 pushExecutor delayTaskEngine.getPushExecutor().doPushWithCallback(each, subscriber, wrapper, new ServicePushCallback(each, subscriber, wrapper.getOriginalData(), delayTask.isPushToAll())); } } catch (Exception e) { Loggers.PUSH.error(\"Push task for service\" + service.getGroupedServiceName() + \" execute failed \", e); delayTaskEngine.addTask(service, new PushDelayTask(service, 1000L)); } } 源码位置: com.alibaba.nacos.naming.push.v2.executor.PushExecutorDelegate#doPushWithCallback @Override public void doPushWithCallback(String clientId, Subscriber subscriber, PushDataWrapper data, NamingPushCallback callBack) { getPushExecuteService(clientId, subscriber).doPushWithCallback(clientId, subscriber, data, callBack); } private PushExecutor getPushExecuteService(String clientId, Subscriber subscriber) { Optional\u003cSpiPushExecutor\u003e result = SpiImplPushExecutorHolder.getInstance() .findPushExecutorSpiImpl(clientId, subscriber); if (result.isPresent()) { return result.get(); } // 获取对应的 pushExecuteService, 之前的 clientId = address + ID_DELIMITER + ephemeral // use nacos default push executor return clientId.contains(IpPortBasedClient.ID_DELIMITER) ? udpPushExecuteService : rpcPushExecuteService; } // udpPushExecuteService 的逻辑比较简单，就是发送一个 udp 的数据包，这里不继续分析了。 // rpcPushExecuteService 的逻辑就是发送一个 rpc 的数据包，后面的文章详说 ","date":"2023-08-21","objectID":"/ooooo-notes/04-%E8%AE%A2%E9%98%85%E6%9C%8D%E5%8A%A1/:3:0","tags":["nacos","source code","源码分析 nacos 系列"],"title":"04 订阅服务","uri":"/ooooo-notes/04-%E8%AE%A2%E9%98%85%E6%9C%8D%E5%8A%A1/"},{"categories":null,"content":"测试类 com.alibaba.nacos.test.naming.SubscribeCluster_ITCase#subscribeAdd ","date":"2023-08-21","objectID":"/ooooo-notes/04-%E8%AE%A2%E9%98%85%E6%9C%8D%E5%8A%A1/:4:0","tags":["nacos","source code","源码分析 nacos 系列"],"title":"04 订阅服务","uri":"/ooooo-notes/04-%E8%AE%A2%E9%98%85%E6%9C%8D%E5%8A%A1/"},{"categories":null,"content":" nacos 基于 2.2.4 版本 ","date":"2023-08-20","objectID":"/ooooo-notes/03-%E6%B3%A8%E9%94%80%E5%AE%9E%E4%BE%8B/:0:0","tags":["nacos","source code","源码分析 nacos 系列"],"title":"03 注销实例","uri":"/ooooo-notes/03-%E6%B3%A8%E9%94%80%E5%AE%9E%E4%BE%8B/"},{"categories":null,"content":"注销实例的 curl curl --location --request DELETE 'http://localhost:8848/nacos/v2/ns/instance' \\ --header 'Content-Type: application/x-www-form-urlencoded' \\ --data-urlencode 'serviceName=test' \\ --data-urlencode 'ip=1.2.3.4' \\ --data-urlencode 'port=80' ","date":"2023-08-20","objectID":"/ooooo-notes/03-%E6%B3%A8%E9%94%80%E5%AE%9E%E4%BE%8B/:1:0","tags":["nacos","source code","源码分析 nacos 系列"],"title":"03 注销实例","uri":"/ooooo-notes/03-%E6%B3%A8%E9%94%80%E5%AE%9E%E4%BE%8B/"},{"categories":null,"content":"注销实例的主流程 源码位置: com.alibaba.nacos.naming.controllers.v2.InstanceControllerV2#deregister public Result\u003cString\u003e deregister(InstanceForm instanceForm) throws NacosException { // check param instanceForm.validate(); checkWeight(instanceForm.getWeight()); // build instance Instance instance = buildInstance(instanceForm); // 移除 instance instanceServiceV2.removeInstance(instanceForm.getNamespaceId(), buildCompositeServiceName(instanceForm), instance); // 发布 DeregisterInstanceTraceEvent 事件 NotifyCenter.publishEvent(new DeregisterInstanceTraceEvent(System.currentTimeMillis(), \"\", false, DeregisterInstanceReason.REQUEST, instanceForm.getNamespaceId(), instanceForm.getGroupName(), instanceForm.getServiceName(), instance.getIp(), instance.getPort())); return Result.success(\"ok\"); } 源码位置: com.alibaba.nacos.naming.core.InstanceOperatorClientImpl#removeInstance @Override public void removeInstance(String namespaceId, String serviceName, Instance instance) { // 判断 instance 是否已经注册过, 如果没有，则不用处理 boolean ephemeral = instance.isEphemeral(); String clientId = IpPortBasedClient.getClientId(instance.toInetAddr(), ephemeral); if (!clientManager.contains(clientId)) { Loggers.SRV_LOG.warn(\"remove instance from non-exist client: {}\", clientId); return; } Service service = getService(namespaceId, serviceName, ephemeral); // 注销实例，如果是临时实例，EphemeralClientOperationServiceImpl，如果是持久化实例，PersistentClientOperationServiceImpl clientOperationService.deregisterInstance(service, instance, clientId); } ","date":"2023-08-20","objectID":"/ooooo-notes/03-%E6%B3%A8%E9%94%80%E5%AE%9E%E4%BE%8B/:2:0","tags":["nacos","source code","源码分析 nacos 系列"],"title":"03 注销实例","uri":"/ooooo-notes/03-%E6%B3%A8%E9%94%80%E5%AE%9E%E4%BE%8B/"},{"categories":null,"content":"临时实例注销 源码位置: com.alibaba.nacos.naming.core.v2.service.impl.EphemeralClientOperationServiceImpl#deregisterInstance @Override public void deregisterInstance(Service service, Instance instance, String clientId) { // 判断 service 是否存在 if (!ServiceManager.getInstance().containSingleton(service)) { Loggers.SRV_LOG.warn(\"remove instance from non-exist service: {}\", service); return; } Service singleton = ServiceManager.getInstance().getSingleton(service); Client client = clientManager.getClient(clientId); if (!clientIsLegal(client, clientId)) { return; } // 移除内存中的 instance 对象，这里会发布 ClientChangedEvent 事件，这个很重要 InstancePublishInfo removedInstance = client.removeServiceInstance(singleton); client.setLastUpdatedTime(); client.recalculateRevision(); if (null != removedInstance) { // 发布 ClientDeregisterServiceEvent 事件 NotifyCenter.publishEvent(new ClientOperationEvent.ClientDeregisterServiceEvent(singleton, clientId)); // 发布 InstanceMetadataEvent 事件 NotifyCenter.publishEvent( new MetadataEvent.InstanceMetadataEvent(singleton, removedInstance.getMetadataId(), true)); } } 源码位置: com.alibaba.nacos.naming.consistency.ephemeral.distro.v2.DistroClientDataProcessor#syncToAllServer // DistroClientDataProcessor 接受 ClientChangedEvent, 负责同步数据给其他节点 private void syncToAllServer(ClientEvent event) { Client client = event.getClient(); // Only ephemeral data sync by Distro, persist client should sync by raft. if (null == client || !client.isEphemeral() || !clientManager.isResponsibleClient(client)) { return; } if (event instanceof ClientEvent.ClientDisconnectEvent) { DistroKey distroKey = new DistroKey(client.getClientId(), TYPE); distroProtocol.sync(distroKey, DataOperation.DELETE); } else if (event instanceof ClientEvent.ClientChangedEvent) { DistroKey distroKey = new DistroKey(client.getClientId(), TYPE); distroProtocol.sync(distroKey, DataOperation.CHANGE); } } 源码位置: com.alibaba.nacos.naming.core.v2.index.ClientServiceIndexesManager#handleClientOperation // ClientServiceIndexesManager 会监听 ClientDeregisterServiceEvent 事件 private void handleClientOperation(ClientOperationEvent event) { Service service = event.getService(); String clientId = event.getClientId(); if (event instanceof ClientOperationEvent.ClientRegisterServiceEvent) { addPublisherIndexes(service, clientId); } else if (event instanceof ClientOperationEvent.ClientDeregisterServiceEvent) { // 移除 service 的 clientId removePublisherIndexes(service, clientId); } else if (event instanceof ClientOperationEvent.ClientSubscribeServiceEvent) { addSubscriberIndexes(service, clientId); } else if (event instanceof ClientOperationEvent.ClientUnsubscribeServiceEvent) { removeSubscriberIndexes(service, clientId); } } private void removePublisherIndexes(Service service, String clientId) { publisherIndexes.computeIfPresent(service, (s, ids) -\u003e { ids.remove(clientId); // 发布 ServiceChangedEvent 事件 NotifyCenter.publishEvent(new ServiceEvent.ServiceChangedEvent(service, true)); return ids.isEmpty() ? null : ids; }); } 源码位置: com.alibaba.nacos.naming.push.v2.NamingSubscriberServiceV2Impl#onEvent // NamingSubscriberServiceV2Impl 会监听 ServiceChangedEvent 事件 @Override public void onEvent(Event event) { if (event instanceof ServiceEvent.ServiceChangedEvent) { // If service changed, push to all subscribers. // 注销 instance， 必须推送给所有的订阅者 ServiceEvent.ServiceChangedEvent serviceChangedEvent = (ServiceEvent.ServiceChangedEvent) event; Service service = serviceChangedEvent.getService(); delayTaskEngine.addTask(service, new PushDelayTask(service, PushConfig.getInstance().getPushTaskDelay())); MetricsMonitor.incrementServiceChangeCount(service.getNamespace(), service.getGroup(), service.getName()); } else if (event instanceof ServiceEvent.ServiceSubscribedEvent) { // If service is subscribed by one client, only push this client. ServiceEvent.ServiceSubscribedEvent subscribedEvent = (ServiceEvent.ServiceSubscribedEvent) event; Service service = subscribedEvent.getService(); delayTaskEngine.addTask(service, new PushDelayTask(service,","date":"2023-08-20","objectID":"/ooooo-notes/03-%E6%B3%A8%E9%94%80%E5%AE%9E%E4%BE%8B/:3:0","tags":["nacos","source code","源码分析 nacos 系列"],"title":"03 注销实例","uri":"/ooooo-notes/03-%E6%B3%A8%E9%94%80%E5%AE%9E%E4%BE%8B/"},{"categories":null,"content":"持久化实例注销 源码位置: com.alibaba.nacos.naming.core.v2.service.impl.PersistentClientOperationServiceImpl#deregisterInstance @Override public void deregisterInstance(Service service, Instance instance, String clientId) { final InstanceStoreRequest request = new InstanceStoreRequest(); request.setService(service); request.setInstance(instance); request.setClientId(clientId); // 注意这里的 group，在构造函数中进行注册对应的 processor final WriteRequest writeRequest = WriteRequest.newBuilder().setGroup(group()) .setData(ByteString.copyFrom(serializer.serialize(request))).setOperation(DataOperation.DELETE.name()) .build(); try { // 由 CPProtcol 写入请求到本地，然后同步到其他节点，最后应用状态机 protocol.write(writeRequest); Loggers.RAFT.info(\"Client unregistered. service={}, clientId={}, instance={}\", service, instance, clientId); } catch (Exception e) { throw new NacosRuntimeException(NacosException.SERVER_ERROR, e); } } // 构造函数中注册 requestProcessor, 这个可以分组的 public PersistentClientOperationServiceImpl(final PersistentIpPortClientManager clientManager) { this.clientManager = clientManager; this.protocol = ApplicationUtils.getBean(ProtocolManager.class).getCpProtocol(); this.protocol.addRequestProcessors(Collections.singletonList(this)); } // 处理状态机 @Override public Response onApply(WriteRequest request) { final Lock lock = readLock; lock.lock(); try { final InstanceStoreRequest instanceRequest = serializer.deserialize(request.getData().toByteArray()); final DataOperation operation = DataOperation.valueOf(request.getOperation()); switch (operation) { case ADD: onInstanceRegister(instanceRequest.service, instanceRequest.instance, instanceRequest.getClientId()); break; case DELETE: // 注销实例 onInstanceDeregister(instanceRequest.service, instanceRequest.getClientId()); break; case CHANGE: if (instanceAndServiceExist(instanceRequest)) { onInstanceRegister(instanceRequest.service, instanceRequest.instance, instanceRequest.getClientId()); } break; default: return Response.newBuilder().setSuccess(false).setErrMsg(\"unsupport operation : \" + operation) .build(); } return Response.newBuilder().setSuccess(true).build(); } catch (Exception e) { Loggers.RAFT.warn(\"Persistent client operation failed. \", e); return Response.newBuilder().setSuccess(false) .setErrMsg(\"Persistent client operation failed. \" + e.getMessage()).build(); } finally { lock.unlock(); } } // 注销实例， 这里的逻辑和临时实例注销的逻辑是一样的，所以就不继续解析了 private void onInstanceDeregister(Service service, String clientId) { Service singleton = ServiceManager.getInstance().getSingleton(service); Client client = clientManager.getClient(clientId); if (client == null) { Loggers.RAFT.warn(\"client not exist onInstanceDeregister, clientId : {} \", clientId); return; } // 移除内存的 instance，发布 ClientChangedEvent 事件 client.removeServiceInstance(singleton); client.setLastUpdatedTime(); if (client.getAllPublishedService().isEmpty()) { clientManager.clientDisconnected(clientId); } // 发布 ClientDeregisterServiceEvent 事件 NotifyCenter.publishEvent(new ClientOperationEvent.ClientDeregisterServiceEvent(singleton, clientId)); } ","date":"2023-08-20","objectID":"/ooooo-notes/03-%E6%B3%A8%E9%94%80%E5%AE%9E%E4%BE%8B/:4:0","tags":["nacos","source code","源码分析 nacos 系列"],"title":"03 注销实例","uri":"/ooooo-notes/03-%E6%B3%A8%E9%94%80%E5%AE%9E%E4%BE%8B/"},{"categories":null,"content":"测试类 com.alibaba.nacos.test.naming.CPInstancesAPI_ITCase#registerInstance_ephemeral_true ","date":"2023-08-20","objectID":"/ooooo-notes/03-%E6%B3%A8%E9%94%80%E5%AE%9E%E4%BE%8B/:5:0","tags":["nacos","source code","源码分析 nacos 系列"],"title":"03 注销实例","uri":"/ooooo-notes/03-%E6%B3%A8%E9%94%80%E5%AE%9E%E4%BE%8B/"},{"categories":null,"content":" nacos 基于 2.2.4 版本 ","date":"2023-08-19","objectID":"/ooooo-notes/02-%E6%B3%A8%E5%86%8C%E5%AE%9E%E4%BE%8B/:0:0","tags":["nacos","source code","源码分析 nacos 系列"],"title":"02 注册实例","uri":"/ooooo-notes/02-%E6%B3%A8%E5%86%8C%E5%AE%9E%E4%BE%8B/"},{"categories":null,"content":"注册实例的 curl curl --location 'http://localhost:8848/nacos/v2/ns/instance' \\ --header 'Content-Type: application/x-www-form-urlencoded' \\ --data-urlencode 'serviceName=test' \\ --data-urlencode 'ip=1.2.3.4' \\ --data-urlencode 'port=80' ","date":"2023-08-19","objectID":"/ooooo-notes/02-%E6%B3%A8%E5%86%8C%E5%AE%9E%E4%BE%8B/:1:0","tags":["nacos","source code","源码分析 nacos 系列"],"title":"02 注册实例","uri":"/ooooo-notes/02-%E6%B3%A8%E5%86%8C%E5%AE%9E%E4%BE%8B/"},{"categories":null,"content":"注册实例的主流程 源码位置: com.alibaba.nacos.naming.controllers.v2.InstanceControllerV2#register public Result\u003cString\u003e register(InstanceForm instanceForm) throws NacosException { // check param instanceForm.validate(); checkWeight(instanceForm.getWeight()); // build instance Instance instance = buildInstance(instanceForm); // 注册实例 instanceServiceV2.registerInstance(instanceForm.getNamespaceId(), buildCompositeServiceName(instanceForm), instance); // 发布 traceEvent NotifyCenter.publishEvent(new RegisterInstanceTraceEvent(System.currentTimeMillis(), \"\", false, instanceForm.getNamespaceId(), instanceForm.getGroupName(), instanceForm.getServiceName(), instance.getIp(), instance.getPort())); return Result.success(\"ok\"); } 源码位置: com.alibaba.nacos.naming.core.InstanceOperatorClientImpl#registerInstance public void registerInstance(String namespaceId, String serviceName, Instance instance) throws NacosException { NamingUtils.checkInstanceIsLegal(instance); boolean ephemeral = instance.isEphemeral(); String clientId = IpPortBasedClient.getClientId(instance.toInetAddr(), ephemeral); // 创建 client createIpPortClientIfAbsent(clientId); // 构建 service 对象，在 nacos2.0 中，临时属性在 service 上, instance 的临时属性已经没有了 Service service = getService(namespaceId, serviceName, ephemeral); // 具体实现类负责注册，如果是临时实例，EphemeralClientOperationServiceImpl，如果是持久化实例，PersistentClientOperationServiceImpl clientOperationService.registerInstance(service, instance, clientId); } ","date":"2023-08-19","objectID":"/ooooo-notes/02-%E6%B3%A8%E5%86%8C%E5%AE%9E%E4%BE%8B/:2:0","tags":["nacos","source code","源码分析 nacos 系列"],"title":"02 注册实例","uri":"/ooooo-notes/02-%E6%B3%A8%E5%86%8C%E5%AE%9E%E4%BE%8B/"},{"categories":null,"content":"临时实例注册 源码位置: com.alibaba.nacos.naming.core.v2.service.impl.EphemeralClientOperationServiceImpl#registerInstance @Override public void registerInstance(Service service, Instance instance, String clientId) throws NacosException { NamingUtils.checkInstanceIsLegal(instance); // 获得单例的 service，如果没有就会注册 Service singleton = ServiceManager.getInstance().getSingleton(service); if (!singleton.isEphemeral()) { throw new NacosRuntimeException(NacosException.INVALID_PARAM, String.format(\"Current service %s is persistent service, can't register ephemeral instance.\", singleton.getGroupedServiceName())); } // 获取 client，并检查 client Client client = clientManager.getClient(clientId); if (!clientIsLegal(client, clientId)) { return; } // InstancePublishInfo 就是 nacos 内部实例 InstancePublishInfo instanceInfo = getPublishInfo(instance); // 添加 service 和 instance，这里会发布 ClientChangedEvent 事件，非常重要 client.addServiceInstance(singleton, instanceInfo); client.setLastUpdatedTime(); client.recalculateRevision(); // 发布 ClientRegisterServiceEvent 事件 NotifyCenter.publishEvent(new ClientOperationEvent.ClientRegisterServiceEvent(singleton, clientId)); // 发布 InstanceMetadataEvent 事件 NotifyCenter .publishEvent(new MetadataEvent.InstanceMetadataEvent(singleton, instanceInfo.getMetadataId(), false)); } 源码位置: com.alibaba.nacos.naming.consistency.ephemeral.distro.v2.DistroClientDataProcessor#syncToAllServer // DistroClientDataProcessor 会监听 ClientChangedEvent 事件 private void syncToAllServer(ClientEvent event) { Client client = event.getClient(); // Only ephemeral data sync by Distro, persist client should sync by raft. if (null == client || !client.isEphemeral() || !clientManager.isResponsibleClient(client)) { return; } if (event instanceof ClientEvent.ClientDisconnectEvent) { DistroKey distroKey = new DistroKey(client.getClientId(), TYPE); distroProtocol.sync(distroKey, DataOperation.DELETE); } else if (event instanceof ClientEvent.ClientChangedEvent) { DistroKey distroKey = new DistroKey(client.getClientId(), TYPE); // 同步到其他节点 distroProtocol.sync(distroKey, DataOperation.CHANGE); } } 源码位置: com.alibaba.nacos.naming.core.v2.index.ClientServiceIndexesManager#handleClientOperation // ClientServiceIndexesManager 会监听 ClientRegisterServiceEvent 事件 private void handleClientOperation(ClientOperationEvent event) { Service service = event.getService(); String clientId = event.getClientId(); if (event instanceof ClientOperationEvent.ClientRegisterServiceEvent) { // 添加 client 的 publishIndex addPublisherIndexes(service, clientId); } else if (event instanceof ClientOperationEvent.ClientDeregisterServiceEvent) { removePublisherIndexes(service, clientId); } else if (event instanceof ClientOperationEvent.ClientSubscribeServiceEvent) { addSubscriberIndexes(service, clientId); } else if (event instanceof ClientOperationEvent.ClientUnsubscribeServiceEvent) { removeSubscriberIndexes(service, clientId); } } private void addPublisherIndexes(Service service, String clientId) { // service 和 clientId 是一对多的关系 publisherIndexes.computeIfAbsent(service, key -\u003e new ConcurrentHashSet\u003c\u003e()); publisherIndexes.get(service).add(clientId); // 发布 ServiceChangedEvent 事件 NotifyCenter.publishEvent(new ServiceEvent.ServiceChangedEvent(service, true)); } 源码位置: com.alibaba.nacos.naming.push.v2.NamingSubscriberServiceV2Impl#onEvent // NamingSubscriberServiceV2Impl 监听 ServiceChangedEvent public void onEvent(Event event) { if (event instanceof ServiceEvent.ServiceChangedEvent) { // If service changed, push to all subscribers. // service 下的 instance 改变之后，要推送给所有的订阅者 ServiceEvent.ServiceChangedEvent serviceChangedEvent = (ServiceEvent.ServiceChangedEvent) event; Service service = serviceChangedEvent.getService(); delayTaskEngine.addTask(service, new PushDelayTask(service, PushConfig.getInstance().getPushTaskDelay())); MetricsMonitor.incrementServiceChangeCount(service.getNamespace(), service.getGroup(), service.getName()); } else if (event instanceof ServiceEvent.ServiceSubscribedEvent) { // If service is subscribed by one cl","date":"2023-08-19","objectID":"/ooooo-notes/02-%E6%B3%A8%E5%86%8C%E5%AE%9E%E4%BE%8B/:3:0","tags":["nacos","source code","源码分析 nacos 系列"],"title":"02 注册实例","uri":"/ooooo-notes/02-%E6%B3%A8%E5%86%8C%E5%AE%9E%E4%BE%8B/"},{"categories":null,"content":"持久化实例注册 源码位置: com.alibaba.nacos.naming.core.v2.service.impl.PersistentClientOperationServiceImpl#registerInstance @Override public void registerInstance(Service service, Instance instance, String clientId) { // 和临时实例注册一样，获取单例的 service Service singleton = ServiceManager.getInstance().getSingleton(service); if (singleton.isEphemeral()) { throw new NacosRuntimeException(NacosException.INVALID_PARAM, String.format(\"Current service %s is ephemeral service, can't register persistent instance.\", singleton.getGroupedServiceName())); } // 包装为 writeRequest 对象 final InstanceStoreRequest request = new InstanceStoreRequest(); request.setService(service); request.setInstance(instance); request.setClientId(clientId); // 这里设置了 group，在构造函数中会初始化 group 的 RequestProcessor final WriteRequest writeRequest = WriteRequest.newBuilder().setGroup(group()) .setData(ByteString.copyFrom(serializer.serialize(request))).setOperation(DataOperation.ADD.name()) .build(); try { // CPProtocol 负责写请求，同步到其他的节点，然后应用状态机 protocol.write(writeRequest); Loggers.RAFT.info(\"Client registered. service={}, clientId={}, instance={}\", service, instance, clientId); } catch (Exception e) { throw new NacosRuntimeException(NacosException.SERVER_ERROR, e); } } // 构造函数中，初始化话了 public PersistentClientOperationServiceImpl(final PersistentIpPortClientManager clientManager) { this.clientManager = clientManager; this.protocol = ApplicationUtils.getBean(ProtocolManager.class).getCpProtocol(); // 自己负责来处理 apply WriteRequest this.protocol.addRequestProcessors(Collections.singletonList(this)); } // 应用 raft 的状态机 // protocol.write(writeRequest) 之后, 就会回调这个方法 @Override public Response onApply(WriteRequest request) { final Lock lock = readLock; lock.lock(); try { final InstanceStoreRequest instanceRequest = serializer.deserialize(request.getData().toByteArray()); final DataOperation operation = DataOperation.valueOf(request.getOperation()); switch (operation) { case ADD: // 处理实例注册 onInstanceRegister(instanceRequest.service, instanceRequest.instance, instanceRequest.getClientId()); break; case DELETE: onInstanceDeregister(instanceRequest.service, instanceRequest.getClientId()); break; case CHANGE: if (instanceAndServiceExist(instanceRequest)) { onInstanceRegister(instanceRequest.service, instanceRequest.instance, instanceRequest.getClientId()); } break; default: return Response.newBuilder().setSuccess(false).setErrMsg(\"unsupport operation : \" + operation) .build(); } return Response.newBuilder().setSuccess(true).build(); } catch (Exception e) { Loggers.RAFT.warn(\"Persistent client operation failed. \", e); return Response.newBuilder().setSuccess(false) .setErrMsg(\"Persistent client operation failed. \" + e.getMessage()).build(); } finally { lock.unlock(); } } // 处理实例注册, 基本和临时实例注册一样, 后面就不重复分析了 private void onInstanceRegister(Service service, Instance instance, String clientId) { // 获取 service 和 client Service singleton = ServiceManager.getInstance().getSingleton(service); if (!clientManager.contains(clientId)) { clientManager.clientConnected(clientId, new ClientAttributes()); } Client client = clientManager.getClient(clientId); InstancePublishInfo instancePublishInfo = getPublishInfo(instance); // 添加 service 和 instance，发布 ClientChangedEvent 事件 client.addServiceInstance(singleton, instancePublishInfo); client.setLastUpdatedTime(); // 发布 ClientRegisterServiceEvent 事件 NotifyCenter.publishEvent(new ClientOperationEvent.ClientRegisterServiceEvent(singleton, clientId)); } ","date":"2023-08-19","objectID":"/ooooo-notes/02-%E6%B3%A8%E5%86%8C%E5%AE%9E%E4%BE%8B/:4:0","tags":["nacos","source code","源码分析 nacos 系列"],"title":"02 注册实例","uri":"/ooooo-notes/02-%E6%B3%A8%E5%86%8C%E5%AE%9E%E4%BE%8B/"},{"categories":null,"content":"测试类 com.alibaba.nacos.test.naming.CPInstancesAPI_ITCase#registerInstance_ephemeral_true ","date":"2023-08-19","objectID":"/ooooo-notes/02-%E6%B3%A8%E5%86%8C%E5%AE%9E%E4%BE%8B/:5:0","tags":["nacos","source code","源码分析 nacos 系列"],"title":"02 注册实例","uri":"/ooooo-notes/02-%E6%B3%A8%E5%86%8C%E5%AE%9E%E4%BE%8B/"},{"categories":null,"content":" nacos 基于 2.2.4 版本 ","date":"2023-08-18","objectID":"/ooooo-notes/01-%E6%90%AD%E5%BB%BA-nacos-%E6%BA%90%E7%A0%81%E8%B0%83%E8%AF%95%E7%8E%AF%E5%A2%83/:0:0","tags":["nacos","source code","源码分析 nacos 系列"],"title":"01 搭建 nacos 源码调试环境","uri":"/ooooo-notes/01-%E6%90%AD%E5%BB%BA-nacos-%E6%BA%90%E7%A0%81%E8%B0%83%E8%AF%95%E7%8E%AF%E5%A2%83/"},{"categories":null,"content":"下载源码和编译 git clone git@github.com:alibaba/nacos.git mvn clean install -U -DskipTests ","date":"2023-08-18","objectID":"/ooooo-notes/01-%E6%90%AD%E5%BB%BA-nacos-%E6%BA%90%E7%A0%81%E8%B0%83%E8%AF%95%E7%8E%AF%E5%A2%83/:1:0","tags":["nacos","source code","源码分析 nacos 系列"],"title":"01 搭建 nacos 源码调试环境","uri":"/ooooo-notes/01-%E6%90%AD%E5%BB%BA-nacos-%E6%BA%90%E7%A0%81%E8%B0%83%E8%AF%95%E7%8E%AF%E5%A2%83/"},{"categories":null,"content":"配置环境 参考 startup.sh 文件，添加相应的 jvm 和 program 的参数。 添加 jvm 参数，-Dnacos.standalone=true, 单机启动 添加 jvm 参数，-Dnacos.home=/Users/ooooo/Code/Demo/nacos/distribution, 集群启动 添加 program 参数，--spring.config.additional-location=/Users/ooooo/Code/Demo/nacos/distribution/conf/application.properties 配置 cluster.conf，添加自己机器的 ip 配置 application.properties, 添加数据库相关配置，脚本位置在 /Users/ooooo/Code/Demo/nacos/distribution/conf/mysql-schema.sql 相关截图如下： run cluster-config application-properties ","date":"2023-08-18","objectID":"/ooooo-notes/01-%E6%90%AD%E5%BB%BA-nacos-%E6%BA%90%E7%A0%81%E8%B0%83%E8%AF%95%E7%8E%AF%E5%A2%83/:2:0","tags":["nacos","source code","源码分析 nacos 系列"],"title":"01 搭建 nacos 源码调试环境","uri":"/ooooo-notes/01-%E6%90%AD%E5%BB%BA-nacos-%E6%BA%90%E7%A0%81%E8%B0%83%E8%AF%95%E7%8E%AF%E5%A2%83/"},{"categories":null,"content":"启动 log ","date":"2023-08-18","objectID":"/ooooo-notes/01-%E6%90%AD%E5%BB%BA-nacos-%E6%BA%90%E7%A0%81%E8%B0%83%E8%AF%95%E7%8E%AF%E5%A2%83/:3:0","tags":["nacos","source code","源码分析 nacos 系列"],"title":"01 搭建 nacos 源码调试环境","uri":"/ooooo-notes/01-%E6%90%AD%E5%BB%BA-nacos-%E6%BA%90%E7%A0%81%E8%B0%83%E8%AF%95%E7%8E%AF%E5%A2%83/"},{"categories":null,"content":"1. goreleaser 的简单说明 # install goreleaser brew install goreleaser # init goreleaser, create .goreleaser.yml goreleaser init # available commands goreleaser build --clean goreleaser release --snapshot --clean ","date":"2023-08-01","objectID":"/ooooo-notes/github-%E4%B8%8A%E4%BD%BF%E7%94%A8-goreleaser/:1:0","tags":["github","tools","goreleaser"],"title":"github 上使用 goreleaser","uri":"/ooooo-notes/github-%E4%B8%8A%E4%BD%BF%E7%94%A8-goreleaser/"},{"categories":null,"content":"2. .goreleaser.yml 示例文件 # This is an example .goreleaser.yml file with some sensible defaults. # Make sure to check the documentation at https://goreleaser.com before: hooks: # You may remove this if you don't use go modules. - go mod tidy # you may remove this if you don't need go generate # - go generate ./... builds: - id: http-tunnel-client binary: http-tunnel-client main: ./cmd/http-tunnel-client env: - CGO_ENABLED=0 goos: - linux - windows - darwin goarch: - amd64 - arm64 - id: http-tunnel-server binary: http-tunnel-server main: ./cmd/http-tunnel-server env: - CGO_ENABLED=0 goos: - linux - windows - darwin goarch: - amd64 - arm64 archives: - format: tar.gz # this name template makes the OS and Arch compatible with the results of uname. name_template: \u003e- {{ .ProjectName }}_ {{- .Version }}_ {{- title .Os }}_ {{- .Arch }} {{- if .Arm }}v{{ .Arm }}{{ end }} # use zip for windows archives format_overrides: - goos: windows format: zip checksum: name_template: 'checksums.txt' snapshot: name_template: \"{{ incpatch .Version }}-next\" changelog: sort: asc filters: exclude: - '^docs:' - '^test:' # The lines beneath this are called `modelines`. See `:help modeline` # Feel free to remove those if you don't want/use them. # yaml-language-server: $schema=https://goreleaser.com/static/schema.json # vim: set ts=2 sw=2 tw=0 fo=cnqoj ","date":"2023-08-01","objectID":"/ooooo-notes/github-%E4%B8%8A%E4%BD%BF%E7%94%A8-goreleaser/:2:0","tags":["github","tools","goreleaser"],"title":"github 上使用 goreleaser","uri":"/ooooo-notes/github-%E4%B8%8A%E4%BD%BF%E7%94%A8-goreleaser/"},{"categories":null,"content":"3. github action 配置 文件路径：.github/workflows/goreleaser.yml name: goreleaser on: push: tags: - '*.*.*' # Trigger the workflow by mannually workflow_dispatch: jobs: goreleaser: runs-on: ubuntu-latest steps: - name: Checkout uses: actions/checkout@v3 with: fetch-depth: 0 - name: Set up Go uses: actions/setup-go@v4 with: go-version: '1.20' - name: Run GoReleaser uses: goreleaser/goreleaser-action@v4 with: version: latest args: release --clean env: GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }} ","date":"2023-08-01","objectID":"/ooooo-notes/github-%E4%B8%8A%E4%BD%BF%E7%94%A8-goreleaser/:3:0","tags":["github","tools","goreleaser"],"title":"github 上使用 goreleaser","uri":"/ooooo-notes/github-%E4%B8%8A%E4%BD%BF%E7%94%A8-goreleaser/"},{"categories":null,"content":"4. github token read write permisson ","date":"2023-08-01","objectID":"/ooooo-notes/github-%E4%B8%8A%E4%BD%BF%E7%94%A8-goreleaser/:4:0","tags":["github","tools","goreleaser"],"title":"github 上使用 goreleaser","uri":"/ooooo-notes/github-%E4%B8%8A%E4%BD%BF%E7%94%A8-goreleaser/"},{"categories":null,"content":"1. 搭建 kafka 环境 这里使用 docker 来搭建。 docker-compose.yml 配置如下，客户端端口:9094 version: \"3\" services: kafka: image: 'bitnami/kafka:latest' ports: - '9092:9092' - '9094:9094' environment: - KAFKA_CFG_NODE_ID=0 - KAFKA_CFG_PROCESS_ROLES=controller,broker - KAFKA_CFG_LISTENERS=PLAINTEXT://:9092,CONTROLLER://:9093,EXTERNAL://:9094 - KAFKA_CFG_ADVERTISED_LISTENERS=PLAINTEXT://kafka:9092,EXTERNAL://localhost:9094 - KAFKA_CFG_LISTENER_SECURITY_PROTOCOL_MAP=CONTROLLER:PLAINTEXT,EXTERNAL:SASL_PLAINTEXT,PLAINTEXT:PLAINTEXT - KAFKA_CFG_CONTROLLER_QUORUM_VOTERS=0@kafka:9093 - KAFKA_CFG_CONTROLLER_LISTENER_NAMES=CONTROLLER - KAFKA_CLIENT_USERS=user - KAFKA_CLIENT_PASSWORDS=password - KAFKA_CLIENT_LISTENER_NAME=SASL_PLAINTEXT 启动kafka docker-compose up -d ","date":"2023-08-01","objectID":"/ooooo-notes/kafka-%E7%9A%84-sasl-%E8%AE%A4%E8%AF%81/:1:0","tags":["kafka"],"title":"kafka 的 SASL 认证","uri":"/ooooo-notes/kafka-%E7%9A%84-sasl-%E8%AE%A4%E8%AF%81/"},{"categories":null,"content":"2. 客户端连接配置 spring: kafka: bootstrap-servers: localhost:9094 properties: security.protocol: SASL_PLAINTEXT sasl.jaas.config: org.apache.kafka.common.security.scram.ScramLoginModule required username='user' password='password'; sasl.mechanism: SCRAM-SHA-512 ","date":"2023-08-01","objectID":"/ooooo-notes/kafka-%E7%9A%84-sasl-%E8%AE%A4%E8%AF%81/:2:0","tags":["kafka"],"title":"kafka 的 SASL 认证","uri":"/ooooo-notes/kafka-%E7%9A%84-sasl-%E8%AE%A4%E8%AF%81/"},{"categories":null,"content":"3. 参考 docker kafka kafka sasl ","date":"2023-08-01","objectID":"/ooooo-notes/kafka-%E7%9A%84-sasl-%E8%AE%A4%E8%AF%81/:3:0","tags":["kafka"],"title":"kafka 的 SASL 认证","uri":"/ooooo-notes/kafka-%E7%9A%84-sasl-%E8%AE%A4%E8%AF%81/"},{"categories":null,"content":" websocket ","date":"2023-07-31","objectID":"/ooooo-notes/protocols/:0:0","tags":["protocol"],"title":"protocols","uri":"/ooooo-notes/protocols/"},{"categories":null,"content":"1. HttpHelloWorldServerHandler 为啥需要使用 SimpleChannelInboundHandler ? HttpObject 的子类有 LastHttpContent, HttpContent, HttpData， 它需要手动调用 release()。 ","date":"2023-07-30","objectID":"/ooooo-notes/%E4%BD%BF%E7%94%A8-netty-%E7%9A%84%E6%B3%A8%E6%84%8F%E7%82%B9/:1:0","tags":["netty"],"title":"使用 netty 的注意点","uri":"/ooooo-notes/%E4%BD%BF%E7%94%A8-netty-%E7%9A%84%E6%B3%A8%E6%84%8F%E7%82%B9/"},{"categories":null,"content":"1. 代码 在自定义封装 MQ 时，要注意 producer 和 consumer 的初始化时机，否则会出现 consumer 占用 consumerQueue 的情况 @Slf4j public class RocketMQHandler extends AbstractMQHandler { private final RocketMQConfig config; private final Supplier\u003cDefaultMQProducer\u003e producer; private final Supplier\u003cDefaultLitePullConsumer\u003e consumer; public RocketMQHandler(RocketMQProperties properties, RocketMQConfig config) { this.config = config; // 这里要延迟初始化，否则启动 consumer 占用 consumerQueue this.producer = SingletonSupplier.of(() -\u003e createProducer(properties, config)); this.consumer = SingletonSupplier.of(() -\u003e createConsumer(properties, config)); } @SneakyThrows @Override public void send(String message) { Message m = new Message(config.getDestinationName(), message.getBytes(StandardCharsets.UTF_8)); if (log.isTraceEnabled()) { log.trace(\"{} send message: {}\", RocketMQHandler.this.getClass().getSimpleName(), message); } producer.get().send(m); } @Override public void receive(MQListener listener) { ConsumerTask task = new ConsumerTask(listener); executorService.schedule(task, PULL_PERIOD, TimeUnit.MILLISECONDS); } @AllArgsConstructor private class ConsumerTask implements Runnable { private MQListener listener; @Override public void run() { List\u003cMessageExt\u003e msgs = consumer.get().poll(PULL_PERIOD); if (CollectionUtils.isNotEmpty(msgs)) { msgs.forEach(m -\u003e { String message = new String(m.getBody(), StandardCharsets.UTF_8); if (log.isTraceEnabled()) { log.trace(\"{} receive message: {}\", RocketMQHandler.this.getClass().getSimpleName(), message); } listener.onMessage(message); }); } executorService.schedule(this, PULL_PERIOD, TimeUnit.MILLISECONDS); } } @SneakyThrows private DefaultMQProducer createProducer(RocketMQProperties properties, RocketMQConfig config) { DefaultMQProducer producer = new DefaultMQProducer(config.getProducerGroup()); producer.setNamesrvAddr(properties.getNamesrvAddr()); producer.start(); return producer; } @SneakyThrows private DefaultLitePullConsumer createConsumer(RocketMQProperties properties, RocketMQConfig config) { DefaultLitePullConsumer consumer = new DefaultLitePullConsumer(config.getConsumerGroup()); consumer.setNamesrvAddr(properties.getNamesrvAddr()); switch (config.getConsumeMode()) { case P2P: consumer.setMessageModel(MessageModel.CLUSTERING); break; case BROADCAST: consumer.setMessageModel(MessageModel.BROADCASTING); break; } consumer.subscribe(config.getDestinationName(), \"*\"); consumer.start(); return consumer; } } ","date":"2023-07-07","objectID":"/ooooo-notes/rocketmq-%E7%9A%84-litepullconsumer-%E4%BD%BF%E7%94%A8/:1:0","tags":["spring","rocketmq"],"title":"rocketmq 的 LitePullConsumer 使用","uri":"/ooooo-notes/rocketmq-%E7%9A%84-litepullconsumer-%E4%BD%BF%E7%94%A8/"},{"categories":null,"content":"1. 配置 dubbo: application: parameters: registry-type: service registries: a: address: nacos://172.16.1.104:7848 group: DUBBO_SERVICE_GROUP parameters: namespace: a b: address: nacos://172.16.1.104:7848 group: DUBBO_SERVICE_GROUP parameters: namespace: b ","date":"2023-07-03","objectID":"/ooooo-notes/dubbo3-%E5%A4%9A%E6%B3%A8%E5%86%8C%E4%B8%AD%E5%BF%83%E7%9A%84%E5%B0%8Fbug/:1:0","tags":["dubbo"],"title":"dubbo3 多注册中心的小 bug","uri":"/ooooo-notes/dubbo3-%E5%A4%9A%E6%B3%A8%E5%86%8C%E4%B8%AD%E5%BF%83%E7%9A%84%E5%B0%8Fbug/"},{"categories":null,"content":"2. 问题 只会注册到一个 namespace 中 ","date":"2023-07-03","objectID":"/ooooo-notes/dubbo3-%E5%A4%9A%E6%B3%A8%E5%86%8C%E4%B8%AD%E5%BF%83%E7%9A%84%E5%B0%8Fbug/:2:0","tags":["dubbo"],"title":"dubbo3 多注册中心的小 bug","uri":"/ooooo-notes/dubbo3-%E5%A4%9A%E6%B3%A8%E5%86%8C%E4%B8%AD%E5%BF%83%E7%9A%84%E5%B0%8Fbug/"},{"categories":null,"content":"3. github dubbo issue ","date":"2023-07-03","objectID":"/ooooo-notes/dubbo3-%E5%A4%9A%E6%B3%A8%E5%86%8C%E4%B8%AD%E5%BF%83%E7%9A%84%E5%B0%8Fbug/:3:0","tags":["dubbo"],"title":"dubbo3 多注册中心的小 bug","uri":"/ooooo-notes/dubbo3-%E5%A4%9A%E6%B3%A8%E5%86%8C%E4%B8%AD%E5%BF%83%E7%9A%84%E5%B0%8Fbug/"},{"categories":null,"content":"1. 问题 在真实的使用过程中，可能不同的 mapper 接口使用的 sqlSessionFactory 不一样。就比如下面这个例子。 // 这个注解虽然可以指定 sqlSessionFactory, 但是最终使用的 configuration 对象是同一份。 @MapperScan(\"com.ooooo.**.mapper1\") @MapperScan(\"com.ooooo.**.mapper2\") public class DemoApplication { public static void main(String[] args) { SpringApplication.run(DemoApplication.class, args); } } ","date":"2023-06-08","objectID":"/ooooo-notes/mybatis-plus-%E8%87%AA%E5%AE%9A%E4%B9%89-mapper/:1:0","tags":["spring","mybatis"],"title":"mybatis-plus 自定义 mapper","uri":"/ooooo-notes/mybatis-plus-%E8%87%AA%E5%AE%9A%E4%B9%89-mapper/"},{"categories":null,"content":"2. 解决方式 可以使用 MapperFactoryBean 来扩展，下面我给出相应的示例代码。 @Configuration public class ComponentConfigMybatiPlusConfiguration { @Getter public static SqlSessionFactory sqlSessionFactory; @Bean public MapperFactoryBean\u003cComponentTreeMapper\u003e componentTreeMapper() { return createMapper(ComponentTreeMapper.class); } private \u003cT\u003e MapperFactoryBean\u003cT\u003e createMapper(Class\u003cT\u003e clazz) { MapperFactoryBean\u003cT\u003e factoryBean = new MapperFactoryBean\u003c\u003e(clazz); factoryBean.setSqlSessionFactory(sqlSessionFactory()); return factoryBean; } private synchronized SqlSessionFactory sqlSessionFactory() { if (sqlSessionFactory != null) { return sqlSessionFactory; } // 使用默认的dataSource DataSource dataSource = SpringUtil.getBean(DataSource.class); Environment environment = new Environment(COMPONENT_CONFIG, new SpringManagedTransactionFactory(), dataSource); // build configuration MybatisConfiguration configuration = new MybatisConfiguration(); configuration.setEnvironment(environment); configuration.setCacheEnabled(false); configuration.setLocalCacheScope(LocalCacheScope.STATEMENT); setInterceptors(configuration); setMapperLocations(configuration, new String[]{\"classpath*:/mapper/**/*.xml\"}); setGlobalConfig(configuration); // factory sqlSessionFactory = new MybatisSqlSessionFactoryBuilder().build(configuration); return sqlSessionFactory; } private void setInterceptors(MybatisConfiguration configuration) { MybatisPlusInterceptor mybatisPlusInterceptor = new MybatisPlusInterceptor(); mybatisPlusInterceptor.addInnerInterceptor(new PaginationInnerInterceptor()); configuration.addInterceptor(mybatisPlusInterceptor); } private void setMapperLocations(MybatisConfiguration configuration, String[] mapperLocations) { ResourcePatternResolver resourceResolver = new PathMatchingResourcePatternResolver(); for (String mapperLocation : mapperLocations) { try { Resource[] resources = resourceResolver.getResources(mapperLocation); for (Resource resource : resources) { if (resource.exists()) { XMLMapperBuilder xmlMapperBuilder = new XMLMapperBuilder(resource.getInputStream(), configuration, resource.toString(), configuration.getSqlFragments()); xmlMapperBuilder.parse(); } } } catch (IOException ignored) { } } } private void setGlobalConfig(MybatisConfiguration configuration) { GlobalConfig globalConfig = GlobalConfigUtils.getGlobalConfig(configuration); // MetaObjectHandler globalConfig.setMetaObjectHandler(new ComponentMetaObjectHandler()); } } ","date":"2023-06-08","objectID":"/ooooo-notes/mybatis-plus-%E8%87%AA%E5%AE%9A%E4%B9%89-mapper/:2:0","tags":["spring","mybatis"],"title":"mybatis-plus 自定义 mapper","uri":"/ooooo-notes/mybatis-plus-%E8%87%AA%E5%AE%9A%E4%B9%89-mapper/"},{"categories":null,"content":"手动创建 Mapper // 用法 DataSource dataSource = DBUtil.buildDataSource(dbProperties); SqlSession sqlSession = DBUtil.buildSqlSession(dbProperties.getName(), dataSource, MockInfoMapper.class); mockInfoMapper = sqlSession.getMapper(MockInfoMapper.class); // 工具类 public class DBUtil { public static DataSource buildDataSource(AbstractDBProperties dbProperties) { HikariConfig config = new HikariConfig(); config.setPoolName(\"pool-\" + dbProperties.getName()); config.setDriverClassName(dbProperties.getDriverClassName()); config.setJdbcUrl(dbProperties.getUrl()); config.setUsername(dbProperties.getUsername()); config.setPassword(dbProperties.getPassword()); return new HikariDataSource(config); } public static SqlSessionTemplate buildSqlSession(String id, DataSource dataSource, Class\u003c?\u003e... mapperClazz) { Environment environment = new Environment(id, new SpringManagedTransactionFactory(), dataSource); MybatisConfiguration configuration = new MybatisConfiguration(environment); if (ArrayUtil.isNotEmpty(mapperClazz)) { for (Class\u003c?\u003e clazz : mapperClazz) { configuration.addMapper(clazz); } } configuration.addInterceptor(buildInterceptor()); SqlSessionFactory sqlSessionFactory = new MybatisSqlSessionFactoryBuilder().build(configuration); return new SqlSessionTemplate(sqlSessionFactory); } private static MybatisPlusInterceptor buildInterceptor() { MybatisPlusInterceptor interceptor = new MybatisPlusInterceptor(); interceptor.addInnerInterceptor(new PaginationInnerInterceptor()); return interceptor; } } ","date":"2023-06-08","objectID":"/ooooo-notes/mybatis-plus-%E8%87%AA%E5%AE%9A%E4%B9%89-mapper/:3:0","tags":["spring","mybatis"],"title":"mybatis-plus 自定义 mapper","uri":"/ooooo-notes/mybatis-plus-%E8%87%AA%E5%AE%9A%E4%B9%89-mapper/"},{"categories":null,"content":"1. 在 docker 上安装 harbor # 下载harbor wget https://github.com/goharbor/harbor/releases/download/v2.8.1/harbor-offline-installer-v2.8.1.tgz # 生成CA秘钥 openssl genrsa -out ca.key 4096 # 生成CA证书 openssl req -x509 -new -nodes -sha512 -days 3650 \\ -subj \"/C=CN/ST=Beijing/L=Beijing/O=example/OU=Personal/CN=yourdomain.com\" \\ -key ca.key \\ -out ca.crt # 生成秘钥 openssl genrsa -out yourdomain.com.key 4096 # 生成证书请求 openssl req -sha512 -new \\ -subj \"/C=CN/ST=Beijing/L=Beijing/O=example/OU=Personal/CN=yourdomain.com\" \\ -key yourdomain.com.key \\ -out yourdomain.com.csr # 生成证书 cat \u003e v3.ext \u003c\u003c-EOF authorityKeyIdentifier=keyid,issuer basicConstraints=CA:FALSE keyUsage = digitalSignature, nonRepudiation, keyEncipherment, dataEncipherment extendedKeyUsage = serverAuth subjectAltName = @alt_names [alt_names] DNS.1=yourdomain.com DNS.2=yourdomain DNS.3=hostname EOF openssl x509 -req -sha512 -days 3650 \\ -extfile v3.ext \\ -CA ca.crt -CAkey ca.key -CAcreateserial \\ -in yourdomain.com.csr \\ -out yourdomain.com.crt # 复制到harbor中， /data/cert/ 是harbor的证书目录 cp yourdomain.com.crt /data/cert/ cp yourdomain.com.key /data/cert/ # 转换为cert格式, 给docker使用 openssl x509 -inform PEM -in yourdomain.com.crt -out yourdomain.com.cert # 复制到docker中，这里是双向tls cp yourdomain.com.cert /etc/docker/certs.d/yourdomain.com/ cp yourdomain.com.key /etc/docker/certs.d/yourdomain.com/ cp ca.crt /etc/docker/certs.d/yourdomain.com/ # 重启docker,加载证书 systemctl restart docker # 执行harbor脚本，启动harbor ./prepare # 关闭 harbor docker-compose down -v # 启动 harbor docker-compose up -d # 验证docker docker login yourdomain.com harbor官方文档 ","date":"2023-06-02","objectID":"/ooooo-notes/%E5%AE%89%E8%A3%85-harbor/:1:0","tags":["docker","harbor"],"title":"安装 harbor","uri":"/ooooo-notes/%E5%AE%89%E8%A3%85-harbor/"},{"categories":null,"content":"2. containerd 配置 harbor # 复制证书到containerd mkdir /etc/containerd/yourdomain.com cp ca.crt /etc/containerd/yourdomain.com/ # 配置containerd vim /etc/containerd/config.toml #配置endpoint连接地址 [plugins.\"io.containerd.grpc.v1.cri\".registry.mirrors] [plugins.\"io.containerd.grpc.v1.cri\".registry.mirrors.\"yourdomain.com\"] endpoint = [\"https://yourdomain.com\"] #配置ca文件路径和用户名密码 [plugins.\"io.containerd.grpc.v1.cri\".registry.configs] [plugins.\"io.containerd.grpc.v1.cri\".registry.configs.\"yourdomain.com\".tls] ca_file = \"/etc/containerd/yourdomain.com/ca.crt\" [plugins.\"io.containerd.grpc.v1.cri\".registry.configs.\"yourdomain.com\".auth] username = \"admin\" password = \"Harbor12345\" 博客 ","date":"2023-06-02","objectID":"/ooooo-notes/%E5%AE%89%E8%A3%85-harbor/:2:0","tags":["docker","harbor"],"title":"安装 harbor","uri":"/ooooo-notes/%E5%AE%89%E8%A3%85-harbor/"},{"categories":null,"content":"1. nacos 的端口 nacos 的 http 端口为 8848，但是 nacos2.0 之后使用 grpc 端口, 而且是偏移量计算的，所以使用 nginx 代理就有坑。 对于 spring 来说, 可以配置多个地址 spring: cloud: nacos: server-addr: 172.168.0.101:8848,172.168.0.102:8848,172.168.0.103:8848 ","date":"2023-06-01","objectID":"/ooooo-notes/nacos-%E9%9B%86%E7%BE%A4%E7%94%A8-nginx-%E4%BB%A3%E7%90%86%E9%97%AE%E9%A2%98/:1:0","tags":["nacos","spring"],"title":"nacos 集群用 nginx 代理问题","uri":"/ooooo-notes/nacos-%E9%9B%86%E7%BE%A4%E7%94%A8-nginx-%E4%BB%A3%E7%90%86%E9%97%AE%E9%A2%98/"},{"categories":null,"content":"2. 源码 发送请求的方法 com.alibaba.nacos.common.remote.client.grpc.GrpcConnection#request @Override public Response request(Request request, long timeouts) throws NacosException { Payload grpcRequest = GrpcUtils.convert(request); // 发送请求 ListenableFuture\u003cPayload\u003e requestFuture = grpcFutureServiceStub.request(grpcRequest); Payload grpcResponse; try { grpcResponse = requestFuture.get(timeouts, TimeUnit.MILLISECONDS); } catch (Exception e) { throw new NacosException(NacosException.SERVER_ERROR, e); } return (Response) GrpcUtils.parse(grpcResponse); } 创建 grpc客户端 的方法 com.alibaba.nacos.common.remote.client.grpc.GrpcClient#connectToServer @Override public Connection connectToServer(ServerInfo serverInfo) { try { if (grpcExecutor == null) { this.grpcExecutor = createGrpcExecutor(serverInfo.getServerIp()); } // 这里就是计算端口的逻辑， serverPort 默认为 8848， rpcPortOffset 为 1000 int port = serverInfo.getServerPort() + rpcPortOffset(); ManagedChannel managedChannel = createNewManagedChannel(serverInfo.getServerIp(), port); // 新建 RequestGrpc.RequestFutureStub newChannelStubTemp = createNewChannelStub(managedChannel); if (newChannelStubTemp != null) { Response response = serverCheck(serverInfo.getServerIp(), port, newChannelStubTemp); if (response == null || !(response instanceof ServerCheckResponse)) { shuntDownChannel(managedChannel); return null; } BiRequestStreamGrpc.BiRequestStreamStub biRequestStreamStub = BiRequestStreamGrpc .newStub(newChannelStubTemp.getChannel()); GrpcConnection grpcConn = new GrpcConnection(serverInfo, grpcExecutor); grpcConn.setConnectionId(((ServerCheckResponse) response).getConnectionId()); //create stream request and bind connection event to this connection. StreamObserver\u003cPayload\u003e payloadStreamObserver = bindRequestStream(biRequestStreamStub, grpcConn); // stream observer to send response to server grpcConn.setPayloadStreamObserver(payloadStreamObserver); grpcConn.setGrpcFutureServiceStub(newChannelStubTemp); grpcConn.setChannel(managedChannel); //send a setup request. ConnectionSetupRequest conSetupRequest = new ConnectionSetupRequest(); conSetupRequest.setClientVersion(VersionUtils.getFullClientVersion()); conSetupRequest.setLabels(super.getLabels()); conSetupRequest.setAbilities(super.clientAbilities); conSetupRequest.setTenant(super.getTenant()); grpcConn.sendRequest(conSetupRequest); //wait to register connection setup Thread.sleep(100L); return grpcConn; } return null; } catch (Exception e) { LOGGER.error(\"[{}]Fail to connect to server!,error={}\", GrpcClient.this.getName(), e); } return null; } ","date":"2023-06-01","objectID":"/ooooo-notes/nacos-%E9%9B%86%E7%BE%A4%E7%94%A8-nginx-%E4%BB%A3%E7%90%86%E9%97%AE%E9%A2%98/:2:0","tags":["nacos","spring"],"title":"nacos 集群用 nginx 代理问题","uri":"/ooooo-notes/nacos-%E9%9B%86%E7%BE%A4%E7%94%A8-nginx-%E4%BB%A3%E7%90%86%E9%97%AE%E9%A2%98/"},{"categories":null,"content":"3. 参考 官方文档-端口说明 ","date":"2023-06-01","objectID":"/ooooo-notes/nacos-%E9%9B%86%E7%BE%A4%E7%94%A8-nginx-%E4%BB%A3%E7%90%86%E9%97%AE%E9%A2%98/:3:0","tags":["nacos","spring"],"title":"nacos 集群用 nginx 代理问题","uri":"/ooooo-notes/nacos-%E9%9B%86%E7%BE%A4%E7%94%A8-nginx-%E4%BB%A3%E7%90%86%E9%97%AE%E9%A2%98/"},{"categories":null,"content":" 我在 chrome 安装了 SwitchyOmega 代理插件，导致资源加载有问题。 检查 clash 代理，刷新 dns 配置，试试全局代理 可以增加 SwitchyOmega 配置 ","date":"2023-05-27","objectID":"/ooooo-notes/github-page-%E5%9B%BE%E7%89%87%E6%97%A0%E6%B3%95%E8%AE%BF%E9%97%AE/:0:0","tags":["resolution"],"title":"github page 图片无法访问","uri":"/ooooo-notes/github-page-%E5%9B%BE%E7%89%87%E6%97%A0%E6%B3%95%E8%AE%BF%E9%97%AE/"},{"categories":null,"content":" -n 以数字显示 -X 显示包体 -i 指定网卡 -w 写入文件 -c 包的个数 ","date":"2023-05-24","objectID":"/ooooo-notes/tcpdump-%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/:0:0","tags":["linux"],"title":"tcpdump 常用命令","uri":"/ooooo-notes/tcpdump-%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/"},{"categories":null,"content":"1. 指定端口 tcpdump -n -X -i any port 1234 -w 1.cap ","date":"2023-05-24","objectID":"/ooooo-notes/tcpdump-%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/:1:0","tags":["linux"],"title":"tcpdump 常用命令","uri":"/ooooo-notes/tcpdump-%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/"},{"categories":null,"content":"2. 指定主机 tcpdump -n -X -i any host 192.168.0.101 -w 1.cap ","date":"2023-05-24","objectID":"/ooooo-notes/tcpdump-%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/:2:0","tags":["linux"],"title":"tcpdump 常用命令","uri":"/ooooo-notes/tcpdump-%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/"},{"categories":null,"content":"3. 其他 # 监视指定主机和端口的数据包 tcpdump -i ens33 port 8080 and host node1 # 监视指定网络的数据包，如本机与192.168网段通信的数据包，\"-c 10\"表示只抓取10个包 tcpdump -i ens33 -c 10 net 192.168 # 抓取ping包 tcpdump -c 5 -nn -i eth0 icmp and src 192.168.100.62 ","date":"2023-05-24","objectID":"/ooooo-notes/tcpdump-%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/:3:0","tags":["linux"],"title":"tcpdump 常用命令","uri":"/ooooo-notes/tcpdump-%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/"},{"categories":null,"content":"3.参考 tcpdump说明 ","date":"2023-05-24","objectID":"/ooooo-notes/tcpdump-%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/:4:0","tags":["linux"],"title":"tcpdump 常用命令","uri":"/ooooo-notes/tcpdump-%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/"},{"categories":null,"content":"1. 检查本机的 dns 配置 # 建议不要配置 search，除非你自己明确, 可用的 dns 域名，如 8.8.8.8 # 修改后，centos系统不需要重启 NetworkManager, 重启可能被覆盖 cat /etc/resolv.conf # 重启 kubelet systemctl restart kubelet # 重启 k8s pod kubectl rollout restart deploy ","date":"2023-05-19","objectID":"/ooooo-notes/k8s-%E4%B8%AD%E7%9A%84-dns-%E9%97%AE%E9%A2%98/:1:0","tags":["resolution","k8s","dns","cloud native"],"title":"k8s 中的 dns 问题","uri":"/ooooo-notes/k8s-%E4%B8%AD%E7%9A%84-dns-%E9%97%AE%E9%A2%98/"},{"categories":null,"content":"2. 检查 pod 的 dns 配置 # 进入容器中 kubectl debug -it some-pod --image=busybox -- sh # 在容器中查看 dns 配置, 这里一定要是 coredns 的 clusterIP, 如果不对，检查 kubelet 的 dns 配置 cat /etc/resolv.conf # 在容器中查看 hosts 配置 cat /etc/hosts ","date":"2023-05-19","objectID":"/ooooo-notes/k8s-%E4%B8%AD%E7%9A%84-dns-%E9%97%AE%E9%A2%98/:2:0","tags":["resolution","k8s","dns","cloud native"],"title":"k8s 中的 dns 问题","uri":"/ooooo-notes/k8s-%E4%B8%AD%E7%9A%84-dns-%E9%97%AE%E9%A2%98/"},{"categories":null,"content":"3. 检查并配置 coredns 配置 # 检查配置 kubectl get cm coredns -n kube-system -oyaml # 自定义配置，如添加 hosts 配置 kubectl apply -f - \u003c\u003cEOF apiVersion: v1 data: Corefile: | .:53 { errors health { lameduck 5s } ready kubernetes cluster.local in-addr.arpa ip6.arpa { pods insecure fallthrough in-addr.arpa ip6.arpa ttl 30 } prometheus :9153 # 添加 hosts 配置 hosts { 172.16.1.36 git.abc.com fallthrough } # 不转发到 /etc/resolv.conf forward . 8.8.8.8 cache 30 loop reload loadbalance } kind: ConfigMap metadata: name: coredns namespace: kube-system EOF ","date":"2023-05-19","objectID":"/ooooo-notes/k8s-%E4%B8%AD%E7%9A%84-dns-%E9%97%AE%E9%A2%98/:3:0","tags":["resolution","k8s","dns","cloud native"],"title":"k8s 中的 dns 问题","uri":"/ooooo-notes/k8s-%E4%B8%AD%E7%9A%84-dns-%E9%97%AE%E9%A2%98/"},{"categories":null,"content":"4. 问题现象 tekton的 pod dns 显示错误，重新设置主机的 /etc/resolv.conf, 重启 kubelet， 重启 tekton. ","date":"2023-05-19","objectID":"/ooooo-notes/k8s-%E4%B8%AD%E7%9A%84-dns-%E9%97%AE%E9%A2%98/:4:0","tags":["resolution","k8s","dns","cloud native"],"title":"k8s 中的 dns 问题","uri":"/ooooo-notes/k8s-%E4%B8%AD%E7%9A%84-dns-%E9%97%AE%E9%A2%98/"},{"categories":null,"content":" # open target dir cd /etc/netplan # edit config file, you must creat if not exist sudo vim 00-installer-config.yaml # network device setting template network: ethernets: ens33: #dhcp4: yes dhcp4: no addresses: - 192.168.130.129/24 routes: - to: default via: 192.168.130.2 #nameservers: # addresses: [192.168.130.2] version: 2 # netplan apply sudo netplan apply # restart reboot ","date":"2023-04-01","objectID":"/ooooo-notes/ubuntu-add-static-ip/:0:0","tags":["ubuntu","resolution"],"title":"ubuntu add static ip","uri":"/ooooo-notes/ubuntu-add-static-ip/"},{"categories":null,"content":"1. 文件夹权限 Volume 为 hostPath, 要注意文件夹权限， chmod 777 /data ","date":"2023-03-21","objectID":"/ooooo-notes/k8s-%E7%9A%84%E5%B0%8F%E9%97%AE%E9%A2%98/:1:0","tags":["k8s","cloud native"],"title":"k8s 的小问题","uri":"/ooooo-notes/k8s-%E7%9A%84%E5%B0%8F%E9%97%AE%E9%A2%98/"},{"categories":null,"content":"1. 检查 kubelet 的 cri 在 k8s 中，由 kubelet 来拉取节点，而 kubelet 又借用了 cri 来操作容器和镜像. # 查看 kubelet 的启动参数, 其中的 --container-runtime-endpoint 就是 cri ps aux | grep kubelet # 我这里使用的是 containerd ","date":"2023-03-21","objectID":"/ooooo-notes/k8s-%E6%8B%89%E5%8F%96%E9%95%9C%E5%83%8F%E6%85%A2/:1:0","tags":["k8s","containerd"],"title":"k8s 拉取镜像慢","uri":"/ooooo-notes/k8s-%E6%8B%89%E5%8F%96%E9%95%9C%E5%83%8F%E6%85%A2/"},{"categories":null,"content":"2. 设置 containerd 代理 # 检查 containerd 服务的 unit 文件, 其中 Loaded 属性就是文件位置 systemctl status containerd # 编辑 containerd.service 文件，我这里的文件位置是 /lib/systemd/system/containerd.service vim /lib/systemd/system/containerd.service # 在 [service] 下添加环境变量 [Service] Environment=HTTP_PROXY=http://ooooo:10800 Environment=HTTPS_PROXY=http://ooooo:10800 # 重启 containerd sudo systemctl daemon-reload sudo systemctl restart containerd ","date":"2023-03-21","objectID":"/ooooo-notes/k8s-%E6%8B%89%E5%8F%96%E9%95%9C%E5%83%8F%E6%85%A2/:2:0","tags":["k8s","containerd"],"title":"k8s 拉取镜像慢","uri":"/ooooo-notes/k8s-%E6%8B%89%E5%8F%96%E9%95%9C%E5%83%8F%E6%85%A2/"},{"categories":null,"content":"3. 参考 cri proxy systemd environment ","date":"2023-03-21","objectID":"/ooooo-notes/k8s-%E6%8B%89%E5%8F%96%E9%95%9C%E5%83%8F%E6%85%A2/:3:0","tags":["k8s","containerd"],"title":"k8s 拉取镜像慢","uri":"/ooooo-notes/k8s-%E6%8B%89%E5%8F%96%E9%95%9C%E5%83%8F%E6%85%A2/"},{"categories":null,"content":"1. go list all 一直加载 go list all 命令需要发送请求，导致连接超时。 goland 配置代理 ","date":"2023-03-20","objectID":"/ooooo-notes/goland-%E6%89%93%E5%BC%80-go-%E9%A1%B9%E7%9B%AE%E4%B8%80%E7%9B%B4-loading/:1:0","tags":["ide","goland"],"title":"goland 打开 go 项目一直 loading","uri":"/ooooo-notes/goland-%E6%89%93%E5%BC%80-go-%E9%A1%B9%E7%9B%AE%E4%B8%80%E7%9B%B4-loading/"},{"categories":null,"content":" vim /etc/docker/daemon.json # 添加以下配置 { \"registry-mirrors\": [ \"https://hub-mirror.c.163.com\", \"https://mirror.baidubce.com\" ] } # 重启docker sudo systemctl daemon-reload sudo systemctl restart docker ","date":"2023-03-18","objectID":"/ooooo-notes/docker-%E8%AE%BE%E7%BD%AE%E9%95%9C%E5%83%8F%E6%BA%90/:0:0","tags":["docker"],"title":"docker 设置镜像源","uri":"/ooooo-notes/docker-%E8%AE%BE%E7%BD%AE%E9%95%9C%E5%83%8F%E6%BA%90/"},{"categories":null,"content":"1. 解决方法 设置区域和语言 ","date":"2023-03-17","objectID":"/ooooo-notes/ubuntu-%E6%89%93%E4%B8%8D%E5%BC%80%E7%BB%88%E7%AB%AF/:1:0","tags":["ubuntu","resolution"],"title":"ubuntu 打不开终端","uri":"/ooooo-notes/ubuntu-%E6%89%93%E4%B8%8D%E5%BC%80%E7%BB%88%E7%AB%AF/"},{"categories":null,"content":"2. 参考 博客 ","date":"2023-03-17","objectID":"/ooooo-notes/ubuntu-%E6%89%93%E4%B8%8D%E5%BC%80%E7%BB%88%E7%AB%AF/:2:0","tags":["ubuntu","resolution"],"title":"ubuntu 打不开终端","uri":"/ooooo-notes/ubuntu-%E6%89%93%E4%B8%8D%E5%BC%80%E7%BB%88%E7%AB%AF/"},{"categories":null,"content":"1. enable enhanced keyboard ","date":"2023-03-17","objectID":"/ooooo-notes/vmware-%E4%B8%80%E4%BA%9B%E8%AE%BE%E7%BD%AE/:1:0","tags":["vmware","resolution"],"title":"vmware 一些设置","uri":"/ooooo-notes/vmware-%E4%B8%80%E4%BA%9B%E8%AE%BE%E7%BD%AE/"},{"categories":null,"content":"2. enable back/forward mouse buttons in vmware path : somepath/Virtual Machines/Ubuntu 64-bit/*.vmx usb.generic.allowHID = \"TRUE\" mouse.vusb.enable = \"TRUE\" reference Back / Forward mouse buttons do not work in VMWare ","date":"2023-03-17","objectID":"/ooooo-notes/vmware-%E4%B8%80%E4%BA%9B%E8%AE%BE%E7%BD%AE/:2:0","tags":["vmware","resolution"],"title":"vmware 一些设置","uri":"/ooooo-notes/vmware-%E4%B8%80%E4%BA%9B%E8%AE%BE%E7%BD%AE/"},{"categories":null,"content":"怎么使用 h2 数据库。 ","date":"2023-03-15","objectID":"/ooooo-notes/h2-%E6%95%B0%E6%8D%AE%E5%BA%93%E4%BD%BF%E7%94%A8/:0:0","tags":["h2"],"title":"h2 数据库使用","uri":"/ooooo-notes/h2-%E6%95%B0%E6%8D%AE%E5%BA%93%E4%BD%BF%E7%94%A8/"},{"categories":null,"content":"1. 引入依赖 dependencies { api('p6spy:p6spy') api('com.h2database:h2') } ","date":"2023-03-15","objectID":"/ooooo-notes/h2-%E6%95%B0%E6%8D%AE%E5%BA%93%E4%BD%BF%E7%94%A8/:1:0","tags":["h2"],"title":"h2 数据库使用","uri":"/ooooo-notes/h2-%E6%95%B0%E6%8D%AE%E5%BA%93%E4%BD%BF%E7%94%A8/"},{"categories":null,"content":"2. 以内存的方式使用 # spring boot 配置 spring: datasource: driverClassName: com.p6spy.engine.spy.P6SpyDriver url: jdbc:p6spy:h2:mem:test;DB_CLOSE_DELAY=1000 ","date":"2023-03-15","objectID":"/ooooo-notes/h2-%E6%95%B0%E6%8D%AE%E5%BA%93%E4%BD%BF%E7%94%A8/:2:0","tags":["h2"],"title":"h2 数据库使用","uri":"/ooooo-notes/h2-%E6%95%B0%E6%8D%AE%E5%BA%93%E4%BD%BF%E7%94%A8/"},{"categories":null,"content":"3. 以进程的方式使用 # 启动 h2 数据库 java -cp h2*.jar org.h2.tools.Server -ifNotExists # 启动 h2 console (可选) java -cp h2*.jar org.h2.tools.Console # 连接配置，会自动创建文件 url: jdbc:h2:tcp://localhost/~/test ","date":"2023-03-15","objectID":"/ooooo-notes/h2-%E6%95%B0%E6%8D%AE%E5%BA%93%E4%BD%BF%E7%94%A8/:3:0","tags":["h2"],"title":"h2 数据库使用","uri":"/ooooo-notes/h2-%E6%95%B0%E6%8D%AE%E5%BA%93%E4%BD%BF%E7%94%A8/"},{"categories":null,"content":"4. 参考 官方文档 ","date":"2023-03-15","objectID":"/ooooo-notes/h2-%E6%95%B0%E6%8D%AE%E5%BA%93%E4%BD%BF%E7%94%A8/:4:0","tags":["h2"],"title":"h2 数据库使用","uri":"/ooooo-notes/h2-%E6%95%B0%E6%8D%AE%E5%BA%93%E4%BD%BF%E7%94%A8/"},{"categories":null,"content":"相关命令 sudo apt install build-essential manpages-dev software-properties-common sudo add-apt-repository ppa:ubuntu-toolchain-r/test sudo apt update \u0026\u0026 sudo apt install gcc-11 g++-11 1. sudo apt update \u0026\u0026 sudo apt upgrade gcc libfontconfig1-dev systemtap-sdt-dev libx11-dev sudo apt-get install libx11-dev libxext-dev libxrender-dev libxrandr-dev libxtst-dev libxt-dev sudo apt-get install libcups2-dev sudo apt-get install libasound2-dev bash configure --build=x86_64-unknown-linux-gnu --enable-debug --with-jvm-variants=server --enable-dtrace bash configure --enable-debug --with-jvm-variants=server bash configure --enable-debug --with-jvm-variants=server --with-toolchain-type=gcc --with-boot-jdk=C:/Users/ooooo/Development/Jdk/jdk17 ","date":"2023-02-01","objectID":"/ooooo-notes/openjdk-build/:1:0","tags":["jdk"],"title":"openjdk build","uri":"/ooooo-notes/openjdk-build/"},{"categories":null,"content":"2. 参考 深入理解Java虚拟机（第3版） jdk build ","date":"2023-02-01","objectID":"/ooooo-notes/openjdk-build/:2:0","tags":["jdk"],"title":"openjdk build","uri":"/ooooo-notes/openjdk-build/"},{"categories":null,"content":"1. 检查 cgroup 的版本 # check if cgroup is supported cat /proc/filesystems | grep cgroup # check cgroup version cat /proc/mounts | grep cgroup ","date":"2023-01-29","objectID":"/ooooo-notes/linux-%E4%B8%AD%E7%9A%84-cgroup-%E6%9C%BA%E5%88%B6/:1:0","tags":["linux"],"title":"linux 中的 cgroup 机制","uri":"/ooooo-notes/linux-%E4%B8%AD%E7%9A%84-cgroup-%E6%9C%BA%E5%88%B6/"},{"categories":null,"content":"2. cgroup v2 操作 # create new dir cd /sys/fs/cgroup mkdir test # creat loop.sh for testing cpu quota vim loop.sh while : do : done # lunch loop.sh, generate pid -\u003e 2584068 nohup sh loop.sh \u0026 # echo pid to cgroup.procs echo 2584068 \u003e test/cgroup.procs # set cpu, at lease 0.1 echo 1000 10000 \u003e test/cpu.max # check top # recovery all kill 2584068 rmdir test 参考： 博客 ","date":"2023-01-29","objectID":"/ooooo-notes/linux-%E4%B8%AD%E7%9A%84-cgroup-%E6%9C%BA%E5%88%B6/:2:0","tags":["linux"],"title":"linux 中的 cgroup 机制","uri":"/ooooo-notes/linux-%E4%B8%AD%E7%9A%84-cgroup-%E6%9C%BA%E5%88%B6/"},{"categories":null,"content":"1. 准备数据 # create new schema create schema test; use test; # create table test create table user ( id int primary key, age int ); alter table user add index age_idx (age); # insert some test data insert into user values (3, 10), (5, 20), (8, 30); ","date":"2023-01-28","objectID":"/ooooo-notes/mysql-%E9%97%B4%E9%9A%99%E9%94%81/:1:0","tags":["mysql"],"title":"mysql 间隙锁","uri":"/ooooo-notes/mysql-%E9%97%B4%E9%9A%99%E9%94%81/"},{"categories":null,"content":"2. 间隙锁测试 ","date":"2023-01-28","objectID":"/ooooo-notes/mysql-%E9%97%B4%E9%9A%99%E9%94%81/:2:0","tags":["mysql"],"title":"mysql 间隙锁","uri":"/ooooo-notes/mysql-%E9%97%B4%E9%9A%99%E9%94%81/"},{"categories":null,"content":"1. 使用主键索引，指定行存在 # session 1, the row is exist for id = 3 , so it doesn't lock. begin; select * from user where id = 3 for update; # session 2, execute successful. begin; insert into user value (1, 20); ","date":"2023-01-28","objectID":"/ooooo-notes/mysql-%E9%97%B4%E9%9A%99%E9%94%81/:2:1","tags":["mysql"],"title":"mysql 间隙锁","uri":"/ooooo-notes/mysql-%E9%97%B4%E9%9A%99%E9%94%81/"},{"categories":null,"content":"2. 使用主键索引，指定行不存在 # session 1, the row isn't exist for id = 2, so it locks range (,3] begin; select * from user where id = 2 for update; # session 2, execute block. begin; insert into user value (1, 20); ","date":"2023-01-28","objectID":"/ooooo-notes/mysql-%E9%97%B4%E9%9A%99%E9%94%81/:2:2","tags":["mysql"],"title":"mysql 间隙锁","uri":"/ooooo-notes/mysql-%E9%97%B4%E9%9A%99%E9%94%81/"},{"categories":null,"content":"3. 使用主键索引，范围查找 # session 1, it locks range [1,5] begin; select * from user where id \u003e= 1 and id \u003c= 5 for update; # session 2, execute block begin; insert into user value (2, 20); ","date":"2023-01-28","objectID":"/ooooo-notes/mysql-%E9%97%B4%E9%9A%99%E9%94%81/:2:3","tags":["mysql"],"title":"mysql 间隙锁","uri":"/ooooo-notes/mysql-%E9%97%B4%E9%9A%99%E9%94%81/"},{"categories":null,"content":"4. 使用二级索引，指定行存在 # session 1, it locks range [3,8] begin; select * from user where age = 20 for update; # session 2, execute block. begin; insert into user value (4, 20); ","date":"2023-01-28","objectID":"/ooooo-notes/mysql-%E9%97%B4%E9%9A%99%E9%94%81/:2:4","tags":["mysql"],"title":"mysql 间隙锁","uri":"/ooooo-notes/mysql-%E9%97%B4%E9%9A%99%E9%94%81/"},{"categories":null,"content":"5. 使用二级索引，指定行不存在 # session 1, it locks range [3,5] begin; select * from user where age = 15 for update; # session 2, execute block. begin; insert into user value (4, 20); ","date":"2023-01-28","objectID":"/ooooo-notes/mysql-%E9%97%B4%E9%9A%99%E9%94%81/:2:5","tags":["mysql"],"title":"mysql 间隙锁","uri":"/ooooo-notes/mysql-%E9%97%B4%E9%9A%99%E9%94%81/"},{"categories":null,"content":"6. 使用二级索引，范围查询 # session 1, it locks range [3,8] begin; select * from user where age \u003e= 12 and age \u003c= 28 for update; # session 2, execute block. begin; insert into user value (4, 20); ","date":"2023-01-28","objectID":"/ooooo-notes/mysql-%E9%97%B4%E9%9A%99%E9%94%81/:2:6","tags":["mysql"],"title":"mysql 间隙锁","uri":"/ooooo-notes/mysql-%E9%97%B4%E9%9A%99%E9%94%81/"},{"categories":null,"content":"7. 结论 使用主键索引，行存在时，才只会锁定这一行。 其他情况都是使用范围锁定 ","date":"2023-01-28","objectID":"/ooooo-notes/mysql-%E9%97%B4%E9%9A%99%E9%94%81/:2:7","tags":["mysql"],"title":"mysql 间隙锁","uri":"/ooooo-notes/mysql-%E9%97%B4%E9%9A%99%E9%94%81/"},{"categories":null,"content":"3. 恢复数据 drop schame test; ","date":"2023-01-28","objectID":"/ooooo-notes/mysql-%E9%97%B4%E9%9A%99%E9%94%81/:3:0","tags":["mysql"],"title":"mysql 间隙锁","uri":"/ooooo-notes/mysql-%E9%97%B4%E9%9A%99%E9%94%81/"},{"categories":null,"content":"1. aufs 存储驱动 Ubuntu 22.04 LTS 不支持 aufs 文件系统 参考： ubuntu官方文档 ","date":"2023-01-24","objectID":"/ooooo-notes/docker-%E5%AD%98%E5%82%A8%E9%A9%B1%E5%8A%A8/:1:0","tags":["docker","storage"],"title":"docker 存储驱动","uri":"/ooooo-notes/docker-%E5%AD%98%E5%82%A8%E9%A9%B1%E5%8A%A8/"},{"categories":null,"content":"2. overlay2 存储驱动 # creat dir mkdir lower upper work mnt # mount lower upper work to mnt mount -t overlay -o lowerdir=lower,upperdir=upper,workdir=work none mnt # testing echo 1 \u003e lower/1 mkdir lower/2 mkdir upper/3 ll mnt # recovery all setting umount mnt rm -rf lower upper work mnt 参考： 文档 linux文档 ","date":"2023-01-24","objectID":"/ooooo-notes/docker-%E5%AD%98%E5%82%A8%E9%A9%B1%E5%8A%A8/:2:0","tags":["docker","storage"],"title":"docker 存储驱动","uri":"/ooooo-notes/docker-%E5%AD%98%E5%82%A8%E9%A9%B1%E5%8A%A8/"},{"categories":null,"content":"这篇文章主要简述 docker 中的 bridge 网络驱动是如何工作的。 ","date":"2023-01-20","objectID":"/ooooo-notes/docker-%E5%8D%95%E4%B8%BB%E6%9C%BA%E7%BD%91%E8%B7%AF/:0:0","tags":["docker","network"],"title":"docker 单主机网络","uri":"/ooooo-notes/docker-%E5%8D%95%E4%B8%BB%E6%9C%BA%E7%BD%91%E8%B7%AF/"},{"categories":null,"content":"1. 测试一，veth1 (ns1) — veth2 (ns2) # create ns1, ns2 ip netns add ns1 ip netns add ns2 # create veth1, veth2 ip link add veth1 type veth peer name veth2 # set veth1 for ns1, set veth2 for ns2 ip link set dev veth1 netns ns1 ip link set dev veth2 netns ns2 # set veth1 ip, set veth2 ip ip netns exec ns1 ip addr add 172.16.0.1/24 dev veth1 ip netns exec ns2 ip addr add 172.16.0.2/24 dev veth2 # set veth1 up, set veth2 up ip netns exec ns1 ip link set dev veth1 up ip netns exec ns2 ip link set dev veth2 up # show ip address ip netns exec ns1 ip addr ip netns exec ns2 ip addr # test for ping ip netns exec ns1 ping 172.16.0.2 ip netns exec ns2 ping 172.16.0.1 # recovery all setting ip netns delete ns1 ip netns delete ns2 ","date":"2023-01-20","objectID":"/ooooo-notes/docker-%E5%8D%95%E4%B8%BB%E6%9C%BA%E7%BD%91%E8%B7%AF/:1:0","tags":["docker","network"],"title":"docker 单主机网络","uri":"/ooooo-notes/docker-%E5%8D%95%E4%B8%BB%E6%9C%BA%E7%BD%91%E8%B7%AF/"},{"categories":null,"content":"2. 测试二，veth0 (bridge0) — veth1 (ns1) # create ns1 ip netns add ns1 # create veth0, veth1 ip link add veth0 type veth peer name veth1 # create bridge0 ip link add bridge0 type bridge # set veth1 for ns1 ip link set dev veth1 netns ns1 # set veth0 for bridge0 ip link set dev veth0 master bridge0 # set veth0 ip address ip addr add 172.16.0.1/24 dev veth0 # set bridge0 ip address ip addr add 172.16.0.0/24 dev bridge0 # set veth1 ip address ip netns exec ns1 ip addr add 172.16.0.2/24 dev veth1 # set veth0, veth1, bridge0 up ip link set dev veth0 up ip link set dev bridge0 up ip netns exec ns1 ip link set dev veth1 up # delete veth0 route ip route del 172.16.0.0/24 dev veth0 # show ip address ip addr ip netns exec ns1 ip addr # test for ping ping 172.16.0.2 ip netns exec ns1 ping 172.16.0.1 # recovery all setting ip netns del ns1 ip link del bridge0 ","date":"2023-01-20","objectID":"/ooooo-notes/docker-%E5%8D%95%E4%B8%BB%E6%9C%BA%E7%BD%91%E8%B7%AF/:2:0","tags":["docker","network"],"title":"docker 单主机网络","uri":"/ooooo-notes/docker-%E5%8D%95%E4%B8%BB%E6%9C%BA%E7%BD%91%E8%B7%AF/"},{"categories":null,"content":"0、持续学习者 Talk is cheap. Show me the code. 英语比编程简单。 学习和实践要平衡。 学会和时间做朋友。 学会投资，学会理财。 学会先做减法，再做加法。 学英语很重要，学英语很重要，学英语很重要。 说明： ⭕ 进行中 ✅ 已完成 ❌ 已废弃 ❓ 有必要 ❗ 重要性 📝 记笔记 🖊️ 写代码 ","date":"2023-01-01","objectID":"/ooooo-notes/2023%E5%B9%B4%E8%AE%A1%E5%88%92/:1:0","tags":["learning"],"title":"2023年学习计划","uri":"/ooooo-notes/2023%E5%B9%B4%E8%AE%A1%E5%88%92/"},{"categories":null,"content":"1、关于英语 《新概念二》 ⭕ ","date":"2023-01-01","objectID":"/ooooo-notes/2023%E5%B9%B4%E8%AE%A1%E5%88%92/:2:0","tags":["learning"],"title":"2023年学习计划","uri":"/ooooo-notes/2023%E5%B9%B4%E8%AE%A1%E5%88%92/"},{"categories":null,"content":"2、关于技术 计划 🎉： 只记录自己认为有用的笔记。 ","date":"2023-01-01","objectID":"/ooooo-notes/2023%E5%B9%B4%E8%AE%A1%E5%88%92/:3:0","tags":["learning"],"title":"2023年学习计划","uri":"/ooooo-notes/2023%E5%B9%B4%E8%AE%A1%E5%88%92/"},{"categories":null,"content":"1、书籍 0️⃣1️⃣. 《深入理解 Kafka：核心设计与实践原理》 0️⃣2️⃣. 《分布式一致性算法开发实战》 ❌ 0️⃣3️⃣. 《Go Web 编程》 ✅ 0️⃣4️⃣. 《Effective C++》 0️⃣5️⃣. 《More Effective C++》 0️⃣6️⃣. 《深度探索C++对象模型》 0️⃣7️⃣. 《Go语言设计与实现》 0️⃣8️⃣. 《Vim实用技巧（第2版）》 ⭕ 0️⃣9️⃣. 《RocketMQ技术内幕 第二版》 ⭕ 1️⃣0️⃣. 《云原生服务网格Istio：原理、实践、架构与源码解析》 ✅ 1️⃣1️⃣. 《MySQL技术内幕》 ⭕ 1️⃣2️⃣. 《深入解析Java虚拟机HotSpot》 1️⃣3️⃣. 《Rust权威指南》 ✅ 1️⃣4️⃣. 《深入剖析Kubernetes》 1️⃣5️⃣. 《Kubernetes编程》 ❌ 1️⃣6️⃣. 《Kubernetes设计模式》 ❌ 1️⃣7️⃣. 《深入剖析Java虚拟机》 1️⃣8️⃣. 《算法训练营：海量图解+竞赛刷题（入门篇》 ✅ 1️⃣9️⃣. 《算法训练营：海量图解+竞赛刷题（进阶篇）》 2️⃣0️⃣. 《TCP/IP详解 卷1：协议》 ✅ 2️⃣1️⃣. 《自己动手写Docker》 ✅ 2️⃣2️⃣. 《UNIX网络编程 卷1：套接字联网API（第3版）》 2️⃣3️⃣. 《Docker——容器与容器云（第2版）》 ✅ 2️⃣4️⃣. 《高性能MySQL（第4版）》 ✅ 2️⃣5️⃣. 《Kubernetes网络权威指南：基础、原理与实践）》 ✅ 2️⃣6️⃣. 《Kubernetes Operator开发进阶》 ❌ 2️⃣7️⃣. 《Kafka权威指南（第2版）》 2️⃣8️⃣. 《操作系统导论》 ⭕ 2️⃣9️⃣. 《Go语言底层原理剖析》 ❌ 3️⃣0️⃣. 《Kubernetes进阶实战（第2版）》 ✅ 3️⃣1️⃣. 《数据密集型应用系统设计》 ✅ 3️⃣2️⃣. 《C++语言导学（第二版）》 ✅ 3️⃣3️⃣. 《Linux内核设计与实现(原书第3版)》 ⭕ ","date":"2023-01-01","objectID":"/ooooo-notes/2023%E5%B9%B4%E8%AE%A1%E5%88%92/:3:1","tags":["learning"],"title":"2023年学习计划","uri":"/ooooo-notes/2023%E5%B9%B4%E8%AE%A1%E5%88%92/"},{"categories":null,"content":"2、文档 0️⃣1️⃣. 《深入拆解 Java 虚拟机》 0️⃣2️⃣. 从 0 开始带你成为JVM实战高手 ❌ 0️⃣3️⃣. Go 语言项目开发实战 ❌ 0️⃣4️⃣. Redis 源码剖析与实战 ✅ 0️⃣5️⃣. 深入 C 语言和程序运行原理 ","date":"2023-01-01","objectID":"/ooooo-notes/2023%E5%B9%B4%E8%AE%A1%E5%88%92/:3:2","tags":["learning"],"title":"2023年学习计划","uri":"/ooooo-notes/2023%E5%B9%B4%E8%AE%A1%E5%88%92/"},{"categories":null,"content":"3. 源码 0️⃣1️⃣. 《rocketmq 源码》 ⭕ 0️⃣2️⃣. 《kubernetes 源码》 ⭕ 0️⃣3️⃣. 《istio 源码》 ❌ 0️⃣4️⃣. 《etcd 源码》 0️⃣5️⃣. 《dubbo 源码》 ⭕ 0️⃣6️⃣. 《arthas 源码》 0️⃣7️⃣. 《nsq 源码》 ❌ 0️⃣8️⃣. 《eventing 源码》 ❌ 0️⃣9️⃣. 《serving 源码》 ❌ 1️⃣0️⃣. 《grpc-go 源码》 1️⃣1️⃣. 《nacos 源码》 ✅ 1️⃣2️⃣. 《activiti 源码》 ✅ ","date":"2023-01-01","objectID":"/ooooo-notes/2023%E5%B9%B4%E8%AE%A1%E5%88%92/:3:3","tags":["learning"],"title":"2023年学习计划","uri":"/ooooo-notes/2023%E5%B9%B4%E8%AE%A1%E5%88%92/"},{"categories":null,"content":"4、视频 0️⃣1️⃣. 《玩转算法系列–图论精讲》 ❌ 0️⃣2️⃣. 《玩转算法面试》 ❌ ","date":"2023-01-01","objectID":"/ooooo-notes/2023%E5%B9%B4%E8%AE%A1%E5%88%92/:3:4","tags":["learning"],"title":"2023年学习计划","uri":"/ooooo-notes/2023%E5%B9%B4%E8%AE%A1%E5%88%92/"},{"categories":null,"content":"4、关于其他 ","date":"2023-01-01","objectID":"/ooooo-notes/2023%E5%B9%B4%E8%AE%A1%E5%88%92/:4:0","tags":["learning"],"title":"2023年学习计划","uri":"/ooooo-notes/2023%E5%B9%B4%E8%AE%A1%E5%88%92/"},{"categories":null,"content":"1、书籍 🎉 0️⃣1️⃣. 《卓有成效的工程师》 ✅ 0️⃣2️⃣. 《非暴力沟通》 ⭕ 0️⃣3️⃣. 《原则》 0️⃣4️⃣. 《刻意练习》 ⭕ 0️⃣5️⃣. 《关键对话》 0️⃣6️⃣. 《当下的启蒙》 0️⃣7️⃣. 《亲密关系：通往灵魂的桥梁》 ⭕ 0️⃣8️⃣. 《人性的弱点》 0️⃣9️⃣. 《当我谈跑步时，我谈些什么》 ","date":"2023-01-01","objectID":"/ooooo-notes/2023%E5%B9%B4%E8%AE%A1%E5%88%92/:4:1","tags":["learning"],"title":"2023年学习计划","uri":"/ooooo-notes/2023%E5%B9%B4%E8%AE%A1%E5%88%92/"},{"categories":null,"content":"2、尝试 🎉 0️⃣1️⃣. 学会使用尤克里里弹奏 ❌ ","date":"2023-01-01","objectID":"/ooooo-notes/2023%E5%B9%B4%E8%AE%A1%E5%88%92/:4:2","tags":["learning"],"title":"2023年学习计划","uri":"/ooooo-notes/2023%E5%B9%B4%E8%AE%A1%E5%88%92/"},{"categories":null,"content":"3、了解 🎉 0️⃣1️⃣. 暂无 ","date":"2023-01-01","objectID":"/ooooo-notes/2023%E5%B9%B4%E8%AE%A1%E5%88%92/:4:3","tags":["learning"],"title":"2023年学习计划","uri":"/ooooo-notes/2023%E5%B9%B4%E8%AE%A1%E5%88%92/"},{"categories":null,"content":"4、娱乐 0️⃣1️⃣. 《奇遇人生 第一季》 0️⃣2️⃣. 《一本好书 1》 0️⃣3️⃣. 《一本好书 2》 0️⃣4️⃣. 《天气之子》 ✅ ","date":"2023-01-01","objectID":"/ooooo-notes/2023%E5%B9%B4%E8%AE%A1%E5%88%92/:4:4","tags":["learning"],"title":"2023年学习计划","uri":"/ooooo-notes/2023%E5%B9%B4%E8%AE%A1%E5%88%92/"},{"categories":null,"content":"1. 前置条件 安装 docker，必须配置 docker 代理，否则 build 失败。 参考 下载 istio 源码。 安装 go 和 dlv 工具。参考 ","date":"2022-12-19","objectID":"/ooooo-notes/%E6%90%AD%E5%BB%BA-istio-%E6%BA%90%E7%A0%81%E8%B0%83%E8%AF%95%E7%8E%AF%E5%A2%83/:1:0","tags":["istio","cloud native"],"title":"搭建 istio 源码调试环境","uri":"/ooooo-notes/%E6%90%AD%E5%BB%BA-istio-%E6%BA%90%E7%A0%81%E8%B0%83%E8%AF%95%E7%8E%AF%E5%A2%83/"},{"categories":null,"content":"2. 设置环境变量 # docker 地址 export HUB=\"docker.io/youwillsee\" # istio 的源码目录 export ISTIO=/root/code/istio # docker 的 tag export TAG=1.17-debug ","date":"2022-12-19","objectID":"/ooooo-notes/%E6%90%AD%E5%BB%BA-istio-%E6%BA%90%E7%A0%81%E8%B0%83%E8%AF%95%E7%8E%AF%E5%A2%83/:2:0","tags":["istio","cloud native"],"title":"搭建 istio 源码调试环境","uri":"/ooooo-notes/%E6%90%AD%E5%BB%BA-istio-%E6%BA%90%E7%A0%81%E8%B0%83%E8%AF%95%E7%8E%AF%E5%A2%83/"},{"categories":null,"content":"3. build istio # 构建 debug 的版本，会输出在 out 目录下 make DEBUG=1 build # 构建 debug 的版本，推到本地的 docker 中 make DEBUG=1 docker # 推送到远端的 docker 中 make docker.push # 清理 make clean 参考 istio-devlopment istio-code-base ","date":"2022-12-19","objectID":"/ooooo-notes/%E6%90%AD%E5%BB%BA-istio-%E6%BA%90%E7%A0%81%E8%B0%83%E8%AF%95%E7%8E%AF%E5%A2%83/:3:0","tags":["istio","cloud native"],"title":"搭建 istio 源码调试环境","uri":"/ooooo-notes/%E6%90%AD%E5%BB%BA-istio-%E6%BA%90%E7%A0%81%E8%B0%83%E8%AF%95%E7%8E%AF%E5%A2%83/"},{"categories":null,"content":"4. dlv 连接 # 找到 pid ps -ef | grep pilot-discovery # attach pid dlv --listen=:2345 --headless=true --api-version=2 --accept-multiclient attach 172965 # 使用 IDE 远程连接 GOland -\u003e go remote ","date":"2022-12-19","objectID":"/ooooo-notes/%E6%90%AD%E5%BB%BA-istio-%E6%BA%90%E7%A0%81%E8%B0%83%E8%AF%95%E7%8E%AF%E5%A2%83/:4:0","tags":["istio","cloud native"],"title":"搭建 istio 源码调试环境","uri":"/ooooo-notes/%E6%90%AD%E5%BB%BA-istio-%E6%BA%90%E7%A0%81%E8%B0%83%E8%AF%95%E7%8E%AF%E5%A2%83/"},{"categories":null,"content":"5. bind dlv to pilot (optional) Dockerfile.pilot # BASE_DISTRIBUTION is used to switch between the old base distribution and distroless base images ARG BASE_DISTRIBUTION=debug # Version is the base image version from the TLD Makefile ARG BASE_VERSION=latest ARG ISTIO_BASE_REGISTRY=gcr.io/istio-release # The following section is used as base image if BASE_DISTRIBUTION=debug FROM ${ISTIO_BASE_REGISTRY}/base:${BASE_VERSION} as debug # The following section is used as base image if BASE_DISTRIBUTION=distroless FROM ${ISTIO_BASE_REGISTRY}/distroless:${BASE_VERSION} as distroless # Add dlv FROM golang:1.20 AS build-dlv ENV GOPROXY=https://goproxy.io,direct RUN go install github.com/go-delve/delve/cmd/dlv@latest # This will build the final image based on either debug or distroless from above # hadolint ignore=DL3006 FROM ${BASE_DISTRIBUTION:-debug} ARG TARGETARCH COPY ${TARGETARCH:-amd64}/pilot-discovery /usr/local/bin/pilot-discovery # Copy templates for bootstrap generation. COPY envoy_bootstrap.json /var/lib/istio/envoy/envoy_bootstrap_tmpl.json COPY gcp_envoy_bootstrap.json /var/lib/istio/envoy/gcp_envoy_bootstrap_tmpl.json COPY --from=build-dlv /go/bin/dlv / USER 1337:1337 ENTRYPOINT [\"/dlv\", \"--listen=:1234\", \"--headless=true\", \"--api-version=2\", \"--accept-multiclient\", \"exec\", \"/usr/local/bin/pilot-discovery\", \"--\"] #ENTRYPOINT [\"/usr/local/bin/pilot-discovery\"] ","date":"2022-12-19","objectID":"/ooooo-notes/%E6%90%AD%E5%BB%BA-istio-%E6%BA%90%E7%A0%81%E8%B0%83%E8%AF%95%E7%8E%AF%E5%A2%83/:5:0","tags":["istio","cloud native"],"title":"搭建 istio 源码调试环境","uri":"/ooooo-notes/%E6%90%AD%E5%BB%BA-istio-%E6%BA%90%E7%A0%81%E8%B0%83%E8%AF%95%E7%8E%AF%E5%A2%83/"},{"categories":null,"content":"1. 配置 docker 代理 # 创建配置目录 mkdir -p /etc/systemd/system/docker.service.d # 创建配置文件 vim /etc/systemd/system/docker.service.d/http-proxy.conf # 配置文件内容 [Service] Environment=\"HTTP_PROXY=http://ooooo:10800\" Environment=\"HTTPS_PROXY=http://ooooo:10800\" # 重启 docker systemctl daemon-reload \u0026\u0026 systemctl restart docker # 查看配置是否生效 systemctl show --property=Environment docker ","date":"2022-12-18","objectID":"/ooooo-notes/%E8%AE%BE%E7%BD%AE-docker-%E4%BB%A3%E7%90%86/:1:0","tags":["docker","cloud native"],"title":"设置 docker 代理","uri":"/ooooo-notes/%E8%AE%BE%E7%BD%AE-docker-%E4%BB%A3%E7%90%86/"},{"categories":null,"content":"1. 实现队列 代码： 使用 head 和 tail 来实现单链表 单链表涉及到两个节点，每次都要判断中间状态 这里使用的是 AtomicReference 来实现的，也可以使用 unsafe 来实现，有兴趣的可以尝试下 这里使用 curTail.next 进行 CAS 来指定下一个节点, 很少这么使用，后面再详细说说 public class LinkedQueue\u003cE\u003e { private final Node\u003cE\u003e dummy = new Node\u003c\u003e(null, null); private final AtomicReference\u003cNode\u003cE\u003e\u003e head = new AtomicReference\u003c\u003e(dummy); private final AtomicReference\u003cNode\u003cE\u003e\u003e tail = new AtomicReference\u003c\u003e(dummy); public boolean put(E item) { Node\u003cE\u003e newNode = new Node\u003c\u003e(item, null); while (true) { Node\u003cE\u003e curTail = tail.get(); Node\u003cE\u003e tailNext = curTail.next.get(); if (curTail == tail.get()) { if (tailNext != null) { // 队列处于中间状态，推进尾节点 tail.compareAndSet(curTail, tailNext); } else { // 处于稳定状态，尝试插入新节点 if (curTail.next.compareAndSet(null, newNode)) { // 插入操作成功，尝试推进尾节点 tail.compareAndSet(curTail, newNode); return true; } } } } } public E take() { while (true) { if (head.get() == tail.get()) { return null; } Node\u003cE\u003e oldHead = head.get(); Node\u003cE\u003e newHead = oldHead.next.get(); // 队列处于中间状态，可能另外一个线程已经 CAS 成功， 只剩下一个元素 dummy 了 if (newHead == null) { return null; } if (head.compareAndSet(oldHead, newHead)) { oldHead.next = null; return oldHead.item; } } } private static class Node\u003cE\u003e { private final E item; private AtomicReference\u003cNode\u003cE\u003e\u003e next; public Node(E item, Node\u003cE\u003e next) { this.item = item; this.next = new AtomicReference\u003c\u003e(next); } } } ","date":"2022-11-16","objectID":"/ooooo-notes/%E5%9C%A8-java-%E4%B8%AD%E4%BD%BF%E7%94%A8-cas-%E6%9D%A5%E5%AE%9E%E7%8E%B0%E9%98%9F%E5%88%97/:1:0","tags":["java"],"title":"在 java 中使用 CAS 来实现队列","uri":"/ooooo-notes/%E5%9C%A8-java-%E4%B8%AD%E4%BD%BF%E7%94%A8-cas-%E6%9D%A5%E5%AE%9E%E7%8E%B0%E9%98%9F%E5%88%97/"},{"categories":null,"content":"2. 代码实现位置 github 地址 ","date":"2022-11-16","objectID":"/ooooo-notes/%E5%9C%A8-java-%E4%B8%AD%E4%BD%BF%E7%94%A8-cas-%E6%9D%A5%E5%AE%9E%E7%8E%B0%E9%98%9F%E5%88%97/:2:0","tags":["java"],"title":"在 java 中使用 CAS 来实现队列","uri":"/ooooo-notes/%E5%9C%A8-java-%E4%B8%AD%E4%BD%BF%E7%94%A8-cas-%E6%9D%A5%E5%AE%9E%E7%8E%B0%E9%98%9F%E5%88%97/"},{"categories":null,"content":"1. 使用 Lock 来实现 Semaphore 代码： Semaphore 的功能就是允许同时有几个线程操作 acquire 方法，permit 会减一，如果为 0，则线程需要等待 release 方法，permit 会加一，唤醒等待的线程 public class SemaphoreOnLock { private final ReentrantLock lock = new ReentrantLock(); private final Condition condition = lock.newCondition(); private int permit; public SemaphoreOnLock(int permit) { this.permit = permit; } /** * 获取锁 */ public void acquire() { lock.lock(); try { while (permit \u003c= 0) { condition.await(); } permit--; } catch (InterruptedException ignored) { } finally { lock.unlock(); } } public void release() { lock.lock(); try { permit++; condition.signal(); } finally { lock.unlock(); } } } ","date":"2022-11-14","objectID":"/ooooo-notes/%E5%9C%A8-java-%E4%B8%AD%E4%BD%BF%E7%94%A8-lock-%E6%9D%A5%E5%AE%9E%E7%8E%B0-semaphore/:1:0","tags":["java"],"title":"在 java 中使用 Lock 来实现 Semaphore","uri":"/ooooo-notes/%E5%9C%A8-java-%E4%B8%AD%E4%BD%BF%E7%94%A8-lock-%E6%9D%A5%E5%AE%9E%E7%8E%B0-semaphore/"},{"categories":null,"content":"2. 代码实现位置 github 地址 ","date":"2022-11-14","objectID":"/ooooo-notes/%E5%9C%A8-java-%E4%B8%AD%E4%BD%BF%E7%94%A8-lock-%E6%9D%A5%E5%AE%9E%E7%8E%B0-semaphore/:2:0","tags":["java"],"title":"在 java 中使用 Lock 来实现 Semaphore","uri":"/ooooo-notes/%E5%9C%A8-java-%E4%B8%AD%E4%BD%BF%E7%94%A8-lock-%E6%9D%A5%E5%AE%9E%E7%8E%B0-semaphore/"},{"categories":null,"content":"1. 使用数组来实现栈 代码： 用数组来实现 用 CTL 来控制 测试类，参考 ConcurrentStackUsingArrayTest public class ConcurrentStackUsingArray\u003cE\u003e { private final AtomicInteger CTL = new AtomicInteger(0); private final AtomicReference\u003cE[]\u003e arr = new AtomicReference\u003c\u003e((E[]) new Object[10]); private final AtomicInteger index = new AtomicInteger(0); public void push(E e) { while (!CTL.compareAndSet(0, 1)) { Thread.yield(); } while (index.get() \u003e= arr.get().length) { E[] oldArr = arr.get(); E[] newArr = (E[]) new Object[oldArr.length * 2]; System.arraycopy(oldArr, 0, newArr, 0, oldArr.length); if (arr.compareAndSet(oldArr, newArr)) { break; } } arr.get()[index.getAndIncrement()] = e; CTL.lazySet(0); } public E pop() { while (!CTL.compareAndSet(0, 1)) { Thread.yield(); } E e = arr.get()[index.decrementAndGet()]; CTL.lazySet(0); return e; } } ","date":"2022-11-13","objectID":"/ooooo-notes/%E5%9C%A8-java-%E4%B8%AD%E4%BD%BF%E7%94%A8-cas-%E6%9D%A5%E5%AE%9E%E7%8E%B0%E6%A0%882/:1:0","tags":["java"],"title":"在 java 中使用 CAS 来实现栈2","uri":"/ooooo-notes/%E5%9C%A8-java-%E4%B8%AD%E4%BD%BF%E7%94%A8-cas-%E6%9D%A5%E5%AE%9E%E7%8E%B0%E6%A0%882/"},{"categories":null,"content":"2. 代码实现位置 github 地址 ","date":"2022-11-13","objectID":"/ooooo-notes/%E5%9C%A8-java-%E4%B8%AD%E4%BD%BF%E7%94%A8-cas-%E6%9D%A5%E5%AE%9E%E7%8E%B0%E6%A0%882/:2:0","tags":["java"],"title":"在 java 中使用 CAS 来实现栈2","uri":"/ooooo-notes/%E5%9C%A8-java-%E4%B8%AD%E4%BD%BF%E7%94%A8-cas-%E6%9D%A5%E5%AE%9E%E7%8E%B0%E6%A0%882/"},{"categories":null,"content":"1. java 多线程测试 在任何语言中，多线程测试都是比较困难的，在这里我介绍下 java 的多线程测试 jcstress. jcstress 是 OpenJDK 提供的一个测试多线程的框架 主要由多个 Actor 来构成，每个 Actor 就是一个线程。 通过匹配 Outcome 的结果来报告测试 运行之后的结果为 html 文件，需要你自己查看。 示例代码: 测试自旋锁，其实也告诉你该怎么编写 CAS 执行命令 gradle jcstress，会生成目录 build/reports/jcstress @JCStressTest @Outcome(id = {\"1, 2\", \"2, 1\"}, expect = Expect.ACCEPTABLE, desc = \"Mutex works\") @Outcome(id = \"1, 1\", expect = Expect.FORBIDDEN, desc = \"Mutex failure\") @State public class Mutex_03_SpinLock { private final AtomicBoolean taken = new AtomicBoolean(false); private int v; @Actor public void actor1(II_Result r) { while (taken.get() || !taken.compareAndSet(false, true)) ; // wait { // critical section r.r1 = ++v; } taken.set(false); } @Actor public void actor2(II_Result r) { while (taken.get() || !taken.compareAndSet(false, true)) ; // wait { // critical section r.r2 = ++v; } taken.set(false); } } ","date":"2022-11-12","objectID":"/ooooo-notes/%E5%9C%A8-java-%E4%B8%AD%E5%A6%82%E4%BD%95%E8%BF%9B%E8%A1%8C%E5%A4%9A%E7%BA%BF%E7%A8%8B%E6%B5%8B%E8%AF%95/:1:0","tags":["java"],"title":"在 java 中如何进行多线程测试","uri":"/ooooo-notes/%E5%9C%A8-java-%E4%B8%AD%E5%A6%82%E4%BD%95%E8%BF%9B%E8%A1%8C%E5%A4%9A%E7%BA%BF%E7%A8%8B%E6%B5%8B%E8%AF%95/"},{"categories":null,"content":"2. 代码实现位置 github 地址 ","date":"2022-11-12","objectID":"/ooooo-notes/%E5%9C%A8-java-%E4%B8%AD%E5%A6%82%E4%BD%95%E8%BF%9B%E8%A1%8C%E5%A4%9A%E7%BA%BF%E7%A8%8B%E6%B5%8B%E8%AF%95/:2:0","tags":["java"],"title":"在 java 中如何进行多线程测试","uri":"/ooooo-notes/%E5%9C%A8-java-%E4%B8%AD%E5%A6%82%E4%BD%95%E8%BF%9B%E8%A1%8C%E5%A4%9A%E7%BA%BF%E7%A8%8B%E6%B5%8B%E8%AF%95/"},{"categories":null,"content":"3. 参考 强烈建议大家看官方代码, 地址: https://github.com/openjdk/jcstress ","date":"2022-11-12","objectID":"/ooooo-notes/%E5%9C%A8-java-%E4%B8%AD%E5%A6%82%E4%BD%95%E8%BF%9B%E8%A1%8C%E5%A4%9A%E7%BA%BF%E7%A8%8B%E6%B5%8B%E8%AF%95/:3:0","tags":["java"],"title":"在 java 中如何进行多线程测试","uri":"/ooooo-notes/%E5%9C%A8-java-%E4%B8%AD%E5%A6%82%E4%BD%95%E8%BF%9B%E8%A1%8C%E5%A4%9A%E7%BA%BF%E7%A8%8B%E6%B5%8B%E8%AF%95/"},{"categories":null,"content":"1. 实现简单的 CAS 例子 CAS 相信大家都听过，就是 compareAndSet(V expectedValue, V newValue), 真正会用的人很少，这里的难点主要是无阻塞算法。 先实现一个简单 CAS 例子，只具有学习的意义。 getValue: 获取值 compareAndSet: 比较旧值，设置新值 public class SimulatedCAS { private int value; public SimulatedCAS(int value) { this.value = value; } public synchronized int getValue() { return value; } public synchronized boolean compareAndSet(int expectedValue, int newValue) { if (expectedValue == value) { this.value = newValue; return true; } return false; } } 重点：真正用 CAS 的时候，都是 while 循环 ","date":"2022-11-09","objectID":"/ooooo-notes/%E5%9C%A8-java-%E4%B8%AD%E4%BD%BF%E7%94%A8-cas-%E6%9D%A5%E5%AE%9E%E7%8E%B0%E6%A0%88/:1:0","tags":["java"],"title":"在 java 中使用 CAS 来实现栈","uri":"/ooooo-notes/%E5%9C%A8-java-%E4%B8%AD%E4%BD%BF%E7%94%A8-cas-%E6%9D%A5%E5%AE%9E%E7%8E%B0%E6%A0%88/"},{"categories":null,"content":"2. 用 CAS 来实现一个栈 代码： 用链表来实现，当然用数组实现也可以，比较麻烦一点，后面我再写一个示例 每次操作都是先 get 来获取 top 对象，然后再 compareAndSet top public class ConcurrentStack\u003cE\u003e { private final AtomicReference\u003cNode\u003cE\u003e\u003e top = new AtomicReference\u003c\u003e(); public void push(E e) { Node\u003cE\u003e newHead = new Node\u003c\u003e(e); Node\u003cE\u003e oldHead; do { oldHead = top.get(); newHead.next = oldHead; } while (!top.compareAndSet(oldHead, newHead)); } public E pop() { Node\u003cE\u003e oldHead; Node\u003cE\u003e newHead; do { oldHead = top.get(); if (oldHead == null) { return null; } newHead = oldHead.next; } while (!top.compareAndSet(oldHead, newHead)); oldHead.next = null; return oldHead.e; } private static class Node\u003cE\u003e { private final E e; private Node\u003cE\u003e next; public Node(E e) { this.e = e; } } } ","date":"2022-11-09","objectID":"/ooooo-notes/%E5%9C%A8-java-%E4%B8%AD%E4%BD%BF%E7%94%A8-cas-%E6%9D%A5%E5%AE%9E%E7%8E%B0%E6%A0%88/:2:0","tags":["java"],"title":"在 java 中使用 CAS 来实现栈","uri":"/ooooo-notes/%E5%9C%A8-java-%E4%B8%AD%E4%BD%BF%E7%94%A8-cas-%E6%9D%A5%E5%AE%9E%E7%8E%B0%E6%A0%88/"},{"categories":null,"content":"3. 代码实现位置 github 地址 ","date":"2022-11-09","objectID":"/ooooo-notes/%E5%9C%A8-java-%E4%B8%AD%E4%BD%BF%E7%94%A8-cas-%E6%9D%A5%E5%AE%9E%E7%8E%B0%E6%A0%88/:3:0","tags":["java"],"title":"在 java 中使用 CAS 来实现栈","uri":"/ooooo-notes/%E5%9C%A8-java-%E4%B8%AD%E4%BD%BF%E7%94%A8-cas-%E6%9D%A5%E5%AE%9E%E7%8E%B0%E6%A0%88/"},{"categories":null,"content":"grpc 使用方式 grpc 作为一个通信方式，现在可以说是非常流行。如果不会 grpc，你可能跟不上时代了, 这里我只是做一个很简单的例子，并说下如何进一步学习 grpc。 grpc 接口需要编写 .proto 文件，如下面的例子： 有一个接口类：Greeter. 有两个方法 SayHello, SayHi. syntax = \"proto3\"; option java_multiple_files = true; option java_package = \"com.ooooo.grpc.helloworld\"; option java_outer_classname = \"HelloWorldProto\"; package helloworld; // The greeting service definition. service Greeter { // Sends a greeting rpc SayHello (HelloRequest) returns (HelloReply) {} rpc SayHi (HelloRequest) returns (HelloReply) {} } // The request message containing the user's name. message HelloRequest { string name = 1; } // The response message containing the greetings message HelloReply { string message = 1; } 在这里，我很推荐大家看下，protobuf 是怎么编码的。 proto3, 官方地址： https://developers.google.com/protocol-buffers/docs/proto3 proto3 encoding, 官方地址： https://developers.google.com/protocol-buffers/docs/encoding 编写完 .proto 文件，执行 gradle 的 generateProto 任务, 就会生成相应的 java 代码。 然后编写入口程序 GrpcClient 和 GrpcServer。 ","date":"2022-10-22","objectID":"/ooooo-notes/%E5%9C%A8-java-%E4%B8%AD%E5%A6%82%E4%BD%95%E4%BD%BF%E7%94%A8-grpc/:1:0","tags":["java","grpc"],"title":"在 java 中如何使用 grpc","uri":"/ooooo-notes/%E5%9C%A8-java-%E4%B8%AD%E5%A6%82%E4%BD%95%E4%BD%BF%E7%94%A8-grpc/"},{"categories":null,"content":"如何进一步学习 grpc 学习 grpc 如何使用，如何扩展，可以看 https://github.com/grpc/grpc-java/tree/master/examples. 在 spring-boot 中如何使用，有开源的 starter. grpc 是基于 http2 协议的，你必须熟悉 http2 协议。 更深入的学习，也就是学习源码，有时间给大家说下 grpc 的源码，也是比较简单的。 ","date":"2022-10-22","objectID":"/ooooo-notes/%E5%9C%A8-java-%E4%B8%AD%E5%A6%82%E4%BD%95%E4%BD%BF%E7%94%A8-grpc/:2:0","tags":["java","grpc"],"title":"在 java 中如何使用 grpc","uri":"/ooooo-notes/%E5%9C%A8-java-%E4%B8%AD%E5%A6%82%E4%BD%95%E4%BD%BF%E7%94%A8-grpc/"},{"categories":null,"content":"2. 代码实现位置 github 地址 ","date":"2022-10-22","objectID":"/ooooo-notes/%E5%9C%A8-java-%E4%B8%AD%E5%A6%82%E4%BD%95%E4%BD%BF%E7%94%A8-grpc/:3:0","tags":["java","grpc"],"title":"在 java 中如何使用 grpc","uri":"/ooooo-notes/%E5%9C%A8-java-%E4%B8%AD%E5%A6%82%E4%BD%95%E4%BD%BF%E7%94%A8-grpc/"},{"categories":null,"content":"1. 添加打印 SQL 的方式 打印 SQL 的方式有很多，比如有 idea 插件，有 mybatis 拦截器，有代理 datasource, 有代理 driver. 我比较认可的方式就是代理 driver. 这种无任何侵入性。 下面来介绍如何使用 p6spy Driver。 示例代码： 使用 BeanPostProcessor 来动态扩展 bean。 判断 bean 是否为 DataSource 类型，并判断开发配置 dev.sql-log.enabled。 根据现有的 driver 配置来创建新的 datasource，并设置 url。 实际开启，还需要 spy.properties 配置文件 @Slf4j @Configuration public class DevDataSourceConfiguration implements BeanPostProcessor, EnvironmentAware { @Setter private Environment environment; @Override public Object postProcessAfterInitialization(Object bean, String beanName) throws BeansException { if (bean instanceof DataSource \u0026\u0026 parseBoolean(environment.resolvePlaceholders(\"${dev.sql-log.enabled:false}\"))) { log.info(\"已开启日志打印，将使用[P6SpyDriver]\"); if (environment.acceptsProfiles(Profiles.of(\"run\"))) { log.warn(\"在生产环境一定要关闭配置[dev.sql-log.enabled]\"); } return proxyDataSource((DataSource) bean); } return bean; } public DataSource proxyDataSource(DataSource dataSource) { if (dataSource instanceof AbstractRoutingDataSource) { AbstractRoutingDataSource abstractRoutingDataSource = (AbstractRoutingDataSource) dataSource; // resolvedDataSources Field resolvedDataSourcesField = findField(AbstractRoutingDataSource.class, \"resolvedDataSources\"); makeAccessible(resolvedDataSourcesField); @SuppressWarnings(\"unchecked\") Map\u003cObject, DataSource\u003e resolvedDataSources = (Map\u003cObject, DataSource\u003e) getField(resolvedDataSourcesField, abstractRoutingDataSource); if (resolvedDataSources != null) { resolvedDataSources.forEach((k, v) -\u003e resolvedDataSources.put(k, convertToProxyDataSource(v))); } // resolvedDefaultDataSource Field resolvedDefaultDataSourceField = findField(AbstractRoutingDataSource.class, \"resolvedDefaultDataSource\"); makeAccessible(resolvedDefaultDataSourceField); DataSource resolvedDefaultDataSource = (DataSource) getField(resolvedDefaultDataSourceField, abstractRoutingDataSource); if (resolvedDefaultDataSource != null) { ReflectionUtils.setField(resolvedDefaultDataSourceField, abstractRoutingDataSource, convertToProxyDataSource(resolvedDefaultDataSource)); } return abstractRoutingDataSource; } return convertToProxyDataSource(dataSource); } public DataSource convertToProxyDataSource(DataSource dataSource) { if (dataSource instanceof HikariDataSource) { HikariConfig oldConfig = (HikariDataSource) dataSource; // jdbc:h2:mem:test to jdbc:p6spy:h2:mem:test String jdbcUrl = oldConfig.getJdbcUrl(); if (!jdbcUrl.contains(\"p6spy\")) { jdbcUrl = \"jdbc:p6spy\" + jdbcUrl.substring(4); } HikariConfig newConfig = new HikariConfig(); newConfig.setPoolName(\"proxy-P6SpyDriver\"); newConfig.setDriverClassName(\"com.p6spy.engine.spy.P6SpyDriver\"); newConfig.setJdbcUrl(jdbcUrl); newConfig.setUsername(oldConfig.getUsername()); newConfig.setPassword(oldConfig.getPassword()); return new HikariDataSource(newConfig); } return dataSource; } } ","date":"2022-10-22","objectID":"/ooooo-notes/%E5%9C%A8-java-%E4%B8%AD%E5%A6%82%E4%BD%95%E6%B7%BB%E5%8A%A0-sql-%E6%97%A5%E5%BF%97/:1:0","tags":["java"],"title":"在 java 中如何添加 SQL 日志","uri":"/ooooo-notes/%E5%9C%A8-java-%E4%B8%AD%E5%A6%82%E4%BD%95%E6%B7%BB%E5%8A%A0-sql-%E6%97%A5%E5%BF%97/"},{"categories":null,"content":"2. 代码实现位置 github 地址 ","date":"2022-10-22","objectID":"/ooooo-notes/%E5%9C%A8-java-%E4%B8%AD%E5%A6%82%E4%BD%95%E6%B7%BB%E5%8A%A0-sql-%E6%97%A5%E5%BF%97/:2:0","tags":["java"],"title":"在 java 中如何添加 SQL 日志","uri":"/ooooo-notes/%E5%9C%A8-java-%E4%B8%AD%E5%A6%82%E4%BD%95%E6%B7%BB%E5%8A%A0-sql-%E6%97%A5%E5%BF%97/"},{"categories":null,"content":"1. 自定义 classloader 有时候，我们在项目开发的时候，会遇到比较恶心的问题，存在两个不同 jar 包，但是类的全限定名是一样的，而这两个包都不能删除，这时候调用可能就会出问题。 如何解决上面的问题？ 我的答案就是自定义类加载器。 场景模拟 module-a: 表示 a.jar module-b: 表示 b.jar module-main: 表示 程序入口 由于在 module-main 项目中同时引入了 module-a 和 module-b 这两个 jar 包，但是存在冲突类 HelloService, 最终导致程序运行错误。 如何自定义 classloader，来解决问题？ 重新定义 loadClass 方法，打破父类委托机制 使用 getResources 方法来获取所有的 class 文件，然后判断 @SneakyThrows public String test1(String message) { // 查找类 ClassLoader classLoader = new ModuleAClassLoader(); Class\u003c?\u003e clazz = classLoader.loadClass(\"com.ooooo.HelloService\"); // 执行 Method test1 = ReflectionUtils.findMethod(clazz, \"test1\", String.class); Object result = test1.invoke(null, message); return (String) result; } @SneakyThrows public String test2(String message) { // 查找类 ClassLoader classLoader = new ModuleBClassLoader(); Class\u003c?\u003e clazz = classLoader.loadClass(\"com.ooooo.HelloService\"); // 执行 Method test2 = ReflectionUtils.findMethod(clazz, \"test2\", String.class); Object result = test2.invoke(null, message); return (String) result; } private static class ModuleAClassLoader extends ClassLoader { public ModuleAClassLoader() { super(ModuleAClassLoader.class.getClassLoader()); } @SneakyThrows @Override protected Class\u003c?\u003e loadClass(String name, boolean resolve) throws ClassNotFoundException { Class\u003c?\u003e c = findLoadedClass(name); if (c == null) { // 当前路径下去找 if (name.contains(\"com.ooooo\")) { String path = name.replace(\".\", \"/\") + \".class\"; Enumeration\u003cURL\u003e resources = getResources(path); URL targetUrl = null; while (resources.hasMoreElements()) { targetUrl = resources.nextElement(); if (targetUrl.toString().contains(\"module-a\")) { break; } } // 读取 class 文件 InputStream in = targetUrl.openStream(); byte[] bytes = StreamUtils.copyToByteArray(in); in.close(); c = defineClass(name, bytes, 0, bytes.length); } } if (c == null) { c = getParent().loadClass(name); } if (resolve) { resolveClass(c); } return c; } } private static class ModuleBClassLoader extends ClassLoader { public ModuleBClassLoader() { super(ModuleAClassLoader.class.getClassLoader()); } @SneakyThrows @Override protected Class\u003c?\u003e loadClass(String name, boolean resolve) throws ClassNotFoundException { Class\u003c?\u003e c = findLoadedClass(name); if (c == null) { // 当前路径下去找 if (name.contains(\"com.ooooo\")) { String path = name.replace(\".\", \"/\") + \".class\"; Enumeration\u003cURL\u003e resources = getResources(path); URL targetUrl = null; while (resources.hasMoreElements()) { targetUrl = resources.nextElement(); if (targetUrl.toString().contains(\"module-b\")) { break; } } // 读取 class 文件 InputStream in = targetUrl.openStream(); byte[] bytes = StreamUtils.copyToByteArray(in); in.close(); c = defineClass(name, bytes, 0, bytes.length); } } if (c == null) { c = getParent().loadClass(name); } if (resolve) { resolveClass(c); } return c; } } } ","date":"2022-10-18","objectID":"/ooooo-notes/%E5%9C%A8-java-%E4%B8%AD%E5%A6%82%E4%BD%95%E8%A7%A3%E5%86%B3%E7%B1%BB%E5%86%B2%E7%AA%81%E9%97%AE%E9%A2%98/:1:0","tags":["java"],"title":"在 java 中如何解决类冲突问题","uri":"/ooooo-notes/%E5%9C%A8-java-%E4%B8%AD%E5%A6%82%E4%BD%95%E8%A7%A3%E5%86%B3%E7%B1%BB%E5%86%B2%E7%AA%81%E9%97%AE%E9%A2%98/"},{"categories":null,"content":"2. 代码实现位置 github 地址 ","date":"2022-10-18","objectID":"/ooooo-notes/%E5%9C%A8-java-%E4%B8%AD%E5%A6%82%E4%BD%95%E8%A7%A3%E5%86%B3%E7%B1%BB%E5%86%B2%E7%AA%81%E9%97%AE%E9%A2%98/:2:0","tags":["java"],"title":"在 java 中如何解决类冲突问题","uri":"/ooooo-notes/%E5%9C%A8-java-%E4%B8%AD%E5%A6%82%E4%BD%95%E8%A7%A3%E5%86%B3%E7%B1%BB%E5%86%B2%E7%AA%81%E9%97%AE%E9%A2%98/"},{"categories":null,"content":"1. methodHandle 调用 在过去，我们调用一个类的方法，除了直接调用，再就是使用反射来调用了。而今天我要说说 jdk 新引入的方式来调用。 比如我们有一个很简单的 UserService 类。 public class UserService { public String getUsername(String id) { return \"username\" + id; } } 直接调用和反射调用的方式比较简单，我就不说明了。 下面来演示 invoke 包的使用。 使用 MethodHandles.lookup() 来查找对应的方法 使用 methodHandle 来调用，分为几种不同的方式 public class UserServiceTest { private MethodType methodType; private Lookup lookup; private MethodHandle methodHandle; @SneakyThrows @BeforeEach public void beforeEach() { methodType = MethodType.methodType(String.class, String.class); lookup = MethodHandles.lookup(); methodHandle = lookup.findVirtual(UserService.class, \"getUsername\", methodType); } @SneakyThrows @Test public void invokeWithArguments() { UserService userService = new UserService(); Object obj = methodHandle.bindTo(userService).invokeWithArguments(\"1\"); assertEquals(\"username1\", obj); } @SneakyThrows @Test public void invoke() { UserService userService = new UserService(); Object obj = methodHandle.invoke(userService, \"1\"); assertEquals(\"username1\", obj); } /** * 要求类型全匹配, 包括返回值类型 */ @SneakyThrows @Test public void invokeExact() { UserService userService = new UserService(); String s = (String) methodHandle.invokeExact(userService, \"1\"); assertEquals(\"username1\", s); } } ","date":"2022-10-17","objectID":"/ooooo-notes/%E5%A6%82%E4%BD%95%E4%BD%BF%E7%94%A8-java-%E4%B8%AD-invoke-%E5%8C%85/:1:0","tags":["java"],"title":"如何使用 java 中 invoke 包?","uri":"/ooooo-notes/%E5%A6%82%E4%BD%95%E4%BD%BF%E7%94%A8-java-%E4%B8%AD-invoke-%E5%8C%85/"},{"categories":null,"content":"2. 代码实现位置 github 地址 ","date":"2022-10-17","objectID":"/ooooo-notes/%E5%A6%82%E4%BD%95%E4%BD%BF%E7%94%A8-java-%E4%B8%AD-invoke-%E5%8C%85/:2:0","tags":["java"],"title":"如何使用 java 中 invoke 包?","uri":"/ooooo-notes/%E5%A6%82%E4%BD%95%E4%BD%BF%E7%94%A8-java-%E4%B8%AD-invoke-%E5%8C%85/"},{"categories":null,"content":"1. jmh 微基准测试 实际上，在 java 中进行微基椎测试并不容易，主要原因在于解释执行，编译执行，而编译执行又分为 C1编译, C2编译。即使是对同一个代码来说，不同的 jvm 参数也会导致测试不一样。 那是否应该了解微基准测试？ 我的答案，是必须掌握的。 jmh 就是我们应该学习的微基准测试的框架。 下面我以一个示例来说明如何快速上手测试，测试 StringBuilder， StringBuffer， String 连接字符的性能。 注意点： Fork 参数可以测试不同的 Jvm 参数。 输出结果的时间单位最好是纳秒。 测试模式可以自由选择，如果测试性能，最好选择平均时间。 如果是在 IDE 中，建议安装 jmh 插件来执行。 代码示例 // 预热的参数 @Warmup(time = 1) // 测试的参数 @Measurement(time = 1) // 可以添加 JVM 参数来测试 @Fork(value = 1) @State(Scope.Thread) @OutputTimeUnit(TimeUnit.NANOSECONDS) @BenchmarkMode(Mode.AverageTime) public class TestStringBenchmark { @Benchmark public String stringBuilder() { StringBuilder sb = new StringBuilder(); sb.append(\"hello\"); sb.append(\"world\"); return sb.toString(); } @Benchmark public String stringBuffer() { StringBuffer sb = new StringBuffer(); sb.append(\"hello\"); sb.append(\"world\"); return sb.toString(); } @Benchmark public String stringConcat() { return \"hello\" + \"world\"; } } ","date":"2022-10-16","objectID":"/ooooo-notes/%E5%9C%A8-java-%E4%B8%AD%E5%A6%82%E4%BD%95%E8%BF%9B%E8%A1%8C%E5%BE%AE%E5%9F%BA%E5%87%86%E6%B5%8B%E8%AF%95/:1:0","tags":["java"],"title":"在 java 中如何进行微基准测试 ?","uri":"/ooooo-notes/%E5%9C%A8-java-%E4%B8%AD%E5%A6%82%E4%BD%95%E8%BF%9B%E8%A1%8C%E5%BE%AE%E5%9F%BA%E5%87%86%E6%B5%8B%E8%AF%95/"},{"categories":null,"content":"2. 代码实现位置 github 地址 ","date":"2022-10-16","objectID":"/ooooo-notes/%E5%9C%A8-java-%E4%B8%AD%E5%A6%82%E4%BD%95%E8%BF%9B%E8%A1%8C%E5%BE%AE%E5%9F%BA%E5%87%86%E6%B5%8B%E8%AF%95/:2:0","tags":["java"],"title":"在 java 中如何进行微基准测试 ?","uri":"/ooooo-notes/%E5%9C%A8-java-%E4%B8%AD%E5%A6%82%E4%BD%95%E8%BF%9B%E8%A1%8C%E5%BE%AE%E5%9F%BA%E5%87%86%E6%B5%8B%E8%AF%95/"},{"categories":null,"content":"3. 参考 强烈建议大家看官方代码, 地址: https://github.com/openjdk/jmh ","date":"2022-10-16","objectID":"/ooooo-notes/%E5%9C%A8-java-%E4%B8%AD%E5%A6%82%E4%BD%95%E8%BF%9B%E8%A1%8C%E5%BE%AE%E5%9F%BA%E5%87%86%E6%B5%8B%E8%AF%95/:3:0","tags":["java"],"title":"在 java 中如何进行微基准测试 ?","uri":"/ooooo-notes/%E5%9C%A8-java-%E4%B8%AD%E5%A6%82%E4%BD%95%E8%BF%9B%E8%A1%8C%E5%BE%AE%E5%9F%BA%E5%87%86%E6%B5%8B%E8%AF%95/"},{"categories":null,"content":"我们在开发过程中，经常会使用 lambda 函数式编程，这样会更加简单。 ","date":"2022-10-11","objectID":"/ooooo-notes/%E5%9C%A8-java-%E4%B8%AD%E5%A6%82%E4%BD%95%E8%8E%B7%E5%8F%96-lambda-%E7%9A%84%E6%96%B9%E6%B3%95%E5%90%8D%E7%A7%B0/:0:0","tags":["IDE"],"title":"在 java 中如何获取 lambda 的方法名称?","uri":"/ooooo-notes/%E5%9C%A8-java-%E4%B8%AD%E5%A6%82%E4%BD%95%E8%8E%B7%E5%8F%96-lambda-%E7%9A%84%E6%96%B9%E6%B3%95%E5%90%8D%E7%A7%B0/"},{"categories":null,"content":"1. 使用方式 比如下面有一个很简单的 User 类。其中有一个属性 username @Data public class User { private String username; public String getPassword(String password) { return password; } } // 使用的方式 // user -\u003e user.getUsername() 等价于 User::getUsername Function\u003cUser, String\u003e getUsername1 = User::getUsername 比如我现在要使用 user -\u003e user.getUsername()， 这样的 lambda 表达式来获取一个 User 对象的 username 属性值。 我现在可以这样来获取 username 这个方法名称。 public class LambdaUtils { // 正常情况，要做缓存 public static \u003cT\u003e String resolveMethod(SFunction\u003cT, ?\u003e func) { SerializedLambda serializedLambda = resovle(func); String methodName = serializedLambda.getImplMethodName(); return methodName; } @SneakyThrows public static SerializedLambda resovle(SFunction\u003c?, ?\u003e func) { Method method = func.getClass().getDeclaredMethod(\"writeReplace\"); method.setAccessible(true); return (SerializedLambda) method.invoke(func); } } ","date":"2022-10-11","objectID":"/ooooo-notes/%E5%9C%A8-java-%E4%B8%AD%E5%A6%82%E4%BD%95%E8%8E%B7%E5%8F%96-lambda-%E7%9A%84%E6%96%B9%E6%B3%95%E5%90%8D%E7%A7%B0/:1:0","tags":["IDE"],"title":"在 java 中如何获取 lambda 的方法名称?","uri":"/ooooo-notes/%E5%9C%A8-java-%E4%B8%AD%E5%A6%82%E4%BD%95%E8%8E%B7%E5%8F%96-lambda-%E7%9A%84%E6%96%B9%E6%B3%95%E5%90%8D%E7%A7%B0/"},{"categories":null,"content":"2. 代码实现位置 本节的内容，我叙述的不是很好，可能看的一脸懵, 使用过 mybatis-plus 的人，可能会有点印象 LambdaQueryWrapper， 推荐看下这个测试类 LambdaUtilsTests github 地址 ","date":"2022-10-11","objectID":"/ooooo-notes/%E5%9C%A8-java-%E4%B8%AD%E5%A6%82%E4%BD%95%E8%8E%B7%E5%8F%96-lambda-%E7%9A%84%E6%96%B9%E6%B3%95%E5%90%8D%E7%A7%B0/:2:0","tags":["IDE"],"title":"在 java 中如何获取 lambda 的方法名称?","uri":"/ooooo-notes/%E5%9C%A8-java-%E4%B8%AD%E5%A6%82%E4%BD%95%E8%8E%B7%E5%8F%96-lambda-%E7%9A%84%E6%96%B9%E6%B3%95%E5%90%8D%E7%A7%B0/"},{"categories":null,"content":"如何设计一个连接池 ? ","date":"2022-09-13","objectID":"/ooooo-notes/%E5%A6%82%E4%BD%95%E8%AE%BE%E8%AE%A1%E4%B8%80%E4%B8%AA%E8%BF%9E%E6%8E%A5%E6%B1%A0/:0:0","tags":["java"],"title":"如何设计一个连接池","uri":"/ooooo-notes/%E5%A6%82%E4%BD%95%E8%AE%BE%E8%AE%A1%E4%B8%80%E4%B8%AA%E8%BF%9E%E6%8E%A5%E6%B1%A0/"},{"categories":null,"content":"1. 需求 ","date":"2022-09-13","objectID":"/ooooo-notes/%E5%A6%82%E4%BD%95%E8%AE%BE%E8%AE%A1%E4%B8%80%E4%B8%AA%E8%BF%9E%E6%8E%A5%E6%B1%A0/:1:0","tags":["java"],"title":"如何设计一个连接池","uri":"/ooooo-notes/%E5%A6%82%E4%BD%95%E8%AE%BE%E8%AE%A1%E4%B8%80%E4%B8%AA%E8%BF%9E%E6%8E%A5%E6%B1%A0/"},{"categories":null,"content":"2. 实现的关键点 ","date":"2022-09-13","objectID":"/ooooo-notes/%E5%A6%82%E4%BD%95%E8%AE%BE%E8%AE%A1%E4%B8%80%E4%B8%AA%E8%BF%9E%E6%8E%A5%E6%B1%A0/:2:0","tags":["java"],"title":"如何设计一个连接池","uri":"/ooooo-notes/%E5%A6%82%E4%BD%95%E8%AE%BE%E8%AE%A1%E4%B8%80%E4%B8%AA%E8%BF%9E%E6%8E%A5%E6%B1%A0/"},{"categories":null,"content":"3. druid datasource ","date":"2022-09-13","objectID":"/ooooo-notes/%E5%A6%82%E4%BD%95%E8%AE%BE%E8%AE%A1%E4%B8%80%E4%B8%AA%E8%BF%9E%E6%8E%A5%E6%B1%A0/:3:0","tags":["java"],"title":"如何设计一个连接池","uri":"/ooooo-notes/%E5%A6%82%E4%BD%95%E8%AE%BE%E8%AE%A1%E4%B8%80%E4%B8%AA%E8%BF%9E%E6%8E%A5%E6%B1%A0/"},{"categories":null,"content":"4. common-pool2 ","date":"2022-09-13","objectID":"/ooooo-notes/%E5%A6%82%E4%BD%95%E8%AE%BE%E8%AE%A1%E4%B8%80%E4%B8%AA%E8%BF%9E%E6%8E%A5%E6%B1%A0/:4:0","tags":["java"],"title":"如何设计一个连接池","uri":"/ooooo-notes/%E5%A6%82%E4%BD%95%E8%AE%BE%E8%AE%A1%E4%B8%80%E4%B8%AA%E8%BF%9E%E6%8E%A5%E6%B1%A0/"},{"categories":null,"content":"如何设计一个对象池 ? ","date":"2022-09-12","objectID":"/ooooo-notes/%E5%A6%82%E4%BD%95%E8%AE%BE%E8%AE%A1%E4%B8%80%E4%B8%AA%E5%AF%B9%E8%B1%A1%E6%B1%A0/:0:0","tags":["java"],"title":"如何设计一个对象池 ?","uri":"/ooooo-notes/%E5%A6%82%E4%BD%95%E8%AE%BE%E8%AE%A1%E4%B8%80%E4%B8%AA%E5%AF%B9%E8%B1%A1%E6%B1%A0/"},{"categories":null,"content":"1. 需求 ","date":"2022-09-12","objectID":"/ooooo-notes/%E5%A6%82%E4%BD%95%E8%AE%BE%E8%AE%A1%E4%B8%80%E4%B8%AA%E5%AF%B9%E8%B1%A1%E6%B1%A0/:1:0","tags":["java"],"title":"如何设计一个对象池 ?","uri":"/ooooo-notes/%E5%A6%82%E4%BD%95%E8%AE%BE%E8%AE%A1%E4%B8%80%E4%B8%AA%E5%AF%B9%E8%B1%A1%E6%B1%A0/"},{"categories":null,"content":"2. 简单的实现 ","date":"2022-09-12","objectID":"/ooooo-notes/%E5%A6%82%E4%BD%95%E8%AE%BE%E8%AE%A1%E4%B8%80%E4%B8%AA%E5%AF%B9%E8%B1%A1%E6%B1%A0/:2:0","tags":["java"],"title":"如何设计一个对象池 ?","uri":"/ooooo-notes/%E5%A6%82%E4%BD%95%E8%AE%BE%E8%AE%A1%E4%B8%80%E4%B8%AA%E5%AF%B9%E8%B1%A1%E6%B1%A0/"},{"categories":null,"content":"3. 实现垃圾回收 软引用，弱引用 ","date":"2022-09-12","objectID":"/ooooo-notes/%E5%A6%82%E4%BD%95%E8%AE%BE%E8%AE%A1%E4%B8%80%E4%B8%AA%E5%AF%B9%E8%B1%A1%E6%B1%A0/:3:0","tags":["java"],"title":"如何设计一个对象池 ?","uri":"/ooooo-notes/%E5%A6%82%E4%BD%95%E8%AE%BE%E8%AE%A1%E4%B8%80%E4%B8%AA%E5%AF%B9%E8%B1%A1%E6%B1%A0/"},{"categories":null,"content":"在 idea 中实际有一个非常有用的功能，那就是远端构建和远端运行。 在我们实际开发项目中，自己的本地环境和服务器环境不太一样，例如 go 开发中的 build-tags, 还有 c/c++ 开发中的API 调用不一样，无法模拟相同的开发环境。 ","date":"2022-09-12","objectID":"/ooooo-notes/%E5%9C%A8-idea-%E4%B8%AD%E4%BD%BF%E7%94%A8%E8%BF%9C%E7%AB%AF-build-%E5%92%8C-run/:0:0","tags":["IDE"],"title":"在 idea 中使用远端 build 和 run","uri":"/ooooo-notes/%E5%9C%A8-idea-%E4%B8%AD%E4%BD%BF%E7%94%A8%E8%BF%9C%E7%AB%AF-build-%E5%92%8C-run/"},{"categories":null,"content":"1. 问题说明 这里拿 运行 kubernetes 来说明这个问题。 我们都知道在 windows 系统 和 mac 系统中，你是很难在本地运行 kubernetes 的，因为需要涉及到很多的组件，很多功能也只有在 linux 系统中才会开启。 一般来说，任何程序都应该是有远程调试这个功能的，但是这个功能在本地学习源码时会有点不方便，主要体现在自己在本地改了源码，需要同步到你远端的机器上，然后 build 和 run， 整套过程不是自动的。 ","date":"2022-09-12","objectID":"/ooooo-notes/%E5%9C%A8-idea-%E4%B8%AD%E4%BD%BF%E7%94%A8%E8%BF%9C%E7%AB%AF-build-%E5%92%8C-run/:1:0","tags":["IDE"],"title":"在 idea 中使用远端 build 和 run","uri":"/ooooo-notes/%E5%9C%A8-idea-%E4%B8%AD%E4%BD%BF%E7%94%A8%E8%BF%9C%E7%AB%AF-build-%E5%92%8C-run/"},{"categories":null,"content":"2. 使用教程 选择在远端机器上运行。 选择在远端机器上构建和编译完成后运行。 指定程序参数。 如下图： 在远端构建和运行 新建一个target配置。 新建或选择一个ssh配置, 对于 rsync 可以不用开启。 设置远端go的执行路径，这个必须要指定，如果 GOPATH 不指定，则使用远端默认配置*。 建议指定一个具体的路径，否则，每次都会生成新的目录，构建比较缓慢。 如下图： 在远端构建和运行 ","date":"2022-09-12","objectID":"/ooooo-notes/%E5%9C%A8-idea-%E4%B8%AD%E4%BD%BF%E7%94%A8%E8%BF%9C%E7%AB%AF-build-%E5%92%8C-run/:2:0","tags":["IDE"],"title":"在 idea 中使用远端 build 和 run","uri":"/ooooo-notes/%E5%9C%A8-idea-%E4%B8%AD%E4%BD%BF%E7%94%A8%E8%BF%9C%E7%AB%AF-build-%E5%92%8C-run/"},{"categories":null,"content":"在 spring 中，我们常常会基于现有的代码来扩展之前的功能，或者换一个实现的方式。 在上一篇中，我使用 BeanPostProcessor 来进行扩展。 而在这一篇中，我使用 BeanDefinitionRegistryPostProcessor 来进行扩展。 由于已经实现过一次，我这里就不多说了。 ","date":"2022-09-05","objectID":"/ooooo-notes/%E5%9C%A8-spring-%E4%B8%AD%E5%A6%82%E4%BD%95%E6%89%A9%E5%B1%95%E7%8E%B0%E6%9C%89%E7%B1%BB%E7%9A%84%E5%8A%9F%E8%83%BD2/:0:0","tags":["java","spring","spring-extension"],"title":"在 spring 中如何扩展现有类的功能2 ?","uri":"/ooooo-notes/%E5%9C%A8-spring-%E4%B8%AD%E5%A6%82%E4%BD%95%E6%89%A9%E5%B1%95%E7%8E%B0%E6%9C%89%E7%B1%BB%E7%9A%84%E5%8A%9F%E8%83%BD2/"},{"categories":null,"content":"1. 实现思路 判断 beanName 删除原有的 beanDefinition 注册新的 beanDefinition 注意点： 新实现的类，必须要是 CompositePropertySources 的子类，否则注入会有问题 所有方法都必须重新实现一遍，无法复用父类的方法 @Component public class CompositePropertySourcesBeanDefinitionRegistry implements BeanDefinitionRegistryPostProcessor { public static final String COMPOSITE_PROPERTY_SOURCES_BEAN_NAME = \"compositePropertySources\"; @Override public void postProcessBeanDefinitionRegistry(BeanDefinitionRegistry registry) throws BeansException { if (registry.containsBeanDefinition(COMPOSITE_PROPERTY_SOURCES_BEAN_NAME)) { registry.removeBeanDefinition(COMPOSITE_PROPERTY_SOURCES_BEAN_NAME); RootBeanDefinition definition = new RootBeanDefinition(ProxyCompositePropertySources.class); definition.setAutowireMode(AbstractBeanDefinition.AUTOWIRE_CONSTRUCTOR); registry.registerBeanDefinition(COMPOSITE_PROPERTY_SOURCES_BEAN_NAME, definition); } } @Override public void postProcessBeanFactory(ConfigurableListableBeanFactory beanFactory) throws BeansException { } } ","date":"2022-09-05","objectID":"/ooooo-notes/%E5%9C%A8-spring-%E4%B8%AD%E5%A6%82%E4%BD%95%E6%89%A9%E5%B1%95%E7%8E%B0%E6%9C%89%E7%B1%BB%E7%9A%84%E5%8A%9F%E8%83%BD2/:1:0","tags":["java","spring","spring-extension"],"title":"在 spring 中如何扩展现有类的功能2 ?","uri":"/ooooo-notes/%E5%9C%A8-spring-%E4%B8%AD%E5%A6%82%E4%BD%95%E6%89%A9%E5%B1%95%E7%8E%B0%E6%9C%89%E7%B1%BB%E7%9A%84%E5%8A%9F%E8%83%BD2/"},{"categories":null,"content":"2. 如何选择 如果是改进之前的功能，就使用第一种方式, BeanPostProcessor 如果是重写之前的功能，就使用第二种方式, BeanDefinitionRegistryPostProcessor 如果不好选择，就啥用第二种方式，也是最强大的。 ","date":"2022-09-05","objectID":"/ooooo-notes/%E5%9C%A8-spring-%E4%B8%AD%E5%A6%82%E4%BD%95%E6%89%A9%E5%B1%95%E7%8E%B0%E6%9C%89%E7%B1%BB%E7%9A%84%E5%8A%9F%E8%83%BD2/:2:0","tags":["java","spring","spring-extension"],"title":"在 spring 中如何扩展现有类的功能2 ?","uri":"/ooooo-notes/%E5%9C%A8-spring-%E4%B8%AD%E5%A6%82%E4%BD%95%E6%89%A9%E5%B1%95%E7%8E%B0%E6%9C%89%E7%B1%BB%E7%9A%84%E5%8A%9F%E8%83%BD2/"},{"categories":null,"content":"2. 完整代码实现 github 地址 ","date":"2022-09-05","objectID":"/ooooo-notes/%E5%9C%A8-spring-%E4%B8%AD%E5%A6%82%E4%BD%95%E6%89%A9%E5%B1%95%E7%8E%B0%E6%9C%89%E7%B1%BB%E7%9A%84%E5%8A%9F%E8%83%BD2/:3:0","tags":["java","spring","spring-extension"],"title":"在 spring 中如何扩展现有类的功能2 ?","uri":"/ooooo-notes/%E5%9C%A8-spring-%E4%B8%AD%E5%A6%82%E4%BD%95%E6%89%A9%E5%B1%95%E7%8E%B0%E6%9C%89%E7%B1%BB%E7%9A%84%E5%8A%9F%E8%83%BD2/"},{"categories":null,"content":"在 spring 中，我们常常会基于现有的代码来扩展之前的功能，或者换一个实现的方式。 ","date":"2022-09-05","objectID":"/ooooo-notes/%E5%9C%A8-spring-%E4%B8%AD%E5%A6%82%E4%BD%95%E6%89%A9%E5%B1%95%E7%8E%B0%E6%9C%89%E7%B1%BB%E7%9A%84%E5%8A%9F%E8%83%BD/:0:0","tags":["java","spring","spring-extension"],"title":"在 spring 中如何扩展现有类的功能 ?","uri":"/ooooo-notes/%E5%9C%A8-spring-%E4%B8%AD%E5%A6%82%E4%BD%95%E6%89%A9%E5%B1%95%E7%8E%B0%E6%9C%89%E7%B1%BB%E7%9A%84%E5%8A%9F%E8%83%BD/"},{"categories":null,"content":"1. 原有的功能 在这里基于之前的功能获取属性来继续深入。 大致代码如下 @Slf4j @Order public class CompositePropertySources implements PropertySources { private final MutablePropertySources mutablePropertySources = new MutablePropertySources(); public CompositePropertySources(List\u003cAbstractSimplePropertySource\u003e sources) { if (sources == null) return; AnnotationAwareOrderComparator.sort(sources); for (AbstractSimplePropertySource source : sources) { mutablePropertySources.addLast(source); } } public boolean containsProperty(String name) { return stream().anyMatch(p -\u003e p.containsProperty(name)); } public String getProperty(String name) { return isBlank(name) ? name : getProperty(name, null); } public String getProperty(String propertyName, String defaultValue) { String value = null; for (PropertySource\u003c?\u003e ps : mutablePropertySources) { value = (String) ps.getProperty(propertyName); if (value != null) { return value; } } return defaultValue; } public Map\u003cString, String\u003e getProperties(String... propertyNames) { if (propertyNames != null) { Map\u003cString, String\u003e map = new HashMap\u003c\u003e(); for (String key : propertyNames) { map.put(key, getProperty(key, null)); } return map; } return null; } } ","date":"2022-09-05","objectID":"/ooooo-notes/%E5%9C%A8-spring-%E4%B8%AD%E5%A6%82%E4%BD%95%E6%89%A9%E5%B1%95%E7%8E%B0%E6%9C%89%E7%B1%BB%E7%9A%84%E5%8A%9F%E8%83%BD/:1:0","tags":["java","spring","spring-extension"],"title":"在 spring 中如何扩展现有类的功能 ?","uri":"/ooooo-notes/%E5%9C%A8-spring-%E4%B8%AD%E5%A6%82%E4%BD%95%E6%89%A9%E5%B1%95%E7%8E%B0%E6%9C%89%E7%B1%BB%E7%9A%84%E5%8A%9F%E8%83%BD/"},{"categories":null,"content":"2. 新的需求 由于之前的功能是根据 key 来获取 value的，而现在需要根据业务编号和 key 来获取 value。 先根据 businType.key 来获取 value 如果结果不是 null，则返回 如果结果是 null， 再根据 key 来获取 value 根据上面的描述，也就是优先取业务类型的配置 因为我们的功能实际上在上一篇就已经完成了，所以在这一节中，只需要扩展原有的功能就行了。 这里我使用 BeanPostProcessor 来进行扩展，选择这个类的原因是原有的 bean 已经生成了，无需更改 bean 定义. 实现如下： 判断 bean 是否为 CompositePropertySources 的实例 使用 ProxyCompositePropertySources 对象来代替原有的类 使用 propertyNamesFunction 来分隔 propertyName @Component public class CompositePropertySourcesBeanPostProcessor implements BeanPostProcessor { @Override public Object postProcessAfterInitialization(Object bean, String beanName) throws BeansException { if (bean instanceof CompositePropertySources) { return new ProxyCompositePropertySources((CompositePropertySources) bean); } return bean; } protected static class ProxyCompositePropertySources extends CompositePropertySources { private final CompositePropertySources compositePropertySources; public ProxyCompositePropertySources(CompositePropertySources compositePropertySources) { super(null); this.compositePropertySources = compositePropertySources; } @Override public String getProperty(String propertyName, String defaultValue) { String[] propertyNames = propertyNamesFunction.apply(propertyName); for (String p : propertyNames) { String v = compositePropertySources.getProperty(p); if (v != null) { return v; } } return defaultValue; } @Override public boolean containsProperty(String propertyName) { String[] propertyNames = propertyNamesFunction.apply(propertyName); for (String p : propertyNames) { boolean contains = compositePropertySources.containsProperty(p); if (contains) { return true; } } return false; } } private static final Function\u003cString, String[]\u003e propertyNamesFunction = (propertyName) -\u003e { if (propertyName == null) { return new String[0]; } propertyName = propertyName.trim(); if (propertyName.contains(\".\")) { return new String[]{propertyName, propertyName.substring(propertyName.lastIndexOf(\".\") + 1)}; } return new String[]{propertyName}; }; } ","date":"2022-09-05","objectID":"/ooooo-notes/%E5%9C%A8-spring-%E4%B8%AD%E5%A6%82%E4%BD%95%E6%89%A9%E5%B1%95%E7%8E%B0%E6%9C%89%E7%B1%BB%E7%9A%84%E5%8A%9F%E8%83%BD/:2:0","tags":["java","spring","spring-extension"],"title":"在 spring 中如何扩展现有类的功能 ?","uri":"/ooooo-notes/%E5%9C%A8-spring-%E4%B8%AD%E5%A6%82%E4%BD%95%E6%89%A9%E5%B1%95%E7%8E%B0%E6%9C%89%E7%B1%BB%E7%9A%84%E5%8A%9F%E8%83%BD/"},{"categories":null,"content":"3. 完整代码实现 github 地址 ","date":"2022-09-05","objectID":"/ooooo-notes/%E5%9C%A8-spring-%E4%B8%AD%E5%A6%82%E4%BD%95%E6%89%A9%E5%B1%95%E7%8E%B0%E6%9C%89%E7%B1%BB%E7%9A%84%E5%8A%9F%E8%83%BD/:3:0","tags":["java","spring","spring-extension"],"title":"在 spring 中如何扩展现有类的功能 ?","uri":"/ooooo-notes/%E5%9C%A8-spring-%E4%B8%AD%E5%A6%82%E4%BD%95%E6%89%A9%E5%B1%95%E7%8E%B0%E6%9C%89%E7%B1%BB%E7%9A%84%E5%8A%9F%E8%83%BD/"},{"categories":null,"content":"1. 需求 希望根据 propertyName 来获取相应的 propertyValue, 这个接口需要支持多种数据来源。 ","date":"2022-09-05","objectID":"/ooooo-notes/%E5%9C%A8-spring-%E4%B8%AD%E5%A6%82%E4%BD%95%E8%AE%BE%E8%AE%A1%E4%B8%80%E4%B8%AA%E8%8E%B7%E5%8F%96%E9%85%8D%E7%BD%AE%E7%9A%84%E6%8E%A5%E5%8F%A3/:1:0","tags":["java","spring","spring-extension"],"title":"在 spring 中如何设计一个获取配置的接口?","uri":"/ooooo-notes/%E5%9C%A8-spring-%E4%B8%AD%E5%A6%82%E4%BD%95%E8%AE%BE%E8%AE%A1%E4%B8%80%E4%B8%AA%E8%8E%B7%E5%8F%96%E9%85%8D%E7%BD%AE%E7%9A%84%E6%8E%A5%E5%8F%A3/"},{"categories":null,"content":"2. 设计接口 很明显这个接口应该设计为这样, 有一个方法为 Object getProperty(String name) 来获取属性。 因为是在 spring 中，所以我就直接复用了 org.springframework.core.env.PropertySource, 但这个类需要泛型，所以我就随便实现了一个 Map\u003cString, String\u003e 的泛型，也不会用到这个。 抽象类的设计如下： public abstract class AbstractSimplePropertySource extends PropertySource\u003cMap\u003cString, String\u003e\u003e implements EnvironmentAware { protected Environment environment; public AbstractSimplePropertySource(String name) { super(name, Collections.emptyMap()); } public void setEnvironment(@NonNull Environment environment) { this.environment = environment; } public Environment getEnvironment() { return environment; } } ","date":"2022-09-05","objectID":"/ooooo-notes/%E5%9C%A8-spring-%E4%B8%AD%E5%A6%82%E4%BD%95%E8%AE%BE%E8%AE%A1%E4%B8%80%E4%B8%AA%E8%8E%B7%E5%8F%96%E9%85%8D%E7%BD%AE%E7%9A%84%E6%8E%A5%E5%8F%A3/:2:0","tags":["java","spring","spring-extension"],"title":"在 spring 中如何设计一个获取配置的接口?","uri":"/ooooo-notes/%E5%9C%A8-spring-%E4%B8%AD%E5%A6%82%E4%BD%95%E8%AE%BE%E8%AE%A1%E4%B8%80%E4%B8%AA%E8%8E%B7%E5%8F%96%E9%85%8D%E7%BD%AE%E7%9A%84%E6%8E%A5%E5%8F%A3/"},{"categories":null,"content":"3. 具体类的实现 ","date":"2022-09-05","objectID":"/ooooo-notes/%E5%9C%A8-spring-%E4%B8%AD%E5%A6%82%E4%BD%95%E8%AE%BE%E8%AE%A1%E4%B8%80%E4%B8%AA%E8%8E%B7%E5%8F%96%E9%85%8D%E7%BD%AE%E7%9A%84%E6%8E%A5%E5%8F%A3/:3:0","tags":["java","spring","spring-extension"],"title":"在 spring 中如何设计一个获取配置的接口?","uri":"/ooooo-notes/%E5%9C%A8-spring-%E4%B8%AD%E5%A6%82%E4%BD%95%E8%AE%BE%E8%AE%A1%E4%B8%80%E4%B8%AA%E8%8E%B7%E5%8F%96%E9%85%8D%E7%BD%AE%E7%9A%84%E6%8E%A5%E5%8F%A3/"},{"categories":null,"content":"3.1 根据请求参数来获取配置 先从请求参数中获取 再从请求头中获取 代码逻辑是比较简单的，我就不解释了。 这种形式，解决了前端可以传入相应的配置，来改变后端的执行逻辑。 @Order(0) public class RequestParamsPropertySource extends AbstractSimplePropertySource { public RequestParamsPropertySource() { super(ENV_PREFIX + \"request_params\"); } @Override public Object getProperty(String name) { String propertyKey = ENV_PREFIX + name.replace(\".\", \"_\"); String propertyValue = null; try { // 先请求参数中获取 ServletRequestAttributes attr = (ServletRequestAttributes) RequestContextHolder.currentRequestAttributes(); HttpServletRequest request = attr.getRequest(); propertyValue = request.getParameter(propertyKey); if (StringUtils.isBlank(propertyValue)) { // 再从请求头中获取 propertyValue = request.getHeader(propertyKey); } } catch (Throwable ignored) { } // 空字符也当做null propertyValue = defaultIfBlank(propertyValue, null); return propertyValue; } } ","date":"2022-09-05","objectID":"/ooooo-notes/%E5%9C%A8-spring-%E4%B8%AD%E5%A6%82%E4%BD%95%E8%AE%BE%E8%AE%A1%E4%B8%80%E4%B8%AA%E8%8E%B7%E5%8F%96%E9%85%8D%E7%BD%AE%E7%9A%84%E6%8E%A5%E5%8F%A3/:3:1","tags":["java","spring","spring-extension"],"title":"在 spring 中如何设计一个获取配置的接口?","uri":"/ooooo-notes/%E5%9C%A8-spring-%E4%B8%AD%E5%A6%82%E4%BD%95%E8%AE%BE%E8%AE%A1%E4%B8%80%E4%B8%AA%E8%8E%B7%E5%8F%96%E9%85%8D%E7%BD%AE%E7%9A%84%E6%8E%A5%E5%8F%A3/"},{"categories":null,"content":"3.2 根据环境变量来获取配置 代码逻辑是比较简单的，我就不解释了。 @Order(2) public class EnvironmentPropertySource extends AbstractSimplePropertySource { public EnvironmentPropertySource() { super(ENV_PREFIX + \"environment\"); } @Override public Object getProperty(String name) { String env_property_key = ENV_PREFIX + name.replace(\".\", \"_\"); return System.getenv(env_property_key.toUpperCase()); } } ","date":"2022-09-05","objectID":"/ooooo-notes/%E5%9C%A8-spring-%E4%B8%AD%E5%A6%82%E4%BD%95%E8%AE%BE%E8%AE%A1%E4%B8%80%E4%B8%AA%E8%8E%B7%E5%8F%96%E9%85%8D%E7%BD%AE%E7%9A%84%E6%8E%A5%E5%8F%A3/:3:2","tags":["java","spring","spring-extension"],"title":"在 spring 中如何设计一个获取配置的接口?","uri":"/ooooo-notes/%E5%9C%A8-spring-%E4%B8%AD%E5%A6%82%E4%BD%95%E8%AE%BE%E8%AE%A1%E4%B8%80%E4%B8%AA%E8%8E%B7%E5%8F%96%E9%85%8D%E7%BD%AE%E7%9A%84%E6%8E%A5%E5%8F%A3/"},{"categories":null,"content":"3.3 根据本地配置文件来获取参数 在实际开发过程，大家可能是共用一套数据库环境，在这个情况下，如果某一个人改了数据库配置，这样会对其他人造成影响，所以必须要设计出一个用于开发的配置类。 使用 apache 的 configuration 包，来实现配置文件的动态刷新 从 builder.getConfiguration() 对象中获取配置 大致代码如下： @Order(1) public class LocalPropertiesPropertySource extends AbstractSimplePropertySource implements InitializingBean { @Autowired private ApplicationEventPublisher publisher; private ReloadingFileBasedConfigurationBuilder\u003cPropertiesConfiguration\u003e builder; private static final String DEFAULT_LOCAL_PROPERTIES_PATH = \"sysoptions.properties\"; public LocalPropertiesPropertySource() { super(ENV_PREFIX + \"local_properties\"); log.debug(\"开发环境，启用[{}]配置\", getClass()); } @Override public void afterPropertiesSet() { Integer sysOptionsLoadInterval = getEnvironment().getProperty(\"dev.localPropertiesLoadInterval\", Integer.class, 1); String sysOptionsPath = getEnvironment().getProperty(\"dev.localPropertiesPath\", DEFAULT_LOCAL_PROPERTIES_PATH); File propertiesFile = new File(sysOptionsPath); if (!propertiesFile.exists()) { return; } // server boot will publish event publisher.publishEvent(new LocalProperitesReReloadingEvent(new Object(), propertiesFile.getAbsolutePath())); builder = new ReloadingFileBasedConfigurationBuilder\u003c\u003e(PropertiesConfiguration.class).configure(new Parameters().fileBased().setFile(propertiesFile)); ReloadingController reloadingController = builder.getReloadingController(); reloadingController.addEventListener(ReloadingEvent.ANY, e -\u003e publisher.publishEvent(new LocalProperitesReReloadingEvent(e, propertiesFile.getAbsolutePath()))); PeriodicReloadingTrigger trigger = new PeriodicReloadingTrigger(reloadingController, null, sysOptionsLoadInterval, SECONDS); trigger.start(); } @Override public Object getProperty(@NonNull String name) { if (builder == null) return null; Configuration configuration = null; try { configuration = builder.getConfiguration(); String config_value = configuration.getString(name); if (config_value != null) { return config_value; } } catch (ConfigurationException e) { log.error(e.getMessage(), e); } return null; } } ","date":"2022-09-05","objectID":"/ooooo-notes/%E5%9C%A8-spring-%E4%B8%AD%E5%A6%82%E4%BD%95%E8%AE%BE%E8%AE%A1%E4%B8%80%E4%B8%AA%E8%8E%B7%E5%8F%96%E9%85%8D%E7%BD%AE%E7%9A%84%E6%8E%A5%E5%8F%A3/:3:3","tags":["java","spring","spring-extension"],"title":"在 spring 中如何设计一个获取配置的接口?","uri":"/ooooo-notes/%E5%9C%A8-spring-%E4%B8%AD%E5%A6%82%E4%BD%95%E8%AE%BE%E8%AE%A1%E4%B8%80%E4%B8%AA%E8%8E%B7%E5%8F%96%E9%85%8D%E7%BD%AE%E7%9A%84%E6%8E%A5%E5%8F%A3/"},{"categories":null,"content":"3.4 其他的扩展 到这里，你就应该很清楚的知道怎么去扩展其他的类了，比如数据库的实现， redis 的实现 ","date":"2022-09-05","objectID":"/ooooo-notes/%E5%9C%A8-spring-%E4%B8%AD%E5%A6%82%E4%BD%95%E8%AE%BE%E8%AE%A1%E4%B8%80%E4%B8%AA%E8%8E%B7%E5%8F%96%E9%85%8D%E7%BD%AE%E7%9A%84%E6%8E%A5%E5%8F%A3/:3:4","tags":["java","spring","spring-extension"],"title":"在 spring 中如何设计一个获取配置的接口?","uri":"/ooooo-notes/%E5%9C%A8-spring-%E4%B8%AD%E5%A6%82%E4%BD%95%E8%AE%BE%E8%AE%A1%E4%B8%80%E4%B8%AA%E8%8E%B7%E5%8F%96%E9%85%8D%E7%BD%AE%E7%9A%84%E6%8E%A5%E5%8F%A3/"},{"categories":null,"content":"4. 使用的入口 上面只是定义了一个接口和几个实现类，统一的入口类，实际上还没有。 使用构造函数的方式来注入所有的 AbstractSimplePropertySource 。 对于每个获取配置的类，肯定有优先级，所以要排序。 使用 MutablePropertySources 这个类做辅助。 大致代码如下： @Slf4j @Order public class CompositePropertySources implements PropertySources { private final MutablePropertySources mutablePropertySources = new MutablePropertySources(); public CompositePropertySources(List\u003cAbstractSimplePropertySource\u003e sources) { if (sources == null) return; AnnotationAwareOrderComparator.sort(sources); for (AbstractSimplePropertySource source : sources) { mutablePropertySources.addLast(source); } } public boolean containsProperty(String name) { return stream().anyMatch(p -\u003e p.containsProperty(name)); } public String getProperty(String name) { return isBlank(name) ? name : getProperty(name, null); } public String getProperty(String propertyName, String defaultValue) { String value = null; for (PropertySource\u003c?\u003e ps : mutablePropertySources) { value = (String) ps.getProperty(propertyName); if (value != null) { return value; } } return defaultValue; } public Map\u003cString, String\u003e getProperties(String... propertyNames) { if (propertyNames != null) { Map\u003cString, String\u003e map = new HashMap\u003c\u003e(); for (String key : propertyNames) { map.put(key, getProperty(key, null)); } return map; } return null; } } ","date":"2022-09-05","objectID":"/ooooo-notes/%E5%9C%A8-spring-%E4%B8%AD%E5%A6%82%E4%BD%95%E8%AE%BE%E8%AE%A1%E4%B8%80%E4%B8%AA%E8%8E%B7%E5%8F%96%E9%85%8D%E7%BD%AE%E7%9A%84%E6%8E%A5%E5%8F%A3/:4:0","tags":["java","spring","spring-extension"],"title":"在 spring 中如何设计一个获取配置的接口?","uri":"/ooooo-notes/%E5%9C%A8-spring-%E4%B8%AD%E5%A6%82%E4%BD%95%E8%AE%BE%E8%AE%A1%E4%B8%80%E4%B8%AA%E8%8E%B7%E5%8F%96%E9%85%8D%E7%BD%AE%E7%9A%84%E6%8E%A5%E5%8F%A3/"},{"categories":null,"content":"5. 完整代码实现 github 地址 ","date":"2022-09-05","objectID":"/ooooo-notes/%E5%9C%A8-spring-%E4%B8%AD%E5%A6%82%E4%BD%95%E8%AE%BE%E8%AE%A1%E4%B8%80%E4%B8%AA%E8%8E%B7%E5%8F%96%E9%85%8D%E7%BD%AE%E7%9A%84%E6%8E%A5%E5%8F%A3/:5:0","tags":["java","spring","spring-extension"],"title":"在 spring 中如何设计一个获取配置的接口?","uri":"/ooooo-notes/%E5%9C%A8-spring-%E4%B8%AD%E5%A6%82%E4%BD%95%E8%AE%BE%E8%AE%A1%E4%B8%80%E4%B8%AA%E8%8E%B7%E5%8F%96%E9%85%8D%E7%BD%AE%E7%9A%84%E6%8E%A5%E5%8F%A3/"},{"categories":null,"content":"作为一个 Java 开发，Spring 的技术可以说是必须要掌握的，不仅仅是会使用，而且要掌握原理，学会扩展。 今天我就说说，哪些核心类和扩展类是必须要掌握的，同时我也说明这些扩展可以干什么，后面 Spring 文章，我会用到这些扩展类，让你学懂这些类。 ","date":"2022-09-05","objectID":"/ooooo-notes/%E5%9C%A8-spring-%E4%B8%AD%E6%9C%89%E5%93%AA%E4%BA%9B%E6%A0%B8%E5%BF%83%E7%B1%BB%E5%92%8C%E6%89%A9%E5%B1%95%E7%B1%BB/:0:0","tags":["java","spring"],"title":"在 spring 中有哪些核心类和扩展类?","uri":"/ooooo-notes/%E5%9C%A8-spring-%E4%B8%AD%E6%9C%89%E5%93%AA%E4%BA%9B%E6%A0%B8%E5%BF%83%E7%B1%BB%E5%92%8C%E6%89%A9%E5%B1%95%E7%B1%BB/"},{"categories":null,"content":"核心类： IOC容器: org.springframework.context.ApplicationContext 配置类: org.springframework.core.env.Environment Bean工厂：org.springframework.beans.factory.BeanFactory 事件发布器： org.springframework.context.ApplicationEventPublisher 资源加载器： org.springframework.core.io.ResourceLoader 上面这几个类，是我们经常会用到的。它们都有相应的 Aware 接口, 如 org.springframework.context.ApplicationContextAware, 可以设置 applicationContext 对象到我们自己定义的 bean 对象中. 注意这样 setApplicationContext 的方式比 @Autowired 注解注入 applicationContext 的方式的时机要早很多，所以一般推荐用 setApplicationContext 的方式。 相应的源码 ApplicationContextAwareProcessor ","date":"2022-09-05","objectID":"/ooooo-notes/%E5%9C%A8-spring-%E4%B8%AD%E6%9C%89%E5%93%AA%E4%BA%9B%E6%A0%B8%E5%BF%83%E7%B1%BB%E5%92%8C%E6%89%A9%E5%B1%95%E7%B1%BB/:1:0","tags":["java","spring"],"title":"在 spring 中有哪些核心类和扩展类?","uri":"/ooooo-notes/%E5%9C%A8-spring-%E4%B8%AD%E6%9C%89%E5%93%AA%E4%BA%9B%E6%A0%B8%E5%BF%83%E7%B1%BB%E5%92%8C%E6%89%A9%E5%B1%95%E7%B1%BB/"},{"categories":null,"content":"扩展类 beanFactory的后置处理器： org.springframework.beans.factory.config.BeanFactoryPostProcessor bean的后置处理器： org.springframework.beans.factory.config.BeanPostProcessor 上面的两个类非常重要，如果你现在还不会熟练使用它们，说明 spring 掌握的很一般。 ","date":"2022-09-05","objectID":"/ooooo-notes/%E5%9C%A8-spring-%E4%B8%AD%E6%9C%89%E5%93%AA%E4%BA%9B%E6%A0%B8%E5%BF%83%E7%B1%BB%E5%92%8C%E6%89%A9%E5%B1%95%E7%B1%BB/:2:0","tags":["java","spring"],"title":"在 spring 中有哪些核心类和扩展类?","uri":"/ooooo-notes/%E5%9C%A8-spring-%E4%B8%AD%E6%9C%89%E5%93%AA%E4%BA%9B%E6%A0%B8%E5%BF%83%E7%B1%BB%E5%92%8C%E6%89%A9%E5%B1%95%E7%B1%BB/"},{"categories":null,"content":"非常有用的类 代理工厂： org.springframework.aop.framework.ProxyFactory bean工厂： org.springframework.beans.factory.ObjectFactory 属性绑定： org.springframework.boot.context.properties.bind.Binder 选择性导入bean: org.springframework.context.annotation.ImportSelector 上面这几个类，一般在扩展功能时，都会用到。 ","date":"2022-09-05","objectID":"/ooooo-notes/%E5%9C%A8-spring-%E4%B8%AD%E6%9C%89%E5%93%AA%E4%BA%9B%E6%A0%B8%E5%BF%83%E7%B1%BB%E5%92%8C%E6%89%A9%E5%B1%95%E7%B1%BB/:3:0","tags":["java","spring"],"title":"在 spring 中有哪些核心类和扩展类?","uri":"/ooooo-notes/%E5%9C%A8-spring-%E4%B8%AD%E6%9C%89%E5%93%AA%E4%BA%9B%E6%A0%B8%E5%BF%83%E7%B1%BB%E5%92%8C%E6%89%A9%E5%B1%95%E7%B1%BB/"},{"categories":null,"content":"1. 下载代码 git clone git@github.com:apache/tomcat.git ","date":"2022-08-10","objectID":"/ooooo-notes/%E6%90%AD%E5%BB%BA-tomcat-%E6%BA%90%E7%A0%81%E8%B0%83%E8%AF%95%E7%8E%AF%E5%A2%83/:1:0","tags":["tomcat","source code"],"title":"搭建 tomcat 源码调试环境","uri":"/ooooo-notes/%E6%90%AD%E5%BB%BA-tomcat-%E6%BA%90%E7%A0%81%E8%B0%83%E8%AF%95%E7%8E%AF%E5%A2%83/"},{"categories":null,"content":"2. 安装ant 我本地安装的是 1.10.12 版本, ant 下载地址 配置环境变量 ANT_HOME, 加入到 PATH 环境变量中 执行命令验证 ant -version ","date":"2022-08-10","objectID":"/ooooo-notes/%E6%90%AD%E5%BB%BA-tomcat-%E6%BA%90%E7%A0%81%E8%B0%83%E8%AF%95%E7%8E%AF%E5%A2%83/:2:0","tags":["tomcat","source code"],"title":"搭建 tomcat 源码调试环境","uri":"/ooooo-notes/%E6%90%AD%E5%BB%BA-tomcat-%E6%BA%90%E7%A0%81%E8%B0%83%E8%AF%95%E7%8E%AF%E5%A2%83/"},{"categories":null,"content":"3. 导入到 idea 中 # 进入 tomcat 根目前 cd tomcat # 复制配置文件 build.properties cp build.properties.default build.properties # 更改 build.properties 中的配置 base.path=第三方jar的下载目录 # 设置 idea ant ide-intellij # 执行编译命令, 会生成 output 目录 ant deploy 然后用 idea 打开项目，idea 会弹出让你配置下面的变量 ANT_HOME = ${ant.home} TOMCAT_BUILD_LIBS = ${base.path} ","date":"2022-08-10","objectID":"/ooooo-notes/%E6%90%AD%E5%BB%BA-tomcat-%E6%BA%90%E7%A0%81%E8%B0%83%E8%AF%95%E7%8E%AF%E5%A2%83/:3:0","tags":["tomcat","source code"],"title":"搭建 tomcat 源码调试环境","uri":"/ooooo-notes/%E6%90%AD%E5%BB%BA-tomcat-%E6%BA%90%E7%A0%81%E8%B0%83%E8%AF%95%E7%8E%AF%E5%A2%83/"},{"categories":null,"content":"4. idea 中 配置 检查你的项目依赖有没有问题 项目依赖配置 上面的三个依赖，其实就是 ServletContainerInitializer 的实现, 比如 res/META-INF/jasper.jar/services/jakarta.servlet.ServletContainerInitializer. 更改配置文件 conf/server.xml # 改为编译输出目录 appBase=\"output/build/webapps\" 运行程序 org.apache.catalina.startup.Bootstrap#main ","date":"2022-08-10","objectID":"/ooooo-notes/%E6%90%AD%E5%BB%BA-tomcat-%E6%BA%90%E7%A0%81%E8%B0%83%E8%AF%95%E7%8E%AF%E5%A2%83/:4:0","tags":["tomcat","source code"],"title":"搭建 tomcat 源码调试环境","uri":"/ooooo-notes/%E6%90%AD%E5%BB%BA-tomcat-%E6%BA%90%E7%A0%81%E8%B0%83%E8%AF%95%E7%8E%AF%E5%A2%83/"},{"categories":null,"content":" deployment 资源是我们经常需要使用的，也是我们最应该熟悉的源码. 对于调试源码，我使用是 deployment_controller_test.go 测试类， TestSyncDeploymentCreatesReplicaSet 方法. ","date":"2022-07-15","objectID":"/ooooo-notes/%E8%B0%83%E8%AF%95-deployment-controller-%E7%9A%84%E6%BA%90%E7%A0%81/:0:0","tags":["k8s","cloud native","source code"],"title":"调试 deployment-controller 的源码","uri":"/ooooo-notes/%E8%B0%83%E8%AF%95-deployment-controller-%E7%9A%84%E6%BA%90%E7%A0%81/"},{"categories":null,"content":"TestSyncDeploymentCreatesReplicaSet 测试方法的结构 源码路径：kubernetes\\pkg\\controller\\deployment\\deployment_controller_test.go 测试配置对象 f := newFixture(t) 创建一个 fixture 对象， 里面有 objects 属性，这个用来模拟 clientSet, 也就是请求 etcd 的接口，后面将会详细描述。 创建一个 Deployment 对象， 标签为 “foo”: “bar” d := newDeployment(\"foo\", 1, nil, nil, nil, map[string]string{\"foo\": \"bar\"}) 添加缓存对象，用于后续的List接口 f.dLister = append(f.dLister, d) f.objects = append(f.objects, d) 创建一个 ReplicaSet 对象 rs := newReplicaSet(d, \"deploymentrs-4186632231\", 1) 希望的测试结果 f.expectCreateRSAction(rs) f.expectUpdateDeploymentStatusAction(d) f.expectUpdateDeploymentStatusAction(d) 从上面的语句就可以发现，kubernetes 的测试类，意图非常明确。 也就是说 我创建一个 Deployment ** 对象，肯定会产生一个 ReplicaSet 对象，并且 DeploymentStatus 会被更新两次, 接下来，我们来看看是kubernetes如何做到的。 ","date":"2022-07-15","objectID":"/ooooo-notes/%E8%B0%83%E8%AF%95-deployment-controller-%E7%9A%84%E6%BA%90%E7%A0%81/:1:0","tags":["k8s","cloud native","source code"],"title":"调试 deployment-controller 的源码","uri":"/ooooo-notes/%E8%B0%83%E8%AF%95-deployment-controller-%E7%9A%84%E6%BA%90%E7%A0%81/"},{"categories":null,"content":"测试方法的执行 f.run(testutil.GetKey(d, t)) 获取 Deployment 的 key 属性 代码 testutil.GetKey(d, t) func GetKey(obj interface{}, t *testing.T) string { // 每个删除的对象都是这个类型, 这里取出了真实的对象 tombstone, ok := obj.(cache.DeletedFinalStateUnknown) if ok { // if tombstone , try getting the value from tombstone.Obj obj = tombstone.Obj } // 取出指针类型中 value，获取 Name 属性 val := reflect.ValueOf(obj).Elem() name := val.FieldByName(\"Name\").String() if len(name) == 0 { t.Errorf(\"Unexpected object %v\", obj) } // 获取key, 结果就是 {namespace}/{name} key, err := keyFunc(obj) if err != nil { t.Errorf(\"Unexpected error getting key for %T %v: %v\", val.Interface(), name, err) return \"\" } return key } 调用 fixture.run_() 方法 这个函数有三个入参：run_(deploymentName string, startInformers bool, expectError bool). 当前的测试方法 TestSyncDeploymentCreatesReplicaSet 的 startInformers 参数为 false, 表示不启动 Informer, 后续会用另外一个测试类来说明 Informer 的启动过程. 接下来详细看看 fixture.run_() 方法都干了啥。 创建一个 controller c, informers, err := f.newController() func (f *fixture) newController() (*DeploymentController, informers.SharedInformerFactory, error) { // 这个也就是之前说的，objects 会用来构建 模拟的 clientSet f.client = fake.NewSimpleClientset(f.objects...) // 创建了 informer 和 deploymentController informers := informers.NewSharedInformerFactory(f.client, controller.NoResyncPeriodFunc()) c, err := NewDeploymentController(informers.Apps().V1().Deployments(), informers.Apps().V1().ReplicaSets(), informers.Core().V1().Pods(), f.client) if err != nil { return nil, nil, err } // 模拟一个 recorder c.eventRecorder = \u0026record.FakeRecorder{} // 所有状态默认为 synced c.dListerSynced = alwaysReady c.rsListerSynced = alwaysReady c.podListerSynced = alwaysReady // 下面这个代码很关键 // 先前在 fixture 对象中加入了相应的 Lister，在这里遍历这些 Lister, 就是为了模拟 Informer 的本地缓存 // kube_controller_manager 程序启动之后，会请求 kube_apiserver 来获取相应的资源，从而更新到自己的缓存中 for _, d := range f.dLister { informers.Apps().V1().Deployments().Informer().GetIndexer().Add(d) } for _, rs := range f.rsLister { informers.Apps().V1().ReplicaSets().Informer().GetIndexer().Add(rs) } for _, pod := range f.podLister { informers.Core().V1().Pods().Informer().GetIndexer().Add(pod) } return c, informers, nil } 详细分析是怎么模拟 clientSet 的？ f.client = fake.NewSimpleClientset(f.objects...) func NewSimpleClientset(objects ...runtime.Object) *Clientset { // 这个是模拟的最终实现对象，所有操作都是依赖它来完成的 o := testing.NewObjectTracker(scheme, codecs.UniversalDecoder()) // 遍历对象，依次添加 for _, obj := range objects { if err := o.Add(obj); err != nil { panic(err) } } // 创建一个 clientSet cs := \u0026Clientset{tracker: o} // 下面三个都是依赖 tracker 来实现的， 通过不同的 Action, 比如 ListActionImpl、GetActionImpl 等 cs.discovery = \u0026fakediscovery.FakeDiscovery{Fake: \u0026cs.Fake} cs.AddReactor(\"*\", \"*\", testing.ObjectReaction(o)) cs.AddWatchReactor(\"*\", func(action testing.Action) (handled bool, ret watch.Interface, err error) { gvr := action.GetResource() ns := action.GetNamespace() watch, err := o.Watch(gvr, ns) if err != nil { return false, nil, err } return true, watch, nil }) return cs } ObjectTracker 如果添加对象的？ testing.NewObjectTracker(scheme, codecs.UniversalDecoder()) func (t *tracker) Add(obj runtime.Object) error { // 添加 List if meta.IsListType(obj) { return t.addList(obj, false) } // 用来获取 namespace objMeta, err := meta.Accessor(obj) // 获取 gvk gvks, _, err := t.scheme.ObjectKinds(obj) for _, gvk := range gvks { // NOTE: UnsafeGuessKindToResource is a heuristic and default match. The // actual registration in apiserver can specify arbitrary route for a // gvk. If a test uses such objects, it cannot preset the tracker with // objects via Add(). Instead, it should trigger the Create() function // of the tracker, where an arbitrary gvr can be specified. gvr, _ := meta.UnsafeGuessKindToResource(gvk) // Resource doesn't have the concept of \"__internal\" version, just set it to \"\". if gvr.Version == runtime.APIVersionInternal { gvr.Version = \"\" } // 添加这个 err := t.add(gvr, obj, objMeta.GetNamespace(), false) if err != nil { return err } } return nil } func (t *tracker) add(gvr schema.GroupVersionResource, obj runtime.Object, ns st","date":"2022-07-15","objectID":"/ooooo-notes/%E8%B0%83%E8%AF%95-deployment-controller-%E7%9A%84%E6%BA%90%E7%A0%81/:2:0","tags":["k8s","cloud native","source code"],"title":"调试 deployment-controller 的源码","uri":"/ooooo-notes/%E8%B0%83%E8%AF%95-deployment-controller-%E7%9A%84%E6%BA%90%E7%A0%81/"},{"categories":null,"content":"1. GVK 定义 GVK(group version kind): 资源组、资源版本、资源类型 表示 apps 组下 v1 版本 Deployment 类型的资源。 apiVersion: apps/v1 kind: Deployment 表示 core 组下 v1 版本 Pod 类型的资源。(没有组信息表示核心组) apiVersion: v1 kind: Pod ","date":"2022-07-02","objectID":"/ooooo-notes/%E5%AD%A6%E4%B9%A0-k8s-%E6%BA%90%E7%A0%81%E7%9A%84%E5%89%8D%E7%BD%AE%E7%9F%A5%E8%AF%86/:1:0","tags":["k8s","cloud native","source code"],"title":"学习 k8s 源码的前置知识","uri":"/ooooo-notes/%E5%AD%A6%E4%B9%A0-k8s-%E6%BA%90%E7%A0%81%E7%9A%84%E5%89%8D%E7%BD%AE%E7%9F%A5%E8%AF%86/"},{"categories":null,"content":"2. kubernetes 对象结构 每个对象都可以分为四个部分。 例如 Deployment 资源： TypeMeta: GVK 信息 ObjectMeta: 对象元数据，比如有属性 name、namespace DeploymentSpec: 对象定义规范，比如有属性 replicas(控制副本数量)、template(定义 Pod 的模板)、selector(标签选择器，与 Pod 标签一样)、strategy(Pod 升级策略) DeploymentStatus: 对象运行时状态， 比如有属性 replicas(总的副本数) 代码路径：kubernetes\\vendor\\k8s.io\\api\\apps\\v1\\types.go type Deployment struct { metav1.TypeMeta `json:\",inline\"` // Standard object's metadata. // More info: https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata // +optional metav1.ObjectMeta `json:\"metadata,omitempty\" protobuf:\"bytes,1,opt,name=metadata\"` // Specification of the desired behavior of the Deployment. // +optional Spec DeploymentSpec `json:\"spec,omitempty\" protobuf:\"bytes,2,opt,name=spec\"` // Most recently observed status of the Deployment. // +optional Status DeploymentStatus `json:\"status,omitempty\" protobuf:\"bytes,3,opt,name=status\"` } ","date":"2022-07-02","objectID":"/ooooo-notes/%E5%AD%A6%E4%B9%A0-k8s-%E6%BA%90%E7%A0%81%E7%9A%84%E5%89%8D%E7%BD%AE%E7%9F%A5%E8%AF%86/:2:0","tags":["k8s","cloud native","source code"],"title":"学习 k8s 源码的前置知识","uri":"/ooooo-notes/%E5%AD%A6%E4%B9%A0-k8s-%E6%BA%90%E7%A0%81%E7%9A%84%E5%89%8D%E7%BD%AE%E7%9F%A5%E8%AF%86/"},{"categories":null,"content":"3. kubernetes 源码目录结构 cmd: 可执行程序包， 例如 kubelet 的入口为 kubernetes\\cmd\\kubelet\\kubelet.go pkg: kubernetes 包路径, 有些子目录与 cmd 目录一样，就是入口文件依赖的包 vendor: 第三方包，其中也有 kubernetes 的包 plugin: 准入插件和认证插件 hack: 脚本路径，非常有用 api: OpenAPI 定义 上述,只是简单的描述了，目前只需要知道 cmd, pkg, vendor 是非常重要的，也是我们经常看的。 ","date":"2022-07-02","objectID":"/ooooo-notes/%E5%AD%A6%E4%B9%A0-k8s-%E6%BA%90%E7%A0%81%E7%9A%84%E5%89%8D%E7%BD%AE%E7%9F%A5%E8%AF%86/:3:0","tags":["k8s","cloud native","source code"],"title":"学习 k8s 源码的前置知识","uri":"/ooooo-notes/%E5%AD%A6%E4%B9%A0-k8s-%E6%BA%90%E7%A0%81%E7%9A%84%E5%89%8D%E7%BD%AE%E7%9F%A5%E8%AF%86/"},{"categories":null,"content":"4. 如何去阅读源码，真的需要把整个项目都运行起来吗？ 我个人认为是完全不需要。 一般来说看源码，只需要了解主线代码，知道哪些类是怎么配合的，一起完成了什么样的功能，即使你把整个程序都运行起来了，有些分支条件的代码，需要特殊的输入数据，在你不熟悉代码的情况下，你也很难去模拟，这时候我们只能看代码的测试类 ，来了解代码是怎样处理这个特殊数据的。 特意说明一下： 以后的阅读代码的部分，我基本以测试类来带领大家阅读。 ","date":"2022-07-02","objectID":"/ooooo-notes/%E5%AD%A6%E4%B9%A0-k8s-%E6%BA%90%E7%A0%81%E7%9A%84%E5%89%8D%E7%BD%AE%E7%9F%A5%E8%AF%86/:4:0","tags":["k8s","cloud native","source code"],"title":"学习 k8s 源码的前置知识","uri":"/ooooo-notes/%E5%AD%A6%E4%B9%A0-k8s-%E6%BA%90%E7%A0%81%E7%9A%84%E5%89%8D%E7%BD%AE%E7%9F%A5%E8%AF%86/"},{"categories":null,"content":"1. 方法1，在本机的 IDE 来调试源码 如果你是 linux 系统，可以在 linux 中搭建一个 kubernetes 单机的集群，在此系统中安装 IDE(Goland) 来调试. 具体步骤如下： ","date":"2022-06-28","objectID":"/ooooo-notes/%E6%90%AD%E5%BB%BA-k8s-1.24-%E7%89%88%E6%9C%AC%E7%9A%84%E6%BA%90%E7%A0%81%E8%B0%83%E8%AF%95%E7%8E%AF%E5%A2%83/:1:0","tags":["k8s","cloud native","source code"],"title":"搭建 k8s 1.24 版本的源码调试环境","uri":"/ooooo-notes/%E6%90%AD%E5%BB%BA-k8s-1.24-%E7%89%88%E6%9C%AC%E7%9A%84%E6%BA%90%E7%A0%81%E8%B0%83%E8%AF%95%E7%8E%AF%E5%A2%83/"},{"categories":null,"content":"1. 下载源码 (go 的版本要求 1.18.x) git clone git@github.com:kubernetes/kubernetes.git cd kubernetes git checkout -b origin/release-1.24 go mod download ","date":"2022-06-28","objectID":"/ooooo-notes/%E6%90%AD%E5%BB%BA-k8s-1.24-%E7%89%88%E6%9C%AC%E7%9A%84%E6%BA%90%E7%A0%81%E8%B0%83%E8%AF%95%E7%8E%AF%E5%A2%83/:1:1","tags":["k8s","cloud native","source code"],"title":"搭建 k8s 1.24 版本的源码调试环境","uri":"/ooooo-notes/%E6%90%AD%E5%BB%BA-k8s-1.24-%E7%89%88%E6%9C%AC%E7%9A%84%E6%BA%90%E7%A0%81%E8%B0%83%E8%AF%95%E7%8E%AF%E5%A2%83/"},{"categories":null,"content":"2. 用 IDE 打开 kubernetes 源码 ","date":"2022-06-28","objectID":"/ooooo-notes/%E6%90%AD%E5%BB%BA-k8s-1.24-%E7%89%88%E6%9C%AC%E7%9A%84%E6%BA%90%E7%A0%81%E8%B0%83%E8%AF%95%E7%8E%AF%E5%A2%83/:1:2","tags":["k8s","cloud native","source code"],"title":"搭建 k8s 1.24 版本的源码调试环境","uri":"/ooooo-notes/%E6%90%AD%E5%BB%BA-k8s-1.24-%E7%89%88%E6%9C%AC%E7%9A%84%E6%BA%90%E7%A0%81%E8%B0%83%E8%AF%95%E7%8E%AF%E5%A2%83/"},{"categories":null,"content":"3. 找到服务的启动参数（比如 kube-controller-manager） # 执行命令 ps aux | grep kube-controller-manager | grep -v grep # 命令执行之后，输出如下, kube-controller-manager 后面的就是程序的参数 root 1584 4.0 0.8 820020 110072 ? Ssl 23:28 0:02 kube-controller-manager --allocate-node-cidrs=true --authentication-kubeconfig=/etc/kubernetes/controller-manager.conf --authorization-kubeconfig=/etc/kubernetes/controller-manager.conf --bind-address=127.0.0.1 --client-ca-file=/etc/kubernetes/pki/ca.crt --cluster-cidr=10.244.0.0/16 --cluster-name=kubernetes --cluster-signing-cert-file=/etc/kubernetes/pki/ca.crt --cluster-signing-key-file=/etc/kubernetes/pki/ca.key --controllers=*,bootstrapsigner,tokencleaner --kubeconfig=/etc/kubernetes/controller-manager.conf --leader-elect=true --requestheader-client-ca-file=/etc/kubernetes/pki/front-proxy-ca.crt --root-ca-file=/etc/kubernetes/pki/ca.crt --service-account-private-key-file=/etc/kubernetes/pki/sa.key --service-cluster-ip-range=10.96.0.0/12 --use-service-account-credentials=true ","date":"2022-06-28","objectID":"/ooooo-notes/%E6%90%AD%E5%BB%BA-k8s-1.24-%E7%89%88%E6%9C%AC%E7%9A%84%E6%BA%90%E7%A0%81%E8%B0%83%E8%AF%95%E7%8E%AF%E5%A2%83/:1:3","tags":["k8s","cloud native","source code"],"title":"搭建 k8s 1.24 版本的源码调试环境","uri":"/ooooo-notes/%E6%90%AD%E5%BB%BA-k8s-1.24-%E7%89%88%E6%9C%AC%E7%9A%84%E6%BA%90%E7%A0%81%E8%B0%83%E8%AF%95%E7%8E%AF%E5%A2%83/"},{"categories":null,"content":"4. 移动 kubernetes 的静态 pod （比如 kube-controller-manager） cd /etc/kubernetes mv manifests/kube-controller-manager.yaml ./ ","date":"2022-06-28","objectID":"/ooooo-notes/%E6%90%AD%E5%BB%BA-k8s-1.24-%E7%89%88%E6%9C%AC%E7%9A%84%E6%BA%90%E7%A0%81%E8%B0%83%E8%AF%95%E7%8E%AF%E5%A2%83/:1:4","tags":["k8s","cloud native","source code"],"title":"搭建 k8s 1.24 版本的源码调试环境","uri":"/ooooo-notes/%E6%90%AD%E5%BB%BA-k8s-1.24-%E7%89%88%E6%9C%AC%E7%9A%84%E6%BA%90%E7%A0%81%E8%B0%83%E8%AF%95%E7%8E%AF%E5%A2%83/"},{"categories":null,"content":"5. 用 IDE 启动服务（比如 kube-controller-manager） 程序的入口： cmd/kube-controller-manager/controller-manager.go (其他的服务也是类似的路径) 点击，配置启动参数，如下图 02-配置启动参数 现在基本就配置好了 ","date":"2022-06-28","objectID":"/ooooo-notes/%E6%90%AD%E5%BB%BA-k8s-1.24-%E7%89%88%E6%9C%AC%E7%9A%84%E6%BA%90%E7%A0%81%E8%B0%83%E8%AF%95%E7%8E%AF%E5%A2%83/:1:5","tags":["k8s","cloud native","source code"],"title":"搭建 k8s 1.24 版本的源码调试环境","uri":"/ooooo-notes/%E6%90%AD%E5%BB%BA-k8s-1.24-%E7%89%88%E6%9C%AC%E7%9A%84%E6%BA%90%E7%A0%81%E8%B0%83%E8%AF%95%E7%8E%AF%E5%A2%83/"},{"categories":null,"content":"6. 检查服务是否正常启动 （比如 kube-controller-manager） # 执行命令看没有 kube-controller-manager kubectl get pods -A ","date":"2022-06-28","objectID":"/ooooo-notes/%E6%90%AD%E5%BB%BA-k8s-1.24-%E7%89%88%E6%9C%AC%E7%9A%84%E6%BA%90%E7%A0%81%E8%B0%83%E8%AF%95%E7%8E%AF%E5%A2%83/:1:6","tags":["k8s","cloud native","source code"],"title":"搭建 k8s 1.24 版本的源码调试环境","uri":"/ooooo-notes/%E6%90%AD%E5%BB%BA-k8s-1.24-%E7%89%88%E6%9C%AC%E7%9A%84%E6%BA%90%E7%A0%81%E8%B0%83%E8%AF%95%E7%8E%AF%E5%A2%83/"},{"categories":null,"content":"2. 方法2. 借助 dlv 来调试源码 如果你是 mac/window 系统，可以借助 dlv 来调试源码。 具体步骤如下： ","date":"2022-06-28","objectID":"/ooooo-notes/%E6%90%AD%E5%BB%BA-k8s-1.24-%E7%89%88%E6%9C%AC%E7%9A%84%E6%BA%90%E7%A0%81%E8%B0%83%E8%AF%95%E7%8E%AF%E5%A2%83/:2:0","tags":["k8s","cloud native","source code"],"title":"搭建 k8s 1.24 版本的源码调试环境","uri":"/ooooo-notes/%E6%90%AD%E5%BB%BA-k8s-1.24-%E7%89%88%E6%9C%AC%E7%9A%84%E6%BA%90%E7%A0%81%E8%B0%83%E8%AF%95%E7%8E%AF%E5%A2%83/"},{"categories":null,"content":"1. 下载源码 (go 的版本要求 1.18.x) git clone git@github.com:kubernetes/kubernetes.git cd kubernetes git checkout -b origin/release-1.24 go mod download 上述下载源码，需要在 本地window 和 k8s节点 上都下载。 注意：由于是远端调试，所以需要在 k8s master 节点上，重新编译源码，去掉 -N -l. ","date":"2022-06-28","objectID":"/ooooo-notes/%E6%90%AD%E5%BB%BA-k8s-1.24-%E7%89%88%E6%9C%AC%E7%9A%84%E6%BA%90%E7%A0%81%E8%B0%83%E8%AF%95%E7%8E%AF%E5%A2%83/:2:1","tags":["k8s","cloud native","source code"],"title":"搭建 k8s 1.24 版本的源码调试环境","uri":"/ooooo-notes/%E6%90%AD%E5%BB%BA-k8s-1.24-%E7%89%88%E6%9C%AC%E7%9A%84%E6%BA%90%E7%A0%81%E8%B0%83%E8%AF%95%E7%8E%AF%E5%A2%83/"},{"categories":null,"content":"2. 在 k8s master 节点上，重新编译源码 cd kubernetes make DBG=1 # 在 hack/lib/golang.sh 中 ","date":"2022-06-28","objectID":"/ooooo-notes/%E6%90%AD%E5%BB%BA-k8s-1.24-%E7%89%88%E6%9C%AC%E7%9A%84%E6%BA%90%E7%A0%81%E8%B0%83%E8%AF%95%E7%8E%AF%E5%A2%83/:2:2","tags":["k8s","cloud native","source code"],"title":"搭建 k8s 1.24 版本的源码调试环境","uri":"/ooooo-notes/%E6%90%AD%E5%BB%BA-k8s-1.24-%E7%89%88%E6%9C%AC%E7%9A%84%E6%BA%90%E7%A0%81%E8%B0%83%E8%AF%95%E7%8E%AF%E5%A2%83/"},{"categories":null,"content":"3. 下载可能用到的工具，如 dlv (你可能需要提前设置 GOPATH 环境变量) # 可能非常慢，需要设置代理 go get -u github.com/cloudflare/cfssl/cmd/cfssl go get -u github.com/cloudflare/cfssl/cmd/cfssljson go get -u github.com/go-delve/delve/cmd/dlv # 配置环境变量 PATH=$PATH:$GOPATH/bin ","date":"2022-06-28","objectID":"/ooooo-notes/%E6%90%AD%E5%BB%BA-k8s-1.24-%E7%89%88%E6%9C%AC%E7%9A%84%E6%BA%90%E7%A0%81%E8%B0%83%E8%AF%95%E7%8E%AF%E5%A2%83/:2:3","tags":["k8s","cloud native","source code"],"title":"搭建 k8s 1.24 版本的源码调试环境","uri":"/ooooo-notes/%E6%90%AD%E5%BB%BA-k8s-1.24-%E7%89%88%E6%9C%AC%E7%9A%84%E6%BA%90%E7%A0%81%E8%B0%83%E8%AF%95%E7%8E%AF%E5%A2%83/"},{"categories":null,"content":"4. 找到服务的启动参数（比如 kube-controller-manager） # 执行命令 ps aux | grep kube-controller-manager | grep -v grep # 命令执行之后，输出如下, kube-controller-manager 后面的就是程序的参数 root 1584 4.0 0.8 820020 110072 ? Ssl 23:28 0:02 kube-controller-manager --allocate-node-cidrs=true --authentication-kubeconfig=/etc/kubernetes/controller-manager.conf --authorization-kubeconfig=/etc/kubernetes/controller-manager.conf --bind-address=127.0.0.1 --client-ca-file=/etc/kubernetes/pki/ca.crt --cluster-cidr=10.244.0.0/16 --cluster-name=kubernetes --cluster-signing-cert-file=/etc/kubernetes/pki/ca.crt --cluster-signing-key-file=/etc/kubernetes/pki/ca.key --controllers=*,bootstrapsigner,tokencleaner --kubeconfig=/etc/kubernetes/controller-manager.conf --leader-elect=true --requestheader-client-ca-file=/etc/kubernetes/pki/front-proxy-ca.crt --root-ca-file=/etc/kubernetes/pki/ca.crt --service-account-private-key-file=/etc/kubernetes/pki/sa.key --service-cluster-ip-range=10.96.0.0/12 --use-service-account-credentials=true ","date":"2022-06-28","objectID":"/ooooo-notes/%E6%90%AD%E5%BB%BA-k8s-1.24-%E7%89%88%E6%9C%AC%E7%9A%84%E6%BA%90%E7%A0%81%E8%B0%83%E8%AF%95%E7%8E%AF%E5%A2%83/:2:4","tags":["k8s","cloud native","source code"],"title":"搭建 k8s 1.24 版本的源码调试环境","uri":"/ooooo-notes/%E6%90%AD%E5%BB%BA-k8s-1.24-%E7%89%88%E6%9C%AC%E7%9A%84%E6%BA%90%E7%A0%81%E8%B0%83%E8%AF%95%E7%8E%AF%E5%A2%83/"},{"categories":null,"content":"5. 移动 kubernetes 的静态 pod （比如 kube-controller-manager） cd /etc/kubernetes mv manifests/kube-controller-manager.yaml ./ ","date":"2022-06-28","objectID":"/ooooo-notes/%E6%90%AD%E5%BB%BA-k8s-1.24-%E7%89%88%E6%9C%AC%E7%9A%84%E6%BA%90%E7%A0%81%E8%B0%83%E8%AF%95%E7%8E%AF%E5%A2%83/:2:5","tags":["k8s","cloud native","source code"],"title":"搭建 k8s 1.24 版本的源码调试环境","uri":"/ooooo-notes/%E6%90%AD%E5%BB%BA-k8s-1.24-%E7%89%88%E6%9C%AC%E7%9A%84%E6%BA%90%E7%A0%81%E8%B0%83%E8%AF%95%E7%8E%AF%E5%A2%83/"},{"categories":null,"content":"6. 用 dlv 启动服务 注意: 启动参数和程序路径，配置成你自己的，监听的端口是 2346 dlv 在配置程序参数时，有 --， 如果后面参数有特殊符号，用 --key=\"value\" 形式 dlv 启动之后，必须要触发(IDE go remote)，才能启动, 否则会一直等着。 dlv --listen=:2346 --headless=true --api-version=2 --accept-multiclient exec /root/kubernetes/_output/bin/kube-controller-manager -- --allocate-node-cidrs=true --authentication-kubeconfig=/etc/kubernetes/controller-manager.conf --authorization-kubeconfig=/etc/kubernetes/controller-manager.conf --bind-address=127.0.0.1 --client-ca-file=/etc/kubernetes/pki/ca.crt --cluster-cidr=10.244.0.0/16 --cluster-name=kubernetes --cluster-signing-cert-file=/etc/kubernetes/pki/ca.crt --cluster-signing-key-file=/etc/kubernetes/pki/ca.key --controllers=*,bootstrapsigner,tokencleaner --kubeconfig=/etc/kubernetes/controller-manager.conf --leader-elect=true --requestheader-client-ca-file=/etc/kubernetes/pki/front-proxy-ca.crt --root-ca-file=/etc/kubernetes/pki/ca.crt --service-account-private-key-file=/etc/kubernetes/pki/sa.key --service-cluster-ip-range=10.96.0.0/12 --use-service-account-credentials=true ","date":"2022-06-28","objectID":"/ooooo-notes/%E6%90%AD%E5%BB%BA-k8s-1.24-%E7%89%88%E6%9C%AC%E7%9A%84%E6%BA%90%E7%A0%81%E8%B0%83%E8%AF%95%E7%8E%AF%E5%A2%83/:2:6","tags":["k8s","cloud native","source code"],"title":"搭建 k8s 1.24 版本的源码调试环境","uri":"/ooooo-notes/%E6%90%AD%E5%BB%BA-k8s-1.24-%E7%89%88%E6%9C%AC%E7%9A%84%E6%BA%90%E7%A0%81%E8%B0%83%E8%AF%95%E7%8E%AF%E5%A2%83/"},{"categories":null,"content":"7. 用 IDE 连接 dlv 服务（比如 kube-controller-manager） 程序的入口： cmd/kube-controller-manager/controller-manager.go (其他的服务也是类似的路径) 添加 Go Remote， 配置 host 和 port。 点击 ok，然后启动服务。 02-连接dlv 现在基本就配置好了 ","date":"2022-06-28","objectID":"/ooooo-notes/%E6%90%AD%E5%BB%BA-k8s-1.24-%E7%89%88%E6%9C%AC%E7%9A%84%E6%BA%90%E7%A0%81%E8%B0%83%E8%AF%95%E7%8E%AF%E5%A2%83/:2:7","tags":["k8s","cloud native","source code"],"title":"搭建 k8s 1.24 版本的源码调试环境","uri":"/ooooo-notes/%E6%90%AD%E5%BB%BA-k8s-1.24-%E7%89%88%E6%9C%AC%E7%9A%84%E6%BA%90%E7%A0%81%E8%B0%83%E8%AF%95%E7%8E%AF%E5%A2%83/"},{"categories":null,"content":"8. 提供一个调试的脚本 （可选） 你现在会发现，如果想要调试，就必须要把 manifests/kube-controller-manager.yaml 移出去，等不需要调试了，再把这个文件移回来，这样非常麻烦。所以使用一个脚本来实现。 脚本内容如下(注意更改你的路径)： cleanup() { mv /etc/kubernetes/kube-controller-manager.yaml /etc/kubernetes/manifests } trap cleanup EXIT mv /etc/kubernetes/manifests/kube-controller-manager.yaml /etc/kubernetes dlv --listen=:2346 --headless=true --api-version=2 --accept-multiclient exec /root/kubernetes/_output/bin/kube-controller-manager -- --allocate-node-cidrs=true --authentication-kubeconfig=/etc/kubernetes/controller-manager.conf --authorization-kubeconfig=/etc/kubernetes/controller-manager.conf --bind-address=127.0.0.1 --client-ca-file=/etc/kubernetes/pki/ca.crt --cluster-cidr=10.244.0.0/16 --cluster-name=kubernetes --cluster-signing-cert-file=/etc/kubernetes/pki/ca.crt --cluster-signing-key-file=/etc/kubernetes/pki/ca.key --controllers=*,bootstrapsigner,tokencleaner --kubeconfig=/etc/kubernetes/controller-manager.conf --leader-elect=true --requestheader-client-ca-file=/etc/kubernetes/pki/front-proxy-ca.crt --root-ca-file=/etc/kubernetes/pki/ca.crt --service-account-private-key-file=/etc/kubernetes/pki/sa.key --service-cluster-ip-range=10.96.0.0/12 --use-service-account-credentials=true ","date":"2022-06-28","objectID":"/ooooo-notes/%E6%90%AD%E5%BB%BA-k8s-1.24-%E7%89%88%E6%9C%AC%E7%9A%84%E6%BA%90%E7%A0%81%E8%B0%83%E8%AF%95%E7%8E%AF%E5%A2%83/:2:8","tags":["k8s","cloud native","source code"],"title":"搭建 k8s 1.24 版本的源码调试环境","uri":"/ooooo-notes/%E6%90%AD%E5%BB%BA-k8s-1.24-%E7%89%88%E6%9C%AC%E7%9A%84%E6%BA%90%E7%A0%81%E8%B0%83%E8%AF%95%E7%8E%AF%E5%A2%83/"},{"categories":null,"content":"1. install logstash refer to logstash document ","date":"2022-06-01","objectID":"/ooooo-notes/logstash-%E7%9A%84%E7%AE%80%E5%8D%95%E4%BD%BF%E7%94%A8/:1:0","tags":["logstash"],"title":"logstash 的简单使用","uri":"/ooooo-notes/logstash-%E7%9A%84%E7%AE%80%E5%8D%95%E4%BD%BF%E7%94%A8/"},{"categories":null,"content":"2. logstash example config file input { tcp { port =\u003e 12345 codec =\u003e \"json_lines\" } } filter{ grok { match =\u003e [\"message\", \"%{TIMESTAMP_ISO8601:logdate}\"] } date { match =\u003e [\"logdate\", \"yyyy-MM-dd HH:mm:ss.SSS\"] target =\u003e \"@timestamp\" } mutate { remove_field =\u003e [\"logdate\"] } ruby { code =\u003e \"event.set('timestamp', event.get('@timestamp').time.localtime + 8*60*60)\" } ruby { code =\u003e \"event.set('@timestamp',event.get('timestamp'))\" } mutate { remove_field =\u003e [\"timestamp\"] } } output { stdout { codec =\u003e rubydebug { metadata =\u003e true } } file { path =\u003e \"./logs/%{+YYYY-MM-dd-HH}.log\" codec =\u003e line { format =\u003e \"%{message}\"} } } notes: it will serve TCP connection on localhost:12345. uses codec named json_lines, json data format such as { \"message\" : \"xxxx\" }. matche date pattern in the log, then use it as its time. reset the field @timestamp, output to the local file. ","date":"2022-06-01","objectID":"/ooooo-notes/logstash-%E7%9A%84%E7%AE%80%E5%8D%95%E4%BD%BF%E7%94%A8/:1:1","tags":["logstash"],"title":"logstash 的简单使用","uri":"/ooooo-notes/logstash-%E7%9A%84%E7%AE%80%E5%8D%95%E4%BD%BF%E7%94%A8/"},{"categories":null,"content":"1. install wsl open Microsoft Store, then search ubuntu and click to install it. open terminal # list all linux subsystem wsl --list # set default linux subsystem, then you can input 'wsl' to inter system wsl --set-default ubuntu # enter default linux subsystem wsl # install cmake, g++, gcc，gdb cd /usr/local wget https://cmake.org/files/v3.22/cmake-3.22.0-linux-x86_64.tar.gz tar xf cmake-3.22.0-linux-x86_64.tar.gz ln -s /usr/local/cmake-3.22.0-linux-x86_64/bin/cmake /usr/bin sudo apt install build-essential ","date":"2022-06-01","objectID":"/ooooo-notes/%E5%9C%A8-windows-%E4%B8%8A%E8%B0%83%E8%AF%95-redis/:1:0","tags":["redis"],"title":"在 windows 上调试 redis","uri":"/ooooo-notes/%E5%9C%A8-windows-%E4%B8%8A%E8%B0%83%E8%AF%95-redis/"},{"categories":null,"content":"2. setting clion open File | Settings | Build, Execution, Deployment | Toolchains menu. add new toolchains and select wsl. clion toolchains setting wsl configuration, you maybe install cmake, gcc, g++, gdb. you must execute command git config core.autocrlf input in your terminal, because windows is CRLF, then git clone . select wsl in File | Settings | Build, Execution, Deployment | Makefile, because building redis by using makefile. login wsl and enter redis directory, for example: cd /mnt/c/Users/ooooo/Development/code/Demo/redis execute command make, you maybe need to execute cd src \u0026\u0026 ls | grep .sh | xargs chmod a+x makefile application ","date":"2022-06-01","objectID":"/ooooo-notes/%E5%9C%A8-windows-%E4%B8%8A%E8%B0%83%E8%AF%95-redis/:2:0","tags":["redis"],"title":"在 windows 上调试 redis","uri":"/ooooo-notes/%E5%9C%A8-windows-%E4%B8%8A%E8%B0%83%E8%AF%95-redis/"},{"categories":null,"content":"1. 机器初始化设置 ","date":"2022-04-01","objectID":"/ooooo-notes/%E4%BD%BF%E7%94%A8-ubuntu-%E6%9D%A5%E5%AE%89%E8%A3%85-kubernetes-1.24-%E7%89%88%E6%9C%AC/:1:0","tags":["k8s","cloud native"],"title":"使用 ubuntu 来安装 kubernetes 1.24 版本","uri":"/ooooo-notes/%E4%BD%BF%E7%94%A8-ubuntu-%E6%9D%A5%E5%AE%89%E8%A3%85-kubernetes-1.24-%E7%89%88%E6%9C%AC/"},{"categories":null,"content":"hostname 设置 hostnamectl ## 查看当前的hostname hostnamectl set-hostname node1 ## 设置主机名为node1 ","date":"2022-04-01","objectID":"/ooooo-notes/%E4%BD%BF%E7%94%A8-ubuntu-%E6%9D%A5%E5%AE%89%E8%A3%85-kubernetes-1.24-%E7%89%88%E6%9C%AC/:1:1","tags":["k8s","cloud native"],"title":"使用 ubuntu 来安装 kubernetes 1.24 版本","uri":"/ooooo-notes/%E4%BD%BF%E7%94%A8-ubuntu-%E6%9D%A5%E5%AE%89%E8%A3%85-kubernetes-1.24-%E7%89%88%E6%9C%AC/"},{"categories":null,"content":"/etc/hosts 文件 和 DNS 配置 # k8s master 192.168.130.131 node1 # 更改dns配置 vim /etc/systemd/resolved.conf # 更改下面内容 [Resolve] DNS=8.8.8.8 8.8.4.4 # 重启dns systemctl restart systemd-resolved.service refer: ubuntu dns resolver ","date":"2022-04-01","objectID":"/ooooo-notes/%E4%BD%BF%E7%94%A8-ubuntu-%E6%9D%A5%E5%AE%89%E8%A3%85-kubernetes-1.24-%E7%89%88%E6%9C%AC/:1:2","tags":["k8s","cloud native"],"title":"使用 ubuntu 来安装 kubernetes 1.24 版本","uri":"/ooooo-notes/%E4%BD%BF%E7%94%A8-ubuntu-%E6%9D%A5%E5%AE%89%E8%A3%85-kubernetes-1.24-%E7%89%88%E6%9C%AC/"},{"categories":null,"content":"创建非 root 用户(可选) # 添加用户 useradd ooooo -g ooooo # 修改用户密码 passwd ooooo ","date":"2022-04-01","objectID":"/ooooo-notes/%E4%BD%BF%E7%94%A8-ubuntu-%E6%9D%A5%E5%AE%89%E8%A3%85-kubernetes-1.24-%E7%89%88%E6%9C%AC/:1:3","tags":["k8s","cloud native"],"title":"使用 ubuntu 来安装 kubernetes 1.24 版本","uri":"/ooooo-notes/%E4%BD%BF%E7%94%A8-ubuntu-%E6%9D%A5%E5%AE%89%E8%A3%85-kubernetes-1.24-%E7%89%88%E6%9C%AC/"},{"categories":null,"content":"安装 containerd 和 runc 安装 containerd wget https://github.com/containerd/containerd/releases/download/v1.6.6/containerd-1.6.6-linux-amd64.tar.gz tar Cxzvf /usr/local containerd-1.6.6-linux-amd64.tar.gz mkdir -p /usr/local/lib/systemd/system/ 通过 systemd 来启动 containerd 将下面的内容写入 /usr/local/lib/systemd/system/containerd.service # Copyright The containerd Authors. # # Licensed under the Apache License, Version 2.0 (the \"License\"); # you may not use this file except in compliance with the License. # You may obtain a copy of the License at # # http://www.apache.org/licenses/LICENSE-2.0 # # Unless required by applicable law or agreed to in writing, software # distributed under the License is distributed on an \"AS IS\" BASIS, # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. # See the License for the specific language governing permissions and # limitations under the License. [Unit] Description=containerd container runtime Documentation=https://containerd.io After=network.target local-fs.target [Service] ExecStartPre=-/sbin/modprobe overlay ExecStart=/usr/local/bin/containerd Type=notify Delegate=yes KillMode=process Restart=always RestartSec=5 # Having non-zero Limit*s causes performance problems due to accounting overhead # in the kernel. We recommend using cgroups to do container-local accounting. LimitNPROC=infinity LimitCORE=infinity LimitNOFILE=infinity # Comment TasksMax if your systemd version does not supports it. # Only systemd 226 and above support this version. TasksMax=infinity OOMScoreAdjust=-999 [Install] WantedBy=multi-user.target 启动 containerd systemctl daemon-reload systemctl enable --now containerd 配置 containerd mkdir -p /etc/containerd # 生成默认配置文件 containerd config default | tee /etc/containerd/config.toml # 修改 /etc/containerd/config.toml 配置 # image 使用阿里云的地址， SystemdCgroup 更改为 true sandbox_image = \"registry.aliyuncs.com/google_containers/pause:3.6\" ... [plugins.\"io.containerd.grpc.v1.cri\".containerd.runtimes.runc.options] ... SystemdCgroup = true # 修改完成后 systemctl restart containerd 安装 runc wget https://github.com/opencontainers/runc/releases/download/v1.1.3/runc.amd64 install -m 755 runc.amd64 /usr/local/sbin/runc 安装 cni 插件 wget https://github.com/containernetworking/plugins/releases/download/v1.1.1/cni-plugins-linux-amd64-v1.1.1.tgz mkdir -p /opt/cni/bin tar Cxzvf /opt/cni/bin cni-plugins-linux-amd64-v1.1.1.tgz ","date":"2022-04-01","objectID":"/ooooo-notes/%E4%BD%BF%E7%94%A8-ubuntu-%E6%9D%A5%E5%AE%89%E8%A3%85-kubernetes-1.24-%E7%89%88%E6%9C%AC/:1:4","tags":["k8s","cloud native"],"title":"使用 ubuntu 来安装 kubernetes 1.24 版本","uri":"/ooooo-notes/%E4%BD%BF%E7%94%A8-ubuntu-%E6%9D%A5%E5%AE%89%E8%A3%85-kubernetes-1.24-%E7%89%88%E6%9C%AC/"},{"categories":null,"content":"2. k8s 安装 官方 k8s 安装文档 # 参考文档检查服务器的状态是否可以安装 k8s 服务 # 临时关闭 swap 分区 swapoff -a # 查看 swap 分区是否关闭，显示 0 表示已关闭 free -h # 永久关闭 swap 分区 编辑 /etc/fstab 文件, 注释最后一行 # 检查 br_netfilter 是否被加载，没有任何输出，表示没有加载 lsmod | grep br_netfilter # 加载 br_netfilter 模块 sudo modprobe br_netfilter ## 配置网络 cat \u003c\u003cEOF | sudo tee /etc/modules-load.d/k8s.conf br_netfilter EOF cat \u003c\u003cEOF | sudo tee /etc/sysctl.d/k8s.conf net.bridge.bridge-nf-call-ip6tables = 1 net.bridge.bridge-nf-call-iptables = 1 net.ipv4.ip_forward = 1 EOF sudo sysctl --system # 安装软件 sudo apt-get update sudo apt-get install -y apt-transport-https ca-certificates curl sudo curl -fsSLo /usr/share/keyrings/kubernetes-archive-keyring.gpg https://mirrors.aliyun.com/kubernetes/apt/doc/apt-key.gpg echo \"deb [signed-by=/usr/share/keyrings/kubernetes-archive-keyring.gpg] https://mirrors.aliyun.com/kubernetes/apt/ kubernetes-xenial main\" | sudo tee /etc/apt/sources.list.d/kubernetes.list sudo apt-get update # 默认安装最新版本 sudo apt-get install -y kubelet kubeadm kubectl # 不自动更新 sudo apt-mark hold kubelet kubeadm kubectl # 查看镜像列表， 报错需要添加配置, crictl 是官方提供的 crictl images # vim /etc/crictl.yaml 添加以下内容 runtime-endpoint: unix:///run/containerd/containerd.sock image-endpoint: unix:///run/containerd/containerd.sock timeout: 10 debug: false # 设置 kubelet 开机启动，并且现在启动 # 启动之后可能会报错，如果原因是 没有读取到 kubelet 的配置文件，这里可以不用管，稍后会重启这个服务 sudo systemctl enable --now kubelet # 查看 kubelet 的状态 sudo systemctl status kubelet # 查看 kubelet 的日志 journalctl -xeu kubelet ","date":"2022-04-01","objectID":"/ooooo-notes/%E4%BD%BF%E7%94%A8-ubuntu-%E6%9D%A5%E5%AE%89%E8%A3%85-kubernetes-1.24-%E7%89%88%E6%9C%AC/:2:0","tags":["k8s","cloud native"],"title":"使用 ubuntu 来安装 kubernetes 1.24 版本","uri":"/ooooo-notes/%E4%BD%BF%E7%94%A8-ubuntu-%E6%9D%A5%E5%AE%89%E8%A3%85-kubernetes-1.24-%E7%89%88%E6%9C%AC/"},{"categories":null,"content":"3. 创建 k8s 集群 创建 k8s 集群官方文档 k8s pod network 插件文档 # 执行 kubeadm init 命令， 在 k8s master 机器上执行，默认情况下， k8s 创建 pod 不会在 master 机器上 # 重点注意: --pod-network-cidr=10.244.0.0/16 这个参数必须要有，没有的话安装 cni 会报错 # 注意 preflight 的前置检查输出，如果有问题，百度自行解决 # 替换为你自己的 ip 和 hostname sudo kubeadm init --image-repository registry.aliyuncs.com/google_containers --apiserver-advertise-address=192.168.130.128 --pod-network-cidr=10.244.0.0/16 --control-plane-endpoint=node1 # 执行命令之后，会有 kubeadm join 输出行 # （分为 master-token 和 worker-token）， 类似于下面的命令，可以在另一个节点上执行 worker-join-token 的命令 sudo kubeadm join 192.168.130.128:6443 --token 8auvt0.zfw0ayr45d80q8pb \\ --discovery-token-ca-cert-hash sha256:efe854739efef5fbaf3f6e28c899481c8d7797c1997fc8315b921a9ede400ca8 # 去掉污点，让单个节点也可以运行, (我这里只有一个节点) kubectl taint nodes --all node-role.kubernetes.io/control-plane- node-role.kubernetes.io/master- ## 在机器上执行 kubeadm join 或者 kubeadm init 命令之后，重启 kubelet 服务 sudo systemctl restart kubelet sudo systemctl status kubelet # 设置 kubectl 的配置文件， 为 $HOME/.kube/config mkdir -p $HOME/.kube sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config sudo chown $(id -u):$(id -g) $HOME/.kube/config # 安装 pod network 插件, 这里使用 calico 插件 curl -o calico-operator.yaml https://projectcalico.docs.tigera.io/manifests/tigera-operator.yaml curl -o calico-custom-resources.yaml https://projectcalico.docs.tigera.io/manifests/custom-resources.yaml # 重点 # 更改 calico-custom-resources.yaml 的 cidr 配置, 值为 --pod-network-cidr （在 kubeadm init 指定了） # 多个网卡，也可以更改，否则可能会报错，搜索 interface kubectl create -f calico-operator.yaml kubectl create -f calico-custom-resources.yaml # 查看 calico 是否已经启动完成, cni 也启动成功 kubectl get pods -A # 成功之后会有下面的服务， 都是 running 状态 calico-apiserver calico-apiserver-78c5f69667-gbxbv 1/1 Running 0 88s calico-apiserver calico-apiserver-78c5f69667-h64wk 1/1 Running 0 88s calico-system calico-kube-controllers-68884f975d-q4l8s 1/1 Running 0 40m calico-system calico-node-4d7hs 1/1 Running 0 40m calico-system calico-typha-854c6b9b4b-s8ls7 1/1 Running 0 40m kube-system coredns-74586cf9b6-4pkxf 1/1 Running 0 76m kube-system coredns-74586cf9b6-9hxwl 1/1 Running 0 76m kube-system etcd-node1 1/1 Running 0 76m kube-system kube-apiserver-node1 1/1 Running 0 76m kube-system kube-controller-manager-node1 1/1 Running 0 76m kube-system kube-proxy-mn6fr 1/1 Running 0 76m kube-system kube-scheduler-node1 1/1 Running 0 76m tigera-operator tigera-operator-5fb55776df-gjs7s 1/1 Running 0 64m ","date":"2022-04-01","objectID":"/ooooo-notes/%E4%BD%BF%E7%94%A8-ubuntu-%E6%9D%A5%E5%AE%89%E8%A3%85-kubernetes-1.24-%E7%89%88%E6%9C%AC/:3:0","tags":["k8s","cloud native"],"title":"使用 ubuntu 来安装 kubernetes 1.24 版本","uri":"/ooooo-notes/%E4%BD%BF%E7%94%A8-ubuntu-%E6%9D%A5%E5%AE%89%E8%A3%85-kubernetes-1.24-%E7%89%88%E6%9C%AC/"},{"categories":null,"content":"install nfs in docker ","date":"2022-03-01","objectID":"/ooooo-notes/%E5%9C%A8-docker-%E4%B8%8A%E5%AE%89%E8%A3%85-nfs/:1:0","tags":["docker","nfs"],"title":"在 docker 上安装 nfs","uri":"/ooooo-notes/%E5%9C%A8-docker-%E4%B8%8A%E5%AE%89%E8%A3%85-nfs/"},{"categories":null,"content":"1. create share directory used by nfs mkdir -p /home/ooooo/shared/nfs ","date":"2022-03-01","objectID":"/ooooo-notes/%E5%9C%A8-docker-%E4%B8%8A%E5%AE%89%E8%A3%85-nfs/:1:1","tags":["docker","nfs"],"title":"在 docker 上安装 nfs","uri":"/ooooo-notes/%E5%9C%A8-docker-%E4%B8%8A%E5%AE%89%E8%A3%85-nfs/"},{"categories":null,"content":"2. create exports.txt used by nfs This the exports.txt mainly used to mount dir (path in the container ) and permission. for example: It indicates read only for all ip. vim /home/ooooo/exports.txt /home/ooooo/shared/nfs *(ro,no_subtree_check) ","date":"2022-03-01","objectID":"/ooooo-notes/%E5%9C%A8-docker-%E4%B8%8A%E5%AE%89%E8%A3%85-nfs/:1:2","tags":["docker","nfs"],"title":"在 docker 上安装 nfs","uri":"/ooooo-notes/%E5%9C%A8-docker-%E4%B8%8A%E5%AE%89%E8%A3%85-nfs/"},{"categories":null,"content":"3. execute docker command docker run -d \\ -v /home/ooooo/shared/nfs:/home/ooooo/shared/nfs \\ -v /home/ooooo/exports.txt:/etc/exports:ro \\ --cap-add SYS_ADMIN \\ -p 2049:2049 \\ erichough/nfs-server # check nfs server netstat -nla | grep 2049 # mount nfs dir (check mount.nfs whether is exist ) # 172.17.0.2 is container ip mount 172.17.0.2:/home/ooooo/shared/nfs /home/ooooo/nfs-mount ","date":"2022-03-01","objectID":"/ooooo-notes/%E5%9C%A8-docker-%E4%B8%8A%E5%AE%89%E8%A3%85-nfs/:1:3","tags":["docker","nfs"],"title":"在 docker 上安装 nfs","uri":"/ooooo-notes/%E5%9C%A8-docker-%E4%B8%8A%E5%AE%89%E8%A3%85-nfs/"},{"categories":null,"content":"5. 参考 docker images ","date":"2022-03-01","objectID":"/ooooo-notes/%E5%9C%A8-docker-%E4%B8%8A%E5%AE%89%E8%A3%85-nfs/:1:4","tags":["docker","nfs"],"title":"在 docker 上安装 nfs","uri":"/ooooo-notes/%E5%9C%A8-docker-%E4%B8%8A%E5%AE%89%E8%A3%85-nfs/"},{"categories":null,"content":" cygwin 的环境变量要放在第一个，这样 rsync 和 ssh 都是 cygwin 的. window 是执行命令 where ssh， 看看 ssh 有几个实现 （比如 openssh 和 cygwin 的 ssh ） refer: window install cygwin ","date":"2022-01-03","objectID":"/ooooo-notes/%E5%9C%A8-windows-%E4%B8%AD-rsync-%E9%97%AE%E9%A2%98/:0:0","tags":["resolution"],"title":"在 windows 中 rsync 问题","uri":"/ooooo-notes/%E5%9C%A8-windows-%E4%B8%AD-rsync-%E9%97%AE%E9%A2%98/"},{"categories":null,"content":" win10安装wireshark经常报“KB2999226 和 KB3118401” install wireshark open installation directory, manually install vc_redist.x64.exe by double click reinstall wireshark ","date":"2022-01-02","objectID":"/ooooo-notes/%E5%9C%A8-windows-%E4%B8%AD%E5%AE%89%E8%A3%85-wireshark-%E6%8A%A5%E9%94%99/:0:0","tags":["resolution","wireshark"],"title":"在 windows 中安装 wireshark 报错","uri":"/ooooo-notes/%E5%9C%A8-windows-%E4%B8%AD%E5%AE%89%E8%A3%85-wireshark-%E6%8A%A5%E9%94%99/"},{"categories":null,"content":"0、持续学习者 Talk is cheap. Show me the code. 英语比编程简单。 学习和实践要平衡。 学会和时间做朋友。 学会投资，学会理财。 学会先做减法，再做加法。 学英语很重要，学英语很重要，学英语很重要。 说明： ⭕ 进行中 ✅ 已完成 ❌ 已废弃 ❓ 有必要 ❗ 重要性 📝 记笔记 🖊️ 写代码 ","date":"2022-01-01","objectID":"/ooooo-notes/2022%E5%B9%B4%E8%AE%A1%E5%88%92/:1:0","tags":["learning"],"title":"2022年学习计划","uri":"/ooooo-notes/2022%E5%B9%B4%E8%AE%A1%E5%88%92/"},{"categories":null,"content":"1、关于英语 《新概念英语》 ⭕ 《每日英语听力 ~ EnglishPod》 ","date":"2022-01-01","objectID":"/ooooo-notes/2022%E5%B9%B4%E8%AE%A1%E5%88%92/:2:0","tags":["learning"],"title":"2022年学习计划","uri":"/ooooo-notes/2022%E5%B9%B4%E8%AE%A1%E5%88%92/"},{"categories":null,"content":"2、关于技术 计划 🎉： 只记录自己认为有用的笔记。 ","date":"2022-01-01","objectID":"/ooooo-notes/2022%E5%B9%B4%E8%AE%A1%E5%88%92/:3:0","tags":["learning"],"title":"2022年学习计划","uri":"/ooooo-notes/2022%E5%B9%B4%E8%AE%A1%E5%88%92/"},{"categories":null,"content":"1、书籍 0️⃣1️⃣. 《Java 性能优化权威指南》 ✅ 0️⃣2️⃣. 《深入理解 Java 虚拟机（第3版）》 ✅ 0️⃣3️⃣. 《Spring Cloud 微服务：入门、实战与进阶》 ❌ 0️⃣4️⃣. 《arthas》 0️⃣5️⃣. 《Java 并发编程实战》 ✅ 0️⃣6️⃣. 《深入理解 Kafka：核心设计与实践原理》 0️⃣7️⃣. 《从零开始学架构》 ✅ 0️⃣8️⃣. 《高可用可伸缩微服务架构》 ❌ 0️⃣9️⃣. 《分布式一致性算法开发实战》 1️⃣0️⃣. 《Go Web 编程》 ⭕ 1️⃣2️⃣. 《Effective C++》 1️⃣3️⃣. 《More Effective C++》 1️⃣4️⃣. 《深度探索C++对象模型》 1️⃣5️⃣. 《Go语言设计与实现》 1️⃣6️⃣. 《wireshark网络分析的艺术》 ✅ 1️⃣7️⃣. 《Vim实用技巧（第2版）》 ⭕ 1️⃣8️⃣. 《RocketMQ技术内幕 第二版》 1️⃣9️⃣. 《Kubernetes in Action中文版》 ✅ 2️⃣1️⃣. 《rocketmq 源码》 ⭕ 2️⃣2️⃣. 《kubernetes 源码》 ⭕ 2️⃣3️⃣. 《istio 源码》 2️⃣4️⃣. 《etcd 源码》 2️⃣5️⃣. 《dubbo 源码》 ⭕ 2️⃣6️⃣. 《pulsar 源码》 ❌ 2️⃣7️⃣. 《nsq 源码》 2️⃣8️⃣. 《eventing 源码》 2️⃣9️⃣. 《serving 源码》 3️⃣0️⃣. 《深入浅出Istio：Service Mesh快速入门与实践》 ✅ 3️⃣1️⃣. 《Istio服务网格技术解析与实践》 ✅ 3️⃣2️⃣. 《云原生服务网格Istio：原理、实践、架构与源码解析》 3️⃣3️⃣. 《gRPC与云原生应用开发》 ✅ 3️⃣4️⃣. 《Quarkus 实战》 ✅ 3️⃣4️⃣. 《gin 源码》 ✅ 3️⃣4️⃣. 《grpc-go 源码》 ⭕ 3️⃣5️⃣. 《Go语言精进之路 I》 ✅ 3️⃣6️⃣. 《Go语言精进之路 II》 ✅ 3️⃣7️⃣. 《Wireshark网络分析就这么简单》 ✅ 3️⃣8️⃣. 《MySQL技术内幕》 3️⃣9️⃣. 《深入解析Java虚拟机HotSpot》 4️⃣0️⃣. 《Rust权威指南》 ⭕ 4️⃣1️⃣. 《深入剖析Kubernetes》 4️⃣2️⃣. 《Kubernetes编程》 ⭕ 4️⃣3️⃣. 《Kubernetes源码剖析》 ✅ 4️⃣4️⃣. 《Kubernetes设计模式》 4️⃣5️⃣. 《Helm学习指南：Kubernetes上的应用程序管理》 ✅ 4️⃣6️⃣. 《Knative实战:基于Kubernetes的无服务器架构实践》 ✅ 4️⃣7️⃣. 《深入剖析Java虚拟机》 4️⃣8️⃣. 《Groovy程序设计》 ✅ ","date":"2022-01-01","objectID":"/ooooo-notes/2022%E5%B9%B4%E8%AE%A1%E5%88%92/:3:1","tags":["learning"],"title":"2022年学习计划","uri":"/ooooo-notes/2022%E5%B9%B4%E8%AE%A1%E5%88%92/"},{"categories":null,"content":"2、文档 0️⃣1️⃣. 《深入拆解 Java 虚拟机》 0️⃣2️⃣. 《How to write Go code》 0️⃣3️⃣. 《Effective Go》 0️⃣4️⃣. 从 0 开始带你成为JVM实战高手 ⭕ 0️⃣4️⃣. Go 语言项目开发实战 ⭕ 0️⃣5️⃣. Redis 核心技术与实战 ✅ 0️⃣6️⃣. Redis 源码剖析与实战 ⭕ 0️⃣7️⃣. 深入拆解 Tomcat \u0026 Jetty ✅ 0️⃣8️⃣. 深入 C 语言和程序运行原理 0️⃣9️⃣. 罗剑锋的 C++ 实战笔记 ✅ ","date":"2022-01-01","objectID":"/ooooo-notes/2022%E5%B9%B4%E8%AE%A1%E5%88%92/:3:2","tags":["learning"],"title":"2022年学习计划","uri":"/ooooo-notes/2022%E5%B9%B4%E8%AE%A1%E5%88%92/"},{"categories":null,"content":"3、视频 0️⃣1️⃣. 《玩转算法系列–图论精讲》 0️⃣2️⃣. 《玩转算法面试》 ⭕ 0️⃣3️⃣. 《看的见的算法》 0️⃣4️⃣. 《极客时间– 算法进阶训练营》 ✅ 0️⃣5️⃣. 《Dubbo 3 深度剖析 - 透过源码认识你》 ✅ 0️⃣5️⃣. 《Go 微服务实战 38 讲》 ✅ 0️⃣6️⃣. 《Netty 源码剖析与实战》 ✅ ","date":"2022-01-01","objectID":"/ooooo-notes/2022%E5%B9%B4%E8%AE%A1%E5%88%92/:3:3","tags":["learning"],"title":"2022年学习计划","uri":"/ooooo-notes/2022%E5%B9%B4%E8%AE%A1%E5%88%92/"},{"categories":null,"content":"4、算法 0️⃣1️⃣. 每周至少 5 道 Leetcode。 ⭕ 0️⃣2️⃣. leetcode 全站排名1000以内。 ⭕ 0️⃣3️⃣. leetcode 周赛全国排名2000以内。 ⭕ 0️⃣4️⃣. leetcode 周赛全球排名10000以内。 ⭕ ","date":"2022-01-01","objectID":"/ooooo-notes/2022%E5%B9%B4%E8%AE%A1%E5%88%92/:3:4","tags":["learning"],"title":"2022年学习计划","uri":"/ooooo-notes/2022%E5%B9%B4%E8%AE%A1%E5%88%92/"},{"categories":null,"content":"4、关于其他 ","date":"2022-01-01","objectID":"/ooooo-notes/2022%E5%B9%B4%E8%AE%A1%E5%88%92/:4:0","tags":["learning"],"title":"2022年学习计划","uri":"/ooooo-notes/2022%E5%B9%B4%E8%AE%A1%E5%88%92/"},{"categories":null,"content":"1、书籍 🎉 0️⃣1️⃣. 《聪明的投资者》 0️⃣2️⃣. 《一万小时天才理论》 ❌ 0️⃣3️⃣. 《番茄工作法图解》 ❌ 0️⃣4️⃣. 《三体》 ✅ 0️⃣5️⃣. 《三体Ⅱ》 0️⃣6️⃣. 《三体Ⅲ》 0️⃣7️⃣. 《非暴力沟通》 ⭕ 0️⃣8️⃣. 《管理你的每一天》 ❌ 0️⃣9️⃣. 《原则》 1️⃣0️⃣. 《思考，快与慢》 1️⃣1️⃣. 《关键对话》 1️⃣2️⃣. 《当下的启蒙》 1️⃣3️⃣. 《把时间当作朋友》 ❌ 1️⃣4️⃣. 《白夜行》 ❌ 1️⃣5️⃣. 《亲密关系：通往灵魂的桥梁》 ⭕ 1️⃣6️⃣. 《蛤蟆先生去看心理医生》 ✅ 1️⃣7️⃣. 《刻意练习》 ⭕ 1️⃣7️⃣. 《卓有成效的工程师》 ⭕ ","date":"2022-01-01","objectID":"/ooooo-notes/2022%E5%B9%B4%E8%AE%A1%E5%88%92/:4:1","tags":["learning"],"title":"2022年学习计划","uri":"/ooooo-notes/2022%E5%B9%B4%E8%AE%A1%E5%88%92/"},{"categories":null,"content":"2、选读 🎉 0️⃣1️⃣. 《人性的弱点》 0️⃣2️⃣. 《算法 4》 0️⃣3️⃣. 《数据密集型应用系统设计》 0️⃣4️⃣. 《当我谈跑步时，我谈些什么》 0️⃣5️⃣. 《Kafka 官网》 ❓ 0️⃣6️⃣. 《MIT 高级数据课程》 ❓ ","date":"2022-01-01","objectID":"/ooooo-notes/2022%E5%B9%B4%E8%AE%A1%E5%88%92/:4:2","tags":["learning"],"title":"2022年学习计划","uri":"/ooooo-notes/2022%E5%B9%B4%E8%AE%A1%E5%88%92/"},{"categories":null,"content":"3、尝试 🎉 0️⃣1️⃣. 学会使用尤克里里弹奏 ","date":"2022-01-01","objectID":"/ooooo-notes/2022%E5%B9%B4%E8%AE%A1%E5%88%92/:4:3","tags":["learning"],"title":"2022年学习计划","uri":"/ooooo-notes/2022%E5%B9%B4%E8%AE%A1%E5%88%92/"},{"categories":null,"content":"4、了解 🎉 0️⃣1️⃣. 暂无 ","date":"2022-01-01","objectID":"/ooooo-notes/2022%E5%B9%B4%E8%AE%A1%E5%88%92/:4:4","tags":["learning"],"title":"2022年学习计划","uri":"/ooooo-notes/2022%E5%B9%B4%E8%AE%A1%E5%88%92/"},{"categories":null,"content":"5、娱乐 0️⃣1️⃣. 《荒岛余生》 ✅ 0️⃣2️⃣. 《星际穿越》 ✅ 0️⃣3️⃣. 《这个杀手不太冷》 ✅ 0️⃣4️⃣. 《美丽人生》 ✅ 0️⃣5️⃣. 《阿甘正传》 ✅ 0️⃣6️⃣. 《奇遇人生 第一季》 0️⃣7️⃣. 《一本好书 1》 0️⃣8️⃣. 《一本好书 2》 0️⃣9️⃣. 《楚门的世界》 ✅ 1️⃣0️⃣. 《穿越时空的少女》 ✅ 1️⃣1️⃣. 《五等分的新娘 剧场版》 ✅ 1️⃣2️⃣. 《你的名字》 ✅ 1️⃣3️⃣. 《工作细胞》 ✅ ","date":"2022-01-01","objectID":"/ooooo-notes/2022%E5%B9%B4%E8%AE%A1%E5%88%92/:4:5","tags":["learning"],"title":"2022年学习计划","uri":"/ooooo-notes/2022%E5%B9%B4%E8%AE%A1%E5%88%92/"},{"categories":null,"content":" idea gradle project show duplicated file tree（project view and packages view） you must select idea as run test and building 项目依赖配置 ","date":"2022-01-01","objectID":"/ooooo-notes/idea-%E4%B8%AD-gradle-%E9%A1%B9%E7%9B%AE%E7%BB%93%E6%9E%84%E6%98%BE%E7%A4%BA%E9%94%99%E8%AF%AF/:0:0","tags":["resolution"],"title":"idea 中 gradle 项目结构显示错误","uri":"/ooooo-notes/idea-%E4%B8%AD-gradle-%E9%A1%B9%E7%9B%AE%E7%BB%93%E6%9E%84%E6%98%BE%E7%A4%BA%E9%94%99%E8%AF%AF/"},{"categories":null,"content":"1. 两台机器初始化设置 ","date":"2021-12-01","objectID":"/ooooo-notes/%E5%9C%A8-centos-%E7%B3%BB%E7%BB%9F%E4%B8%AD%E5%AE%89%E8%A3%85-k8s/:1:0","tags":["k8s","cloud native"],"title":"在 centos 上安装 k8s","uri":"/ooooo-notes/%E5%9C%A8-centos-%E7%B3%BB%E7%BB%9F%E4%B8%AD%E5%AE%89%E8%A3%85-k8s/"},{"categories":null,"content":"1.1 hostname 设置 hostnamectl ## 查看当前的hostname hostnamectl set-hostname centos1 ## 设置主机名为centos1, 在 192.168.130.131 上执行 hostnamectl set-hostname centos2 ## 设置主机名为centos1, 在 192.168.130.132 上执行 ","date":"2021-12-01","objectID":"/ooooo-notes/%E5%9C%A8-centos-%E7%B3%BB%E7%BB%9F%E4%B8%AD%E5%AE%89%E8%A3%85-k8s/:1:1","tags":["k8s","cloud native"],"title":"在 centos 上安装 k8s","uri":"/ooooo-notes/%E5%9C%A8-centos-%E7%B3%BB%E7%BB%9F%E4%B8%AD%E5%AE%89%E8%A3%85-k8s/"},{"categories":null,"content":"1.2 /etc/hosts 文件 (两个机器都需要) 192.168.1.8 ooooo 192.168.130.131 centos1 ## k8s master 192.168.130.132 centos2 ## k8s worker ","date":"2021-12-01","objectID":"/ooooo-notes/%E5%9C%A8-centos-%E7%B3%BB%E7%BB%9F%E4%B8%AD%E5%AE%89%E8%A3%85-k8s/:1:2","tags":["k8s","cloud native"],"title":"在 centos 上安装 k8s","uri":"/ooooo-notes/%E5%9C%A8-centos-%E7%B3%BB%E7%BB%9F%E4%B8%AD%E5%AE%89%E8%A3%85-k8s/"},{"categories":null,"content":"1.3 创建非 root 用户 (两个机器都需要) useradd ooooo -g ooooo ## 添加用户，两个机器都执行 passwd ooooo ## 修改用户密码，两个机器都执行 ","date":"2021-12-01","objectID":"/ooooo-notes/%E5%9C%A8-centos-%E7%B3%BB%E7%BB%9F%E4%B8%AD%E5%AE%89%E8%A3%85-k8s/:1:3","tags":["k8s","cloud native"],"title":"在 centos 上安装 k8s","uri":"/ooooo-notes/%E5%9C%A8-centos-%E7%B3%BB%E7%BB%9F%E4%B8%AD%E5%AE%89%E8%A3%85-k8s/"},{"categories":null,"content":"1.4 添加 yum 代理 (两个机器都需要) sudo vim /etc/yum.conf ## 编辑 yum 配置文件 proxy=http://ooooo:10800 ## 在文件中添加一行 ","date":"2021-12-01","objectID":"/ooooo-notes/%E5%9C%A8-centos-%E7%B3%BB%E7%BB%9F%E4%B8%AD%E5%AE%89%E8%A3%85-k8s/:1:4","tags":["k8s","cloud native"],"title":"在 centos 上安装 k8s","uri":"/ooooo-notes/%E5%9C%A8-centos-%E7%B3%BB%E7%BB%9F%E4%B8%AD%E5%AE%89%E8%A3%85-k8s/"},{"categories":null,"content":"1.5 安装 docker 服务 (两个机器都需要) 官方 docker 安装文档 参考文档安装 docker sudo vim /etc/docker/daemon.json ## 编辑 docker 配置文件， 添加下面 json 配置，这是因为 k8s 默认使用的 cgroup driver 是 systemd { \"exec-opts\": [\"native.cgroupdriver=systemd\"] } sudo systemctl enable --now docker.service ## 设置 docker 服务开机启动，并且现在启动 sudo systemctl status docker ## 查看 docker 服务的状态， 失败了，使用下一条命令查看日志 journalctl -xeu docker ## 查看 docker 日志服务 ","date":"2021-12-01","objectID":"/ooooo-notes/%E5%9C%A8-centos-%E7%B3%BB%E7%BB%9F%E4%B8%AD%E5%AE%89%E8%A3%85-k8s/:1:5","tags":["k8s","cloud native"],"title":"在 centos 上安装 k8s","uri":"/ooooo-notes/%E5%9C%A8-centos-%E7%B3%BB%E7%BB%9F%E4%B8%AD%E5%AE%89%E8%A3%85-k8s/"},{"categories":null,"content":"2. k8s 的 kubeadm 安装 (两台都需要) 官方 k8s 安装文档 参考文档检查服务器的状态是否可以安装 k8s 服务 ## 关闭 swap 分区 swapoff -a sudo echo vm.swappiness=0 \u003e\u003e /etc/sysctl.con ## 永久关闭 swap 分区， k8s 不能运行在有 swap 分区的机器上 free -h ## 查看 swap 分区是否关闭，显示 0 表示已关闭 ## 检查 br_netfilter 是否被加载，没有任何输出，表示没有加载 lsmod | grep br_netfilter sudo modprobe br_netfilter ## 加载 br_netfilter 模块 ## 配置网络 cat \u003c\u003cEOF | sudo tee /etc/modules-load.d/k8s.conf br_netfilter EOF cat \u003c\u003cEOF | sudo tee /etc/sysctl.d/k8s.conf net.bridge.bridge-nf-call-ip6tables = 1 net.bridge.bridge-nf-call-iptables = 1 EOF sudo sysctl --system 安装容器运行时(runtime),k8s 高版本采用自动检查方式,不用做任何处理 ## 添加 k8s 镜像仓库，在前面中，设置了 yum 代理 ## 在官方文档中多了 exclude=kubelet kubeadm kubectl ，这里去掉, 直接安装最新版本的 cat \u003c\u003cEOF | sudo tee /etc/yum.repos.d/kubernetes.repo [kubernetes] name=Kubernetes baseurl=http://mirrors.aliyun.com/kubernetes/yum/repos/kubernetes-el7-\\$basearch enabled=1 gpgcheck=1 gpgkey=http://mirrors.aliyun.com/kubernetes/yum/doc/rpm-package-key.gpg EOF ## 关闭 selinux sudo setenforce 0 sudo sed -i 's/^SELINUX=enforcing$/SELINUX=permissive/' /etc/selinux/config ## 安装 k8s 服务, --disableexcludes=kubernetes 表示排除 kubernetes 之外的镜像源 sudo yum install -y kubelet kubeadm kubectl --disableexcludes=kubernetes ## 设置 kubelet 开机启动，并且现在启动 ## 启动之后可能会报错，如果原因是 没有读取到 kubelet 的配置文件，这里可以不用管，稍后会重启这个服务 sudo systemctl enable --now kubelet sudo systemctl status kubelet ## 查看 kubelet 的状态 journalctl -xeu kubelet ## 查看 kubelet 的日志 ","date":"2021-12-01","objectID":"/ooooo-notes/%E5%9C%A8-centos-%E7%B3%BB%E7%BB%9F%E4%B8%AD%E5%AE%89%E8%A3%85-k8s/:2:0","tags":["k8s","cloud native"],"title":"在 centos 上安装 k8s","uri":"/ooooo-notes/%E5%9C%A8-centos-%E7%B3%BB%E7%BB%9F%E4%B8%AD%E5%AE%89%E8%A3%85-k8s/"},{"categories":null,"content":"3. 创建 k8s 集群 (两台都需要) 创建 k8s 集群官方文档 k8s pod network 插件文档 ## 执行 kubeadm init 命令， 在 k8s master 机器上执行，默认情况下， k8s 创建 pod 不会在 master 机器上 ## 重点注意: --pod-network-cidr=10.244.0.0/16 这个参数必须要有，没有的话安装 cni 会报错 ## 注意 preflight 的前置检查输出，可能需要添加 docker group，这个会输出有提示的命令 sudo kubeadm init --image-repository registry.aliyuncs.com/google_containers --apiserver-advertise-address=192.168.130.131 --pod-network-cidr=10.244.0.0/16 ## 执行命令之后，会有 kubeadm join 输出行 ## （分为 master-token 和 worker-token）， 类似于下面的命令，在 centos2 上执行 worker-join-token 的命令 sudo kubeadm join 192.168.130.131:6443 --token 8auvt0.zfw0ayr45d80q8pb \\ --discovery-token-ca-cert-hash sha256:efe854739efef5fbaf3f6e28c899481c8d7797c1997fc8315b921a9ede400ca8 ## 在机器上执行 kubeadm join 或者 kubeadm init 命令之后，重启 kubelet 服务 sudo systemctl restart kubelet sudo systemctl status kubelet ## 设置 kubectl 的配置文件， 为 $HOME/.kube/config mkdir -p $HOME/.kube sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config sudo chown $(id -u):$(id -g) $HOME/.kube/config ## 安装 pod network 插件, 这里使用 flannel 插件 kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml ## 查看 flannel 是否已经启动完成, cni 也启动成功 kubectl get pods -A ## 成功之后会有下面的服务， 都是 running 状态 kube-system coredns-7f6cbbb7b8-5hqt5 1/1 Running 15 (76m ago) 26h kube-system coredns-7f6cbbb7b8-lwdrv 1/1 Running 15 (76m ago) 26h kube-system etcd-centos1 1/1 Running 18 (76m ago) 26h kube-system kube-apiserver-centos1 1/1 Running 25 (76m ago) 26h kube-system kube-controller-manager-centos1 1/1 Running 12 (76m ago) 26h kube-system kube-flannel-ds-6lx7s 1/1 Running 6 (76m ago) 21h kube-system kube-flannel-ds-n5tfn 1/1 Running 6 (76m ago) 21h kube-system kube-proxy-78jrm 1/1 Running 8 (76m ago) 26h kube-system kube-proxy-wl5jg 1/1 Running 8 (76m ago) 26h kube-system kube-scheduler-centos1 1/1 Running 16 (76m ago) 26h ","date":"2021-12-01","objectID":"/ooooo-notes/%E5%9C%A8-centos-%E7%B3%BB%E7%BB%9F%E4%B8%AD%E5%AE%89%E8%A3%85-k8s/:3:0","tags":["k8s","cloud native"],"title":"在 centos 上安装 k8s","uri":"/ooooo-notes/%E5%9C%A8-centos-%E7%B3%BB%E7%BB%9F%E4%B8%AD%E5%AE%89%E8%A3%85-k8s/"},{"categories":null,"content":"在 ~\\.gradle 目录下新建文件 init.gradle, 内容如下 allprojects { repositories { mavenLocal() maven { name \"Alibaba\" ; url \"https://maven.aliyun.com/repository/public\" } maven { name \"Bstek\" ; url \"http://nexus.bsdn.org/content/groups/public/\" } } buildscript { repositories { maven { name \"Alibaba\" ; url 'https://maven.aliyun.com/repository/public' } maven { name \"Bstek\" ; url 'https://nexus.bsdn.org/content/groups/public/' } maven { name \"M2\" ; url 'https://plugins.gradle.org/m2/' } } } } ","date":"2021-01-02","objectID":"/ooooo-notes/gradle-%E5%85%A8%E5%B1%80%E8%AE%BE%E7%BD%AE%E4%BB%93%E5%BA%93%E9%95%9C%E5%83%8F/:0:0","tags":["resolution"],"title":"gradle 全局设置仓库镜像","uri":"/ooooo-notes/gradle-%E5%85%A8%E5%B1%80%E8%AE%BE%E7%BD%AE%E4%BB%93%E5%BA%93%E9%95%9C%E5%83%8F/"},{"categories":null,"content":"0、持续学习者 Talk is cheap. Show me the code. 英语比编程简单。 学习和实践要平衡。 学会和时间做朋友。 学会投资，学会理财。 学会先做减法，再做加法。 学英语很重要，学英语很重要，学英语很重要。 说明： ⭕ 进行中 ✅ 已完成 ❌ 已废弃 ❓ 有必要 ❗ 重要性 📝 记笔记 🖊️ 写代码 ","date":"2021-01-01","objectID":"/ooooo-notes/2021%E5%B9%B4%E8%AE%A1%E5%88%92/:1:0","tags":["learning"],"title":"2021年学习计划","uri":"/ooooo-notes/2021%E5%B9%B4%E8%AE%A1%E5%88%92/"},{"categories":null,"content":"1、关于英语 听说读写，目前的学习重点是日常沟通，所以放弃背单词。 计划 🎉： 目前我已经背单词 518 多天，我将会继续背单词（墨墨背单词）。 ❌ 学习《新概念英语一》 ✅ 看 Spring Framework。 学习《赖世雄美语音标》 学习《新概念英语二》 目前状态: 《新概念英语二》。 ","date":"2021-01-01","objectID":"/ooooo-notes/2021%E5%B9%B4%E8%AE%A1%E5%88%92/:2:0","tags":["learning"],"title":"2021年学习计划","uri":"/ooooo-notes/2021%E5%B9%B4%E8%AE%A1%E5%88%92/"},{"categories":null,"content":"2、关于技术 计划 🎉： 只记录自己认为有用的笔记。 ","date":"2021-01-01","objectID":"/ooooo-notes/2021%E5%B9%B4%E8%AE%A1%E5%88%92/:3:0","tags":["learning"],"title":"2021年学习计划","uri":"/ooooo-notes/2021%E5%B9%B4%E8%AE%A1%E5%88%92/"},{"categories":null,"content":"1、书籍 0️⃣1️⃣. 《Java 性能优化权威指南》 0️⃣2️⃣. 《Netty 实战》 ✅ 0️⃣3️⃣. 《程序员面试金典（第6版）》 ⭕ 0️⃣4️⃣. 《图解TCP/IP》 ✅ 0️⃣5️⃣. 《Spring 源码深度解析》 ✅ 0️⃣6️⃣. 《深入理解 Java 虚拟机（第3版）》 ⭕ 0️⃣7️⃣. 《Spring Cloud 微服务：入门、实战与进阶》 0️⃣8️⃣. 《Spring Cloud Alibaba 微服务原理与实战》 ✅ 0️⃣9️⃣. 《深入理解 Apache Dubbo 与实战》 ✅ 1️⃣0️⃣. 《arthas》 1️⃣1️⃣. 《Java 并发编程实战》 1️⃣2️⃣. 《深入理解 Kafka：核心设计与实践原理》 1️⃣3️⃣. 《Spring 5核心原理与30个类手写实战》 ❌ 1️⃣4️⃣. 《Netty 4核心原理与手写RPC框架实战》 ❌ 1️⃣5️⃣. 《从零开始学架构》 1️⃣6️⃣. 《高可用可伸缩微服务架构》 1️⃣7️⃣. 《实战Java虚拟机》 ❓ 1️⃣9️⃣. 《分布式一致性算法开发实战》 2️⃣0️⃣. 《Go Web 编程》 2️⃣1️⃣. 《consul》 ❌ 2️⃣2️⃣. 《Java 异步编程实战》 ❓ 2️⃣3️⃣. 《Effective C++》 2️⃣4️⃣. More Effective C++ 2️⃣5️⃣. 深度探索C++对象模型 2️⃣6️⃣. 《深入浅出 Docker》 ✅ 2️⃣7️⃣. 《码出高效：Java开发手册》 2️⃣8️⃣. 《Go 专家编程》 2️⃣9️⃣. 《流畅的 Python》 3️⃣0️⃣. 《wireshark网络分析的艺术》 3️⃣2️⃣. 《RocketMQ技术内幕》 ✅ 3️⃣3️⃣. 《RocketMQ分布式消息中间件：核心原理与最佳实践》 ✅ 3️⃣4️⃣. 《RocketMQ实战与原理解析》 ✅ 3️⃣5️⃣. 《Vim实用技巧（第2版）》 ⭕ 3️⃣6️⃣. 《RocketMQ技术内幕 第二版》 3️⃣7️⃣. 《Kubernetes in Action中文版》 ⭕ 3️⃣8️⃣. 《rocketmq》 ⭕ 3️⃣9️⃣. 《spring cloud stream》 ✅ 3️⃣9️⃣. 《dubbo》 ⭕ 4️⃣0️⃣. 《pulsar》 ⭕ 4️⃣0️⃣. 《nsq》 4️⃣0️⃣. 《eventing》 4️⃣0️⃣. 《serving》 4️⃣1️⃣. 《Activiti》 ✅ ","date":"2021-01-01","objectID":"/ooooo-notes/2021%E5%B9%B4%E8%AE%A1%E5%88%92/:3:1","tags":["learning"],"title":"2021年学习计划","uri":"/ooooo-notes/2021%E5%B9%B4%E8%AE%A1%E5%88%92/"},{"categories":null,"content":"2、文档 0️⃣1️⃣. 《深入拆解 Java 虚拟机》 0️⃣2️⃣. 通读 Spring 官网，+实践+代码+笔记。 0️⃣1️⃣ spring-cloud-netflix-eureka-clients 0️⃣2️⃣ spring-cloud-netflix-eureka-server 0️⃣3️⃣ spring-cloud-task ✅ 0️⃣3️⃣. 学习 Go 语言，通读 Go 官网，+实践+代码+笔记。 0️⃣1️⃣. 《A Tour of Go》 ✅ 0️⃣2️⃣. 《Tutorial: Create a module》 ✅ 0️⃣3️⃣. 《Writing Web Applications》 ✅ 0️⃣4️⃣. 《How to write Go code》 0️⃣5️⃣. 《Effective Go》 0️⃣4️⃣. Protobuf javaTutorial ✅ 0️⃣5️⃣. Go语言核心36讲 ✅ 0️⃣6️⃣. 消息队列高手课 ✅ 0️⃣7️⃣. 从 0 开始带你成为消息中间件实战高手 ✅ 0️⃣8️⃣. 从 0 开始带你成为JVM实战高手 ⭕ ","date":"2021-01-01","objectID":"/ooooo-notes/2021%E5%B9%B4%E8%AE%A1%E5%88%92/:3:2","tags":["learning"],"title":"2021年学习计划","uri":"/ooooo-notes/2021%E5%B9%B4%E8%AE%A1%E5%88%92/"},{"categories":null,"content":"3、视频 0️⃣1️⃣. 《玩转算法系列–图论精讲》 0️⃣2️⃣. 《玩转算法面试》 ⭕ 0️⃣3️⃣. 《利用Go优越的性能设计与实现高性能企业级微服务网关》 ⭕ 0️⃣3️⃣. 《看的见的算法》 0️⃣4️⃣. 《极客时间– Java 进阶训练营》 ❌ 0️⃣5️⃣. 《极客时间– go 进阶训练营》 ❌ 0️⃣5️⃣. 《极客时间– 算法进阶训练营》 ⭕ ","date":"2021-01-01","objectID":"/ooooo-notes/2021%E5%B9%B4%E8%AE%A1%E5%88%92/:3:3","tags":["learning"],"title":"2021年学习计划","uri":"/ooooo-notes/2021%E5%B9%B4%E8%AE%A1%E5%88%92/"},{"categories":null,"content":"4、算法 0️⃣1️⃣. 每周至少 5 道 Leetcode。 ⭕ 0️⃣2️⃣. leetcode 全站排名1000以内。 ⭕ 0️⃣3️⃣. leetcode 周赛全国排名2000以内。 ⭕ 0️⃣4️⃣. leetcode 周赛全球排名10000以内。 ⭕ ","date":"2021-01-01","objectID":"/ooooo-notes/2021%E5%B9%B4%E8%AE%A1%E5%88%92/:3:4","tags":["learning"],"title":"2021年学习计划","uri":"/ooooo-notes/2021%E5%B9%B4%E8%AE%A1%E5%88%92/"},{"categories":null,"content":"4、关于其他 ","date":"2021-01-01","objectID":"/ooooo-notes/2021%E5%B9%B4%E8%AE%A1%E5%88%92/:4:0","tags":["learning"],"title":"2021年学习计划","uri":"/ooooo-notes/2021%E5%B9%B4%E8%AE%A1%E5%88%92/"},{"categories":null,"content":"1、书籍 🎉 0️⃣1️⃣. 《聪明的投资者》 0️⃣2️⃣. 《一万小时天才理论》 ⭕ 0️⃣3️⃣. 《番茄工作法图解》 ⭕ 0️⃣4️⃣. 《三体》 ✅ 0️⃣5️⃣. 《三体Ⅱ》 0️⃣6️⃣. 《三体Ⅲ》 0️⃣7️⃣. 《非暴力沟通》 ⭕ 0️⃣8️⃣. 《管理你的每一天》 ⭕ 0️⃣9️⃣. 《原则》 1️⃣0️⃣. 《思考，快与慢》 1️⃣1️⃣. 《关键对话》 1️⃣2️⃣. 《当下的启蒙》 1️⃣3️⃣. 《把时间当作朋友》 ⭕ 1️⃣3️⃣. 《活着》 ✅ 1️⃣4️⃣. 《白夜行》 ⭕ 1️⃣5️⃣. 《亲密关系：通往灵魂的桥梁》 ⭕ ","date":"2021-01-01","objectID":"/ooooo-notes/2021%E5%B9%B4%E8%AE%A1%E5%88%92/:4:1","tags":["learning"],"title":"2021年学习计划","uri":"/ooooo-notes/2021%E5%B9%B4%E8%AE%A1%E5%88%92/"},{"categories":null,"content":"2、选读 🎉 0️⃣1️⃣. 《C++ Prime（第5版）》 ❌ 0️⃣2️⃣. 《算法 4》 0️⃣3️⃣. 《JMC 工具(Java Mission Control)》 0️⃣4️⃣. 《ZooKeeper》 0️⃣5️⃣. 《MIT 高级数据课程》 0️⃣6️⃣. 《Resilience4j》 ❓ 0️⃣7️⃣. 《Google Guava》 ❓ 0️⃣8️⃣. 《Kafka 官网》 0️⃣9️⃣. 《Spring Security 实战》 ❌ 1️⃣0️⃣. 《Jvisualvm》 1️⃣1️⃣. 《深入理解 Nginx（第 2 版）》 ❓ 1️⃣2️⃣. 《分布式服务框架：原理与实践》 1️⃣3️⃣. 《chrome-devtools》 ❌ 1️⃣4️⃣. 《人性的弱点》 1️⃣5️⃣. 《深入剖析 Tomcat》 1️⃣6️⃣. 《Java 编程方法论：响应式Spring Reactor 3设计与实现》 1️⃣7️⃣. 《数据密集型应用系统设计》 1️⃣8️⃣. 《Go 程序设计语言》 ❌ 1️⃣9️⃣. 《机器学习实战：基于Scikit-Learn、Keras和TensorFlow》 2️⃣0️⃣. 《Python深度学习》 2️⃣1️⃣. 《当我谈跑步时，我谈些什么》 ","date":"2021-01-01","objectID":"/ooooo-notes/2021%E5%B9%B4%E8%AE%A1%E5%88%92/:4:2","tags":["learning"],"title":"2021年学习计划","uri":"/ooooo-notes/2021%E5%B9%B4%E8%AE%A1%E5%88%92/"},{"categories":null,"content":"3、尝试 🎉 0️⃣1️⃣. 学会使用尤克里里弹奏 ","date":"2021-01-01","objectID":"/ooooo-notes/2021%E5%B9%B4%E8%AE%A1%E5%88%92/:4:3","tags":["learning"],"title":"2021年学习计划","uri":"/ooooo-notes/2021%E5%B9%B4%E8%AE%A1%E5%88%92/"},{"categories":null,"content":"4、了解 🎉 0️⃣1️⃣. 《hugo》 ✅ ","date":"2021-01-01","objectID":"/ooooo-notes/2021%E5%B9%B4%E8%AE%A1%E5%88%92/:4:4","tags":["learning"],"title":"2021年学习计划","uri":"/ooooo-notes/2021%E5%B9%B4%E8%AE%A1%E5%88%92/"},{"categories":null,"content":"5、娱乐 0️⃣1️⃣. 《傲慢与偏见》 ✅ 0️⃣2️⃣. 《肖申克的救赎》 ✅ 0️⃣3️⃣. 《志明与春娇》 ✅ 0️⃣4️⃣. 《春娇与志明》 ✅ 0️⃣4️⃣. 《春娇救志明》 ✅ ","date":"2021-01-01","objectID":"/ooooo-notes/2021%E5%B9%B4%E8%AE%A1%E5%88%92/:4:5","tags":["learning"],"title":"2021年学习计划","uri":"/ooooo-notes/2021%E5%B9%B4%E8%AE%A1%E5%88%92/"},{"categories":null,"content":"6. 总结 今年大部分的时间都放在阅读源码上，导致很多的书籍没有看完，也放弃了英语学习。 年初定的计划在实际执行过程中，两次改变了学习重点， 1. mq 源码 2. k8s 源码。 认真思考指定 2022 的计划 ","date":"2021-01-01","objectID":"/ooooo-notes/2021%E5%B9%B4%E8%AE%A1%E5%88%92/:4:6","tags":["learning"],"title":"2021年学习计划","uri":"/ooooo-notes/2021%E5%B9%B4%E8%AE%A1%E5%88%92/"},{"categories":null,"content":"在 conf/server.xml 中的 Host标签添加 \u003cHost name=\"localhost\" appBase=\"webapps\" unpackWARs=\"true\" autoDeploy=\"true\"\u003e \u003cValve className=\"org.apache.catalina.valves.AccessLogValve\" directory=\"logs\" prefix=\"localhost_access_log\" suffix=\".txt\" pattern=\"%h %l %u %t \u0026quot;%r\u0026quot; %s %b\" /\u003e // 这是新加的 \u003cValve className=\"org.apache.catalina.valves.ErrorReportValve\" errorCode.400=\"webapps/ROOT/error.jsp\" errorCode.0=\"webapps/ROOT/error.jsp\" showReport=\"false\" showServerInfo=\"false\" /\u003e // 这是新加的 \u003c/Host\u003e 上面的 error.jsp 放在 webapps/ROOT/ 参考 https://stackoverflow.com/questions/52814582/tomcat-is-not-redirecting-to-400-bad-request-custom-error-page 参考 https://tomcat.apache.org/tomcat-9.0-doc/config/valve.html#Error_Report_Valve ","date":"2021-01-01","objectID":"/ooooo-notes/tomcat-%E8%87%AA%E5%AE%9A%E4%B9%89%E9%94%99%E8%AF%AF%E9%A1%B5/:0:0","tags":["resolution"],"title":"tomcat 自定义错误页","uri":"/ooooo-notes/tomcat-%E8%87%AA%E5%AE%9A%E4%B9%89%E9%94%99%E8%AF%AF%E9%A1%B5/"},{"categories":null,"content":" 重写方法XMLHttpRequest.prototype.send XMLHttpRequest.prototype._send = XMLHttpRequest.prototype.send XMLHttpRequest.prototype.send = function (params) { var attached_params = mdcUtil.MDC_DEVICE_ID + \"=\" + mdcUtil.getMdcDeviceId(); if (params) { params += \"\u0026\" + attached_params; } else { params = attached_params; } return this._send(params) } ","date":"2020-01-05","objectID":"/ooooo-notes/%E5%9C%A8-js-%E4%B8%AD%E7%BB%9F%E4%B8%80%E8%AE%BE%E7%BD%AE%E8%AF%B7%E6%B1%82%E5%8F%82%E6%95%B0%E7%9A%84%E5%8F%A6%E4%B8%80%E7%A7%8D%E6%96%B9%E6%B3%95/:0:0","tags":["resolution"],"title":"在 js 中统一设置请求参数的另一种方法","uri":"/ooooo-notes/%E5%9C%A8-js-%E4%B8%AD%E7%BB%9F%E4%B8%80%E8%AE%BE%E7%BD%AE%E8%AF%B7%E6%B1%82%E5%8F%82%E6%95%B0%E7%9A%84%E5%8F%A6%E4%B8%80%E7%A7%8D%E6%96%B9%E6%B3%95/"},{"categories":null,"content":" 编辑运行配置，设置环境变量中的工作目录为当前模块目录。 ","date":"2020-01-04","objectID":"/ooooo-notes/idea-%E5%A4%9A%E6%A8%A1%E5%9D%97%E9%A1%B9%E7%9B%AE%E5%90%AF%E5%8A%A8webapp-%E7%9B%AE%E5%BD%95%E4%B8%8B%E6%96%87%E4%BB%B6%E8%AE%BF%E9%97%AE%E4%B8%8D%E5%88%B0/:0:0","tags":["resolution"],"title":"idea 多模块项目启动，webapp 目录下文件访问不到","uri":"/ooooo-notes/idea-%E5%A4%9A%E6%A8%A1%E5%9D%97%E9%A1%B9%E7%9B%AE%E5%90%AF%E5%8A%A8webapp-%E7%9B%AE%E5%BD%95%E4%B8%8B%E6%96%87%E4%BB%B6%E8%AE%BF%E9%97%AE%E4%B8%8D%E5%88%B0/"},{"categories":null,"content":" 去掉属性 required，添加 rules 规则 { required: true, message: '请输入姓名', trigger: 'blur' } ","date":"2020-01-03","objectID":"/ooooo-notes/element-ui-%E4%B8%AD-el-form-item-%E6%A0%A1%E9%AA%8C%E5%87%BA%E7%8E%B0%E8%8B%B1%E6%96%87/:0:0","tags":["resolution"],"title":"element-ui 中 el-form-item 校验出现英文","uri":"/ooooo-notes/element-ui-%E4%B8%AD-el-form-item-%E6%A0%A1%E9%AA%8C%E5%87%BA%E7%8E%B0%E8%8B%B1%E6%96%87/"},{"categories":null,"content":"问题 If you see that the storm process is getting crashed even though you have enough memory (swap/free) available then you should also check the “/proc/sys/vm/overcommit_memory” This switch knows 3 different settings: =\u003e 0: The Linux kernel is free to over commit memory(this is the default), a heuristic algorithm is applied to figure out if enough memory is available. =\u003e 1: The Linux kernel will always over commit memory, and never check if enough memory is available. This increases the risk of out-of-memory situations, but also improves memory-intensive workloads. =\u003e 2: The Linux kernel will not over commit memory, and only allocate as much memory as defined in over commit_ratio. As sometimes OS kills /crashes a process due to a system OS setting, the system OS memory overcommit setting was 2 (when it should have been set to 0) - ","date":"2020-01-02","objectID":"/ooooo-notes/java-%E5%87%BA%E7%8E%B0%E5%86%85%E5%AD%98%E5%88%86%E9%85%8D%E5%A4%B1%E8%B4%A5/:0:0","tags":["resolution"],"title":"java 出现内存分配失败","uri":"/ooooo-notes/java-%E5%87%BA%E7%8E%B0%E5%86%85%E5%AD%98%E5%88%86%E9%85%8D%E5%A4%B1%E8%B4%A5/"},{"categories":null,"content":"0、持续学习者 Talk is cheap. Show me the code. 英语比编程简单。 学习和实践要平衡。 学会和时间做朋友。 学会投资，学会理财。 学会先做减法，再做加法。 学英语很重要，学英语很重要，学英语很重要。 说明： ⭕ 进行中 ✅ 已完成 ❌ 已废弃 ❓ 有必要 ❗ 重要 📝 记笔记* 🖊️ 写代码 ","date":"2020-01-01","objectID":"/ooooo-notes/2020%E5%B9%B4%E8%AE%A1%E5%88%92/:1:0","tags":["learning"],"title":"2020年学习计划","uri":"/ooooo-notes/2020%E5%B9%B4%E8%AE%A1%E5%88%92/"},{"categories":null,"content":"1、关于英语 听说读写，其中最容易的应该是读，然后再是写，我有很大的信心能在两三年之内（2022）正常读写。剩余就是听和说了，目前对我真的太难了。 计划 🎉： 目前我已经背单词 430 多天，我将会继续背单词（墨墨背单词）😄 。 看 YouTube - English with Lucy。 ❓ 看 Spring。 看 Friends。 ❓ 目前状态: 扇贝阅读。 ❗ ","date":"2020-01-01","objectID":"/ooooo-notes/2020%E5%B9%B4%E8%AE%A1%E5%88%92/:2:0","tags":["learning"],"title":"2020年学习计划","uri":"/ooooo-notes/2020%E5%B9%B4%E8%AE%A1%E5%88%92/"},{"categories":null,"content":"2、关于技术 只记录自己认为有用的笔记。 计划 🎉： ","date":"2020-01-01","objectID":"/ooooo-notes/2020%E5%B9%B4%E8%AE%A1%E5%88%92/:3:0","tags":["learning"],"title":"2020年学习计划","uri":"/ooooo-notes/2020%E5%B9%B4%E8%AE%A1%E5%88%92/"},{"categories":null,"content":"1. 书籍 0️⃣1️⃣. 《Java 并发编程的艺术》 ✅ 0️⃣2️⃣. 《Redis 开发与运维》 ✅ 0️⃣3️⃣. 《剑指 Offer》 ✅ 0️⃣4️⃣. 《Effective Java中文版（第3版）》 ✅ 0️⃣5️⃣. 《Java 8 函数式编程 》 ✅ 0️⃣6️⃣. 《算法图解》 ✅ 0️⃣7️⃣. 《设计模式》 ✅ 0️⃣8️⃣. 《图解 HTTP 》 ✅ 0️⃣9️⃣. 《Spring Boot编程思想（核心篇）》 ✅ ","date":"2020-01-01","objectID":"/ooooo-notes/2020%E5%B9%B4%E8%AE%A1%E5%88%92/:3:1","tags":["learning"],"title":"2020年学习计划","uri":"/ooooo-notes/2020%E5%B9%B4%E8%AE%A1%E5%88%92/"},{"categories":null,"content":"2. 文档 0️⃣1️⃣. 《MySQL 实战 45 讲》 ✅ 0️⃣2️⃣. 《Kafka 核心技术与实战》 ✅ 0️⃣3️⃣. 《Java 核心技术 36 讲》 ✅ 0️⃣4️⃣. 《Java 并发编程实战》 ✅ ","date":"2020-01-01","objectID":"/ooooo-notes/2020%E5%B9%B4%E8%AE%A1%E5%88%92/:3:2","tags":["learning"],"title":"2020年学习计划","uri":"/ooooo-notes/2020%E5%B9%B4%E8%AE%A1%E5%88%92/"},{"categories":null,"content":"3. 视频 0️⃣1️⃣. 《算法面试通关 40 讲》 ✅ 0️⃣2️⃣. 《玩转算法系列–玩转数据结构 Java 版》 ✅ 0️⃣3️⃣. 《算法与数据结构-综合提升 C++ 版》 ✅ 0️⃣4️⃣. 《玩转 Spring 全家桶》 ✅ 0️⃣5️⃣. 《Go 语言从入门到实战》 ✅ ","date":"2020-01-01","objectID":"/ooooo-notes/2020%E5%B9%B4%E8%AE%A1%E5%88%92/:3:3","tags":["learning"],"title":"2020年学习计划","uri":"/ooooo-notes/2020%E5%B9%B4%E8%AE%A1%E5%88%92/"},{"categories":null,"content":"4. 通读 Spring 官网 0️⃣1️⃣ spring-cloud-stream ✅ 0️⃣2️⃣ spring-cloud-netflix-zuul ✅ ","date":"2020-01-01","objectID":"/ooooo-notes/2020%E5%B9%B4%E8%AE%A1%E5%88%92/:3:4","tags":["learning"],"title":"2020年学习计划","uri":"/ooooo-notes/2020%E5%B9%B4%E8%AE%A1%E5%88%92/"},{"categories":null,"content":"4、关于其他 阅读，+笔记 0️⃣1️⃣. 《指数基金投资指南》 ✅ 0️⃣2️⃣. 《富爸爸穷爸爸》 ✅ 0️⃣3️⃣. 《解读基金》 ✅ 0️⃣4️⃣. 《富爸爸财务自由之路》 ✅ 0️⃣5️⃣. 《克莱因壶》 ✅ ","date":"2020-01-01","objectID":"/ooooo-notes/2020%E5%B9%B4%E8%AE%A1%E5%88%92/:4:0","tags":["learning"],"title":"2020年学习计划","uri":"/ooooo-notes/2020%E5%B9%B4%E8%AE%A1%E5%88%92/"},{"categories":null,"content":"5、选读 🎉 0️⃣1️⃣. 《阿里巴巴 Java 开发手册》 ✅ 0️⃣2️⃣. 《Spring 实战（第5版 ）》 ✅ 0️⃣3️⃣. 《Cloud Native Java》 ✅ 0️⃣4️⃣. 《MyBatis 技术内幕》 ✅ 0️⃣5️⃣. 《看透 Spring MVC》 ✅ 0️⃣6️⃣. 《Redis 深度历险：核心原理与应用实践》 ✅ 0️⃣7️⃣. 《Offer来了：Java面试核心知识点精讲（原理篇）》 ✅ 0️⃣8️⃣. 《Offer来了：Java面试核心知识点精讲（框架篇）》 ✅ ","date":"2020-01-01","objectID":"/ooooo-notes/2020%E5%B9%B4%E8%AE%A1%E5%88%92/:5:0","tags":["learning"],"title":"2020年学习计划","uri":"/ooooo-notes/2020%E5%B9%B4%E8%AE%A1%E5%88%92/"},{"categories":null,"content":" 执行命令 rm -rf ~/.zcompdump* ","date":"2020-01-01","objectID":"/ooooo-notes/zsh-%E6%B7%BB%E5%8A%A0%E6%8F%92%E4%BB%B6%E5%90%8E%E4%B8%8D%E7%94%9F%E6%95%88/:0:0","tags":["resolution"],"title":"zsh 添加插件后，不生效","uri":"/ooooo-notes/zsh-%E6%B7%BB%E5%8A%A0%E6%8F%92%E4%BB%B6%E5%90%8E%E4%B8%8D%E7%94%9F%E6%95%88/"},{"categories":null,"content":"1、Redis 特性 ","date":"0001-01-01","objectID":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/01/:1:0","tags":null,"title":"","uri":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/01/"},{"categories":null,"content":"1、速度快 Redis 的所有数据都是放在内存中的。 Redis 是 C 语言实现的。 Redis 使用单线程架构，避免多线程环境上下文切换。 ","date":"0001-01-01","objectID":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/01/:1:1","tags":null,"title":"","uri":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/01/"},{"categories":null,"content":"2、基于键值对的数据结构服务器 Redis 主要提供五种基本数据结构：string（字符串）、hash（哈希）、list（列表）、set（集合）、zset（有序集合），还提供 Bitmaps（位图）、HyperLogLog（基数统计算法）、GEO（地理位置）高级数据结构。 ","date":"0001-01-01","objectID":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/01/:1:2","tags":null,"title":"","uri":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/01/"},{"categories":null,"content":"3、丰富的功能 提供了键过期功能，可以用来实现缓存。 提供了发布订阅功能，可以用来实现消息系统。 支持 Lua 脚本功能，可以利用 Lua 创造新的 Redis 命令。 提供了简单事务功能。 提供了 Pipeline（流水线）功能。 ","date":"0001-01-01","objectID":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/01/:1:3","tags":null,"title":"","uri":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/01/"},{"categories":null,"content":"4、简单稳定 早期的 Redis 源码只有两万行，3.0 版本后添加了集群特性，代码增到 5 万行左右，Redis 自己实现了事件处理功能。 ","date":"0001-01-01","objectID":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/01/:1:4","tags":null,"title":"","uri":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/01/"},{"categories":null,"content":"5、客户端语言多 Redis 提供了简单的 TCP 通信协议，主流的编程语言都有其客户端实现。 ","date":"0001-01-01","objectID":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/01/:1:5","tags":null,"title":"","uri":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/01/"},{"categories":null,"content":"6、持久化 Redis 提供了 AOF 、 RDB 两种持久化方式。 ","date":"0001-01-01","objectID":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/01/:1:6","tags":null,"title":"","uri":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/01/"},{"categories":null,"content":"7、主从复制、高可用和分布式 Redis Sentinel 、Redis Cluster ","date":"0001-01-01","objectID":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/01/:1:7","tags":null,"title":"","uri":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/01/"},{"categories":null,"content":"2、Redis 使用场景 ","date":"0001-01-01","objectID":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/01/:2:0","tags":null,"title":"","uri":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/01/"},{"categories":null,"content":"Redis 可以做什么 缓存。（ Redis 提供了键过期时间设置） 排行榜系统。（ Redis 提供了列表 list 和有序集合 set 数据结构） 计数器应用。（ Redis 提供了计数功能 incr、decr ） 社交网络。（赞/踩、粉丝、共同好友/喜好） 消息队列。（ Redis 提供了列表 list 的 rpush, blpop 命令 ） ","date":"0001-01-01","objectID":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/01/:2:1","tags":null,"title":"","uri":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/01/"},{"categories":null,"content":"Redis 不可以做什么 Redis 基于内存，不能做存储。 避免用 Redis 来缓存冷数据。 ","date":"0001-01-01","objectID":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/01/:2:2","tags":null,"title":"","uri":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/01/"},{"categories":null,"content":"1、预备 ","date":"0001-01-01","objectID":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/02/:1:0","tags":null,"title":"","uri":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/02/"},{"categories":null,"content":"1、全局命令 help HELP command 查看所有的key KEYS * 键总数 DBSIZE 检查键是否存在 EXISTS key 删除键 DEL key 键过期 EXPIRE key sencond ttl 返回过期时间 TTL key \u003e0: 剩余过期时间 -1: 没有设置过期时间 -2: 键不存在 key的数据结构类型 TYPE key ","date":"0001-01-01","objectID":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/02/:1:1","tags":null,"title":"","uri":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/02/"},{"categories":null,"content":"2、数据结构与内部编码 每种数据结构都有自己底层的内部编码实现，通过命令 OBJECT ENCODING key 来查看。 string 内部编码： raw、int、embstr hash 内部编码： ziplist、hashtable list 内部编码： ziplist、linkedlist、quicklist set 内部编码： intset、hashtable zset 内部编码： ziplist、skiplist ","date":"0001-01-01","objectID":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/02/:1:2","tags":null,"title":"","uri":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/02/"},{"categories":null,"content":"2、string ","date":"0001-01-01","objectID":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/02/:2:0","tags":null,"title":"","uri":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/02/"},{"categories":null,"content":"1、常用命令 设置值、获取值 SET key value [expiration EX seconds|PX milliseconds] [NX|XX] GET key 说明： NX：不存在 key，才设置成功。同命令 SETNX key value XX：存在 key，才设置成功。 批量设置、批量获取 MSET key value [key value ...] MGET key [key ...] 说明： 批量操作可以减少网络时间。 计数 INCR key INCRBY key increment INCRBYFLOAT key increment DECR key DECRBY key decrement ","date":"0001-01-01","objectID":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/02/:2:1","tags":null,"title":"","uri":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/02/"},{"categories":null,"content":"2、不常用的命令 追加值 APPEND key value 字符串长度 STRLEN key 设置并返回原值 GETSET key value 设置指定位置的字符 SETRANGE key offset value 获取部分字符串 GETRANGE key start end ","date":"0001-01-01","objectID":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/02/:2:2","tags":null,"title":"","uri":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/02/"},{"categories":null,"content":"3、内部编码 字符串内部编码有三种： int：8 个字节的长整型 embstr：小于等于 39 个字节的字符串 raw：大于 39 个字节的字符串 ","date":"0001-01-01","objectID":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/02/:2:3","tags":null,"title":"","uri":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/02/"},{"categories":null,"content":"4、使用场景 缓存 （网站请求数据缓存） 计数 （网站的浏览数和播放数） 共享 Session （用户登录信息） 限速 （验证码接口） ","date":"0001-01-01","objectID":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/02/:2:4","tags":null,"title":"","uri":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/02/"},{"categories":null,"content":"3、hash ","date":"0001-01-01","objectID":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/02/:3:0","tags":null,"title":"","uri":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/02/"},{"categories":null,"content":"1、命令 设置值、获取值 HSET key field value HGET key field 删除 field HDEL key field [field ...] 计算 field 的个数 HLEN key 批量设置、批量获取 HMSET key field value [field value ...] HMGET key field [field ...] 是否存在 field HEXISTS key field 获取所有的 field HKEYS key 获取所有的 value HVALS key 获取所有的 field-value HGETALL key 说明： field 个数比较多时，会阻塞 redis。 计数 HINCRBY key field increment HINCRBYFLOAT key field increment 获取 value 长度 HSTRLEN key field ","date":"0001-01-01","objectID":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/02/:3:1","tags":null,"title":"","uri":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/02/"},{"categories":null,"content":"2、内部编码 ziplist（压缩表）：元素个数小于 hash-max-ziplist-entries = 512 ，同时 value 小于 hash-max-ziplist-value = 64，就使用 ziplist， 配置参数在 redis.conf 中。 hashtable（哈希表）：无法满足 ziplist 的条件，会使用 hashtable。 ","date":"0001-01-01","objectID":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/02/:3:2","tags":null,"title":"","uri":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/02/"},{"categories":null,"content":"4、list ","date":"0001-01-01","objectID":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/02/:4:0","tags":null,"title":"","uri":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/02/"},{"categories":null,"content":"1、命令 添加 RPUSH key value [value ...] # 右边添加 LPUSH key value [value ...] # 左边添加 LINSERT key BEFORE|AFTER pivot value # 指定位置插入 查询 LRANGE key start stop # 范围为[start, stop], 查询所有是 start = 0, stop = -1 LLEN key # 列表长度 删除 LPOP key # 左边弹出 RPOP key # 右边弹出 LREM key count value ## 删除 count 个 value 值, count \u003e 0,从左边删除；count \u003c 0,从右边删除；count = 0, 删除所有 LTRIM key start stop # 只保留[start, stop]的元素 修改 LSET key index value # 设置指定索引的值 阻塞 BLPOP key [key ...] timeout # 从左边弹出元素，如果为空，则阻塞 BRPOP key [key ...] timeout # 从右边弹出元素，如果为空，则阻塞 # timeout：阻塞时间。多个 key, 从左扫描。 ","date":"0001-01-01","objectID":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/02/:4:1","tags":null,"title":"","uri":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/02/"},{"categories":null,"content":"2、内部编码 ziplist（压缩表）：元素个数小于 list-max-ziplist-entries = 512 ，同时 value 小于 list-max-ziplist-value = 64，就使用 ziplist， 配置参数在 redis.conf 中。 linkedlist（链表）：无法满足 ziplist 的条件，会使用 linkedlist。 ","date":"0001-01-01","objectID":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/02/:4:2","tags":null,"title":"","uri":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/02/"},{"categories":null,"content":"5、set ","date":"0001-01-01","objectID":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/02/:5:0","tags":null,"title":"","uri":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/02/"},{"categories":null,"content":"1、集合内操作 添加 SADD key member [member ...] 删除 SREM key member [member ...] 计算元素个数 SCARD key 是否在集合中 SISMEMBER key member # 随机返回 count 个元素，不会删除元素 SRANDMEMBER key [count] 随机弹出 count 个元素，会删除元素 SPOP key [count] 获取所有元素 SMEMBERS key ","date":"0001-01-01","objectID":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/02/:5:1","tags":null,"title":"","uri":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/02/"},{"categories":null,"content":"2、集合间操作 多个集合的交集 SINTER key [key ...] 多个集合的并集 SUNION key [key ...] 多个集合的差集 SDIFF key [key ...] 将交集、并集、差集的结果保存 SINTERSTORE destination key [key ...] SUNIONSTORE destination key [key ...] SDIFFSTORE destination key [key ...] 说明： destination 表示目标 key。 key 表示需要操作的 key。 ","date":"0001-01-01","objectID":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/02/:5:2","tags":null,"title":"","uri":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/02/"},{"categories":null,"content":"3、内部编码 intset（整数集合）：value 值为整型，个数小于 set-max-intset-entries = 512 时，使用 intset。配置参数在 redis.conf 中。 hashtable（哈希表）：不满足 intset 条件时，使用 hashtable。 ","date":"0001-01-01","objectID":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/02/:5:3","tags":null,"title":"","uri":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/02/"},{"categories":null,"content":"4、使用场景 标签系统：计算不同人相同喜好的标签 (SINTER命令)。 SADD user1:tags tag1 tag2 SADD user2:tags tag2 tag3 SINTER user1:tags user2:tags ","date":"0001-01-01","objectID":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/02/:5:4","tags":null,"title":"","uri":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/02/"},{"categories":null,"content":"6、zset ","date":"0001-01-01","objectID":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/02/:6:0","tags":null,"title":"","uri":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/02/"},{"categories":null,"content":"1、集合内操作 添加 ZADD key [NX|XX] [CH] [INCR] score member [score member ...] # NX: 不存在 key，才设置成功 # XX: 存在 可以，才设置成功 计算成员个数 ZCARD key 获取某个成员的分数 ZSCORE key member 获取某个成员的排名 ZRANK key member # 从低到高 ZREVRANK key member # 从高到低 删除 ZREM key member [member ...] # 删除成员 ZREMRANGEBYRANK key start stop # 删除指定排名范围的成员 ZREMRANGEBYSCORE key min max # 删除指定分数范围的成员 增加成员的分数 ZINCRBY key increment member 获取指定排名范围的成员 ZRANGE key start stop [WITHSCORES] # 从低到高 ZREVRANGE key start stop [WITHSCORES] # 从高到底 获取指定分数范围的成员 ZRANGEBYSCORE key min max [WITHSCORES] [LIMIT offset count] # WITHSCORES：结果返回分数 # LIMIT offset count：限制返回个数 获取指定分数范围的成员个数 ZCOUNT key min max ","date":"0001-01-01","objectID":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/02/:6:1","tags":null,"title":"","uri":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/02/"},{"categories":null,"content":"2、集合间操作 交集 ZINTERSTORE destination numkeys key [key ...] [WEIGHTS weight] [AGGREGATE SUM|MIN|MAX] # destination：计算结果保存的键 # numkeys：参与的键，也就是 key 的总数 # weight：每一个 key 参与的权重，默认为 1 # AGGREGATE：聚合操作，默认为 sum 并集 ZUNIONSTORE destination numkeys key [key ...] [WEIGHTS weight] [AGGREGATE SUM|MIN|MAX] # 参数同 ZINTERSTORE ","date":"0001-01-01","objectID":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/02/:6:2","tags":null,"title":"","uri":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/02/"},{"categories":null,"content":"3、内部编码 ziplist（压缩列表）：个数小于 zset-max-ziplist-entries = 128 时，value 小于 zet-max-ziplist-value = 64 使用 ziplist。配置参数在 redis.conf 中。 skiplist（跳跃表）：不满足 ziplist 条件时，使用 skiplist。 ","date":"0001-01-01","objectID":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/02/:6:3","tags":null,"title":"","uri":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/02/"},{"categories":null,"content":"6、键管理 ","date":"0001-01-01","objectID":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/02/:7:0","tags":null,"title":"","uri":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/02/"},{"categories":null,"content":"1、单个键管理 键重命名 RENAME key newkey # 存在 key，会覆盖 RENAMENX key newkey # 不存在 key，才重命名成功 随机返回一个键 RANDOMKEY 键过期 EXPIRE key seconds # \u003e0: 剩余过期时间 # -1: 没有设置过期时间； # -2: 键不存在 Redis 不支持二级数据结构（哈希表、列表）过期 setex 原子命令设置 value 和 expire 迁移键 MOVE key db # 迁移到另一个db, 不建议使用，因为集群环境只能使用一个数据库 DUMP key; RESTORE key ttl serialized-value [REPLACE] # 操作麻烦，不建议使用 MIGRATE host port key| destination-db timeout [COPY] [REPLACE] [KEYS key] # 可以使用 # COPY: 迁移后不会删除源键 # REPLACE: 迁移后会覆盖目标库的键 示例： 迁移到 localhost:6380 的 db0 库上，timeout为 1000ms，命令为 MIGRATE localhost 6380 hello 0 1000。 迁移多个键 k1, k2, k3，命令为 MIGRATE localhost 6380 \"\" 0 1000 KEYS k1 k2 k3。 ","date":"0001-01-01","objectID":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/02/:7:1","tags":null,"title":"","uri":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/02/"},{"categories":null,"content":"2、遍历键 全量遍历键 KEYS pattern # 键很多时，会阻塞 Redis 渐进式遍历建 SCAN cursor [MATCH pattern] [COUNT count] # count: 每次查询 key 的个数。 # pattern: 同命令 scan。 说明： 第一次查询设置 cursor 为 0，结果会返回 cursor，如果 cursor 为 0，表示遍历结束，否则设置 cursor 为当前返回值，再次查询。 ","date":"0001-01-01","objectID":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/02/:7:2","tags":null,"title":"","uri":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/02/"},{"categories":null,"content":"3、数据库管理 无，因为集群模式下，只能使用一个数据库，生产环境也是如此。 ","date":"0001-01-01","objectID":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/02/:7:3","tags":null,"title":"","uri":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/02/"},{"categories":null,"content":"1、慢查询 ","date":"0001-01-01","objectID":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/03/:1:0","tags":null,"title":"","uri":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/03/"},{"categories":null,"content":"1、慢查询配置 slowlog-log-slower-than: 10000 (默认值，单位微秒)，超过 10 毫秒的语句就会被记录下来。 slowlog-max-len: 128（默认值），Redis 内部使用列表来保存慢查询日志。 lowlog-log-slower-than = 0, 会记录所有命令。 lowlog-log-slower-than \u003c 0, 不会记录任何命令。 配置方式： 修改配置文件 redis.conf。 动态修改 config set lowlog-log-slower-than 20000 # 设置慢查询时间 config set slowlog-max-len 1000 # 设置慢查询日志大小 config rewrite # 持久化到配置文件 慢查询命令 SLOWLOG GET 10 # 获取最近 10 条日志 SLOWLOG LEN # 获取日志条数 SLOWLOG RESET # 慢查询日志重置 ","date":"0001-01-01","objectID":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/03/:1:1","tags":null,"title":"","uri":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/03/"},{"categories":null,"content":"2、最佳实践 参数 slowlog-max-len，建议调大日志列表，比如 1000以上；参数 slowlog-log-slower-than，默认超过 10ms 就判断为慢查询，如果每条命令执行时间在 1ms 以上，则 1s 的并发量不足 1000，所以对于高 OPS 场景设置为 1ms。 慢查询只记录命令执行时间，不包括命令排队和网络传输时间。 慢查询日志只是一个先进先出的队列，如果查询较多，可能会丢失日志数据，可以利用 SLOWLOG GET 命令将日志存入 mysql 中，也可以利用开源工具 CacheCloud。 ","date":"0001-01-01","objectID":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/03/:1:2","tags":null,"title":"","uri":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/03/"},{"categories":null,"content":"2、Redis shell ","date":"0001-01-01","objectID":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/03/:2:0","tags":null,"title":"","uri":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/03/"},{"categories":null,"content":"1、redis-cli 命令 -x 参数 echo \"world\" | redis-cli -x set hello # 设置key为 hello， value为 world -c 参数 集群参数，防止 moved 和 asked 异常。 –rdb 参数 请求 Redis 实例生成 RDB 文件，保存在本地。 –bigkeys 参数 选出大 key，这些 key 可能是系统瓶颈。 –eval 参数 运行 lua 脚本。 latency 参数 –latency: 客户端与主机延迟 。 –latency-history: 分时段展示延迟，用 -i 参数来指定，默认为 15s。 –latency-dist: 统计图表形式展示延迟。 –stat 参数 实时获取 Redis 重要统计信息，信息比 info 命令少。 ","date":"0001-01-01","objectID":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/03/:2:1","tags":null,"title":"","uri":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/03/"},{"categories":null,"content":"2、redis-server 命令 –test-memory 参数 redis-server --test-memory 1024 # 检测是否可以给 Redis 分配 1G 内存 ","date":"0001-01-01","objectID":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/03/:2:2","tags":null,"title":"","uri":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/03/"},{"categories":null,"content":"3、redis-benchmark 命令 用来做基准性能测试。 redis-benchmark -c 100 -n 20000 -q -r 10000 -t get,set --csv # -c 客户端并发数 # -n 客户端请求总数 # -q 仅仅显示 requests per second 信息 # -r 随机键的范围（0-9999），不是个数 # -t 指定命令 # --csv 结果按照 csv 格式输出 ","date":"0001-01-01","objectID":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/03/:2:3","tags":null,"title":"","uri":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/03/"},{"categories":null,"content":"3、Pipeline ","date":"0001-01-01","objectID":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/03/:3:0","tags":null,"title":"","uri":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/03/"},{"categories":null,"content":"1、pipeline 概念 redis 执行一条命令可以分为四个过程： 发送命令 命令排队 执行命令 返回结果 其中 1. 和 4. 称为 RTT (往返时间)。 pipeline 可以将一组 redis 命令通过一次 RTT 发给 Redis，再按照执行结果返回给客户端。 redis-cli 脚本的 –pipe 选项就是使用 pipeline 机制。 ","date":"0001-01-01","objectID":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/03/:3:1","tags":null,"title":"","uri":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/03/"},{"categories":null,"content":"2、性能测试 pipeline 执行速度一般比逐条执行快，客户端与服务端网路延时越大，效果越明显。 ","date":"0001-01-01","objectID":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/03/:3:2","tags":null,"title":"","uri":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/03/"},{"categories":null,"content":"3、原生批量和 pipeline 原生批量命令是原子的，pipeline 不是原子的（中间可以执行其他命令）。 原生批量命令是一个命令对应多个 key, pipeline 支持多个命令。 原生批零命令是 Redis 服务端实现的，pipeline 是客户端和服务端共同实现的。 ","date":"0001-01-01","objectID":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/03/:3:3","tags":null,"title":"","uri":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/03/"},{"categories":null,"content":"4、最佳实践 pipeline 封装的数据不能过多，即大数据可以拆分为批量的小 pipeline 命令。 pipeline 只能操作一个 Redis 实例。 ","date":"0001-01-01","objectID":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/03/:3:4","tags":null,"title":"","uri":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/03/"},{"categories":null,"content":"4、事务与Lua 为了保证多个命令组合的原子性，Redis 提供了简单事务功能和 lua 脚本。 ","date":"0001-01-01","objectID":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/03/:4:0","tags":null,"title":"","uri":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/03/"},{"categories":null,"content":"1、事务 MULTI # 开启事务 set a 1 # 执行命令，实际上把命令放到队列中 set b 1 # 执行命令，实际上把命令放到队列中 EXEC # 真正的执行命令 命令错误，会导致事务执行失败，比如 set a 1 写成了 sett a 1。 运行时错误，redis 不支持回滚，比如 sadd a 1 写成了 zadd a 1 b，假设 a 这个 key 已经存在，就会抛出错误。 事务简单主要原因就是，redis 不支持回滚。 ","date":"0001-01-01","objectID":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/03/:4:1","tags":null,"title":"","uri":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/03/"},{"categories":null,"content":"2、Lua 在 Redis 中使用 Lua，有两种方式 eval 和 evalsha。 eval EVAL script numkeys key [key ...] arg [arg ...] eval 'return \"hello \" .. KEYS[1] .. ARGV[1]' 1 world redis # 例子 # 输出 \"hello worldredis\" 如果 Lua 脚本较长，可以使用 redis-cli –eval 选项来执行。 evalsha 使用 eval 命令，每次都需要将脚本发送到服务端，使用 evalsha 命令就避免了开销。 redis-cli script load hello.lua # 加载 lua 脚本到服务端，会返回 sha1 值。 EVALSHA sha1 numkeys key [key ...] arg [arg ...] # 执行 lua 脚本，参数 sha1 就是返回的 sha1 值，其他参数同 eval 命令。 lua 中使用 redis API redis.call(\"set\", \"a\" , 1) redis.call(\"get\", \"a\" ) 也可以使用 redis.pcall 命令，两者差别在于 pcall 命令会忽略错误继续执行，call 遇到错误停止。 lua 脚本执行是原子性的，中间不会插入别的命令。 管理 lua 脚本命令 SCRIPT LOAD [script] # 加载 lua 脚本，返回 sha1 值 SCRIPT EXISTS sha1 [sha1] # 是否存在 sha1 的脚本 SCRIPT FLUSH # 清空 lua 脚本 SCRIPT KILL # 杀掉 lua 脚本 ","date":"0001-01-01","objectID":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/03/:4:2","tags":null,"title":"","uri":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/03/"},{"categories":null,"content":"5、Bitmaps ","date":"0001-01-01","objectID":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/03/:5:0","tags":null,"title":"","uri":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/03/"},{"categories":null,"content":"1、数据结构模型 Bitmaps 不是一种数据结构，实际上它是字符串，但它可以对字符串的位进行操作，你可以想象一个以位为单位的数组，每个单元只能存储 0 和1。 ","date":"0001-01-01","objectID":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/03/:5:1","tags":null,"title":"","uri":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/03/"},{"categories":null,"content":"2、命令 设置值 SETBIT key offset value # offset 从 0 开始 获取值 GETBIT key offset # 结果只有 0 或者 1 BITCOUNT key [start end] # 对[start, end]范围获取值为 1 的个数 Bitmaps 间的运算 BITOP operation destkey key [key ...] # operation 可以是 and(交集)、or(并集)、not(非)、xor(异或) 获取第一个为 bit 的 offset 值 BITPOS key bit [start] [end] # [start,end]范围中第一个出现 bit 的 offset ","date":"0001-01-01","objectID":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/03/:5:2","tags":null,"title":"","uri":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/03/"},{"categories":null,"content":"3、分析 利用 Bitmaps 来统计网站访问用户： SETBIT users:2020-03-22 1 1 # 2020-03-22 这一天 1 号访问了。 SETBIT users:2020-03-23 2 1 # 2020-03-23 这一天 2 号访问了。 BITCOUNT users:2020-03-23 # 2020-03-23 这一天 访问用户量 BITOP and users:2020-03-22_23 users:2020-03-23 users:2020-03-22 # 两天都访问的用户量 set 和 bitmaps 对比： 数据类型 每个用户 id 占用空间 需要存储用户量 全部内存量 set 64 位 5 千万 64 位 * 5 千万 = 400 MB bitmaps 1 位 1 亿 1 位 * 1 亿 = 12.5 MB 从表格可以看出 bitmaps 节省内存。 但如果每天的活跃用户很少，set 可能比 bitmaps 好，因为 set 需要内存 64 位 * 10 万 = 800 KB，而 bitmap 还是需要 12.5 MB 内存。 ","date":"0001-01-01","objectID":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/03/:5:3","tags":null,"title":"","uri":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/03/"},{"categories":null,"content":"6、HyperLogLog HyperLoglog 不是一种新的数据结构，而是一种基数算法，可以利用极小的内存空间完成独立总数统计，数据集可以 ID、Email、IP。 命令 PFADD key element [element ...] # 添加元素 PFCOUNT key [key ...] # 计数 PFMERGE destkey sourcekey [sourcekey ...] # merge 注意： 只是计算独立总数，不需要获取单条数据 HyperLogLog 有误差 ","date":"0001-01-01","objectID":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/03/:6:0","tags":null,"title":"","uri":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/03/"},{"categories":null,"content":"7、发布订阅 Redis 提供发布/订阅模式的消息机制。 命令 PUBLISH channel message # 向指定的 channel 发布消息 SUBSCRIBE channel [channel ...] # 向指定的 channel 订阅消息 PSUBSCRIBE pattern [pattern ...] # 模式订阅消息 UNSUBSCRIBE [channel [channel ...]] # 取消订阅 PUNSUBSCRIBE [pattern [pattern ...]] # 模式取消订阅 PUBSUB subcommand [argument [argument ...]] # 查看订阅 # PUBSUB channels [pattern] # 频道 # PUBSUB numsub [channel ...] # channel 订阅数 # PUBSUB numpat # 模式订阅数 注意： Redis 提供的消息机制，无法实现消息堆积、回溯 消息队列的优点：异步、解耦、削峰，缺点：复杂度提高。 ","date":"0001-01-01","objectID":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/03/:7:0","tags":null,"title":"","uri":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/03/"},{"categories":null,"content":"8、GEO Redis 提供了 GEO（地址位置）功能，支持存储地理位置信息。 添加位置信息 GEOADD key longitude latitude member [longitude latitude member ...] 获取位置信息 GEOPOS key member [member ...] 获取两个地理位置的距离 GEODIST key member1 member2 [unit] # unit: m(米)；（km）公里；（mi）英里；（fl）尺 获取指定范围内的地理位置集合 GEORADIUS key longitude latitude radius m|km|ft|mi [WITHCOORD] [WITHDIST] [WITHHASH] [COUNT count] [ASC|DESC] [STORE key] [STOREDIST key] # 根据具体的经纬度来获取 GEORADIUSBYMEMBER key member radius m|km|ft|mi [WITHCOORD] [WITHDIST] [WITHHASH] [COUNT count] [ASC|DESC] [STORE key] [STOREDIST key] # 根据某一个成员来获取 获取 geohash GEOHASH key member [member ...] # Redis 将二维的经纬度转化为一维字符串 删除地理位置 ZREM key member [member ...] # Redis 没有提供专门的删除命令，可以借助 ZREM 命令来删除 # GEO 的数据类型为 zset ","date":"0001-01-01","objectID":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/03/:8:0","tags":null,"title":"","uri":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/03/"},{"categories":null,"content":"1、客户端通信协议 Redis 制定了 RESP（redis序列化协议）实现客户端和服务端的正常交互。 ","date":"0001-01-01","objectID":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/04/:1:0","tags":null,"title":"","uri":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/04/"},{"categories":null,"content":"1、发送命令格式 CRLF 为 ‘\\r\\n’ *\u003c参数数量\u003e CRLF $\u003c参数 1 的字节数量\u003e CRLF \u003c参数 1\u003e CRLF ... $\u003c参数 N 的字节数量\u003e CRLF \u003c参数 N\u003e CRLF 以 set hello world 命令为例： *3 $3 set $5 hello $5 world ","date":"0001-01-01","objectID":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/04/:1:1","tags":null,"title":"","uri":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/04/"},{"categories":null,"content":"2、返回结果格式 状态回复，第一个字节为 “+\"。如 set 错误回复，第一个字节为 “-\"。如 错误命令 整数回复，第一个字节为 “:\"。如 incr 字符串回复，第一个字节为 “$\"。如 get 多条字符串回复，第一个字节为 “*\"。如 mget ","date":"0001-01-01","objectID":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/04/:1:2","tags":null,"title":"","uri":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/04/"},{"categories":null,"content":"2、Java 客户端 Jedis jedis 用的很少了，请参考 lettuce、redisson ","date":"0001-01-01","objectID":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/04/:2:0","tags":null,"title":"","uri":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/04/"},{"categories":null,"content":"3、Python 客户端 redis-py 略 ","date":"0001-01-01","objectID":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/04/:3:0","tags":null,"title":"","uri":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/04/"},{"categories":null,"content":"4、客户端管理 ","date":"0001-01-01","objectID":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/04/:4:0","tags":null,"title":"","uri":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/04/"},{"categories":null,"content":"1、客户端 API 1、client list 与 Redis 服务端相连的所有客户端连接信息 说明： id: 客户端连接唯一标识，递增，但 Redis 重启后重置为 0。 addr: 客户端 IP 和 PORT。 fd: socket 文件描述符，与 lsof 命令中 fd 是同一个。 name: 客户端名字，与 client setName 和 client getName 有关 Redis 为每个客户端分配了输入缓冲区，它的作用将客户端发送的命令临时保存，Redis 会从输入缓冲区中拉取命令并执行。不受 maxmemory 参数影响。 qbuf: 客户端的输入缓冲区总容量 qbuf-free: 客户端的输入缓冲区剩余容量 输入缓冲区过大的原因： Redis 处理速度跟不上输入缓冲区的输入速度，可能存在 bigKey。 Redis 发生了阻塞。 Redis 为每个客户端分配了输出缓冲区，它的作用是保存命令执行的结果返回给客户端。通过配置文件中的 client-output-buffer-limit \u003cclass\u003e \u003chard limit\u003e \u003csoft limit\u003e \u003csoft seconds\u003e 来配置。不受 maxmemory 参数影响。 obl: 固定输出缓冲区大小 oll: 动态输出缓冲区大小，当固定缓冲区满了，就会使用动态缓冲区 omem: 输出缓冲区总计的字节数 其他信息： age: 已连接的时间 idle: 最近一次空闲时间 flag: S 表示 slave 客户端，N 表示普通客户端，O 表示执行 monitor 命令的客户端 db: 数据库索引下标 sub/psub: 当前客户端订阅的频道 multi: 当前事务已执行命令个数 客户端限制 maxclients (默认为 1000) 和 timeout，通过 config set maxclients 10000 命令和 config set timeout 30 命令来设置。 监控缓冲区方法： 定期执行 client list 命令，收集 qbuf 和 qbuf-free。 执行 info clients 命令，找到最大的输入缓冲区 client_recent_max_input_buffer 2、client getName / setName 给当前客户端设置名字 3、client kill 杀掉指定 ip 和 port 的客户端 client kill ip:port 4、client pause 阻塞客户端 timeout 毫秒 client pause timeout(毫秒) 5、monitor 监控 Redis 正在执行的命令，如果并发量过大，会造成输出缓冲区暴涨。 monitor ","date":"0001-01-01","objectID":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/04/:4:1","tags":null,"title":"","uri":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/04/"},{"categories":null,"content":"2、客户端相关配置 客户端的配置如下： timeout： 空闲连接超时时间 tcp-keepalive： 检查死的连接 tcp-backlog: TCP 连接过后，会将接受的连接放入队列中，tcp-backlog 就是这个队列的大小。 ","date":"0001-01-01","objectID":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/04/:4:2","tags":null,"title":"","uri":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/04/"},{"categories":null,"content":"3、客户端统计信息 1、info clients 运行命令如下： 2、info stats 运行命令如下： 客户端相关的指标 total_connections_received： 总共接受的连接数 rejected_connections： 拒绝的连接数 ","date":"0001-01-01","objectID":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/04/:4:3","tags":null,"title":"","uri":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/04/"},{"categories":null,"content":"5、客户端常见异常 ","date":"0001-01-01","objectID":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/04/:5:0","tags":null,"title":"","uri":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/04/"},{"categories":null,"content":"1、无法从连接池中获取连接 可能的原因： 连接池设置过小。 没有正确使用连接池，用过后没有释放。 具体还是要看选用的客户端，没有连接了是怎么处理的？（是等待还是直接拒接抛出异常） ","date":"0001-01-01","objectID":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/04/:5:1","tags":null,"title":"","uri":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/04/"},{"categories":null,"content":"2、客户端读写超时 可能的原因： 读写超时时间设置短。 命令本身就很慢。 网络不正常。 Redis 阻塞。 ","date":"0001-01-01","objectID":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/04/:5:2","tags":null,"title":"","uri":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/04/"},{"categories":null,"content":"3、客户端连接超时 可能的原因： 连接超时时间设置短。 网络不正常。 Redis 发生阻塞，导致 tcp-backlog 已满。 ","date":"0001-01-01","objectID":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/04/:5:3","tags":null,"title":"","uri":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/04/"},{"categories":null,"content":"4、客户端缓冲区异常 可能的原因： 输出缓冲区满，比如用 get 命令来获取一个bigKey。 长时间空闲连接被服务端主动断开。 不正常的并发读写，Redis 对象被多个线程并发操作。 ","date":"0001-01-01","objectID":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/04/:5:4","tags":null,"title":"","uri":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/04/"},{"categories":null,"content":"5、Lua 脚本执行 可能的原因： lua 脚本执行时间超过参数 lua-time-limit。 ","date":"0001-01-01","objectID":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/04/:5:5","tags":null,"title":"","uri":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/04/"},{"categories":null,"content":"6、客户端连接数过大 客户端连接数超过 maxclients，新的连接就会被拒绝。 从两个方面来解决： 客户端：通过下线部分应用节点，使 Redis 的连接数降下来，从而继续找其根本原因，或者调整 maxclients 参数。 服务端：如果 Redis 是高可用模式，可以把当前的节点故障转移。 ","date":"0001-01-01","objectID":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/04/:5:6","tags":null,"title":"","uri":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/04/"},{"categories":null,"content":"6、客户端案例分析 ","date":"0001-01-01","objectID":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/04/:6:0","tags":null,"title":"","uri":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/04/"},{"categories":null,"content":"1、Redis 内存陡增 现象： 服务端：Redis 主节点内存陡增，从节点内存无变化。 客户端：产生 OOM 异常。 可能的原因： 确实有大量的写入，通过执行命令 dbsize 来获取主从节点的键个数。 排查是否由客户端缓冲区应引发的问题，通过执行命令 info clients来查看。 处理方法： 通过命令 redis-cli info list | grep -v \"omemo=0\"， 找到非零的客户端连接，然后 kill 掉。 可能就是运行命令 monitor 造成的，一般都建议在生厂环境中禁用 monitor。 ","date":"0001-01-01","objectID":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/04/:6:1","tags":null,"title":"","uri":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/04/"},{"categories":null,"content":"2、客户端周期性超时 现象： 客户端：客户端周期性超时 服务端：无明显现象，只是一些慢查询。 可能的原因： 网络不正常。 执行命令造成慢查询导致的周期性超时。 处理方法： 运维层面，监控慢查询，一旦超多阈值，就发出报警。 避免不正确使用命令，如 KEYS *、HGETALL key 等。 ","date":"0001-01-01","objectID":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/04/:6:2","tags":null,"title":"","uri":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/04/"},{"categories":null,"content":"1、RDB ","date":"0001-01-01","objectID":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/05/:1:0","tags":null,"title":"","uri":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/05/"},{"categories":null,"content":"1、触发机制 手动触发，分别有 save 和 bgsave 两个命令。 save： 会阻塞当前 Redis 服务器，直到 RDB 过程完成为止，不建议使用。 bgsave： Redis 进程会 fork 出子进程，子进程进行 RDB 持久化，阻塞只会发生在 fork 阶段。 自动触发的场景： save m n 配置，表示在 m 秒中存在 n 次数据改变，才会触发 bgsave。 从节点全量复制过程中，主节点会执行 bgsave 生成 RDB 文件，发送子节点。 默认关闭情况下，如果没有开启 AOF，也会执行 bgsave。 ","date":"0001-01-01","objectID":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/05/:1:1","tags":null,"title":"","uri":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/05/"},{"categories":null,"content":"2、触发流程 bgsave 命令的运行流程如下图： 说明； 执行 bgsave 命令， 判断是否有 AOF/RDB 进程。 执行 info stats 命令，选项 latest_fork_usec 表示最后一次 fork 使用的秒数。 bgsave 命令执行完成后，会出现 Background saving started 提示。 子进程创建 RDB 文件成功后，对原有的文件进行原子替换, 执行 lastsave 命令获取最后一次生成 RDB 文件的时间，对应 info Persistence 命令中的选项 rdb_last_save_time。 ","date":"0001-01-01","objectID":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/05/:1:2","tags":null,"title":"","uri":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/05/"},{"categories":null,"content":"3、RDB 文件的处理 RDB 文件通过配置文件参数 dbfilename 和 dir来配置，也可以通过命令 config set dir {dir} 和 config set dbfilename {dbfilename} 来动态配置。 RDB 文件默认采用 LZF 压缩，通常建议开启，因为主从复制时，需要发送 RDB 文件到从节点，这样可以节省带宽。 RDB 默认也开启校验，可以通过脚本 redis-check-rdb 来校验生成相应的错误报告。 ","date":"0001-01-01","objectID":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/05/:1:3","tags":null,"title":"","uri":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/05/"},{"categories":null,"content":"4、RDB 的优缺点 优点： RDB 非常适合备份、全量复制等场景，比如每 6 小时定时执行 bgsave，可用于灾难恢复。 RDB 的恢复数据远远快于 AOF 方式。 缺点： RDB 无法做到秒级持久化，fork 创建子进程也属于重量级操作。 RDB 用特定的二进制格式保存，可能有版本不兼容问题。 ","date":"0001-01-01","objectID":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/05/:1:4","tags":null,"title":"","uri":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/05/"},{"categories":null,"content":"2、AOF 以独立的日志记录每次写命令，重启时再重新执行 AOF 文件中的命令达到恢复数据的目的。 AOF 解决了数据持久化的实时性。 ","date":"0001-01-01","objectID":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/05/:2:0","tags":null,"title":"","uri":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/05/"},{"categories":null,"content":"1、AOF 工作流程 开启 AOF 需要设置参数 appendonly yes。 通过参数 appendfilename 来设置文件名。 工作流程如下图： 说明： 所有的写入命令会追加到 aof_buf (缓冲区)中。 AOF 缓冲区会根据同步策略（参数默认设置 appendfsync everysec）来做同步操作。 会定期对 AOF 文件进行 rewrite，达到压缩的目的，因为可能有些 key 过期了。 机器重启时，如果开启了 AOF，则使用 AOF 加载数据。 ","date":"0001-01-01","objectID":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/05/:2:1","tags":null,"title":"","uri":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/05/"},{"categories":null,"content":"2、命令写入 AOF 采用文本协议格式，也就是说 AOF 文件中存储就是写入的命令，这样具有阅读性、便于修改。 AOF 把命令先写入 aof_buf 中，根据不同的同步策略可以在性能和安全上做出平衡，没有特殊要求，就设置为 everysec。 三种策略； no: don’t fsync, just let the OS flush the data when it wants. Faster. always: fsync after every write to the append only log. Slow, Safest. everysec: fsync only one time every second. Compromise. ","date":"0001-01-01","objectID":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/05/:2:2","tags":null,"title":"","uri":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/05/"},{"categories":null,"content":"3、重写机制 AOF 文件可以变小的原因： 超时的数据，可以不用再写入文件中。 key 过期了，可能含有无效命令，如 del key1。 多个命令可以合并成一个，如 lpush list a 和 lpush list b 可以合并为 lpush list a b。 触发 AOF 重写方式： 手动执行命令 bgrewriteaof 自动触发，根据配置参数 auto-aof-rewrite-percentage 100 和 auto-aof-rewrite-min-size 64mb。 参数说明： This is how it works: Redis remembers the size of the AOF file after the latest rewrite (if no rewrite has happened since the restart, the size of the AOF at startup is used). This base size is compared to the current size. If the current size is bigger than the specified percentage, the rewrite is triggered. Also you need to specify a minimal size for the AOF file to be rewritten, this is useful to avoid rewriting the AOF file even if the percentage increase is reached but it is still pretty small 自动触发时机: aof_current_size \u003e auto-aof-rewrite-min-size \u0026\u0026 (aof_current_size - aof_base_size) / aof_base_size \u003e auto-aof-rewrite-percentage AOF 重写流程图如下： 说明： 执行 AOF 重写请求，如果有子进程在执行 bgsave 则等待完成之后再操作。 fork 子进程进行重写，父进程接受请求，修改命令写入 aof_buf 中根据策略同步到磁盘。 fork 操作运用写时复制技术，所以子进程只能共享操作 fork 时的内存，这时父进程可能还在响应请求，所以把重写后的新命令放入 aof_rewrite_buf 缓冲区中。 把 aof_rewrite_buf 中数据写入新的 AOF 文件中，根据开启参数 aof-rewrite-incremental-fsync yes，每 32MB 同步到磁盘。 ","date":"0001-01-01","objectID":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/05/:2:3","tags":null,"title":"","uri":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/05/"},{"categories":null,"content":"4、重启加载 重启加载图： ","date":"0001-01-01","objectID":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/05/:2:4","tags":null,"title":"","uri":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/05/"},{"categories":null,"content":"5、文件校验 加载损坏的 AOF 文件会拒绝启动，可以先备份文件，然后再执行命令 redis-check-aof [--fix] \u003cfile.aof\u003e 来进行修复。 ","date":"0001-01-01","objectID":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/05/:2:5","tags":null,"title":"","uri":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/05/"},{"categories":null,"content":"3、问题定位与优化 ","date":"0001-01-01","objectID":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/05/:3:0","tags":null,"title":"","uri":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/05/"},{"categories":null,"content":"1、fork 操作 Redis 做 RDB 或者 AOF 重写时，必不可少的操作就是 fork。fork 用的写时复制技术，会复制父进程的内存页表。 改善 fork操作的耗时： 优先使用物理机或者高效支持 fork 操作的虚拟化技术。 fork 耗时和内存量成正比，单个 Redis 实例建议不超过 10G。 linux 内存分配策略，避免物理内存不足导致 fork 失败。 降低 fork 操作频率，比如避免不必要的全量复制，适当放宽 AOF 自动触发时机。 ","date":"0001-01-01","objectID":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/05/:3:1","tags":null,"title":"","uri":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/05/"},{"categories":null,"content":"2、子进程开销监控和优化 CPU，子进程负责把内存中的数据写入文件中，属于 IO 密集型操作，不要和其他 IO 密集型服务部署在一起。 内存，写时复制技术，避免在大量写入时做子进程重写操作，导致父进程维护大量页副本，造成内存消耗，可以关闭 THP。 磁盘，AOF 重写会消耗大量磁盘 IO，可以关闭，参数设置为 no-appendfsync-on-rewrite yes，默认是关闭的，但是开启后，可能丢失数据。 ","date":"0001-01-01","objectID":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/05/:3:2","tags":null,"title":"","uri":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/05/"},{"categories":null,"content":"3、AOF 追加阻塞 AOF 持久化，常用的同步策略是 everysec，用于平衡性能和安全性，对于这种方式，Redis 使用另一个线程每秒执行 fsync 同步磁盘，当系统磁盘繁忙时，可能造成 Redis 主进程阻塞。 ","date":"0001-01-01","objectID":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/05/:3:3","tags":null,"title":"","uri":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/05/"},{"categories":null,"content":"4、多实例部署 略 ","date":"0001-01-01","objectID":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/05/:4:0","tags":null,"title":"","uri":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/05/"},{"categories":null,"content":"1、配置 ","date":"0001-01-01","objectID":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/06/:1:0","tags":null,"title":"","uri":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/06/"},{"categories":null,"content":"1、建立复制 建立复制前会删除全部数据。 配置复制的方式有三种； 配置文件中加入 slaveof {masterHost} {masterPort}。 redis 启动命令后加入 slaveof {masterHost} {masterPort}。 直接执行命令 slaveof {masterHost} {masterPort}。 ","date":"0001-01-01","objectID":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/06/:1:1","tags":null,"title":"","uri":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/06/"},{"categories":null,"content":"2、断开复制 在从节点执行命令 slaveof no one 来断开复制。所谓切主操作，就是先断开复制，然后再建立复制。 ","date":"0001-01-01","objectID":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/06/:1:2","tags":null,"title":"","uri":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/06/"},{"categories":null,"content":"3、安全性 为了安全性，一般都会在主节点上设置 requirepass 123456，所有客户端访问必须使用 auth 123456 验证。因此从节点开启复制时，也要设置 masterauth 123456。 ","date":"0001-01-01","objectID":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/06/:1:3","tags":null,"title":"","uri":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/06/"},{"categories":null,"content":"4、只读 默认情况下，从节点使用 slave-read-only=yes 配置为只读模式。由于已经开启了复制，建议从节点保持只读模式。 ","date":"0001-01-01","objectID":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/06/:1:4","tags":null,"title":"","uri":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/06/"},{"categories":null,"content":"5、传输延迟 主从节点之间复制数据，肯定会有延迟。 redis 提供了 repl-disable-tcp-nodelay 参数用于关闭 TCP_NODELAY，默认关闭。 当关闭时，主节点的数据无论大小都会发送到从节点，这样就降低了延迟，但增加了带宽。 当开启时，主节点会合并较小的 TCP 数据包，从而节省带宽。 ","date":"0001-01-01","objectID":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/06/:1:5","tags":null,"title":"","uri":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/06/"},{"categories":null,"content":"2、拓扑 主要有三种；一主一从，一主多从，树状主从。 一主一从： 最简单的结构，一般只在 从节点上开启 AOF 操作。 一主多从： 用于读多写少、读写分离的场景 树状主从： 从节点不但可以复制主节点的数据，还可以作为其他的节点的主节点。 ","date":"0001-01-01","objectID":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/06/:2:0","tags":null,"title":"","uri":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/06/"},{"categories":null,"content":"3、原理 ","date":"0001-01-01","objectID":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/06/:3:0","tags":null,"title":"","uri":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/06/"},{"categories":null,"content":"1、复制过程 执行 slaveof {masterHost} {masterPort} 命令后，保存主节点信息。 从节点每秒运行定时任务维护复制逻辑，当发现新的主节点后，建立连接。 发送 ping 命令，主要是检查网络是否可用和是否可以处理命令（可能主节点阻塞了）。 权限验证，requirepass 和 masterauth 是否匹配。 同步数据集，分为全量同步和部分同步。 命令持续复制，新的命令持续发给从节点。 ","date":"0001-01-01","objectID":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/06/:3:1","tags":null,"title":"","uri":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/06/"},{"categories":null,"content":"2、数据同步 同步过程分为全量复制和部分复制。 参与主从复制的节点都会维护自身复制偏移量。命令 info 中 master_repl_offset 和 slave_repl_offset 。 复制积压缓冲区，主节点把命令发送从节点，还会把命令写入复制积压缓冲区，这个用于部分复制和命令丢失的场景。命令 info 中 repl_backlog_*。 主节点运行 ID，用来唯一识别 Redis 节点，当运行 ID 变化了，从节点将做全量复制了。节点重启了，运行 ID 也会变化。可以执行命令 debug reload 来重新加载并保持运行 ID 不变，但是会阻塞当前 Redis。 命令 info 中 run_id。 ","date":"0001-01-01","objectID":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/06/:3:2","tags":null,"title":"","uri":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/06/"},{"categories":null,"content":"3、全量复制 ","date":"0001-01-01","objectID":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/06/:3:3","tags":null,"title":"","uri":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/06/"},{"categories":null,"content":"4、部分复制 ","date":"0001-01-01","objectID":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/06/:3:4","tags":null,"title":"","uri":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/06/"},{"categories":null,"content":"5、心跳 ","date":"0001-01-01","objectID":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/06/:3:5","tags":null,"title":"","uri":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/06/"},{"categories":null,"content":"6、异步复制 ","date":"0001-01-01","objectID":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/06/:3:6","tags":null,"title":"","uri":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/06/"},{"categories":null,"content":"4、开发与运维中的问题 ","date":"0001-01-01","objectID":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/06/:4:0","tags":null,"title":"","uri":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/06/"},{"categories":null,"content":"Redis 开发与运维 第一章 初识 Redis 第二章 API 的理解和使用 第三章 小功能大用处 第四章 客户端 第五章 持久化 第六章 复制 ","date":"0001-01-01","objectID":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/readme/:1:0","tags":null,"title":"","uri":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/readme/"},{"categories":null,"content":"1、上下文切换 CPU 通过时间片分配算法来循环执行任务，当前任务执行完一个时间片后会切换到下一个任务。但是在切换前会保存上一个任务的状态，以便下次切换回这个任务时，再次加载该任务状态。这就是上下文切换。 创建过多的线程，会使上下文切换频繁，执行效率也可能不如单线程。 上图的 cs (context switch) 表示上下文切换次数。 减少上下文切换的方法： 无锁并发编程，多线程处理数据时，可以用一个方法来避免锁。如将数据的 ID 按照 Hash 算法取余分段，不同的线程处理不同段的数据。 CAS 算法，Java 的 Atomic 包。 使用最少线程，避免创建不需要的线程，比如任务很少，创建的线程较多。 协程，单线程实现多任务的调度，并维持多任务状态切换。 减少上下文切换示例 jstack 命令来 dump 线程 jstack 31177 \u003e /home/xxx/dump-31177 统计线程都处于什么状态 grep java.lang.Thread.State dump-31177 | awk '{print $2$3$4$5}' | sort | uniq -c 查看这些 waiting 的线程，根据需要合理配置线程数。 ","date":"0001-01-01","objectID":"/ooooo-notes/old-notes/books/the-art-of-java-concurrent-programming/01/:1:0","tags":null,"title":"","uri":"/ooooo-notes/old-notes/books/the-art-of-java-concurrent-programming/01/"},{"categories":null,"content":"2、死锁 死锁示例： public static void main(String[] args) { Object lockA = new Object(); Object lockB = new Object(); Thread t1 = new Thread(() -\u003e { synchronized (lockA) { System.out.println(\"get lockA\"); timeSleep(2); synchronized (lockB) { System.out.println(\"get lockB\"); } } }); Thread t2 = new Thread(() -\u003e { synchronized (lockB) { System.out.println(\"get lockB\"); synchronized (lockA) { System.out.println(\"get lockA\"); } } }); t1.start(); t2.start(); } 避免死锁的方法： 避免一个线程同时获取多个锁，也就是同时申请所有的资源。 尝试使用定时锁，如 lock.tryLock(timeout) 来替换内部锁。 对于数据库锁，加锁和加锁必须在一个数据库连接里。 ","date":"0001-01-01","objectID":"/ooooo-notes/old-notes/books/the-art-of-java-concurrent-programming/01/:2:0","tags":null,"title":"","uri":"/ooooo-notes/old-notes/books/the-art-of-java-concurrent-programming/01/"},{"categories":null,"content":"3、资源限制的挑战 带宽，比如带宽只有 20M, 一个线程最多只能使用 10M，也就是说线程数最大只能是 2，多余的线程没有资源可以使用。 磁盘 ","date":"0001-01-01","objectID":"/ooooo-notes/old-notes/books/the-art-of-java-concurrent-programming/01/:3:0","tags":null,"title":"","uri":"/ooooo-notes/old-notes/books/the-art-of-java-concurrent-programming/01/"},{"categories":null,"content":"4、总结 强烈建议使用 JDK 并发包提供的并发容器和工具类来解决并发问题。 ","date":"0001-01-01","objectID":"/ooooo-notes/old-notes/books/the-art-of-java-concurrent-programming/01/:4:0","tags":null,"title":"","uri":"/ooooo-notes/old-notes/books/the-art-of-java-concurrent-programming/01/"},{"categories":null,"content":"Java 并发编程的艺术 第一章 并发编程的挑战 第二章 Java 并发机制的底层实现原理 第三章 Java 内存模型 ","date":"0001-01-01","objectID":"/ooooo-notes/old-notes/books/the-art-of-java-concurrent-programming/readme/:1:0","tags":null,"title":"","uri":"/ooooo-notes/old-notes/books/the-art-of-java-concurrent-programming/readme/"},{"categories":null,"content":"Java 并发编程实战 ","date":"0001-01-01","objectID":"/ooooo-notes/old-notes/geektime/java-con-practice/readme/:1:0","tags":null,"title":"","uri":"/ooooo-notes/old-notes/geektime/java-con-practice/readme/"},{"categories":null,"content":"1、Java本身有两个显著的特性 JRE 就是 Java 运行环境， JDK 就是 Java 开发工具包 跨平台运行（一次编写，到处运行） 垃圾回收器（程序员不用手动回收内存，但仍然可能存在内存泄漏） ","date":"0001-01-01","objectID":"/ooooo-notes/old-notes/geektime/java-core-36/01/:1:0","tags":null,"title":"","uri":"/ooooo-notes/old-notes/geektime/java-core-36/01/"},{"categories":null,"content":"2、Java是解析执行？（不太正确） 我们开发的 java 源代码，经过 javac 编译成为字节码，在运行时，通过 JVM 内置的解析器将字节码装换为机器码。 常见的 JVM， 比如 Oracle 的 Hotspot JVM，提供了 JIT（Just-In-Time）动态编译器。 在主流的 Java 版本中，Java 8 采用混合模式-Xmixed进行。 Oracle Hotspot JVM 提供了两种不同的 JIT 编译器，C1 对应 client 模式，适用于启动敏感的应用，C2 对应 server 模式，适用于长时间运行的服务器。默认采用的是分层编译。 JVM 启动时，可以通过指定不同的参数对运行模式选择。 -Xint JVM 只进行解释执行。 -Xcomp JVM 只进行编译执行。 除了上面的编译方式，还有一种新的编译方式（AOT），就是直接把字节码编译为机器码。 利用下面的命令把某个类或者某个模块编译成为AOT库 jaotc --output libHelloWorld.so HelloWorld.class jaotc --output libjava.base.so --module java.base 然后在启动时直接指定 java -XX:AOTLibrary=./libHelloWorld.so,./libjava.base.so HelloWorld ","date":"0001-01-01","objectID":"/ooooo-notes/old-notes/geektime/java-core-36/01/:2:0","tags":null,"title":"","uri":"/ooooo-notes/old-notes/geektime/java-core-36/01/"},{"categories":null,"content":"1、Exception 和 Error Exception 和 Error 都继承 Throwable 类，只有 Throwable 类的实例才可以抛出。 Exception 是可以预料的意外情况，可以被捕获进行相应的处理。而 Error 是不太可能出现的情况，可能会造成程序终止，如 OutOfMemoryError（内存溢出）。 Exception 分为可检查（checked）异常和不检查（unchecked）异常，可检查异常必须显式捕获处理，不检查异常就是运行时异常。如 NullPointerException 。 常见的 Exception NullPointerException （空指针异常） ArrayIndexOutOfBoundsException （数组越界异常） NoSuchFileException （文件没有找到异常） InterruptedException （线程被打断异常） ClassCastException （类型转换异常） 常见的 Error NoClassDefFoundError （类没有被找到错误） OutOfMemoryError （堆内存溢出错误） StackOverflowError （栈内存溢出错误） ","date":"0001-01-01","objectID":"/ooooo-notes/old-notes/geektime/java-core-36/02/:1:0","tags":null,"title":"","uri":"/ooooo-notes/old-notes/geektime/java-core-36/02/"},{"categories":null,"content":"2、try-catch-finally try (BuferedReader br = new BuferedReader(...); BuferedWriter writer = new BuferedWriter(...)) { // do something catch ( IOException | XEception e) { // Multiple catch // Handle it } finally { // do something } 注意 尽量不要捕获 Exception 类型的异常，具体异常具体处理。 不要生吞（swallow）异常，避免错误后出现难以诊断的情况，可以输出到日志中。 Java 的异常处理机制会有额外的开销 try-catch 的代码段会影响 JVM 的优化，尽量只捕获有必要的代码段。 Java 每实例化一个 Exception，就会对当前栈进行快照。 ","date":"0001-01-01","objectID":"/ooooo-notes/old-notes/geektime/java-core-36/02/:2:0","tags":null,"title":"","uri":"/ooooo-notes/old-notes/geektime/java-core-36/02/"},{"categories":null,"content":"1、final final 修饰的类，不能被继承。 final 修饰的变量，不能被修改。 final 修饰的方法，不能被重写。 final 不是 immutable，对象的属性还是可以改变的。 ","date":"0001-01-01","objectID":"/ooooo-notes/old-notes/geektime/java-core-36/03/:1:0","tags":null,"title":"","uri":"/ooooo-notes/old-notes/geektime/java-core-36/03/"},{"categories":null,"content":"2、finally finally 是 Java 保证代码一定会被执行的机制，可以使用 try-catch-finally、try-finally 来关闭数据库连接，unlock()等。 如果是利用 finally 机制来关闭资源，最好是用 try-with-resources。 特例 try { // do something Sysem.exit(1); } finally{ Sysem.out.println(“Print from fnally”); } 上面的 finally 语句不会被执行。 ","date":"0001-01-01","objectID":"/ooooo-notes/old-notes/geektime/java-core-36/03/:2:0","tags":null,"title":"","uri":"/ooooo-notes/old-notes/geektime/java-core-36/03/"},{"categories":null,"content":"3、finalize finalize 方法是 Object 中一个方法，它的设计目的是保证对象在垃圾收集前完成资源的回收，现在已经不推荐使用，在 Java 9 中已被标记为 @Deprecated。 使用 finalize 可能会使程序性能降低，因为 JVM 会做额外处理。 Java 目前使用 Cleaner 来替换 finalize，Cleaner的实现利用了幻象引用（虚引用）和引用队列，比如 mysql-connector-java 就是利用幻象引用来回收资源。 ","date":"0001-01-01","objectID":"/ooooo-notes/old-notes/geektime/java-core-36/03/:3:0","tags":null,"title":"","uri":"/ooooo-notes/old-notes/geektime/java-core-36/03/"},{"categories":null,"content":"1、kafka概念 ​ Apache Kafka是一款开源的消息引擎系统，也是一个分布式流处理平台；消息引擎系统是一组规范，企业利用这组规范在不同系统之间传递语义准确的消息，实现松耦合的异步式数据传输。 ","date":"0001-01-01","objectID":"/ooooo-notes/old-notes/geektime/java-core-36/04/:1:0","tags":null,"title":"","uri":"/ooooo-notes/old-notes/geektime/java-core-36/04/"},{"categories":null,"content":"2、kafka特点 使用纯二进制的字节序列 同时支持两种消息引擎模型（点对点、发布/订阅） ","date":"0001-01-01","objectID":"/ooooo-notes/old-notes/geektime/java-core-36/04/:2:0","tags":null,"title":"","uri":"/ooooo-notes/old-notes/geektime/java-core-36/04/"},{"categories":null,"content":"3、kafka的架构 Topic：主题，承载消息的逻辑容器，用来区分业务； Producer：生产者，向主题发布新消息的应用程序； Consumer：消费者，向主题订阅新消息的应用程序； Partition：分区，每个Topic可以设置多个分区； Replica：副本，同一个消息以提供数据冗余可以有多个副本，分为领导者副本（可对外提供服务）和追随者副本（不可以对外提供服务）；对于分区实现高可用； Consumer Group：消费者组，多个消费者可组成一个消费者组，同时消费多个分区实现高吞吐；同一个消费者组内的消费者不可重复消费同一分区的消息； Rebalance：重平衡，消费者组内的某个消费者挂掉后，会重新分配订阅主题分区； Offset：位移，有分区位移和消费者位移两个概念；分区位移是消息的位置标记（从0开始），是固定的；消费者位移是消费者在订阅消息时的消费进度，是动态的； 图解partition和replication的概念 ","date":"0001-01-01","objectID":"/ooooo-notes/old-notes/geektime/java-core-36/04/:3:0","tags":null,"title":"","uri":"/ooooo-notes/old-notes/geektime/java-core-36/04/"},{"categories":null,"content":"4、多种Kafka对比 Apache Kafka：社区版kafka；迭代速度快，社区响应快，但是仅提供核心功能，缺少高级特性； Confluent Kafka：Confluent公司提供的Kafka；集成了很多高级特性，分免费版和收费版，但是相关资料不全，普及率低； CDH/HDP Kafka：大数据平台内嵌的Apache Kafka，操作简单，节省运维成本，但是把控度低，演进速度慢； ","date":"0001-01-01","objectID":"/ooooo-notes/old-notes/geektime/java-core-36/04/:4:0","tags":null,"title":"","uri":"/ooooo-notes/old-notes/geektime/java-core-36/04/"},{"categories":null,"content":"5、Kafka版本演变 ​ kafka版本命名规范：例kafka_2.11-2.1.1.tgz（2.11表示scala版本，2.1.1表示kafka版本） kafka版本号规范：大版本号 - 小版本号 - Patch 号（修订号） 0.7版本：只提供最基础的消息队列功能； 0.8版本：引入了副本机制， 成为了一个真正意义上完备的分布式高可靠消息队列解决方案；（但是生产和消费的客户端还是老版本的，应指定zk地址而不是broker地址）； 0.8.2.0版本：引入了新版本Producer API（但是bug还有点多，不建议使用）； 0.8.2.2版本：老版本的Consumer API比较稳定了； 0.9版本：增加了基础的安全认证/权限功能，同时使用java重写了Consumer API，还引入了Kafka Connect组件用于实现高性能的数据抽取；另外新版本的Producer API比较稳定了，不建议使用新版本的Consumer API； 0.10版本：引入了Kafka Streams，正式升级为分布式流处理平台； 0.10.2.2版本：新版本的Consumer API比较稳定了，该版本也修复了一个可能导致Producer性能降低的bug； 0.11版本：提供了幂等性Producer API（幂等性就是消息去重，默认不开启）和事务API（实现流处理结果正确性的基石），还对Kafka消息格式做了重构；（该版本也是主流版本）； 0.11.0.3版本：消息引擎功能非常完善了； 1.1版本：实现故障转移（即Failover）； 1.0版本和2.0版本：只要是是对Kafka Streams的改进； ","date":"0001-01-01","objectID":"/ooooo-notes/old-notes/geektime/java-core-36/04/:5:0","tags":null,"title":"","uri":"/ooooo-notes/old-notes/geektime/java-core-36/04/"},{"categories":null,"content":"6、Kafka的核心参数 ","date":"0001-01-01","objectID":"/ooooo-notes/old-notes/geektime/java-core-36/04/:6:0","tags":null,"title":"","uri":"/ooooo-notes/old-notes/geektime/java-core-36/04/"},{"categories":null,"content":"1、配置文件参数 log.dirs：指定broker需要使用的若干个文件目录路径，无默认值；生产环境必须配置，CSV格式（例如：/home/kafka1,/home/kafka2） zookeeper.connect：指定zk的地址和端口（例hadoop01:2181,hadoop02:2181,hadoop03:2181），zk保存了topic、分区的信息等等，如果多个kafka集群共有一个zk集群，加上chroot即可，chroot是别名，则指定格式为hadoop01:2181,hadoop02:2181,hadoop03:2181/kafka1或hadoop01:2181,hadoop02:2181,hadoop03:2181/kafka2; listeners：监听器，指定协议、主机名、端口； advertised.listeners：指该监听器是broker对外发布的； host.name/port：过期参数，可以不指定； auto.create.topics.enable：是否允许自动创建topic； unclean.leader.election.enable：是否允许unclean leader选举，原本数据多的分区才有资格选举leader，该参数设置为true后，数据少的也可以参与选举，会造成数据丢失，建议设置为false； auto.leader.rebalance.enable：是否允许定期选举leader，不建议开启； log.retention.{hour|minutes|ms}：控制一条消息被保存多长时间，ms优先级最高； log.retention.bytes：指定broker为消息保存的总磁盘容量大小，默认值为-1，表示没有上限； message.max.bytes：控制broker能够接收的最大消息大小，默认值为1000012，太小，建议重设置； ","date":"0001-01-01","objectID":"/ooooo-notes/old-notes/geektime/java-core-36/04/:6:1","tags":null,"title":"","uri":"/ooooo-notes/old-notes/geektime/java-core-36/04/"},{"categories":null,"content":"2、Topic级别参数 retention.ms：规定了该topic消息被保存的时长； retention.bytes：规定了要为该topic预留多大的磁盘空间（默认-1，表示没有上限）； max.message.bytes：Broker能够接收的该topic的最大消息大小； 以上参数可以通过两种方式设置 创建topic时进行设置：例bin/kafka-topics.sh –zookeeper hadoop01:2181,hadoop02:2181,hadoop03:2181 –create –topic my-topic –partitions 1 –replication-factor 1 –config max.message.bytes=64000 –config flush.messages=1 修改topic时设置：例bin/kafka-topics.sh –zookeeper hadoop01:2181,hadoop02:2181,hadoop03:2181 –alter –topic my-topic –config max.message.bytes=128000 ","date":"0001-01-01","objectID":"/ooooo-notes/old-notes/geektime/java-core-36/04/:6:2","tags":null,"title":"","uri":"/ooooo-notes/old-notes/geektime/java-core-36/04/"},{"categories":null,"content":"3、JVM参数 KAFKA_HEAP_OPTS：指定堆大小； KAFKA_JVM_PERFORMANCE_OPTS：指定GC参数； 在启动kafka前设置这两个环境变量。 ","date":"0001-01-01","objectID":"/ooooo-notes/old-notes/geektime/java-core-36/04/:6:3","tags":null,"title":"","uri":"/ooooo-notes/old-notes/geektime/java-core-36/04/"},{"categories":null,"content":"4、操作系统参数 ulimit -n：打开文件描述符最大值（例ulimit -n 100000）； 文件系统类型：建议选择XFS； swappniess：swap空间大小，建议设置略大于0的值； 提交时间：即flush落盘时间，kafka的数据会先写到操作系统的页缓存上，然后会根据LRU算法定期将页缓存的数据落盘到磁盘，默认为5秒，可适当增大时间间隔； ","date":"0001-01-01","objectID":"/ooooo-notes/old-notes/geektime/java-core-36/04/:6:4","tags":null,"title":"","uri":"/ooooo-notes/old-notes/geektime/java-core-36/04/"},{"categories":null,"content":"7、分区策略 ","date":"0001-01-01","objectID":"/ooooo-notes/old-notes/geektime/java-core-36/04/:7:0","tags":null,"title":"","uri":"/ooooo-notes/old-notes/geektime/java-core-36/04/"},{"categories":null,"content":"1、生产者分区策略 轮询策略（Round-robin）：kafka生产者API默认的分区策略，最大限度负载均衡； 随机策略（Randomness）：可自定义实现该策略； 按消息键保存策略（Key-ordering）：key可以是业务上的字段信息，按业务场景自定义分区； ","date":"0001-01-01","objectID":"/ooooo-notes/old-notes/geektime/java-core-36/04/:7:1","tags":null,"title":"","uri":"/ooooo-notes/old-notes/geektime/java-core-36/04/"},{"categories":null,"content":"2、消费者分区策略 按范围分配：指定分区消费； 轮询分配：按顺序分配给消费者； 自定义：设置partition.assignment.strategy为自定义的类； ","date":"0001-01-01","objectID":"/ooooo-notes/old-notes/geektime/java-core-36/04/:7:2","tags":null,"title":"","uri":"/ooooo-notes/old-notes/geektime/java-core-36/04/"},{"categories":null,"content":"8、压缩 ​ kafka的消息层次分为消息集合和消息。一个消息集合包含若干条日志项，日志项就是装消息的地方；kafka底层的消息日志由一系列消息集合日志项组成，kafka是在消息集合层面上进行写入操作； ","date":"0001-01-01","objectID":"/ooooo-notes/old-notes/geektime/java-core-36/04/:8:0","tags":null,"title":"","uri":"/ooooo-notes/old-notes/geektime/java-core-36/04/"},{"categories":null,"content":"1、kafka消息格式 ​ kafka有两大消息格式：V1和V2（0.11.0.0版本后引入的）； ​ V1中每条消息需要执行CRC校验，但是在某些情况下CRC值是会变化的，会浪费空间和耽误CPU时间；在保存压缩消息上，是把多条消息进行压缩然后保存到外层消息的消息体字段中。 ​ V2对V1改进了很多，CRC校验工作移到了消息集合这一层，而且在保存压缩消息上，是对整个消息集合进行压缩。 ","date":"0001-01-01","objectID":"/ooooo-notes/old-notes/geektime/java-core-36/04/:8:1","tags":null,"title":"","uri":"/ooooo-notes/old-notes/geektime/java-core-36/04/"},{"categories":null,"content":"2、何时压缩和解压缩 生产者端：生产消息时指定压缩方法。 broker端：默认的压缩方式是producer，如果指定了跟producer不同的压缩方式时，会先解压缩再按新指定的方式压缩；或者broker端发生了消息格式转换也会重压缩。 consumer端：解压缩。 ","date":"0001-01-01","objectID":"/ooooo-notes/old-notes/geektime/java-core-36/04/:8:2","tags":null,"title":"","uri":"/ooooo-notes/old-notes/geektime/java-core-36/04/"},{"categories":null,"content":"3、压缩算法 （kafka2.1.0版本前支持的算法：GZIP、Snappy、LZ4；该版本开始后支持Zstandard算法） 压缩算法的优劣有两个指标：压缩比和压缩/解压缩吞吐量； 压缩比：zstd \u003e LZ4 \u003e GZIP \u003e Snappy 吞吐量：LZ4 \u003e Snappy \u003e zstd / GZIP ","date":"0001-01-01","objectID":"/ooooo-notes/old-notes/geektime/java-core-36/04/:8:3","tags":null,"title":"","uri":"/ooooo-notes/old-notes/geektime/java-core-36/04/"},{"categories":null,"content":"9、无消息丢失配置 kafka只对已提交的消息做有限度的持久化保证。 不要使用producer.send(msg),而要使用producer.send(msg,callback); 设置acks = all；表明所有副本broker都要接收到消息，保证消息”已提交“； 设置retries为一个较大的值； 设置unclean.leader.election.enable = false；表示禁止落后的broker被选为leader。 设置replication.factor \u003e= 3； 设置min.insync.replicas \u003e 1；控制消息至少被写入多少个副本才算“已提交”； 确保replication.factor \u003e min.insync.replicas；推荐replication.factor = min.insync.replicas + 1； 设置enable.auto.commit = false；确保消息消费完成再提交； ","date":"0001-01-01","objectID":"/ooooo-notes/old-notes/geektime/java-core-36/04/:9:0","tags":null,"title":"","uri":"/ooooo-notes/old-notes/geektime/java-core-36/04/"},{"categories":null,"content":"10、幂等性和事务性 ​ 幂等性：指某些操作执行一次或多次的结果是一样的，在kafka中是对重复消息去重。 ​ 引入事务的作用：1、 生产者多次发送消息可以封装成一个原子操作，要么都成功，要么失败；2、 consumer-transform-producer模式下，因为消费者提交偏移量出现问题，导致在重复消费消息时，生产者重复生产消息。需要将这个模式下消费者提交偏移量操作和生产者一系列生成消息的操作封装成一个原子操作。 ​ kafka事务一般为两种：1、 只有Producer生产消息 ；2、生产消费并存（consumer-transform-producer）；3、只有Consumer消费消息。 幂等性Producer：只能保证单分区、单会话上的消息幂等性（设置幂等性：props.put(“enable.idempotence”, true)或props.put(ProducerConfig.ENABLE_IDEMPOTENCE_CONFIG, true)） 事务提供的ACID特性：原子性（Atomicity）、一致性（Consistency）、隔离性（Isolation）、持久性（Durability） 隔离性：表示并发执行的事务彼此相互隔离，互不影响。 事务型Producer：能够保证跨分区、跨会话间的幂等性(设置事务型Producer：开启enable.idempotence = true，然后设置Producer端参数transctional.id,还要调用一些事务API，如下列代码；表示record1和record2要么全部写入成功要么失败。在consumer端要设置isolation.level，read_uncommitted是默认值，表示可以读取到kafka任何消息，不管事务型Producer是提交事务还是终止事务；建议使用read_committed，表示只读取事务型成功提交的消息以及非事务型Producer写入的消息。) producer.initTransactions(); try{ producer.beginTransaction(); producer.send(record1); producer.send(record2); producer.commitTransaction(); }catch(KafkaException e){ producer.abortTransaction(); } ","date":"0001-01-01","objectID":"/ooooo-notes/old-notes/geektime/java-core-36/04/:10:0","tags":null,"title":"","uri":"/ooooo-notes/old-notes/geektime/java-core-36/04/"},{"categories":null,"content":"11、Consumer Group ","date":"0001-01-01","objectID":"/ooooo-notes/old-notes/geektime/java-core-36/04/:11:0","tags":null,"title":"","uri":"/ooooo-notes/old-notes/geektime/java-core-36/04/"},{"categories":null,"content":"1、特性 一个Consumer Group可以有一个或多个Consumer； Droup ID是一个字符串，标识这一个唯一的Consumer Group； 同一个Group中的Consumer不能重复订阅一个分区； （老版本的Consumer Group把消费者位移保存在zk中，由于频繁读写会导致zk集群性能降低，新版本把消费者位移保存在kafka的_consumer_offsets的topic中。） ","date":"0001-01-01","objectID":"/ooooo-notes/old-notes/geektime/java-core-36/04/:11:1","tags":null,"title":"","uri":"/ooooo-notes/old-notes/geektime/java-core-36/04/"},{"categories":null,"content":"2、rebalance的触发条件 Consumer Group中的成员数变更； 订阅的topic数变更（比如订阅了用正则匹配的topic，新增了一个符合的topic）； 订阅的topic的分区数变更； ","date":"0001-01-01","objectID":"/ooooo-notes/old-notes/geektime/java-core-36/04/:11:2","tags":null,"title":"","uri":"/ooooo-notes/old-notes/geektime/java-core-36/04/"},{"categories":null,"content":"3、rebalance的弊端 rebalance影响Consumer端TPS；（rebalance期间，consumer会停止工作） rebalance过程很慢； rebalance效率不高； ","date":"0001-01-01","objectID":"/ooooo-notes/old-notes/geektime/java-core-36/04/:11:3","tags":null,"title":"","uri":"/ooooo-notes/old-notes/geektime/java-core-36/04/"},{"categories":null,"content":"4、非必要rebalance consumer未能及时给coordinator发送心跳，导致consumer被踢出Consumer Group；需设置合理的session.timeout.ms（默认值是10s，表示coordinator在10s内没收到consumer的心跳消息，该consumer被判定为dead）和heartbeat.interval.ms（表示consumer发送心跳请求的频率）的值 （推荐session.timeout.ms=2s，heartbeat.interval.ms=6s） consumer消费时间过长；需设置max.poll.interval.ms的值，表示下游处理数据的时间； ","date":"0001-01-01","objectID":"/ooooo-notes/old-notes/geektime/java-core-36/04/:11:4","tags":null,"title":"","uri":"/ooooo-notes/old-notes/geektime/java-core-36/04/"},{"categories":null,"content":"12、位移主题（_consumer_offsets） ​ _consumer_offsets的主要作用就是保存Kafka消费者的位移消息。消息格式是KV对，Key保存的是\u003cGroup ID,主题名，分区号\u003e；另外还有两种消息格式：1.用于保存Consumer Group信息的消息；2.用于删除Group过期位移甚至是删除Group的消息。 ​ 当有第一个Consumer消费数据时，Kafka就会自动创建_consumer_offsets这个主题，默认有50个分区，3个副本。 ","date":"0001-01-01","objectID":"/ooooo-notes/old-notes/geektime/java-core-36/04/:12:0","tags":null,"title":"","uri":"/ooooo-notes/old-notes/geektime/java-core-36/04/"},{"categories":null,"content":"13、提交位移 ​ 从用户角度，分为自动提交和手动提交；从Consumer角度分为同步提交和异步提交。 org.apache.kafka.common.serialization.StringSerializer 自动提交 设置为自动提交后，调用poll方法时，会提交上次poll返回的所有消息，poll方法的逻辑是先提交上一批消息的位移，再处理下一批消息，可以保证不出现消费丢失的情况，缺点是可能出现重复消费。 Properties props = new Properties(); props.put(\"bootstrap.servers\",\"localhost:9092\"); props.put(\"group.id\",\"test\"); props.put(\"enable.auto.commit\",\"true\"); props.put(\"auto.commit.interval.ms\",\"2000\"); props.put(\"key.deserializer\",\"org.apache.kafka.common.serialization.String*Serializer\"); props.put(\"value.deserializer\",\"org.apache.kafka.common.serialization.String*Serializer\"); KafkaConsumer\u003cString,String\u003e consumer = new KafkaConsumer\u003c\u003e(props); consumer.subscribe(Arrays.asList(\"foo\",\"bar\")); while(true){ ConsumerRecords\u003cString,String\u003e records = consumer.poll(100); for(ConsumerRecord\u003cString,String\u003e record : records){ System.out.printf(\"offset = %d, key = %s, value = %s%n\", record.offset(), record.key(), resord.value()); } } 同步提交commitSync() 手动提交在调用commitSync()时，Consumer会处于阻塞状态，直到Broker端返回结果，影响整个程序的TPS。 while(true){ ConsumerRecords\u003cString,String\u003e records = consumer.poll(Duration.ofSeconds(1)); process(records); //处理消息 try{ consumer.commitSync(); }catch (CommitFailedException e){ handle(e); } } 异步提交commitAsync() 在调用commitAsync()时，会立即返回结果，不会阻塞；缺点是出现问题时不能自动重试。 while(true){ ConsumerRecords\u003cString,String\u003e records = consumer.poll(Duration.ofSeconds(1)); process(records); //处理消息 consumer.commitAsync((offsets,exception) -\u003e { if(exception != null) handle(exception); }); } 同步+异步提交 手动提交中，commitSync()和commitAsync()结合使用会有很好的效果，利用commitSync()的自动重试避免瞬时错误，利用commitAsync()不会阻塞。 try { while (true) { ConsumerRecords\u003cString, String\u003e records = consumer.poll(Duration.ofSeconds(1)); process(records); // 处理消息 commitAysnc(); // 使用异步提交规避阻塞 } } catch (Exception e) { handle(e); // 处理异常 } finally { try { consumer.commitSync(); // 最后一次提交使用同步阻塞式提交 } finally { consumer.close(); } } 同步/异步的细粒度提交 通常poll的数据全部处理完后再提交位移，如果poll的总数很大，而处理过程中出现差错了，下一次会重复消费，就需要设置细粒度的提交位移。 private Map\u003cTopicPartition, OffsetAndMetadata\u003e offsets = new HashMap\u003c\u003e(); int count = 0; …… while (true) { ConsumerRecords\u003cString, String\u003e records = consumer.poll(Duration.ofSeconds(1)); for (ConsumerRecord\u003cString, String\u003e record: records) { process(record); // 处理消息 offsets.put(new TopicPartition(record.topic(), record.partition()), new OffsetAndMetadata(record.offset() + 1)); if（count % 100 == 0） consumer.commitAsync(offsets, null); // 回调处理逻辑是 count++; } } ","date":"0001-01-01","objectID":"/ooooo-notes/old-notes/geektime/java-core-36/04/:13:0","tags":null,"title":"","uri":"/ooooo-notes/old-notes/geektime/java-core-36/04/"},{"categories":null,"content":"14、CommitFailedException ​ CommitFailedException是指Consumer客户端在提交位移时出现了错误或异常，而且不可恢复。 ","date":"0001-01-01","objectID":"/ooooo-notes/old-notes/geektime/java-core-36/04/:14:0","tags":null,"title":"","uri":"/ooooo-notes/old-notes/geektime/java-core-36/04/"},{"categories":null,"content":"1、场景一 ​ 当消息处理的总时间超过预设的 max.poll.interval.ms 参数值时，Kafka Consumer 端会抛出 CommitFailedException 异常。 解决方法如代码： … Properties props = new Properties(); … props.put(\"max.poll.interval.ms\", 5000); consumer.subscribe(Arrays.asList(\"test-topic\")); while (true) { ConsumerRecords\u003cString, String\u003e records = consumer.poll(Duration.ofSeconds(1)); // 使用 Thread.sleep 模拟真实的消息处理逻辑 Thread.sleep(6000L); consumer.commitSync(); } 防止出现该异常的办法： 缩短单条消息处理时间； 增加Comsumer端允许下游系统消费一批消息的最大时长（设置max.poll.interval.ms的值，在0.10.1.0版本后才有该参数）； 减少下游系统一次性消费的消息总数（设置max.poll.records的值）； 下游系统使用多线程加速消费； ","date":"0001-01-01","objectID":"/ooooo-notes/old-notes/geektime/java-core-36/04/:14:1","tags":null,"title":"","uri":"/ooooo-notes/old-notes/geektime/java-core-36/04/"},{"categories":null,"content":"2、场景二 ​ Standalone Consumer在消费时也需要指定groud.id，如果出现了一个相同group.id的Consumer Group，kafka也会抛出异常；这种情况很少见，目前没有解决办法，要尽量去避免。 ","date":"0001-01-01","objectID":"/ooooo-notes/old-notes/geektime/java-core-36/04/:14:2","tags":null,"title":"","uri":"/ooooo-notes/old-notes/geektime/java-core-36/04/"},{"categories":null,"content":"15、多线程 ​ kafka 0.10.1.0版本开始，KafkaConsumer就是双线程设计，即用户主线程和心跳线程；用户主线程就是启动Consumer应用程序main方法的那个程序，心跳线程只负责定期给对应的的Broker机器发送心跳请求，以标识消费者应用的存活性。所以在消费层面上，Consumer依然是单线程设计。 多线程方案： 方案一： ​ 消费者程序启动多个线程，每个线程维护专属的KafkaConsumer实例，负责完整的消息获取和消息处理流程； 优点： 实现起来简单； 线程之间没有交互，省去保障线程安全的开销； 由于每个线程使用专属的Consumer实例执行消息获取和消息处理，可以保证分区内的消费顺序； 缺点： 由于每个线程要维护自己的Consumer实例，会占用很多系统资源； 线程数受限于Consumer订阅主题的总分区数； 如有某个线程处理较慢，会造成rebalance； 实现代码如下： public class KafkaConsumerRunner implements Runnable { private final AtomicBoolean closed = new AtomicBoolean(false); private final KafkaConsumer consumer; public void run() { try { consumer.subscribe(Arrays.asList(\"topic\")); while (!closed.get()) { ConsumerRecords records = consumer.poll(Duration.ofMillis(10000)); // 执行消息处理逻辑 ... } } catch (WakeupException e) { // Ignore exception if closing if (!closed.get()) throw e; } finally { consumer.close(); } } // Shutdown hook which can be called from a separate thread public void shutdown() { closed.set(true); consumer.wakeup(); } } 方案二： ​ 消费者程序使用单或多线程获取消息，同时创建多个消费线程执行消息处理逻辑； 优点： 高伸缩性，自由调节消息获取和消息处理的线程数； 缺点： 实现难度大； 由于消息获取和消息处理解耦，会破坏消息在分区中的顺序； 会使得整个消息消费链路被拉长，位移提交可能会出错，导致重复消费； private final KafkaConsumer\u003cString, String\u003e consumer; private ExecutorService executors; ... private int workerNum = ...; executors = new ThreadPoolExecutor( workerNum, workerNum, 0L, TimeUnit.MILLISECONDS, new ArrayBlockingQueue\u003c\u003e(1000), new ThreadPoolExecutor.CallerRunsPolicy() ); ... while (true) { ConsumerRecords\u003cString, String\u003e records = consumer.poll(Duration.ofSeconds(1)); for (final ConsumerRecord record : records) { executors.submit(new Worker(record)); } } .. ","date":"0001-01-01","objectID":"/ooooo-notes/old-notes/geektime/java-core-36/04/:15:0","tags":null,"title":"","uri":"/ooooo-notes/old-notes/geektime/java-core-36/04/"},{"categories":null,"content":"Java 核心技术 36讲 01、谈谈你对 Java 的理解 02、Exception 和 Error 的区别 03、final、finally、finalize 的区别 ","date":"0001-01-01","objectID":"/ooooo-notes/old-notes/geektime/java-core-36/readme/:1:0","tags":null,"title":"","uri":"/ooooo-notes/old-notes/geektime/java-core-36/readme/"},{"categories":null,"content":"1、Kafka 是一款消息引擎系统 消息引擎系统是一组规范。企业利用这组规范在不同系统之间传递语义准确的消息，实现松耦合的异步式数据传递。 Kafka 的消息编码格式是 二进制的字节序列 Kafka 支持的两种消息模型 点对点模型，系统 A 发送的消息只能被 B 系统消费，其他系统不能读取 A 系统发送的消息，一对一的关系。 发布 / 订阅模型，有 Topic（主题）、Producer（生产者）、Consumer（消费者）的概念，可能会存在多个 Producer 向 Topic 发送消息，也可能存在多个 Consumer 来消费消息，多对多的关系。 消息队列的优点：解耦、异步、削峰 ","date":"0001-01-01","objectID":"/ooooo-notes/old-notes/geektime/kafka-core-tech/01/:1:0","tags":null,"title":"","uri":"/ooooo-notes/old-notes/geektime/kafka-core-tech/01/"},{"categories":null,"content":"1、Kafka 术语 ","date":"0001-01-01","objectID":"/ooooo-notes/old-notes/geektime/kafka-core-tech/02/:1:0","tags":null,"title":"","uri":"/ooooo-notes/old-notes/geektime/kafka-core-tech/02/"},{"categories":null,"content":"Record（消息） Kafka 处理的主要对象。 ","date":"0001-01-01","objectID":"/ooooo-notes/old-notes/geektime/kafka-core-tech/02/:1:1","tags":null,"title":"","uri":"/ooooo-notes/old-notes/geektime/kafka-core-tech/02/"},{"categories":null,"content":"Topic（主题） 主题是承载消息的逻辑容器，在实际使用中多用来区分具体的业务。 你可以为每个业务、每个应用甚至是每类数据都创建专属的主题。 ","date":"0001-01-01","objectID":"/ooooo-notes/old-notes/geektime/kafka-core-tech/02/:1:2","tags":null,"title":"","uri":"/ooooo-notes/old-notes/geektime/kafka-core-tech/02/"},{"categories":null,"content":"Partition（分区） 一个有序不变的消息序列。每个主题下可以有多个分区。 ","date":"0001-01-01","objectID":"/ooooo-notes/old-notes/geektime/kafka-core-tech/02/:1:3","tags":null,"title":"","uri":"/ooooo-notes/old-notes/geektime/kafka-core-tech/02/"},{"categories":null,"content":"Offset（消息位移） 分区中每条消息的位置信息，是一个单调递增且不变的值。 ","date":"0001-01-01","objectID":"/ooooo-notes/old-notes/geektime/kafka-core-tech/02/:1:4","tags":null,"title":"","uri":"/ooooo-notes/old-notes/geektime/kafka-core-tech/02/"},{"categories":null,"content":"Producer（生产者） 向主题发布新消息的应用程序。 ","date":"0001-01-01","objectID":"/ooooo-notes/old-notes/geektime/kafka-core-tech/02/:1:5","tags":null,"title":"","uri":"/ooooo-notes/old-notes/geektime/kafka-core-tech/02/"},{"categories":null,"content":"Consumer（消费者） 从主题订阅新消息的应用程序。 ","date":"0001-01-01","objectID":"/ooooo-notes/old-notes/geektime/kafka-core-tech/02/:1:6","tags":null,"title":"","uri":"/ooooo-notes/old-notes/geektime/kafka-core-tech/02/"},{"categories":null,"content":"Consumer Offset（消费者位移） 消费者消费进度，每个消费者都有自己的消费者位移。 ","date":"0001-01-01","objectID":"/ooooo-notes/old-notes/geektime/kafka-core-tech/02/:1:7","tags":null,"title":"","uri":"/ooooo-notes/old-notes/geektime/kafka-core-tech/02/"},{"categories":null,"content":"Consumer Group（消费者组） 多个消费者实例共同组成的一个组，同时消费多个分区以实现高吞吐。 ","date":"0001-01-01","objectID":"/ooooo-notes/old-notes/geektime/kafka-core-tech/02/:1:8","tags":null,"title":"","uri":"/ooooo-notes/old-notes/geektime/kafka-core-tech/02/"},{"categories":null,"content":"Rebalance （重平衡） 消费者组内某个消费者实例挂掉后，其他消费者实例自动重新分配订阅主题分区的过程。 Rebalance 是 Kafka 消费者端实现高可用的重要手段。 ","date":"0001-01-01","objectID":"/ooooo-notes/old-notes/geektime/kafka-core-tech/02/:1:9","tags":null,"title":"","uri":"/ooooo-notes/old-notes/geektime/kafka-core-tech/02/"},{"categories":null,"content":"2、Kafka 三层消息架构 第一层是主题层，每个主题可以配置 M 个分区，而每个分区又可以配置 N 个副本。 第二层是分区层，每个分区的 N 个副本中只能有一个充当领导者角色，对外提供服务；其他 N-1 个副本是追随者副本，只是提供数据冗余之用。 第三层是消息层，分区中包含若干条消息，每条消息的位移从 0 开始，依次递增。 最后，客户端程序只能与分区的领导者副本进行交互。 ","date":"0001-01-01","objectID":"/ooooo-notes/old-notes/geektime/kafka-core-tech/02/:2:0","tags":null,"title":"","uri":"/ooooo-notes/old-notes/geektime/kafka-core-tech/02/"},{"categories":null,"content":"3、Kafka 持久化数据 Kafka 使用消息日志（Log）来保存数据，消息日志只能追加写入，所以避免了随机 I/O 操作，改为性能较好的顺序 I/O 写操作。 在 Kafka 底层，一个日志会细分成多个日志段，消息被追加写到当前最新的日志段中，当写满了一个日志段后，Kafka 会自动切分出一个新的日志段，并将老的日志段封存起来。Kafka 在后台还有定时任务会定期地检查老的日志段是否能够被删除，从而实现回收磁盘空间的目的。 ","date":"0001-01-01","objectID":"/ooooo-notes/old-notes/geektime/kafka-core-tech/02/:3:0","tags":null,"title":"","uri":"/ooooo-notes/old-notes/geektime/kafka-core-tech/02/"},{"categories":null,"content":"1、Kafka 是分布式流处理平台 Apache Kafka 是消息引擎系统，也是一个分布式流处理平台。 Kafka 的特性： 提供一套 API 实现生产者和消费者。 降低网络传输和磁盘存储开销。 实现高伸缩性架构。 可以实现端到端的精确一次处理语义。 ","date":"0001-01-01","objectID":"/ooooo-notes/old-notes/geektime/kafka-core-tech/03/:1:0","tags":null,"title":"","uri":"/ooooo-notes/old-notes/geektime/kafka-core-tech/03/"},{"categories":null,"content":"1、Kafka 不同的\"发行版\" ","date":"0001-01-01","objectID":"/ooooo-notes/old-notes/geektime/kafka-core-tech/04/:1:0","tags":null,"title":"","uri":"/ooooo-notes/old-notes/geektime/kafka-core-tech/04/"},{"categories":null,"content":"1、Apache Kafka 只提供最基础的组件，没有任何监控框架或工具。 如果你仅仅需要一个消息引擎系统亦或是简单的流处理应用场景，同时需要对系统有较大把控度，那么我推荐你使用 Apache Kafka。 优势在于迭代速度快，社区响应度高，使用它可以让你有更高的把控度；缺陷在于仅提供基础核心组件，缺失一些高级的特性。 ","date":"0001-01-01","objectID":"/ooooo-notes/old-notes/geektime/kafka-core-tech/04/:1:1","tags":null,"title":"","uri":"/ooooo-notes/old-notes/geektime/kafka-core-tech/04/"},{"categories":null,"content":"2、Confluent Kafka 分为免费版和企业版。 免费版包含 Schema 注册中心和 REST proxy 两大功能。前者是帮助你集中管理 Kafka 消息格式以实现数据前向 / 后向兼容；后者用开放 HTTP 接口的方式允许你通过网络访问 Kafka 的各种功能，这两个都是 Apache Kafka 所没有的。 企业版包含跨数据中心备份和集群监控两大功能。 优势在于集成了很多高级特性且由 Kafka 原班人马打造，质量上有保证；缺陷在于相关文档资料不全，普及率较低，没有太多可供参考的范例。 ","date":"0001-01-01","objectID":"/ooooo-notes/old-notes/geektime/kafka-core-tech/04/:1:2","tags":null,"title":"","uri":"/ooooo-notes/old-notes/geektime/kafka-core-tech/04/"},{"categories":null,"content":"3、Cloudera/Hortonworks Kafka 大数据云公司发布的 Kafka（CDH/HDP Kafka）。这些大数据平台天然集成了 Apache Kafka，通过便捷化的界面操作将 Kafka 的安装、运维、管理、监控全部统一在控制台中。 如果你需要快速地搭建消息引擎系统，或者你需要搭建的是多框架构成的数据平台且 Kafka 只是其中一个组件，那么我推荐你使用这些大数据云公司提供的 Kafka。 优势在于操作简单，节省运维成本；缺陷在于把控度低，演进速度较慢。 ","date":"0001-01-01","objectID":"/ooooo-notes/old-notes/geektime/kafka-core-tech/04/:1:3","tags":null,"title":"","uri":"/ooooo-notes/old-notes/geektime/kafka-core-tech/04/"},{"categories":null,"content":"1、Kafka 的版本 [scala-version] - [kafka-version].tar.gz 0.10.0.0 引进 Kafka Stream 0.11.0.0 添加幂等性 Producer 和事务 API，并对 Kafka 消息格式进行重构 总结： 如果只使用 Kafka 的消息引擎功能， 最少 0.10.2.2， 可以使用新版的 Consumer API。 如果使用 Kafka Stream，最少 2.0.0。 ","date":"0001-01-01","objectID":"/ooooo-notes/old-notes/geektime/kafka-core-tech/05/:1:0","tags":null,"title":"","uri":"/ooooo-notes/old-notes/geektime/kafka-core-tech/05/"},{"categories":null,"content":"1、操作系统 三个方面选择： I/O 模型的使用 数据网络传输效率 社区支持度 五种 I/O 模型：阻塞式 I/O、非阻塞式 I/O、I/O 多路复用、信号驱动 I/O 和异步 I/O。 Linux 中的系统调用 select 函数就属于 I/O 多路复用模型；epoll 系统调用则介于第三种和第四种模型之间。 Linux 还可以使用 zero copy。 总结； 高性能只能选择 linux。 ","date":"0001-01-01","objectID":"/ooooo-notes/old-notes/geektime/kafka-core-tech/06/:1:0","tags":null,"title":"","uri":"/ooooo-notes/old-notes/geektime/kafka-core-tech/06/"},{"categories":null,"content":"2、磁盘 总结： 追求性价比的公司可以不搭建 RAID。 使用机械磁盘完全能够胜任 Kafka 线上环境。 ","date":"0001-01-01","objectID":"/ooooo-notes/old-notes/geektime/kafka-core-tech/06/:2:0","tags":null,"title":"","uri":"/ooooo-notes/old-notes/geektime/kafka-core-tech/06/"},{"categories":null,"content":"3、磁盘容量 规划磁盘容量的思考因素： 新增消息数 消息留存时间 平均消息大小 备份数 是否启用压缩 例如： 每一天 1 亿条 1KB 大小的消息，保存两份且存留两周的时间。 一天数据大小： 1 亿 * 1 KB * 2 份 / 1000 / 1000 = 200 GB 预留 10% 的磁盘空间： 200 GB * 1.1 = 220 GB 两周时间： 220 GB * 14 = 3 TB 开启了压缩比，比如 0.75： 3 TB * 0.75 = 2.25 TB ","date":"0001-01-01","objectID":"/ooooo-notes/old-notes/geektime/kafka-core-tech/06/:3:0","tags":null,"title":"","uri":"/ooooo-notes/old-notes/geektime/kafka-core-tech/06/"},{"categories":null,"content":"4、带宽 以 1Gbps 的千兆网络为例， 在 1 小时内处理 1TB 的业务数据，需要机器数？ 带宽是 1Gbps，即每秒处理 1Gb 的数据，通常假设 Kafka 只能用到 70% 的带宽资源，毕竟其他进程也需要一些资源，也就是Kafka 最大只能使用 700MB 的带宽资源，常规性使用要预留 2/3，所以单台 Kafka 使用的带宽为 700 Mb / 3 = 240 Mbps。 1 个小时处理 1TB 数据， 1024 * 1024 / 3600 / (240 / 8) = 9.7 (注意 240 Mbps 是带宽，变为 MB 单位时，要除以 8)。 如果消息的副本数为 3，大约就是 30 台机器。 ","date":"0001-01-01","objectID":"/ooooo-notes/old-notes/geektime/kafka-core-tech/06/:4:0","tags":null,"title":"","uri":"/ooooo-notes/old-notes/geektime/kafka-core-tech/06/"},{"categories":null,"content":"1、Broker 端参数 ","date":"0001-01-01","objectID":"/ooooo-notes/old-notes/geektime/kafka-core-tech/07/:1:0","tags":null,"title":"","uri":"/ooooo-notes/old-notes/geektime/kafka-core-tech/07/"},{"categories":null,"content":"1、broker 存储信息 log.dirs: 日志目录，例如 /home/kafka1,/home/kafka2,/home/kafka3。 log.dir: 只需要设置参数log.dirs，此参数不需要。 ","date":"0001-01-01","objectID":"/ooooo-notes/old-notes/geektime/kafka-core-tech/07/:1:1","tags":null,"title":"","uri":"/ooooo-notes/old-notes/geektime/kafka-core-tech/07/"},{"categories":null,"content":"2、zk 信息 zookeeper.connect: 连接 zk 的参数，例如 zk1:2181,zk2:2181,zk3:2181。 如果让多个 Kafka 集群使用同一套 zk 集群，利用 zk 的 chroot 设置，例如 zookeeper.connect 可以设置为 zk1:2181,zk2:2181,zk3:2181/kafka1 和 zk1:2181,zk2:2181,zk3:2181/kafka2。 ","date":"0001-01-01","objectID":"/ooooo-notes/old-notes/geektime/kafka-core-tech/07/:1:2","tags":null,"title":"","uri":"/ooooo-notes/old-notes/geektime/kafka-core-tech/07/"},{"categories":null,"content":"3、broker 连接信息 listeners: 监听器，也就是通过什么协议访问指定主机名和端口开放的 Kafka 服务。 advertised.listeners: Broker 用于对外发布的监听器。 监听器配置，由三元组 \u003c协议名称，主机名，端口号\u003e 构成，例如你自己定义的协议名字 CONTROLLER://localhost:9092。 一旦你自己定义了协议名称，你必须还要指定 listener.security.protocol.map 参数告诉这个协议底层使用了哪种安全协议，比如指定 listener.security.protocol.map=CONTROLLER:PLAINTEXT 表示 CONTROLLER 这个自定义协议底层使用明文不加密传输数据。 host.name/port: 过期。 ","date":"0001-01-01","objectID":"/ooooo-notes/old-notes/geektime/kafka-core-tech/07/:1:3","tags":null,"title":"","uri":"/ooooo-notes/old-notes/geektime/kafka-core-tech/07/"},{"categories":null,"content":"4、topic 管理 auto.create.topics.enable: 是否允许自动创建 Topic, 建议为 false。 unclean.leader.election.enable： 是否允许 Unclean Leader 选举， 建议为 false。 Unclean Leader 选举，指的是落后太多的副本参与选举，可能会使数据丢失。 auto.leader.rebalance.enable: 是否允许定期进行 Leader 选举，建议为 false。 ","date":"0001-01-01","objectID":"/ooooo-notes/old-notes/geektime/kafka-core-tech/07/:1:4","tags":null,"title":"","uri":"/ooooo-notes/old-notes/geektime/kafka-core-tech/07/"},{"categories":null,"content":"5、数据留存 log.retention.{hour|minutes|ms}: 日志保留时间。例如 log.retention.hour=168 表示默认保存 7 天的数据。 log.retention.bytes: 消息保存的总磁盘容量大小。默认为 -1，表示容量无限制，在云上的多租户才用到此参数。 message.max.bytes: 最大消息大小。实际上，1MB 的消息很常见。 注意：上述的参数都不能使用默认值。 ","date":"0001-01-01","objectID":"/ooooo-notes/old-notes/geektime/kafka-core-tech/07/:1:5","tags":null,"title":"","uri":"/ooooo-notes/old-notes/geektime/kafka-core-tech/07/"},{"categories":null,"content":"1、Topic 级别参数 如果同时设置了 Topic 级别参数和全局 Broker 参数，Topic 级别参数会覆盖全局 Broker 参数的值，而每个 Topic 都能设置自己的参数值，这就是所谓的 Topic 级别参数。 retention.ms: 该 Topic 消息被保存的时长，会覆盖掉 Broker 端的全局参数值。 retention.bytes: 该 Topic 预留多大的磁盘空间，当前默认值是 -1，表示可以无限使用磁盘空间，在多租户的 Kafka 集群中用到。 max.message.bytes: Topic 的最大消息大小。 Topic 设置方式（🎉Kafka官方文档🎉）： 创建 Topic 时设置 –config 后面指定了想要设置的 Topic 级别参数。 bin/kafka-topics.sh --bootstrap-server localhost:9092 --create --topic my-topic --partitions 1 --replication-factor 1 --config retention.ms=15552000000 --config max.message.bytes=5242880 修改 Topic 时设置 bin/kafka-configs.sh --zookeeper localhost:2181 --entity-type topics --entity-name my-topic --alter --add-config max.message.bytes=10485760 ","date":"0001-01-01","objectID":"/ooooo-notes/old-notes/geektime/kafka-core-tech/08/:1:0","tags":null,"title":"","uri":"/ooooo-notes/old-notes/geektime/kafka-core-tech/08/"},{"categories":null,"content":"2、JVM 参数 Java7 中，如果 Broker 所在机器的 CPU 资源非常充裕，则建议开启 CMS 垃圾回收器, -XX:+UseCurrentMarkSweepGC。否则，使用吞吐量收集器。开启方法是指定 -XX:+UseParallelGC。 Java8 中，建议使用 G1 垃圾回收器。 建议使用 Java8。 KAFKA_HEAP_OPTS: 堆大小, 建议为 6GB，这是比较公认的合理值。 KAFKA_JVM_PERFORMANCE_OPTS: 指定 GC 参数。 比如你可以这样启动 Kafka： export KAFKA_HEAP_OPTS=--Xms6g --Xmx6g export KAFKA_JVM_PERFORMANCE_OPTS= -server -XX:+UseG1GC -XX:MaxGCPauseMillis=20 -XX:InitiatingHeapOccupancyPercent=35 -XX:+ExplicitGCInvokesConcurrent -Djava.awt.headless=true bin/kafka-server-start.sh config/server.properties ","date":"0001-01-01","objectID":"/ooooo-notes/old-notes/geektime/kafka-core-tech/08/:2:0","tags":null,"title":"","uri":"/ooooo-notes/old-notes/geektime/kafka-core-tech/08/"},{"categories":null,"content":"3、操作系统参数 主要系统参数： 文件描述符限制 执行命令 ulimit -n 1000000 来设置。 文件系统类型 XFS 的性能要强于 ext4。 Swappiness 将 swap 交换内存配置成一个接近 0 但不为 0 的值，比如 1。 提交时间 向 Kafka 发送数据并不是真要等数据被写入磁盘才会认为成功，而是只要数据被写入到操作系统的页缓存（Page Cache）上就可以了，随后操作系统根据 LRU 算法会定期将页缓存上的\"脏\"数据落盘到物理磁盘上。这个定期就是由提交时间来确定的，默认是 5 秒。由于 Kafka 的多副本的冗余机制，可以稍微拉大提交间隔来提高性能。 ","date":"0001-01-01","objectID":"/ooooo-notes/old-notes/geektime/kafka-core-tech/08/:2:1","tags":null,"title":"","uri":"/ooooo-notes/old-notes/geektime/kafka-core-tech/08/"},{"categories":null,"content":"1、为什么分区 Kafka 的消息组织方式实际上是三级结构：主题 - 分区 - 消息，主题下的每条消息只会保存在某一个分区中。 Kafka 的三级结构： 分区是为了实现系统的高伸缩性（Scalability），可以通过添加新的节点来增加整体系统的吞吐量。 ","date":"0001-01-01","objectID":"/ooooo-notes/old-notes/geektime/kafka-core-tech/09/:1:0","tags":null,"title":"","uri":"/ooooo-notes/old-notes/geektime/kafka-core-tech/09/"},{"categories":null,"content":"2、分区策略 所谓分区策略是决定生产者将消息发送到哪个分区的算法。 ","date":"0001-01-01","objectID":"/ooooo-notes/old-notes/geektime/kafka-core-tech/09/:2:0","tags":null,"title":"","uri":"/ooooo-notes/old-notes/geektime/kafka-core-tech/09/"},{"categories":null,"content":"1、自定义分区策略 你需要显式地配置生产者端的参数 partitioner.class，在编写生产者程序时，你可以编写一个具体的类实现 org.apache.kafka.clients.producer.Partitioner 接口，实现 partition() 和 close() 接口。 方法签名： int partition(String topic, Object key, byte[] keyBytes, Object value, byte[] valueBytes, Cluster cluster); ","date":"0001-01-01","objectID":"/ooooo-notes/old-notes/geektime/kafka-core-tech/09/:2:1","tags":null,"title":"","uri":"/ooooo-notes/old-notes/geektime/kafka-core-tech/09/"},{"categories":null,"content":"2、轮询策略 Kafka 默认的分区策略就是轮询策略，轮询策略有非常优秀的负载均衡表现，它总是能保证消息最大限度地被平均分配到所有分区上。 ","date":"0001-01-01","objectID":"/ooooo-notes/old-notes/geektime/kafka-core-tech/09/:2:2","tags":null,"title":"","uri":"/ooooo-notes/old-notes/geektime/kafka-core-tech/09/"},{"categories":null,"content":"3、随机策略 要实现随机策略版的 partition 方法(自定义分区策略)，如下： List\u003cPartitionInfo\u003e partitions = cluster.partitionsForTopic(topic); return ThreadLocalRandom.current().nextInt(partitions.size()); ","date":"0001-01-01","objectID":"/ooooo-notes/old-notes/geektime/kafka-core-tech/09/:2:3","tags":null,"title":"","uri":"/ooooo-notes/old-notes/geektime/kafka-core-tech/09/"},{"categories":null,"content":"4、按消息键保序策略 实现这个策略的 partition 方法(自定义分区策略)，如下： List\u003cPartitionInfo\u003e partitions = cluster.partitionsForTopic(topic); return Math.abs(key.hashCode()) % partitions.size(); Kafka 默认分区策略实际上同时实现了两种策略：如果指定了 Key，那么默认实现按消息键保序策略；如果没有指定 Key，则使用轮询策略。 ","date":"0001-01-01","objectID":"/ooooo-notes/old-notes/geektime/kafka-core-tech/09/:2:4","tags":null,"title":"","uri":"/ooooo-notes/old-notes/geektime/kafka-core-tech/09/"},{"categories":null,"content":"5、其他分区策略 比较常见的一种就是基于地理位置的分区策略。 比如根据 Broker 所在的 IP 地址实现定制化的分区策略，如下： List\u003cPartitionInfo\u003e partitions = cluster.partitionsForTopic(topic); return partitions.stream().filter(p -\u003e isSouth(p.leader().host())).map(PartitionInfo::partition).findAny().get(); 我们可以从所有分区中找出那些 Leader 副本在南方的所有分区，然后随机挑选一个进行消息发送。 ","date":"0001-01-01","objectID":"/ooooo-notes/old-notes/geektime/kafka-core-tech/09/:2:5","tags":null,"title":"","uri":"/ooooo-notes/old-notes/geektime/kafka-core-tech/09/"},{"categories":null,"content":"1、消息格式 Kafka 的消息格式目前有两种：V1 和 V2。 不论是哪个版本，Kafka 的消息层次都分为两层：消息集合(message set) 和 消息(message)。一个消息集合包含若干条日志项(record item)，日志项中包含多条消息。 在 V1 版本中，每条消息都要执行 CRC 校验，但有些情况下消息的 CRC 值是会发生变化的。比如在 Broker 端可能会对消息时间戳字段进行更新，或者在执行消息格式转换（兼容老版本客户端程序）。对于这些情况，每条消息都执行 CRC 校验就有点没必要了。在 V2 版本中，消息的 CRC 校验工作就被移到了消息集合这一层。 在 V1 版本中，保存压缩消息的方法是把多条消息进行压缩然后保存到外层消息的消息体字段中；而在 V2 版本中，是对整个消息集合进行压缩。显然后者应该比前者有更好的压缩效果。 在相同条件下，不论是否启用压缩，V2 版本都比 V1 版本节省磁盘空间。当启用压缩时，这种节省空间的效果更加明显。 ","date":"0001-01-01","objectID":"/ooooo-notes/old-notes/geektime/kafka-core-tech/10/:1:0","tags":null,"title":"","uri":"/ooooo-notes/old-notes/geektime/kafka-core-tech/10/"},{"categories":null,"content":"2、何时压缩 在 Kafka 中，压缩可能发生在两个地方：生产者端和 Broker 端。 Properties props = new Properties(); props.put(\"bootstrap.servers\", \"localhost:9092\"); props.put(\"compression.type\", \"gzip\"); Producer\u003cString, String\u003e producer = new KafkaProducer\u003c\u003e(props); 上述代码，表明该 Producer 的压缩算法使用的是 GZIP。 但有两种例外情况让 Broker 重新压缩消息： Broker 端指定了和 Producer 端不同的压缩算法。 Broker 端发生了消息格式转换（兼容老的客户端程序）。 ","date":"0001-01-01","objectID":"/ooooo-notes/old-notes/geektime/kafka-core-tech/10/:2:0","tags":null,"title":"","uri":"/ooooo-notes/old-notes/geektime/kafka-core-tech/10/"},{"categories":null,"content":"3、何时解压缩 一句话：Producer 端压缩、Broker 端保持、Consumer 端解压缩。 除了在 Consumer 端解压缩，Broker 端也会进行解压缩，目的是为了对消息执行各种验证。 ","date":"0001-01-01","objectID":"/ooooo-notes/old-notes/geektime/kafka-core-tech/10/:3:0","tags":null,"title":"","uri":"/ooooo-notes/old-notes/geektime/kafka-core-tech/10/"},{"categories":null,"content":"4、压缩算法对比 在 Kafka 2.1.0 版本之前，Kafka 支持 3 种压缩算法：GZIP、Snappy 和 LZ4。从 2.1.0 开始，Kafka 正式支持 Zstandard 算法（zstd）。 压缩算法有两个重要指标：压缩比、压缩/解压缩吞吐量。 吞吐量方面: LZ4 \u003e Snappy \u003e zstd 和 GZIP。 压缩比方面：zstd \u003e LZ4 \u003e GZIP \u003e Snappy 总结： 机器的 CPU 资源不充足，不建议开启压缩，因为压缩需要消耗大量 CPU。 机器的 CPU 资源充足，强烈建议你开启 zstd 压缩，这样能极大地节省网络资源消耗。 ","date":"0001-01-01","objectID":"/ooooo-notes/old-notes/geektime/kafka-core-tech/10/:4:0","tags":null,"title":"","uri":"/ooooo-notes/old-notes/geektime/kafka-core-tech/10/"},{"categories":null,"content":"Kafka 核心技术与实战 01、消息引擎系统 02、Kafka 术语 ","date":"0001-01-01","objectID":"/ooooo-notes/old-notes/geektime/kafka-core-tech/readme/:1:0","tags":null,"title":"","uri":"/ooooo-notes/old-notes/geektime/kafka-core-tech/readme/"},{"categories":null,"content":"1、MySQL 的基本架构 主要分为Server层和存储引擎层。 Server层包括连接器、查询缓存、分析器、优化器、执行器等，涵盖MySQL的大多数核心服务功能，以及所有的内置函数（如日期、时间、数学和加密函数等），所有跨存储引擎的功能都在这一层实现，比如存储过程、触发器、视图等。 存储引擎层负责数据的存储和提取。其架构模式是插件式的，支持 InnoDB 、 MyISAM 、 Memory 等多个存储引擎。现在最常用的存储引擎是 InnoDB ，它从 MySQL 5.5.5版本开始成为了默认存储引擎。 ","date":"0001-01-01","objectID":"/ooooo-notes/old-notes/geektime/mysql-45/01/:1:0","tags":null,"title":"","uri":"/ooooo-notes/old-notes/geektime/mysql-45/01/"},{"categories":null,"content":"2、查询语句的执行流程 ","date":"0001-01-01","objectID":"/ooooo-notes/old-notes/geektime/mysql-45/01/:2:0","tags":null,"title":"","uri":"/ooooo-notes/old-notes/geektime/mysql-45/01/"},{"categories":null,"content":"1、连接器 连接器负责跟客户端建立连接、获取权限、维持和管理连接。 连接命令： mysql -h$ip -P$port -u$user -p 如果用户名密码认证通过，连接器会到权限表里面查出你拥有的权限。 连接完成后，如果你没有后续的动作，这个连接就处于空闲状态（Sleep），通过 show processlist 命令查看如下： 客户端长时间处于 Sleep 状态，连接器就会自动将它断开。由参数 wait_timeout 控制的，默认值是8小时。 数据库里面，长连接是指连接成功后，如果客户端持续有请求，则一直使用同一个连接。短连接则是指每次执行完很少的几次查询就断开连接，下次查询再重新建立一个。建立连接的过程通常是比较复杂的，所以尽量使用长连接。 但全部使用长连接后，有些时候 MySQL 占用内存涨得特别快，这是因为 MySQL 在执行过程中临时使用的内存是管理在连接对象里面的，这些资源会在连接断开的时候才释放。所以如果长连接累积下来，可能导致内存占用太大，被系统强行杀掉（OOM），从现象看就是 MySQL 异常重启了。 长连接解决方案： 定期断开长连接. 在 MySQL 5.7 版本后，通过执行 mysql_reset_connection来重新初始化连接资源， 无需重新连接和权限认证。 ","date":"0001-01-01","objectID":"/ooooo-notes/old-notes/geektime/mysql-45/01/:2:1","tags":null,"title":"","uri":"/ooooo-notes/old-notes/geektime/mysql-45/01/"},{"categories":null,"content":"2、查询缓存 只要对一个表更新，这个表上所有的查询缓存都会被清空。 MySQL 8.0 缓存功能已经被删除。 ","date":"0001-01-01","objectID":"/ooooo-notes/old-notes/geektime/mysql-45/01/:2:2","tags":null,"title":"","uri":"/ooooo-notes/old-notes/geektime/mysql-45/01/"},{"categories":null,"content":"3、分析器 对SQL语句做解析，词法分析（select、insert、delete、update）、语法分析（语法是否正确）。 ","date":"0001-01-01","objectID":"/ooooo-notes/old-notes/geektime/mysql-45/01/:2:3","tags":null,"title":"","uri":"/ooooo-notes/old-notes/geektime/mysql-45/01/"},{"categories":null,"content":"4、优化器 优化器选择索引。 例如，对于下面的语句，有两种查询逻辑： mysql\u003e select * from t1 join t2 using(ID) where t1.c=10 and t2.d=20; 先从 t1 里面取出 c = 10 的记录的ID值，再根据 ID 值关联到 t2，再判断 t2 里面 d 的值是否等于 20。 先从 t2 里面取出 d = 20 的记录的ID值，再根据 ID 值关联到 t1，再判断 t1 里面 c 的值是否等于 10。 ","date":"0001-01-01","objectID":"/ooooo-notes/old-notes/geektime/mysql-45/01/:2:4","tags":null,"title":"","uri":"/ooooo-notes/old-notes/geektime/mysql-45/01/"},{"categories":null,"content":"5、执行器 先判断查询权限，如果有权限，执行器就会调用存储引擎提供的接口。 ","date":"0001-01-01","objectID":"/ooooo-notes/old-notes/geektime/mysql-45/01/:2:5","tags":null,"title":"","uri":"/ooooo-notes/old-notes/geektime/mysql-45/01/"},{"categories":null,"content":"3、问题 如果你执行 select * from T where k=1，报不存在 K 这一列。是在哪个阶段报出来的？ 分析器阶段 ","date":"0001-01-01","objectID":"/ooooo-notes/old-notes/geektime/mysql-45/01/:3:0","tags":null,"title":"","uri":"/ooooo-notes/old-notes/geektime/mysql-45/01/"},{"categories":null,"content":"1、更新语句的执行流程 创建表 T: mysql\u003e create table T(ID int primary key, c int); 将 ID = 2 这一行的值加 1 的 SQL 语句: mysql\u003e update T set c=c+1 where ID=2; 查询语句的那一套流程，更新语句也是同样会走一遍。 经过连接器和查询缓存之后，分析器通过词法和语法分析知道这是一条更新语句，优化器决定要使用 ID 这个索引，执行器负责具体执行，找到这一行，然后更新。 与查询流程不一样的是，更新流程还涉及两个重要的日志模块： **redo log（重做日志）**和 binlog（归档日志）。 ","date":"0001-01-01","objectID":"/ooooo-notes/old-notes/geektime/mysql-45/02/:1:0","tags":null,"title":"","uri":"/ooooo-notes/old-notes/geektime/mysql-45/02/"},{"categories":null,"content":"2、redo log redo log 是 InnoDB 引擎特有的日志模块。 在 MySQL 中，如果每一次的更新操作都需要写进磁盘，然后磁盘也要找到对应的那条记录，然后再更新，整个过程 IO 成本、查找成本都很高。为了提高更新效率，MySQL 使用 WAL（Write-Ahead Logging）技术，它的关键点就是先写日志，再写磁盘。 当有记录需要更新时，InnoDB 引擎就会先把记录写到 redo log 里面，并更新内存，这时更新就算完成了。同时，InnoDB引擎会在适当的时候（系统比较空闲），将这个记录更新到磁盘里面。 InnoDB的 redo log 是固定大小的，比如可以配置为一组 4 个文件，每个文件的大小是 1GB，那么总共可以记录 4GB 的操作。从头开始写，写到末尾又回到开头循环写。 write pos 是当前记录的位置，一边写一边后移。 checkpoint 是当前要擦除的位置，擦除记录前要把记录更新到数据文件。 有了 redo log，InnoDB 就可以保证即使数据库发生异常重启，之前提交的记录都不会丢失，这个能力称为 crash-safe。 ","date":"0001-01-01","objectID":"/ooooo-notes/old-notes/geektime/mysql-45/02/:2:0","tags":null,"title":"","uri":"/ooooo-notes/old-notes/geektime/mysql-45/02/"},{"categories":null,"content":"3、binlog binlog 是 Sever 层的日志模块，只能用于归档，比如主从复制。 两种日志不同： redo log 是InnoDB引擎特有的；binlog是MySQL的Server层实现的，所有引擎都可以使用。 redo log 是物理日志，记录的是“在某个数据页上做了什么修改”；binlog 是逻辑日志，记录的是这个语句的原始逻辑，比如“给ID=2这一行的c字段加1”。 redo log 是循环写，写到末尾又回到开头写；binlog是追加写入，写到一定大小后会切换到下一个，并不会覆盖以前的日志。 ","date":"0001-01-01","objectID":"/ooooo-notes/old-notes/geektime/mysql-45/02/:3:0","tags":null,"title":"","uri":"/ooooo-notes/old-notes/geektime/mysql-45/02/"},{"categories":null,"content":"4、两阶段提交 上面简单的 updata 语句的执行流程。 执行器先找引擎取 ID = 2 这一行。ID 是主键，引擎直接用树搜索找到这一行。如果ID=2 这一行所在的数据页本来就在内存中，就直接返回给执行器；否则，需要先从磁盘读入内存，然后再返回。 执行器拿到引擎给的行数据，把这个值加上1，比如原来是N，现在就是N+1，得到新的一行数据，再调用引擎接口写入这行新数据。 引擎将这行新数据更新到内存中，同时将这个更新操作记录到 redo log 里面，此时 redo log 处于 prepare 状态。然后告知执行器执行完成了，随时可以提交事务。 执行器生成这个操作的 binlog，并把 binlog 写入磁盘。 执行器调用引擎的提交事务接口，引擎把刚刚写入的 redo log 改成 commit 状态，更新完成。 如果不使用两阶段提交，会有什么问题？ 先写 redo log 后写 binlog。假设在 redo log 写完，binlog 还没有写完的时候，MySQL进程异常重启，binlog 中没有更新的数据。 先写 binlog 后写 redo log。如果在 binlog 写完之后 crash，由于 redo log 没有写，崩溃恢复以后这个事务无效，所以这一行 c 的值是 0，但 binlog 中 c = 1 了。 ","date":"0001-01-01","objectID":"/ooooo-notes/old-notes/geektime/mysql-45/02/:4:0","tags":null,"title":"","uri":"/ooooo-notes/old-notes/geektime/mysql-45/02/"},{"categories":null,"content":"5、配置 双1设置： innodb_flush_log_at_trx_commit = 1 表示每次事务的 redo log 都直接持久化到磁盘。这样可以保证 MySQL 异常重启之后数据不丢失。 sync_binlog = 1 表示每次事务的 binlog 都持久化到磁盘。这样可以保证 MySQL 异常重启之后 binlog 不丢失。 ","date":"0001-01-01","objectID":"/ooooo-notes/old-notes/geektime/mysql-45/02/:5:0","tags":null,"title":"","uri":"/ooooo-notes/old-notes/geektime/mysql-45/02/"},{"categories":null,"content":"6、问题 在什么场景下，一天一备会比一周一备更有优势呢？ 好处是“最长恢复时间”更短。 在一天一备的模式里，最坏情况下需要应用一天的 binlog。比如，你每天 0 点做一次全量备份，而要恢复出一个到昨天晚上 23 点的备份。 一周一备最坏情况就要应用一周的 binlog 了。 ","date":"0001-01-01","objectID":"/ooooo-notes/old-notes/geektime/mysql-45/02/:6:0","tags":null,"title":"","uri":"/ooooo-notes/old-notes/geektime/mysql-45/02/"},{"categories":null,"content":"1、事务隔离级别 事务的 ACID 中的 I 指的就是隔离性（Isolation）。 ","date":"0001-01-01","objectID":"/ooooo-notes/old-notes/geektime/mysql-45/03/:1:0","tags":null,"title":"","uri":"/ooooo-notes/old-notes/geektime/mysql-45/03/"},{"categories":null,"content":"1、隔离级别 读未提交（read-uncommitted）：一个事务还没提交时，它做的变更就能被别的事务看到。 读提交（read-committed）：一个事务提交之后，它做的变更才会被其他事务看到。 可重复读（repeatable-read）：一个事务执行过程中看到的数据，总是跟这个事务在启动时看到的数据是一致的。 串行化（serializable）：当出现读写锁冲突的时候，后执行事务必须等前一个事务执行完成，才能继续执行。 MySQL 的隔离级别设置为\"读提交\"。 ","date":"0001-01-01","objectID":"/ooooo-notes/old-notes/geektime/mysql-45/03/:1:1","tags":null,"title":"","uri":"/ooooo-notes/old-notes/geektime/mysql-45/03/"},{"categories":null,"content":"2、事务隔离例子 mysql\u003e create table T(c int) engine=InnoDB; insert into T(c) values(1); 在不同的隔离级别下的结果： 隔离级别为读未提交，v1 = 2, v2 = 2, v3 = 2。 隔离级别为读提交，v1 = 1, v2 = 2, v3 = 2。 隔离级别为可重复读，v1 = 1, v2 = 1, v3 = 2。 隔离级别为串行化，v1 = 1, v2 = 1, v3 = 2。 在实现上，数据库里面会创建一个视图，访问的时候以视图的逻辑结果为准。 在可重复读隔离级别下，这个视图是在事务启动时创建的，整个事务存在期间都用这个视图。 在读提交隔离级别下，这个视图是在每个SQL语句开始执行的时候创建的。 在读未提交隔离级别下，直接返回记录上的最新值，没有视图概念。 在串行化隔离级别下，直接用加锁的方式来避免并行访问。 ","date":"0001-01-01","objectID":"/ooooo-notes/old-notes/geektime/mysql-45/03/:1:2","tags":null,"title":"","uri":"/ooooo-notes/old-notes/geektime/mysql-45/03/"},{"categories":null,"content":"3、事务配置方式 通过命令 show variables like 'transaction_isolation'; 来查看当前隔离级别。通过命令 set transaction_isolation = 'read-committed'; 来设置隔离级别。 ","date":"0001-01-01","objectID":"/ooooo-notes/old-notes/geektime/mysql-45/03/:1:3","tags":null,"title":"","uri":"/ooooo-notes/old-notes/geektime/mysql-45/03/"},{"categories":null,"content":"2、事务隔离的实现 在 MySQL 中，实际上每条记录在更新的时候都会同时记录一条回滚操作。记录上的最新值，通过回滚操作，都可以得到前一个状态的值。 假设一个值从 1 被按顺序改成了 2、3、4，在回滚日志里面就会有类似下面的记录。 长事务意味着系统里面会存在很老的事务视图。由于这些事务随时可能访问数据库里面的任何数据，所以这个事务提交之前，数据库里面它可能用到的回滚记录都必须保留，这就会导致大量占用存储空间。长事务还占用锁资源，也可能拖垮整个库。 ","date":"0001-01-01","objectID":"/ooooo-notes/old-notes/geektime/mysql-45/03/:2:0","tags":null,"title":"","uri":"/ooooo-notes/old-notes/geektime/mysql-45/03/"},{"categories":null,"content":"3、事务的启动方式 建议你总是使用 set autocommit = 1, 通过显式语句的方式来启动事务。 在 autocommit = 1 的情况下，用 begin 显式启动的事务，如果执行 commit 则提交事务。如果执行 commit work and chain，则是提交事务并自动启动下一个事务。 你可以在 information_schema 库的 innodb_trx 这个表中查询长事务，比如下面这个语句，用于查找持续时间超过 60s 的事务。 mysql\u003e select * from information_schema.innodb_trx where TIME_TO_SEC(timediff(now(),trx_started))\u003e60 ","date":"0001-01-01","objectID":"/ooooo-notes/old-notes/geektime/mysql-45/03/:3:0","tags":null,"title":"","uri":"/ooooo-notes/old-notes/geektime/mysql-45/03/"},{"categories":null,"content":"4、问题 怎样避免长事务？ 在开发端： 设置set autocommit = 0。 确认是否有不必要的只读事务。 通过 SET MAX_EXECUTION_TIME 命令，来控制每个语句执行的最长时间，避免单个语句意外执行太长时间。 在数据库端： 控 information_schema.Innodb_trx 表，设置长事务阈值，超过就报警/或者 kill。 ercona 的 pt-kill 这个工具不错，推荐使用。 在业务功能测试阶段要求输出所有的 general_log，分析日志行为提前发现问题。 如果使用的是MySQL 5.6或者更新版本，把 innodb_undo_tablespaces 设置成2（或更大的值）。如果真的出现大事务导致回滚段过大，这样设置后清理起来更方便。 ","date":"0001-01-01","objectID":"/ooooo-notes/old-notes/geektime/mysql-45/03/:4:0","tags":null,"title":"","uri":"/ooooo-notes/old-notes/geektime/mysql-45/03/"},{"categories":null,"content":"1、索引的常见模型 ","date":"0001-01-01","objectID":"/ooooo-notes/old-notes/geektime/mysql-45/04/:1:0","tags":null,"title":"","uri":"/ooooo-notes/old-notes/geektime/mysql-45/04/"},{"categories":null,"content":"1、哈希表 以键-值（key-value）存储数据的结构。只适用于等值查询的场景。 ","date":"0001-01-01","objectID":"/ooooo-notes/old-notes/geektime/mysql-45/04/:1:1","tags":null,"title":"","uri":"/ooooo-notes/old-notes/geektime/mysql-45/04/"},{"categories":null,"content":"2、有序数组 查询效率高，但更新数据，成本高。只适用静态存储引擎。 上面数组的按照 ID_card 升序排列，如果查询条件是 where ID_card = '?'，可以用二分法查询。 ","date":"0001-01-01","objectID":"/ooooo-notes/old-notes/geektime/mysql-45/04/:1:2","tags":null,"title":"","uri":"/ooooo-notes/old-notes/geektime/mysql-45/04/"},{"categories":null,"content":"3、搜索树 InnoDB 引擎中使用 B+ 树（ N 叉树）。可以减少磁盘 IO。 ","date":"0001-01-01","objectID":"/ooooo-notes/old-notes/geektime/mysql-45/04/:1:3","tags":null,"title":"","uri":"/ooooo-notes/old-notes/geektime/mysql-45/04/"},{"categories":null,"content":"2、InnoDB的索引模型 我们有一个主键列为 ID 的表，表中有字段 k，并且在 k 上有索引，建表语句如下： mysql\u003e create table T( id int primary key, k int not null, name varchar(16), index (k))engine=InnoDB; 表中 R1 ~ R5 的 (ID,k) 值分别为 (100,1)、(200,2)、(300,3)、(500,5) 和 (600,6)。 在 InnoDB 引擎中，每个索引就是一颗 B+ 树。两颗索引树如下。 根据叶子节点的内容，索引类型分为主键索引和非主键索引。 主键索引的叶子节点存放的是整行数据。 非主键索引的叶子节点存放的是主键的值。 基于主键索引和普通索引的查询有什么区别？ 如果语句是 select * from T where ID=500，即主键查询方式，则只需要搜索 ID 这棵B+树； 如果语句是 select * from T where k=5，即普通索引查询方式，则需要先搜索 k 索引树，得到 ID 的值为 500，再到 ID 索引树搜索一次。这个过程称为回表。 ","date":"0001-01-01","objectID":"/ooooo-notes/old-notes/geektime/mysql-45/04/:2:0","tags":null,"title":"","uri":"/ooooo-notes/old-notes/geektime/mysql-45/04/"},{"categories":null,"content":"3、索引维护 B+ 树为了维护索引有序性，在插入新值的时候需要做必要的维护。以上面这个图为例，如果插入新的行 ID 值为 700，则只需要在 R5 的记录后面插入一个新记录。如果新插入的 ID 值为 400，就相对麻烦了，需要逻辑上挪动后面的数据，空出位置。 而更糟的情况是，如果 R5 所在的数据页已经满了，根据 B+ 树的算法，这时候需要申请一个新的数据页，然后挪动部分数据过去。这个过程称为页分裂。在这种情况下，性能自然会受影响。 除了性能外，页分裂操作还影响数据页的利用率。原本放在一个页的数据，现在分到两个页中，整体空间利用率降低大约50%。当相邻两个页由于删除了数据，利用率很低之后，会将数据页做合并。 总结： 如果主键是自增的，每次插入一条新的数据，就是追加操作，就不会触发页分裂。 主键长度越小，普通索引的叶子节点就越小，占用的空间也就越小。 ","date":"0001-01-01","objectID":"/ooooo-notes/old-notes/geektime/mysql-45/04/:3:0","tags":null,"title":"","uri":"/ooooo-notes/old-notes/geektime/mysql-45/04/"},{"categories":null,"content":"4、问题 对于上面例子中的 InnoDB 表 T，如果你要重建索引 k，你的两个 SQL 语句可以这么写： alter table T drop index k; alter table T add index(k); 如果你要重建主键索引，也可以这么写： alter table T drop primary key; alter table T add primary key(id); 可以执行 alter table T engine=InnoDB; 来重建索引。 ","date":"0001-01-01","objectID":"/ooooo-notes/old-notes/geektime/mysql-45/04/:4:0","tags":null,"title":"","uri":"/ooooo-notes/old-notes/geektime/mysql-45/04/"},{"categories":null,"content":"1、覆盖索引 创建表 T : mysql\u003e create table T ( ID int primary key, k int NOT NULL DEFAULT 0, s varchar(16) NOT NULL DEFAULT '', index k(k)) engine=InnoDB; insert into T values(100,1, 'aa'),(200,2,'bb'),(300,3,'cc'),(500,5,'ee'),(600,6,'ff'),(700,7,'gg'); 执行语句 select ID from T where k between 3 and 5，只需要扫描 k 索引树。因为结果只需要查询 ID，而 ID 在 k 索引树上，减少了回表操作。 索引树已经覆盖了查询结果，称之为覆盖索引。覆盖索引可以减少树的搜索次数，显著提升查询性能。 ","date":"0001-01-01","objectID":"/ooooo-notes/old-notes/geektime/mysql-45/05/:1:0","tags":null,"title":"","uri":"/ooooo-notes/old-notes/geektime/mysql-45/05/"},{"categories":null,"content":"2、最左前缀匹配 B+ 树这种索引结构，可以利用索引的最左前缀，来定位记录。 创建表 tuser ： CREATE TABLE `tuser` ( `id` int(11) NOT NULL, `id_card` varchar(32) DEFAULT NULL, `name` varchar(32) DEFAULT NULL, `age` int(11) DEFAULT NULL, `ismale` tinyint(1) DEFAULT NULL, PRIMARY KEY (`id`), KEY `id_card` (`id_card`), KEY `name_age` (`name`,`age`) ) ENGINE=InnoDB 当你的查询条件是 where name like ‘张%’，也是可以用到索引（name,age）的。 联合索引建立规则： 通过调整顺序，可以少维护一个索引，这是优先考虑的。 索引的空间。 ","date":"0001-01-01","objectID":"/ooooo-notes/old-notes/geektime/mysql-45/05/:2:0","tags":null,"title":"","uri":"/ooooo-notes/old-notes/geektime/mysql-45/05/"},{"categories":null,"content":"3、索引下推 可以对索引中存在的字段先做判断，减少回表次数。 当查询条件是 where name like '张%' and age=10 and ismale=1;，也是可以用到索引树（name,age）的。 ","date":"0001-01-01","objectID":"/ooooo-notes/old-notes/geektime/mysql-45/05/:3:0","tags":null,"title":"","uri":"/ooooo-notes/old-notes/geektime/mysql-45/05/"},{"categories":null,"content":"4、问题 有如下表： CREATE TABLE `geek` ( `a` int(11) NOT NULL, `b` int(11) NOT NULL, `c` int(11) NOT NULL, `d` int(11) NOT NULL, PRIMARY KEY (`a`,`b`), KEY `c` (`c`), KEY `ca` (`c`,`a`), KEY `cb` (`c`,`b`) ) ENGINE=InnoDB; 由于历史原因，这个表需要a、b做联合主键。 查询语句如下： select * from geek where c=N order by a limit 1; select * from geek where c=N order by b limit 1; 索引（c,a）、（c,b）是否都是必须的? 索引 (c,a) 不需要。 ","date":"0001-01-01","objectID":"/ooooo-notes/old-notes/geektime/mysql-45/05/:4:0","tags":null,"title":"","uri":"/ooooo-notes/old-notes/geektime/mysql-45/05/"},{"categories":null,"content":"1、全局表 根据加锁的范围，MySQL 里面的锁大致可以分成全局锁、表级锁和行锁三类。 MySQL 加全局读锁的命令：flush tables with read lock;（FTWRL）, 解锁命令：unlock tables;。让整个库处于只读状态。任何增删改语句都会被阻塞。 全局锁的典型使用场景是，做全库逻辑备份，也就是 select 所有的数据。 对于 MyISAM 引擎来说，备份只能加全局锁。 对于 Innodb 引擎来说，可以使用脚本 mysqldump，参数为 –-single-transaction，来启动一个事务，确保拿到一致性视图来备份数据。MyISAM 不支持事务，所以无法用此脚本。 全库只读，为什么不使用 set global readonly=true 的方式，主要原因有两点： 在有些系统中，readonly 会被用来做其他逻辑，比如判断一个库是主库还是备库。 异常处理机制有差异。 执行 FTWRL 命令后，客户端发生异常断开，MySQL 会自动释放这个全局锁。如果整库处理 readonly 状态，客户端发生异常，数据库会一直处于 readonly 状态。 ","date":"0001-01-01","objectID":"/ooooo-notes/old-notes/geektime/mysql-45/06/:1:0","tags":null,"title":"","uri":"/ooooo-notes/old-notes/geektime/mysql-45/06/"},{"categories":null,"content":"2、表级锁 MySQL 里面表级别的锁有两种：一种是表锁，一种是元数据锁（meta data lock，MDL)。 例如：如果在某个线程 A 中执行 lock tables t1 read, t2 write; 这个语句，则其他线程写 t1 、读写 t2 的语句都会被阻塞。线程 A 执行 unlock tables; 语句来解锁。 MDL 不需要显式使用，在访问一个表的时候会被自动加上。MDL 的作用是，保证读写的正确性。 在 MySQL 5.5 版本中引入了 MDL，当对一个表做增删改查操作的时候，加MDL读锁；当要对表做结构变更操作的时候，加MDL写锁。 给一个小表加个字段，导致整个库挂了。 session A 和 session B 都会加上 MDL读锁。 session C 要变更表结构，必要要加上 MDL写锁，此时只能阻塞。 session D 申请 MDL读锁 就会被 session C 阻塞。 MDL 锁必须要等整个事务提交后再释放。 如何安全地给小表加字段？ 首先要解决长事务，事务不提交，就会一直占着 MDL锁。在 MySQL 的 information_schema 库的 innodb_trx 表中，你可以查到当前执行中的事务。如果你要做 DDL 变更的表刚好有长事务在执行，要考虑先暂停 DDL，或者 kill 掉这个长事务。 如果要变更的表是一个热点表，虽然数据量不大，但是上面的请求很频繁，而你不得不加个字段, kill 可能未必管用，比较理想的机制是，在 alter table 语句里面设定等待时间，如果在这个等待时间里面能够拿到 MDL写锁 最好，拿不到也不要阻塞后面的业务语句，先放弃。之后再通过重试命令重复这个过程。 ALTER TABLE tbl_name NOWAIT add column ... ALTER TABLE tbl_name WAIT N add column ... ","date":"0001-01-01","objectID":"/ooooo-notes/old-notes/geektime/mysql-45/06/:2:0","tags":null,"title":"","uri":"/ooooo-notes/old-notes/geektime/mysql-45/06/"},{"categories":null,"content":"3、问题 备份一般都会在备库上执行，你在用 –-single-transaction 方法做逻辑备份的过程中，如果主库上的一个小表做了一个DDL，比如给一个表上加了一列。这时候，从备库上会看到什么现象呢？ 假设这个 DDL 是针对表 t1 的， 这里把备份过程中几个关键的语句列出来： Q1:SET SESSION TRANSACTION ISOLATION LEVEL REPEATABLE READ; Q2:START TRANSACTION WITH CONSISTENT SNAPSHOT; /* other tables */ Q3:SAVEPOINT sp; /* 时刻 1 */ Q4:show create table `t1`; /* 时刻 2 */ Q5:SELECT * FROM `t1`; /* 时刻 3 */ Q6:ROLLBACK TO SAVEPOINT sp; /* 时刻 4 */ /* other tables */ 在备份开始的时候，为了确保RR（可重复读）隔离级别，再设置一次RR隔离级别(Q1); 启动事务(Q2); 设置一个保存点，这个很重要(Q3); show create 是为了拿到表结构(Q4)，然后正式导数据 （Q5），回滚到SAVEPOINT sp，在这里的作用是释放 t1 的 MDL锁。 答案如下： 如果在 Q4 语句执行之前到达，现象：没有影响，备份拿到的是DDL后的表结构。 如果在\"时刻 2\"到达，则 Q5 执行的时候表结构被改过，报 Table definition has changed, please retry transaction，现象：mysqldump 终止； 如果在\"时刻2\"和\"时刻3\"之间到达，mysqldump 占着 t1 的 MDL读锁，binlog 被阻塞，现象：主从延迟，直到 Q6 执行完成。 从\"时刻4\"开始，mysqldump 释放了 MDL读锁，现象：没有影响，备份拿到的是 DDL 前的表结构。 ","date":"0001-01-01","objectID":"/ooooo-notes/old-notes/geektime/mysql-45/06/:3:0","tags":null,"title":"","uri":"/ooooo-notes/old-notes/geektime/mysql-45/06/"},{"categories":null,"content":"1、行锁 MySQL 的行锁是在引擎层由各个引擎自己实现的，并不是所有的引擎都支持行锁，比如 MyISAM 引擎就不支持，InnoDB 引擎支持行锁。 两阶段锁： 实际上事务B的 update 语句会被阻塞，直到事务A执行 commit 之后，事务B才能继续执行。 在 InnoDB 事务中，行锁是在需要的时候才加上的，但并不是不需要了就立刻释放，而是要等到事务结束时才释放。这个就是两阶段锁协议。 如果你的事务中需要锁多个行，要把最可能造成锁冲突、最可能影响并发度的锁尽量往后放。 比如，电影票在线交易业务： 从顾客A账户余额中扣除电影票价。 给影院B的账户余额增加这张电影票价。 记录一条交易日志。 要保证交易的原子性，就要把这三个操作放在一个事务中，按照 3 -\u003e 1 -\u003e 2 的顺序执行，就减少了事务之间的锁等待，提升了并发度。 ","date":"0001-01-01","objectID":"/ooooo-notes/old-notes/geektime/mysql-45/07/:1:0","tags":null,"title":"","uri":"/ooooo-notes/old-notes/geektime/mysql-45/07/"},{"categories":null,"content":"2、死锁和死锁检测 死锁例子： 事务A在等待事务B释放 id=2 的行锁，而事务B在等待事务A释放 id=1 的行锁。 事务A和事务B在互相等待对方的资源释放，就是进入了死锁状态。 当出现死锁以后，有两种策略 直接进入等待，直到超时，这个超时时间通过参数 innodb_lock_wait_timeout 来设置。 发起死锁检测，发现死锁后，主动回滚死锁链条中的某一个事务，让其他事务得以继续执行。通过参数 innodb_deadlock_detect 设置为 on。 在 InnoDB 中，innodb_lock_wait_timeout 的默认值是 50s，当出现死锁以后，第一个被锁住的线程要过 50s 才会超时退出，这个等待时间肯定是无法接受的，但如果设置为 1s，可能出现不是死锁的情况（大事务），造成误伤。 正常情况下，我们要采用第二种策略：主动死锁检测。innodb_deadlock_detect 的默认值为 on。 当更新同一行时，每个线程检查的时间复杂度为 O(n), 也就是说 1000 个线程，要操作 100w 次，所以死锁检测要耗费大量的 CPU 资源。 怎么解决由这种热点行更新导致的性能问题呢？ 一种头痛医头的方法，就是如果你能确保这个业务一定不会出现死锁，可以临时把死锁检测关掉。业务有损。 控制并发度。比如同一行同时最多只有 10 个线程在更新，那么死锁检测的成本很低，就不会出现这个问题。 并发控制要做在数据库服务端，如果你有中间件，可以考虑在中间件实现；如果你的团队有能修改 MySQL 源码的人，也可以做在 MySQL 里面。基本思路就是，对于相同行的更新，在进入引擎之前排队。这样在 InnoDB 内部就不会有大量的死锁检测工作了。 控制并发度也可以从业务设计上优化。考虑将一行改成逻辑上的多行来减少锁冲突。 还是以影院账户为例，可以考虑放在多条记录上，比如 10 个记录，影院的账户总额等于这 10 个记录的值的总和。这样每次要给影院账户加金额的时候，随机选其中一条记录来加。这样每次冲突概率变成原来的 1/10，可以减少锁等待个数，也就减少了死锁检测的 CPU 消耗。 如果账户余额可能会减少，比如退票逻辑，那么这时候就需要考虑当一部分行记录变成 0 的时候，代码要有特殊处理。 ","date":"0001-01-01","objectID":"/ooooo-notes/old-notes/geektime/mysql-45/07/:2:0","tags":null,"title":"","uri":"/ooooo-notes/old-notes/geektime/mysql-45/07/"},{"categories":null,"content":"3、问题 如果你要删除一个表里面的前 10000 行数据，有以下三种方法可以做到： 第一种，直接执行 delete from T limit 10000; 第二种，在一个连接中循环执行20次 delete from T limit 500; 第三种，在 20 个连接中同时执行 delete from T limit 500。 你会选择哪一种方法呢？为什么呢？ 第二种方式是相对较好的。 第一种方式单个语句占用时间长，锁的时间也比较长；而且大事务还会导致主从延迟。 第三种方式会人为造成锁冲突。 ","date":"0001-01-01","objectID":"/ooooo-notes/old-notes/geektime/mysql-45/07/:3:0","tags":null,"title":"","uri":"/ooooo-notes/old-notes/geektime/mysql-45/07/"},{"categories":null,"content":"1、事务 在第 3 篇文章和你讲事务隔离级别的时候提到过，如果是可重复读隔离级别，事务 T 启动的时候会创建一个视图 read-view，之后事务 T 执行期间，即使有其他事务修改了数据，事务 T 看到的仍然跟在启动时看到的一样。 例子： mysql\u003e CREATE TABLE `t` ( `id` int(11) NOT NULL, `k` int(11) DEFAULT NULL, PRIMARY KEY (`id`) ) ENGINE=InnoDB; insert into t(id, k) values(1,1),(2,2); 在可重复读隔离级别下，begin/start transaction 并不是事务的起点，只有执行到第一个语句时才会真正启动事务。如果你想要马上启动一个事务，可以使用 start transaction with consistent snapshot 这个命令。 在这个例子中，事务 C 没有显式地使用 begin/commit ，表示这个 update 语句本身就是一个事务，语句完成的时候会自动提交。 结果是事务 B 的 k 值为 3，事务A的 k 值为 1。 在 MySQL 里，有两个\"视图\"的概念: 一个是 view，它是一个用查询语句定义的虚拟表。 另一个是 InnoDB 在实现 MVCC 时用到的一致性读视图，即 consistent read view。 ","date":"0001-01-01","objectID":"/ooooo-notes/old-notes/geektime/mysql-45/08/:1:0","tags":null,"title":"","uri":"/ooooo-notes/old-notes/geektime/mysql-45/08/"},{"categories":null,"content":"2、“快照\"怎么工作的 在可重复读隔离级别下，事务在启动的时候就\"拍了个快照”。这个快照是基于整库的。 InnoDB 里面每个事务有一个唯一的事务 ID，叫作 transaction id 。它是在事务开始的时候向 InnoDB 的事务系统申请的，是按申请顺序严格递增的。 而每行数据也都是有多个版本的。每次事务更新数据的时候，都会生成一个新的数据版本，并且把transaction id赋值给这个数据版本的事务 ID，记为 row trx_id。也就是说，表中的一行记录，其实可能有多个版本(row)，每个版本有自己的 row trx_id。 一个记录被多个事务连续更新后的状态: 图中虚线框里是同一行数据的4个版本，当前最新版本是 V4，k 的值是 22，它是被transaction id 为 25 的事务更新的，因此它的 row trx_id 也是 25。 图中三个虚线箭头，就是 undo log，而 V1、V2、V3 并不是物理上真实存在的，而是每次需要的时候根据当前版本和 undo log 计算出来的。 一个事务只需要在启动的时候声明说，“以我启动的时刻为准，如果一个数据版本是在我启动之前生成的，就认；如果是我启动以后才生成的，我就不认，我必须要找到它的上一个版本”。如果\"上一个版本\"也不可见，那就得继续往前找。 在实现上， InnoDB 为每个事务构造了一个数组，用来保存这个事务启动瞬间，当前正在 活跃 (启动了但还没提交)的所有事务 ID。 数组里面事务 ID 的最小值记为低水位，当前系统里面已经创建过的事务 ID 的最大值加1记为高水位。 对于一个数据版本的 row trx_id, 有以下几种可能： 如果落在绿色部分，表示这个版本是已提交的事务或者是当前事务自己生成的，这个数据是可见的； 如果落在红色部分，表示这个版本是由将来启动的事务生成的，是肯定不可见的； 如果落在黄色部分，那就包括两种情况 a. 若 row trx_id 在数组中，表示这个版本是由还没提交的事务生成的，不可见； b. 若 row trx_id 不在数组中，表示这个版本是已经提交了的事务生成的，可见。 分析下图 1 中的三个事务，事务 A 为什么是 k=1 ？ 假设： 事务A开始前，系统里面只有一个活跃事务 ID 是 99； 事务 A、B、C 的版本号分别是 100、101、102，且当前系统里只有这四个事务； 三个事务开始前，(1,1）这一行数据的 row trx_id是 90。 这样，事务 A 的视图数组就是 [99,100] , 事务B的视图数组是 [99,100,101], 事务C的视图数组是 [99,100,101,102]。 事务A查询逻辑有关的操作: 总结： 一个数据版本，对于一个事务视图来说，除了自己的更新总是可见以外，有三种情况： 版本未提交，不可见； 版本已提交，但是是在视图创建后提交的，不可见； 版本已提交，而且是在视图创建前提交的，可见。 ","date":"0001-01-01","objectID":"/ooooo-notes/old-notes/geektime/mysql-45/08/:2:0","tags":null,"title":"","uri":"/ooooo-notes/old-notes/geektime/mysql-45/08/"},{"categories":null,"content":"3、更新逻辑 更新数据都是先读后写的，而这个读，只能读当前的值，称为\"当前读\"（current read）。 在执行事务 B 语句的时候，update 语句是当前读，这是 row trx_id 为 101，所以 select 语句能读到 k=3。 除了 update 语句外，select 语句如果加锁，也是当前读。 mysql\u003e select k from t where id=1 lock in share mode; # 读锁（S锁，共享锁） mysql\u003e select k from t where id=1 for update; # 写锁（X锁，排他锁） 假设事务 C 不是马上提交的，而是变成了下面的事务 C’，会怎么样呢？ 事务 C’ 没提交，也就是说 (1,2) 这个版本上的写锁还没释放。而事务B 是当前读，必须要读最新版本，而且必须加锁，因此就被锁住了，必须等到事务 C’ 释放这个锁，才能继续它的当前读。 总结： 可重复读的核心就是一致性读（consistent read）；而事务更新数据的时候，只能用当前读。如果当前的记录的行锁被其他事务占用的话，就需要进入锁等待。 读提交的逻辑和可重复读的逻辑类似，它们最主要的区别是： 在可重复读隔离级别下，只需要在事务开始的时候创建一致性视图，之后事务里的其他查询都共用这个一致性视图； 在读提交隔离级别下，每一个语句执行前都会重新算出一个新的视图。 注意，语句 start transaction with consistent snapshot; 在读提交隔离级别下，没有意义。 读提交时的状态图，注意是事务 C。 事务 A 查询语句返回的是 k=2，事务 B 查询结果 k=3。 ","date":"0001-01-01","objectID":"/ooooo-notes/old-notes/geektime/mysql-45/08/:2:1","tags":null,"title":"","uri":"/ooooo-notes/old-notes/geektime/mysql-45/08/"},{"categories":null,"content":"4、最重要的总结 一个数据版本，对于一个事务视图来说，除了自己的更新总是可见以外，有三种情况： 版本未提交，不可见； 版本已提交，但是是在视图创建后提交的，不可见； 版本已提交，而且是在视图创建前提交的，可见。 语句 update 、for update 、lock in share mode 是当前读。 ","date":"0001-01-01","objectID":"/ooooo-notes/old-notes/geektime/mysql-45/08/:3:0","tags":null,"title":"","uri":"/ooooo-notes/old-notes/geektime/mysql-45/08/"},{"categories":null,"content":"5、问题 我用下面的表结构和初始化语句作为试验环境，事务隔离级别是可重复读。现在，我要把所有\"字段c和id值相等的行\"的 c 值清零，但是却发现了一个“诡异”的、改不掉的情况。请你构造出这种情况，并说明其原理。 mysql\u003e CREATE TABLE `t` ( `id` int(11) NOT NULL, `c` int(11) DEFAULT NULL, PRIMARY KEY (`id`) ) ENGINE=InnoDB; insert into t(id, c) values(1,1),(2,2),(3,3),(4,4); 出现的情况： 第一种情况 2. 第二种情况 ","date":"0001-01-01","objectID":"/ooooo-notes/old-notes/geektime/mysql-45/08/:4:0","tags":null,"title":"","uri":"/ooooo-notes/old-notes/geektime/mysql-45/08/"},{"categories":null,"content":"1、查询过程 主键索引 ID 和 普通索引 K: 假设执行查询的语句是 select id from T where k=5, 使用二分法查询： 对于普通索引来说，查找到满足条件的第一个记录(5,500)后，需要查找下一个记录，直到碰到第一个不满足 k=5 条件的记录。 对于唯一索引来说，由于索引定义了唯一性，查找到第一个满足条件的记录后，就会停止继续检索。 两个索引的性能差别，微乎其微。 InnoDB 的数据是按数据页为单位来读写，也就是说，当找到 k=5 的记录的时候，它所在的数据页就都在内存里了，所以判断下一条记录是很快的。 ","date":"0001-01-01","objectID":"/ooooo-notes/old-notes/geektime/mysql-45/09/:1:0","tags":null,"title":"","uri":"/ooooo-notes/old-notes/geektime/mysql-45/09/"},{"categories":null,"content":"2、更新过程 当需要更新一个数据页时，如果数据页在内存中就直接更新，如果这个数据页还没有在内存中，在不影响数据一致性的前提下，InooDB 会将这些更新操作缓存在 change buffer 中，这样就不需要从磁盘中读入这个数据页了。在下次查询需要访问这个数据页的时候，将数据页读入内存，然后执行 change buffer 中与这个页有关的操作。通过这种方式就能保证这个数据逻辑的正确性。 将 change buffer 中的操作应用到原数据页，得到最新结果的过程称为 merge。除了访问这个数据页会触发 merge 外，系统有后台线程会定期 merge 。在数据库正常关闭（shutdown）的过程中，也会执行 merge 操作。 对于唯一索引来说，所有的更新操作都要先判断这个操作是否违反唯一性约束，所以必须要将数据页读入内存才能判断，这时 change buffer 不能使用了。 change buffer 用的是 buffer pool 里的内存，因此不能无限增大。change buffer 的大小，可以通过参数 innodb_change_buffer_max_size 来动态设置。这个参数设置为 50 的时候，表示 change buffer 的大小最多只能占用 buffer pool 的 50%。 ","date":"0001-01-01","objectID":"/ooooo-notes/old-notes/geektime/mysql-45/09/:2:0","tags":null,"title":"","uri":"/ooooo-notes/old-notes/geektime/mysql-45/09/"},{"categories":null,"content":"3、change buffer 的使用场景 change buffer 的主要目的就是将记录的变更动作缓存下来，所以在一个数据页做 merge 之前，change buffer 记录的变更越多（也就是这个页面上要更新的次数越多），收益就越大。 对于写多读少的业务来说， change buffer 的使用效果最好。这种业务模型常见的就是账单类、日志类的系统。 假设一个业务的更新模式是写入之后马上会做查询，那么即使满足了条件，将更新先记录在 change buffer，但之后由于马上要访问这个数据页，会立即触发 merge 过程。这样随机访问 IO 的次数不会减少，反而增加了 change buffer 的维护代价。 ","date":"0001-01-01","objectID":"/ooooo-notes/old-notes/geektime/mysql-45/09/:3:0","tags":null,"title":"","uri":"/ooooo-notes/old-notes/geektime/mysql-45/09/"},{"categories":null,"content":"4、索引选择和实践 在不影响业务的情况下，建议你尽量选择普通索引。 普通索引和 change buffer 的配合使用，对于数据量大的表的更新优化还是很明显的。 ","date":"0001-01-01","objectID":"/ooooo-notes/old-notes/geektime/mysql-45/09/:4:0","tags":null,"title":"","uri":"/ooooo-notes/old-notes/geektime/mysql-45/09/"},{"categories":null,"content":"5、change buffer 和 redo log 执行插入语句： mysql\u003e insert into t(id,k) values(id1,k1),(id2,k2); 我们假设\b当前 k 索引树的状态，查找到位置后，k1 所在的数据页在内存(InnoDB buffer pool)中，k2 所在的数据页不在内存中。下图是带 change buffer 的更新状态图。 这条插入语句做了如下的操作: Page 1 在内存中，直接更新内存。 Page 2 没有在内存中，就在内存的 change buffer 区域，记录下\"我要往 Page 2 插入一行\"这个信息。 将上述两个动作记入 redo log 中（图中3和4）。 图中的两个虚线箭头，是后台操作，不影响更新的响应时间。 现在要执行语句 select * from t where k in (k1, k2)，这两个读请求的流程图： 读 Page 1 的时候，直接从内存返回。 要读 Page 2 的时候，需要把 Page 2 从磁盘读入内存中，然后应用 change buffer 里面的操作日志，生成一个正确的版本并返回结果。 从上图可知，redo log 主要节省的是随机写磁盘的 IO 消耗（转成顺序写），而 change buffer 主要节省的则是随机读磁盘的 IO 消耗。 ","date":"0001-01-01","objectID":"/ooooo-notes/old-notes/geektime/mysql-45/09/:5:0","tags":null,"title":"","uri":"/ooooo-notes/old-notes/geektime/mysql-45/09/"},{"categories":null,"content":"6、问题 change buffer 一开始是写内存的，那么如果这个时候机器掉电重启，会不会导致 change buffer 丢失呢？change buffer 丢失可不是小事儿，再从磁盘读入数据可就没有了 merge 过程，就等于是数据丢失了。会不会出现这种情况呢？ 不会丢失。虽然是只更新内存，但是在事务提交的时候，我们把 change buffer 的操作也记录到 redo log 里了，所以崩溃恢复的时候，change buffer 也能找回来。 merge 的执行流程是这样的： 从磁盘读入数据页到内存（老版本的数据页）； 从 change buffer 里找出这个数据页的 change buffer 记录(可能有多个），依次应用，得到新版数据页； 写 redo log。这个 redo log 包含了数据的变更和 change buffer 的变更。 ","date":"0001-01-01","objectID":"/ooooo-notes/old-notes/geektime/mysql-45/09/:6:0","tags":null,"title":"","uri":"/ooooo-notes/old-notes/geektime/mysql-45/09/"},{"categories":null,"content":"1、选错索引 一个例子，建表语句如下： CREATE TABLE `x` ( `id` int(11) NOT NULL, `a` int(11) DEFAULT NULL, `b` int(11) DEFAULT NULL, PRIMARY KEY (`id`), KEY `a` (`a`), KEY `b` (`b`) ) ENGINE=InnoDB; 往表 x 中插入 10 万行记录，取值按整数递增，即：(1,1,1)，(2,2,2)，(3,3,3) 直到(100000,100000,100000)，存储过程如下： delimiter ;; create procedure idata() begin declare i int; set i=1; while(i\u003c=100000)do insert into x values(i, i, i); set i=i+1; end while; end;; delimiter ; call idata(); 分析一条 SQL 语句 select * from x where a between 10000 and 20000; 我们再做如下操作。 这时候，session B 的查询语句 select * from t where a between 10000 and 20000 就不会再选择索引 a 了。 查看慢查询： 临时开启慢查询： set global slow_query_log='ON'; set global slow_query_log_file='/var/lib/mysql/instance-1-slow.log'; 实验过程就是这三个语句： set long_query_time=0; # 记录所有的查询到慢查询日志中 select * from x where a between 10000 and 20000; # Q1 查询 select * from x force index(a) where a between 10000 and 20000; # Q2 强制使用索引 a 这三条 SQL 语句执行完成后的慢查询日志: Q1 扫描了 10 万行，显然是走了全表扫描，执行时间是 40 毫秒。Q2 扫描了 10001 行，执行了21毫秒，很显然 mysql 选错了索引。 ","date":"0001-01-01","objectID":"/ooooo-notes/old-notes/geektime/mysql-45/10/:1:0","tags":null,"title":"","uri":"/ooooo-notes/old-notes/geektime/mysql-45/10/"},{"categories":null,"content":"2、优化器的逻辑 在第一篇文章中，我们就提到过，选择索引是优化器的工作。 优化器选择索引会根据扫描行数、是否使用临时表、是否排序等因素进行综合判断。 当然，这个例子中只有扫描行数这个因素，对于扫描行数，MySQL 根据统计信息来估算记录数，抽样来得到索引的基数信息（区别度），如下图。 采样统计的时候，InnoDB 默认会选择 N 个数据页，统计这些页面上的不同值，得到一个平均值，然后乘以这个索引的页面数，就得到了这个索引的基数。 在 MySQL 中，有两种存储索引统计的方式，可以通过设置参数 innodb_stats_persistent 的值来选择： 设置为 on，表示统计信息会持久化存储。这时，默认的 N 是 20，M 是 10。 设置为 off，表示统计信息只存储在内存中。这时，默认的 N是 8，M 是 16。 MySQL 选错索引，是因为索引统计信息不准确，修正统计信息，执行 analyze table x; 命令。 另外一个语句： mysql\u003e select * from t where (a between 1 and 1000) and (b between 50000 and 100000) order by b limit 1; 从条件上看，这个查询没有符合条件的记录，因此会返回空集合。 如果优化器使用索引 a 的话，执行速度明显会快很多，执行 explain命令后： 返回结果中 key 字段显示，这次优化器选择了索引 b。 从这个结果中，你可以得到两个结论： 扫描行数的估计值依然不准确。 这个例子里 MySQL 又选错了索引。 ","date":"0001-01-01","objectID":"/ooooo-notes/old-notes/geektime/mysql-45/10/:2:0","tags":null,"title":"","uri":"/ooooo-notes/old-notes/geektime/mysql-45/10/"},{"categories":null,"content":"3、索引选择异常和处理 一种方法是，采用 force index 强行选择一个索引。 另一种方法是，我们可以考虑修改语句，引导 MySQL 使用我们期望的索引。 修改语句后： 之前优化器选择使用索引 b，是因为它认为使用索引 b 可以避免排序（ b 本身是索引，已经是有序的了，如果选择索引 b 的话，不需要再做排序，只需要遍历），所以即使扫描行数多，也判定为代价更小。 现在 order by b,a 这种写法，要求按照 b,a 排序，就意味着使用这两个索引都需要排序。因此，扫描行数成了影响决策的主要条件，于是此时优化器选了只需要扫描 1000 行的索引 a。 这种修改并不是通用的优化手段，只是刚好在这个语句里面有 limit 1,order by b limit 1 和 order by b,a limit 1 都会返回b 是最小的那一行，逻辑上一致，才可以这么做。 另一种修改语句： 在这个例子里，我们用 limit 100 让优化器意识到，使用 b 索引代价是很高的。其实是我们根据数据特征诱导了一下优化器，也不具备通用性。 第三种方法是，有些场景下，我们可以新建一个更合适的索引，来提供给优化器做选择，或删掉误用的索引。 第四种方法是，删掉索引 b。 ","date":"0001-01-01","objectID":"/ooooo-notes/old-notes/geektime/mysql-45/10/:3:0","tags":null,"title":"","uri":"/ooooo-notes/old-notes/geektime/mysql-45/10/"},{"categories":null,"content":"4、问题 前面我们在构造第一个例子的过程中，通过 session A 的配合，让 session B 删除数据后又重新插入了一遍数据，然后就发现 explain 结果中，rows 字段从 10001 变成 37000 多。 而如果没有 session A 的配合，只是单独执行 delete from t 、call idata()、explain 这三句话，会看到 rows 字段其实还是10000左右。 答案： delete 语句删掉了所有的数据，然后再通过 call idata() 插入了 10 万行数据，看上去是覆盖了原来的 10 万行。 但是，session A 开启了事务并没有提交，所以之前插入的 10 万行数据是不能删除的。这样，之前的数据每一行数据都有两个版本，旧版本是 delete 之前的数据，新版本是标记为 deleted 的数据。 这样，索引 a 上的数据其实就有两份。 表的行数，优化器直接用的是 show table status 的值。 ","date":"0001-01-01","objectID":"/ooooo-notes/old-notes/geektime/mysql-45/10/:4:0","tags":null,"title":"","uri":"/ooooo-notes/old-notes/geektime/mysql-45/10/"},{"categories":null,"content":"1、字符串索引 假设，你现在维护一个支持邮箱登录的系统，用户表是这么定义的 mysql\u003e create table SUser( ID bigint unsigned primary key, email varchar(64), ... )engine=innodb; 要使用邮箱登录，一定会出现类似下面的语句： mysql\u003e select f1, f2 from SUser where email='xxx'; email 这个字段上没有索引，那么这个语句就只能做全表扫描。 MySQL 是支持前缀索引的，你可以定义字符串的一部分作为索引，例如： mysql\u003e alter table SUser add index index1(email); ## 整个字符串 mysql\u003e alter table SUser add index index2(email(6)); ## 前 6 个字节 两个索引图差别如下： 使用前缀索引的优势，占用的空间会更小，但可能会增加额外的记录扫描次数。 例如分析执行语句 select id,name,email from SUser where email='zhangssxyz@xxx.com'; 的过程。 使用 index1 (整个字符串)： 这个过程中，只需要从 index1 索引树找到满足索引值是 ’zhangssxyz@xxx.com’ 的这条记录，再进行回表操作，就可以返回结果。 使用 index2 (前 6 个字节)： 这个过程中，需要从 index2 索引树找到满足索引值是 ’zhangs’ 的全部记录，再进行回表操作，逐一判断 email 是 ’zhangssxyz@xxx.com’，最后返回结果。可能有大量以 ’zhangs’ 开头的记录， 所以可能会增加额外的记录扫描次数。 使用前缀索引，定义好长度，就可以做到既节省空间，又不用额外增加太多的查询成本。 如果建立前缀索引，就要关注区分度，区分度越高越好。 可以使用如下语句，统计不同索引的个数： mysql\u003e select count(distinct email) as L from SUser; 例如，统计 4~7 个字节的前缀索引： mysql\u003e select count(distinct left(email,4)）as L4, count(distinct left(email,5)）as L5, count(distinct left(email,6)）as L6, count(distinct left(email,7)）as L7, from SUser; ","date":"0001-01-01","objectID":"/ooooo-notes/old-notes/geektime/mysql-45/11/:1:0","tags":null,"title":"","uri":"/ooooo-notes/old-notes/geektime/mysql-45/11/"},{"categories":null,"content":"2、其他方式 对于类似于邮箱这样的字段来说，使用前缀索引的效果可能还不错,遇到前缀的区分度不够好的情况时，我们要怎么办呢？ 例如对身份证号的解决方法。 第一种方式是使用倒序存储。 mysql\u003e select field_list from t where id_card = reverse('input_id_card_string'); 第二种方式是使用hash字段 ## 添加索引，用来存储 crc32。 mysql\u003e alter table t add id_card_crc int unsigned, add index(id_card_crc); # 查询时判断 crc32 值 mysql\u003e select field_list from t where id_card_crc=crc32('input_id_card_string') and id_card='input_id_card_string' 它们的相同点是，都不支持范围查询。 ","date":"0001-01-01","objectID":"/ooooo-notes/old-notes/geektime/mysql-45/11/:2:0","tags":null,"title":"","uri":"/ooooo-notes/old-notes/geektime/mysql-45/11/"},{"categories":null,"content":"3、问题 如果你在维护一个学校的学生信息数据库，学生登录名的统一格式是 ”学号@gmail.com\", 而学号的规则是：十五位的数字，其中前三位是所在城市编号、第四到第六位是学校编号、第七位到第十位是入学年份、最后五位是顺序编号。 系统登录的时候都需要学生输入登录名和密码，验证正确后才能继续使用系统。就只考虑登录验证这个行为的话，你会怎么设计这个登录名的索引呢？ 答案： 可以只存入学年份加顺序编号，它们的长度是 9 位。如果数字类型来存这 9 位数字。比如 201100001 ，这样只需要占 4 个字节。 ","date":"0001-01-01","objectID":"/ooooo-notes/old-notes/geektime/mysql-45/11/:3:0","tags":null,"title":"","uri":"/ooooo-notes/old-notes/geektime/mysql-45/11/"},{"categories":null,"content":"1、SQL语句为什么变\"慢\"了 在前面第 2 篇文章《日志系统：一条SQL更新语句是如何执行的？》中，InnoDB 在处理更新语句时，只做了写日志这一个磁盘操作，这个日志叫作 redo log。更新内存写完 redo log 后，就返回给客户端，本次更新成功。。 当内存数据页跟磁盘数据页内容不一致的时候，我们称这个内存页为\"脏页\"。内存数据写入到磁盘后，内存和磁盘上的数据页的内容就一致了，称为\"干净页\"。 MySQL 偶尔\"抖\"一下的那个瞬间，可能就是在刷脏页（flush）。 引发数据库的 flush 过程的几种情况： InnoDB 的 redo log 写满了。 系统内存不足。当需要新的内存页，而内存不够用的时候，就要淘汰一些数据页，空出内存给别的数据页使用。如果淘汰的是\"脏页\"，就要先将脏页写到磁盘。 MySQL 认为系统\"空闲\"的时候。 MySQL 正常关闭。 上面四种场景对性能的影响： 第 3 种情况和第 4 种场景是 MySQL 正常情况，不用太关心性能。 第 1 种是 “redo log 写满了，要 flush 脏页”，出现这种情况了，整个系统就不能再接受更新，如果你从监控上看，这时候更新数会跌为 0。 第 2 种是\"内存不够用了，要先将脏页写到磁盘\"，这种情况其实是常态。InnoDB用缓冲池（buffer pool）管理内存，缓冲池中的内存页有三种状态： 第一种是，还没有使用的； 第二种是，使用了并且是干净页； 第三种是，使用了并且是脏页。 InnoDB 的策略是尽量使用内存，因此对于一个长时间运行的库来说，未被使用的页面很少。刷脏页虽然是常态，但是出现以下这两种情况，都是会明显影响性能的： 一个查询要淘汰的脏页个数太多，会导致查询的响应时间明显变长。 日志写满，更新全部堵住，写性能跌为 0，这种情况对敏感业务来说，是不能接受的。 ","date":"0001-01-01","objectID":"/ooooo-notes/old-notes/geektime/mysql-45/12/:1:0","tags":null,"title":"","uri":"/ooooo-notes/old-notes/geektime/mysql-45/12/"},{"categories":null,"content":"2、InnoDB 刷脏页的控制策略 innodb_io_capacity 参数：告诉 InnoDB 所在主机的 IO 能力。 可以使用 fio 这个工具来测试 IO 能力： fio -filename=$filename -direct=1 -iodepth 1 -thread -rw=randrw -ioengine=psync -bs=16k -size=500M -numjobs=10 -runtime=10 -group_reporting -name=mytest InnoDB 的刷盘速度就是要参考这两个因素：一个是脏页比例，一个是redo log写盘速度。 参数 innodb_max_dirty_pages_pct 是脏页比例上限，默认值是 75%。 脏页比例是通过 Innodb_buffer_pool_pages_dirty/Innodb_buffer_pool_pages_total 得到的，具体的命令如下： select VARIABLE_VALUE into @a from global_status where VARIABLE_NAME = 'Innodb_buffer_pool_pages_dirty'; select VARIABLE_VALUE into @b from global_status where VARIABLE_NAME = 'Innodb_buffer_pool_pages_total'; select @a/@b; 要尽量避免刷脏页这种情况，你就要合理地设置 innodb_io_capacity 的值，并且平时要多关注脏页比例，不要让它经常接近 75%。 一个有趣的策略： 一旦一个查询请求需要在执行过程中先 flush 掉一个脏页时，这个查询就可能要比平时慢了。而 MySQL中 的一个机制，可能让你的查询会更慢：在准备刷一个脏页的时候，如果这个数据页旁边的数据页刚好是脏页，就会把这个\"邻居\"也带着一起刷掉；而且这个把\"邻居\"拖下水的逻辑还可以继续蔓延，也就是对于每个邻居数据页，如果跟它相邻的数据页也还是脏页的话，也会被放到一起刷。 在 InnoDB 中，innodb_flush_neighbors 参数就是用来控制这个行为的，值为 1 的时候会有上述的\"连坐\"机制，值为 0 时表示不找邻居，自己刷自己的。 如果是 SSD 这种， 建议 innodb_flush_neighbors = 0。 ","date":"0001-01-01","objectID":"/ooooo-notes/old-notes/geektime/mysql-45/12/:2:0","tags":null,"title":"","uri":"/ooooo-notes/old-notes/geektime/mysql-45/12/"},{"categories":null,"content":"3、问题 一个内存配置为 128GB、innodb_io_capacity设置为 20000 的大规格实例，正常会建议你将redo log 设置成 4 个 1GB 的文件。 但如果你在配置的时候不慎将 redo log 设置成了 1个 100M 的文件，会发生什么情况呢？又为什么会出现这样的情况呢？ 答案： redo log 太小，很快就会被写满，就必须要 flush，在这种情况下， change buffer 的优化也失效了，因为 flush 时，必须要进行 merge 操作。你看到的现象就是磁盘压力很小，但是数据库出现间歇性的性能下跌。 ","date":"0001-01-01","objectID":"/ooooo-notes/old-notes/geektime/mysql-45/12/:2:1","tags":null,"title":"","uri":"/ooooo-notes/old-notes/geektime/mysql-45/12/"},{"categories":null,"content":"MySQL 实战 45 讲 01、SQL 查询语句的执行过程 ","date":"0001-01-01","objectID":"/ooooo-notes/old-notes/geektime/mysql-45/readme/:1:0","tags":null,"title":"","uri":"/ooooo-notes/old-notes/geektime/mysql-45/readme/"},{"categories":null,"content":" ArrayList LinkedList HashMap LinkedHashMap TreeMap ThreadLocal ThreadPoolExecutor CopyOnWriteArrayList ConcurrentHashMap ConcurrentSkipListMap ArrayBlockingQueue LinkedBlockingQueue PriorityQueue AtomicInteger AQS (AbstractQueuedSynchronizer) CountDownLatch Semaphore CyclicBarrier ReentrantLock ReentrantReadWriteLock ","date":"0001-01-01","objectID":"/ooooo-notes/outline/:0:0","tags":null,"title":"","uri":"/ooooo-notes/outline/"},{"categories":null,"content":" 搭建 rocketmq 源码调试环境 netty 设计 broker 注册 namesvr producer 发送消息 broker 存储消息 consumer 消费消息 broker 获取消息 producer 发送延时消息 producer 发送事务消息 broker 主从同步 proxy 计算存储分离 静态 topic admin 查找消息 ","date":"0001-01-01","objectID":"/ooooo-notes/outline/:0:0","tags":null,"title":"","uri":"/ooooo-notes/outline/"},{"categories":null,"content":"大纲: spring cache 原理 spring mvc 请求流程 spring boot 启动流程 适配多种 servlet 容器 spring 常用扩展点 spring bean 初始化 spring websocket 原理 spring security 原理 ","date":"0001-01-01","objectID":"/ooooo-notes/outline/:0:0","tags":null,"title":"","uri":"/ooooo-notes/outline/"},{"categories":null,"content":"大纲： 异步 servlet 原理 ","date":"0001-01-01","objectID":"/ooooo-notes/outline/:0:0","tags":null,"title":"","uri":"/ooooo-notes/outline/"}]