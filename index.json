[{"categories":["计划"],"content":"0、持续学习者 Talk is cheap. Show me the code. 英语比编程简单。 学习和实践要平衡。 学会和时间做朋友。 学会投资，学会理财。 学会先做减法，再做加法。 学英语很重要，学英语很重要，学英语很重要。 说明： ⭕ 进行中 ✅ 已完成 ❌ 已废弃 ❓ 有必要 ❗ 重要性 📝 记笔记 🖊 写代码 ","date":"2023-01-01 09:00:00","objectID":"/ooooo-notes/2023%E5%B9%B4%E8%AE%A1%E5%88%92/:1:0","tags":["learning"],"title":"2023年学习计划","uri":"/ooooo-notes/2023%E5%B9%B4%E8%AE%A1%E5%88%92/"},{"categories":["计划"],"content":"1、关于英语 《新概念二》 ⭕ ","date":"2023-01-01 09:00:00","objectID":"/ooooo-notes/2023%E5%B9%B4%E8%AE%A1%E5%88%92/:2:0","tags":["learning"],"title":"2023年学习计划","uri":"/ooooo-notes/2023%E5%B9%B4%E8%AE%A1%E5%88%92/"},{"categories":["计划"],"content":"2、关于技术 计划 🎉： 只记录自己认为有用的笔记。 ","date":"2023-01-01 09:00:00","objectID":"/ooooo-notes/2023%E5%B9%B4%E8%AE%A1%E5%88%92/:3:0","tags":["learning"],"title":"2023年学习计划","uri":"/ooooo-notes/2023%E5%B9%B4%E8%AE%A1%E5%88%92/"},{"categories":["计划"],"content":"1、书籍 0️⃣1️⃣. 《深入理解 Kafka：核心设计与实践原理》 0️⃣2️⃣. 《分布式一致性算法开发实战》 0️⃣3️⃣. 《Go Web 编程》 ✅ 0️⃣4️⃣. 《Effective C++》 0️⃣5️⃣. 《More Effective C++》 0️⃣6️⃣. 《深度探索C++对象模型》 0️⃣7️⃣. 《Go语言设计与实现》 0️⃣8️⃣. 《Vim实用技巧（第2版）》 ⭕ 0️⃣9️⃣. 《RocketMQ技术内幕 第二版》 1️⃣0️⃣. 《云原生服务网格Istio：原理、实践、架构与源码解析》 1️⃣1️⃣. 《MySQL技术内幕》 1️⃣2️⃣. 《深入解析Java虚拟机HotSpot》 1️⃣3️⃣. 《Rust权威指南》 ⭕ 1️⃣4️⃣. 《深入剖析Kubernetes》 1️⃣5️⃣. 《Kubernetes编程》 ⭕ 1️⃣6️⃣. 《Kubernetes设计模式》 1️⃣7️⃣. 《深入剖析Java虚拟机》 1️⃣8️⃣. 《算法训练营：海量图解+竞赛刷题（入门篇》 1️⃣9️⃣. 《算法训练营：海量图解+竞赛刷题（进阶篇）》 2️⃣0️⃣. 《TCP/IP详解 卷1：协议》 2️⃣1️⃣. 《自己动手写Docker》 ⭕ 2️⃣2️⃣. 《UNIX网络编程 卷1：套接字联网API（第3版）》 ","date":"2023-01-01 09:00:00","objectID":"/ooooo-notes/2023%E5%B9%B4%E8%AE%A1%E5%88%92/:3:1","tags":["learning"],"title":"2023年学习计划","uri":"/ooooo-notes/2023%E5%B9%B4%E8%AE%A1%E5%88%92/"},{"categories":["计划"],"content":"2、文档 0️⃣1️⃣. 《深入拆解 Java 虚拟机》 0️⃣2️⃣. 从 0 开始带你成为JVM实战高手 ⭕ 0️⃣3️⃣. Go 语言项目开发实战 ⭕ 0️⃣4️⃣. Redis 源码剖析与实战 ⭕ 0️⃣5️⃣. 深入 C 语言和程序运行原理 ","date":"2023-01-01 09:00:00","objectID":"/ooooo-notes/2023%E5%B9%B4%E8%AE%A1%E5%88%92/:3:2","tags":["learning"],"title":"2023年学习计划","uri":"/ooooo-notes/2023%E5%B9%B4%E8%AE%A1%E5%88%92/"},{"categories":["计划"],"content":"3. 源码 0️⃣1️⃣. 《rocketmq 源码》 ⭕ 0️⃣2️⃣. 《kubernetes 源码》 ⭕ 0️⃣3️⃣. 《istio 源码》 0️⃣4️⃣. 《etcd 源码》 0️⃣5️⃣. 《dubbo 源码》 ⭕ 0️⃣6️⃣. 《arthas》 0️⃣7️⃣. 《nsq 源码》 0️⃣8️⃣. 《eventing 源码》 0️⃣9️⃣. 《serving 源码》 1️⃣0️⃣. 《grpc-go 源码》 ⭕ ","date":"2023-01-01 09:00:00","objectID":"/ooooo-notes/2023%E5%B9%B4%E8%AE%A1%E5%88%92/:3:3","tags":["learning"],"title":"2023年学习计划","uri":"/ooooo-notes/2023%E5%B9%B4%E8%AE%A1%E5%88%92/"},{"categories":["计划"],"content":"4、视频 0️⃣1️⃣. 《玩转算法系列–图论精讲》 0️⃣2️⃣. 《玩转算法面试》 ⭕ ","date":"2023-01-01 09:00:00","objectID":"/ooooo-notes/2023%E5%B9%B4%E8%AE%A1%E5%88%92/:3:4","tags":["learning"],"title":"2023年学习计划","uri":"/ooooo-notes/2023%E5%B9%B4%E8%AE%A1%E5%88%92/"},{"categories":["计划"],"content":"4、关于其他 ","date":"2023-01-01 09:00:00","objectID":"/ooooo-notes/2023%E5%B9%B4%E8%AE%A1%E5%88%92/:4:0","tags":["learning"],"title":"2023年学习计划","uri":"/ooooo-notes/2023%E5%B9%B4%E8%AE%A1%E5%88%92/"},{"categories":["计划"],"content":"1、书籍 🎉 0️⃣1️⃣. 《卓有成效的工程师》 ✅ 0️⃣2️⃣. 《非暴力沟通》 ⭕ 0️⃣3️⃣. 《原则》 0️⃣4️⃣. 《刻意练习》 ⭕ 0️⃣5️⃣. 《关键对话》 0️⃣6️⃣. 《当下的启蒙》 0️⃣7️⃣. 《亲密关系：通往灵魂的桥梁》 ⭕ 0️⃣8️⃣. 《人性的弱点》 0️⃣9️⃣. 《数据密集型应用系统设计》 1️⃣0️⃣. 《当我谈跑步时，我谈些什么》 ","date":"2023-01-01 09:00:00","objectID":"/ooooo-notes/2023%E5%B9%B4%E8%AE%A1%E5%88%92/:4:1","tags":["learning"],"title":"2023年学习计划","uri":"/ooooo-notes/2023%E5%B9%B4%E8%AE%A1%E5%88%92/"},{"categories":["计划"],"content":"2、尝试 🎉 0️⃣1️⃣. 学会使用尤克里里弹奏 ","date":"2023-01-01 09:00:00","objectID":"/ooooo-notes/2023%E5%B9%B4%E8%AE%A1%E5%88%92/:4:2","tags":["learning"],"title":"2023年学习计划","uri":"/ooooo-notes/2023%E5%B9%B4%E8%AE%A1%E5%88%92/"},{"categories":["计划"],"content":"3、了解 🎉 0️⃣1️⃣. 暂无 ","date":"2023-01-01 09:00:00","objectID":"/ooooo-notes/2023%E5%B9%B4%E8%AE%A1%E5%88%92/:4:3","tags":["learning"],"title":"2023年学习计划","uri":"/ooooo-notes/2023%E5%B9%B4%E8%AE%A1%E5%88%92/"},{"categories":["计划"],"content":"4、娱乐 0️⃣1️⃣. 《奇遇人生 第一季》 0️⃣2️⃣. 《一本好书 1》 0️⃣3️⃣. 《一本好书 2》 ","date":"2023-01-01 09:00:00","objectID":"/ooooo-notes/2023%E5%B9%B4%E8%AE%A1%E5%88%92/:4:4","tags":["learning"],"title":"2023年学习计划","uri":"/ooooo-notes/2023%E5%B9%B4%E8%AE%A1%E5%88%92/"},{"categories":["随笔"],"content":"1. 前置条件 安装 docker，必须配置 docker 代理，否则 build 失败。 参考 下载 istio 源码。 安装 go 和 dlv 工具。参考 ","date":"2022-12-19 09:00:00","objectID":"/ooooo-notes/%E6%90%AD%E5%BB%BA-istio-%E6%BA%90%E7%A0%81%E8%B0%83%E8%AF%95%E7%8E%AF%E5%A2%83/:1:0","tags":["istio","cloud native"],"title":"搭建 istio 源码调试环境","uri":"/ooooo-notes/%E6%90%AD%E5%BB%BA-istio-%E6%BA%90%E7%A0%81%E8%B0%83%E8%AF%95%E7%8E%AF%E5%A2%83/"},{"categories":["随笔"],"content":"2. 设置环境变量 # docker 地址 export HUB=\"docker.io/youwillsee\" # istio 的源码目录 export ISTIO=/root/code/istio # docker 的 tag export TAG=1.16.0-debug ","date":"2022-12-19 09:00:00","objectID":"/ooooo-notes/%E6%90%AD%E5%BB%BA-istio-%E6%BA%90%E7%A0%81%E8%B0%83%E8%AF%95%E7%8E%AF%E5%A2%83/:2:0","tags":["istio","cloud native"],"title":"搭建 istio 源码调试环境","uri":"/ooooo-notes/%E6%90%AD%E5%BB%BA-istio-%E6%BA%90%E7%A0%81%E8%B0%83%E8%AF%95%E7%8E%AF%E5%A2%83/"},{"categories":["随笔"],"content":"3. build istio # 构建 debug 的版本，会输出在 out 目录下 make DEBUG=1 build # 构建 debug 的版本，推到本地的 docker 中 make DEBUG=1 docker # 推送到远端的 docker 中 make docker.push # 清理 make clean 参考 istio-devlopment istio-code-base ","date":"2022-12-19 09:00:00","objectID":"/ooooo-notes/%E6%90%AD%E5%BB%BA-istio-%E6%BA%90%E7%A0%81%E8%B0%83%E8%AF%95%E7%8E%AF%E5%A2%83/:3:0","tags":["istio","cloud native"],"title":"搭建 istio 源码调试环境","uri":"/ooooo-notes/%E6%90%AD%E5%BB%BA-istio-%E6%BA%90%E7%A0%81%E8%B0%83%E8%AF%95%E7%8E%AF%E5%A2%83/"},{"categories":["随笔"],"content":"4. dlv 连接 # 找到 pid ps -ef | grep pilot-discovery # attach pid dlv --listen=:2345 --headless=true --api-version=2 --accept-multiclient attach 172965 # 使用 IDE 远程连接 GOland -\u003e go remote ","date":"2022-12-19 09:00:00","objectID":"/ooooo-notes/%E6%90%AD%E5%BB%BA-istio-%E6%BA%90%E7%A0%81%E8%B0%83%E8%AF%95%E7%8E%AF%E5%A2%83/:4:0","tags":["istio","cloud native"],"title":"搭建 istio 源码调试环境","uri":"/ooooo-notes/%E6%90%AD%E5%BB%BA-istio-%E6%BA%90%E7%A0%81%E8%B0%83%E8%AF%95%E7%8E%AF%E5%A2%83/"},{"categories":["随笔"],"content":"1. 配置 docker 代理 # 创建配置目录 mkdir -p /etc/systemd/system/docker.service.d # 创建配置文件 vim /etc/systemd/system/docker.service.d/http-proxy.conf # 配置文件内容 [Service] Environment=\"HTTP_PROXY=http://ooooo:10800\" Environment=\"HTTPS_PROXY=http://ooooo:10800\" # 重启 docker systemctl daemon-reload \u0026\u0026 systemctl restart docker # 查看配置是否生效 systemctl show --property=Environment docker ","date":"2022-12-18 09:00:00","objectID":"/ooooo-notes/%E8%AE%BE%E7%BD%AE-docker-%E4%BB%A3%E7%90%86/:1:0","tags":["docker","cloud native"],"title":"设置 docker 代理","uri":"/ooooo-notes/%E8%AE%BE%E7%BD%AE-docker-%E4%BB%A3%E7%90%86/"},{"categories":["微信文章"],"content":"1. 实现队列 代码： 使用 head 和 tail 来实现单链表 单链表涉及到两个节点，每次都要判断中间状态 这里使用的是 AtomicReference 来实现的，也可以使用 unsafe 来实现，有兴趣的可以尝试下 这里使用 curTail.next 进行 CAS 来指定下一个节点, 很少这么使用，后面再详细说说 public class LinkedQueue\u003cE\u003e { private final Node\u003cE\u003e dummy = new Node\u003c\u003e(null, null); private final AtomicReference\u003cNode\u003cE\u003e\u003e head = new AtomicReference\u003c\u003e(dummy); private final AtomicReference\u003cNode\u003cE\u003e\u003e tail = new AtomicReference\u003c\u003e(dummy); public boolean put(E item) { Node\u003cE\u003e newNode = new Node\u003c\u003e(item, null); while (true) { Node\u003cE\u003e curTail = tail.get(); Node\u003cE\u003e tailNext = curTail.next.get(); if (curTail == tail.get()) { if (tailNext != null) { // 队列处于中间状态，推进尾节点 tail.compareAndSet(curTail, tailNext); } else { // 处于稳定状态，尝试插入新节点 if (curTail.next.compareAndSet(null, newNode)) { // 插入操作成功，尝试推进尾节点 tail.compareAndSet(curTail, newNode); return true; } } } } } public E take() { while (true) { if (head.get() == tail.get()) { return null; } Node\u003cE\u003e oldHead = head.get(); Node\u003cE\u003e newHead = oldHead.next.get(); // 队列处于中间状态，可能另外一个线程已经 CAS 成功， 只剩下一个元素 dummy 了 if (newHead == null) { return null; } if (head.compareAndSet(oldHead, newHead)) { oldHead.next = null; return oldHead.item; } } } private static class Node\u003cE\u003e { private final E item; private AtomicReference\u003cNode\u003cE\u003e\u003e next; public Node(E item, Node\u003cE\u003e next) { this.item = item; this.next = new AtomicReference\u003c\u003e(next); } } } ","date":"2022-11-16 08:00:00","objectID":"/ooooo-notes/%E5%9C%A8-java-%E4%B8%AD%E4%BD%BF%E7%94%A8-cas-%E6%9D%A5%E5%AE%9E%E7%8E%B0%E9%98%9F%E5%88%97/:1:0","tags":["java"],"title":"在 java 中使用 CAS 来实现队列","uri":"/ooooo-notes/%E5%9C%A8-java-%E4%B8%AD%E4%BD%BF%E7%94%A8-cas-%E6%9D%A5%E5%AE%9E%E7%8E%B0%E9%98%9F%E5%88%97/"},{"categories":["微信文章"],"content":"2. 代码实现位置 github 地址 ","date":"2022-11-16 08:00:00","objectID":"/ooooo-notes/%E5%9C%A8-java-%E4%B8%AD%E4%BD%BF%E7%94%A8-cas-%E6%9D%A5%E5%AE%9E%E7%8E%B0%E9%98%9F%E5%88%97/:2:0","tags":["java"],"title":"在 java 中使用 CAS 来实现队列","uri":"/ooooo-notes/%E5%9C%A8-java-%E4%B8%AD%E4%BD%BF%E7%94%A8-cas-%E6%9D%A5%E5%AE%9E%E7%8E%B0%E9%98%9F%E5%88%97/"},{"categories":["微信文章"],"content":"1. 使用 Lock 来实现 Semaphore 代码： Semaphore 的功能就是允许同时有几个线程操作 acquire 方法，permit 会减一，如果为 0，则线程需要等待 release 方法，permit 会加一，唤醒等待的线程 public class SemaphoreOnLock { private final ReentrantLock lock = new ReentrantLock(); private final Condition condition = lock.newCondition(); private int permit; public SemaphoreOnLock(int permit) { this.permit = permit; } /** * 获取锁 */ public void acquire() { lock.lock(); try { while (permit \u003c= 0) { condition.await(); } permit--; } catch (InterruptedException ignored) { } finally { lock.unlock(); } } public void release() { lock.lock(); try { permit++; condition.signal(); } finally { lock.unlock(); } } } ","date":"2022-11-14 08:00:00","objectID":"/ooooo-notes/%E5%9C%A8-java-%E4%B8%AD%E4%BD%BF%E7%94%A8-lock-%E6%9D%A5%E5%AE%9E%E7%8E%B0-semaphore/:1:0","tags":["java"],"title":"在 java 中使用 Lock 来实现 Semaphore","uri":"/ooooo-notes/%E5%9C%A8-java-%E4%B8%AD%E4%BD%BF%E7%94%A8-lock-%E6%9D%A5%E5%AE%9E%E7%8E%B0-semaphore/"},{"categories":["微信文章"],"content":"2. 代码实现位置 github 地址 ","date":"2022-11-14 08:00:00","objectID":"/ooooo-notes/%E5%9C%A8-java-%E4%B8%AD%E4%BD%BF%E7%94%A8-lock-%E6%9D%A5%E5%AE%9E%E7%8E%B0-semaphore/:2:0","tags":["java"],"title":"在 java 中使用 Lock 来实现 Semaphore","uri":"/ooooo-notes/%E5%9C%A8-java-%E4%B8%AD%E4%BD%BF%E7%94%A8-lock-%E6%9D%A5%E5%AE%9E%E7%8E%B0-semaphore/"},{"categories":["微信文章"],"content":"1. 使用数组来实现栈 代码： 用数组来实现 用 CTL 来控制 测试类，参考 ConcurrentStackUsingArrayTest public class ConcurrentStackUsingArray\u003cE\u003e { private final AtomicInteger CTL = new AtomicInteger(0); private final AtomicReference\u003cE[]\u003e arr = new AtomicReference\u003c\u003e((E[]) new Object[10]); private final AtomicInteger index = new AtomicInteger(0); public void push(E e) { while (!CTL.compareAndSet(0, 1)) { Thread.yield(); } while (index.get() \u003e= arr.get().length) { E[] oldArr = arr.get(); E[] newArr = (E[]) new Object[oldArr.length * 2]; System.arraycopy(oldArr, 0, newArr, 0, oldArr.length); if (arr.compareAndSet(oldArr, newArr)) { break; } } arr.get()[index.getAndIncrement()] = e; CTL.lazySet(0); } public E pop() { while (!CTL.compareAndSet(0, 1)) { Thread.yield(); } E e = arr.get()[index.decrementAndGet()]; CTL.lazySet(0); return e; } } ","date":"2022-11-13 08:00:00","objectID":"/ooooo-notes/%E5%9C%A8-java-%E4%B8%AD%E4%BD%BF%E7%94%A8-cas-%E6%9D%A5%E5%AE%9E%E7%8E%B0%E6%A0%882/:1:0","tags":["java"],"title":"在 java 中使用 CAS 来实现栈2","uri":"/ooooo-notes/%E5%9C%A8-java-%E4%B8%AD%E4%BD%BF%E7%94%A8-cas-%E6%9D%A5%E5%AE%9E%E7%8E%B0%E6%A0%882/"},{"categories":["微信文章"],"content":"2. 代码实现位置 github 地址 ","date":"2022-11-13 08:00:00","objectID":"/ooooo-notes/%E5%9C%A8-java-%E4%B8%AD%E4%BD%BF%E7%94%A8-cas-%E6%9D%A5%E5%AE%9E%E7%8E%B0%E6%A0%882/:2:0","tags":["java"],"title":"在 java 中使用 CAS 来实现栈2","uri":"/ooooo-notes/%E5%9C%A8-java-%E4%B8%AD%E4%BD%BF%E7%94%A8-cas-%E6%9D%A5%E5%AE%9E%E7%8E%B0%E6%A0%882/"},{"categories":["微信文章"],"content":"1. java 多线程测试 在任何语言中，多线程测试都是比较困难的，在这里我介绍下 java 的多线程测试 jcstress. jcstress 是 OpenJDK 提供的一个测试多线程的框架 主要由多个 Actor 来构成，每个 Actor 就是一个线程。 通过匹配 Outcome 的结果来报告测试 运行之后的结果为 html 文件，需要你自己查看。 示例代码: 测试自旋锁，其实也告诉你该怎么编写 CAS 执行命令 gradle jcstress，会生成目录 build/reports/jcstress @JCStressTest @Outcome(id = {\"1, 2\", \"2, 1\"}, expect = Expect.ACCEPTABLE, desc = \"Mutex works\") @Outcome(id = \"1, 1\", expect = Expect.FORBIDDEN, desc = \"Mutex failure\") @State public class Mutex_03_SpinLock { private final AtomicBoolean taken = new AtomicBoolean(false); private int v; @Actor public void actor1(II_Result r) { while (taken.get() || !taken.compareAndSet(false, true)) ; // wait { // critical section r.r1 = ++v; } taken.set(false); } @Actor public void actor2(II_Result r) { while (taken.get() || !taken.compareAndSet(false, true)) ; // wait { // critical section r.r2 = ++v; } taken.set(false); } } ","date":"2022-11-12 09:00:00","objectID":"/ooooo-notes/%E5%9C%A8-java-%E4%B8%AD%E5%A6%82%E4%BD%95%E8%BF%9B%E8%A1%8C%E5%A4%9A%E7%BA%BF%E7%A8%8B%E6%B5%8B%E8%AF%95/:1:0","tags":["java"],"title":"在 java 中如何进行多线程测试","uri":"/ooooo-notes/%E5%9C%A8-java-%E4%B8%AD%E5%A6%82%E4%BD%95%E8%BF%9B%E8%A1%8C%E5%A4%9A%E7%BA%BF%E7%A8%8B%E6%B5%8B%E8%AF%95/"},{"categories":["微信文章"],"content":"2. 代码实现位置 github 地址 ","date":"2022-11-12 09:00:00","objectID":"/ooooo-notes/%E5%9C%A8-java-%E4%B8%AD%E5%A6%82%E4%BD%95%E8%BF%9B%E8%A1%8C%E5%A4%9A%E7%BA%BF%E7%A8%8B%E6%B5%8B%E8%AF%95/:2:0","tags":["java"],"title":"在 java 中如何进行多线程测试","uri":"/ooooo-notes/%E5%9C%A8-java-%E4%B8%AD%E5%A6%82%E4%BD%95%E8%BF%9B%E8%A1%8C%E5%A4%9A%E7%BA%BF%E7%A8%8B%E6%B5%8B%E8%AF%95/"},{"categories":["微信文章"],"content":"3. 参考 强烈建议大家看官方代码, 地址: https://github.com/openjdk/jcstress ","date":"2022-11-12 09:00:00","objectID":"/ooooo-notes/%E5%9C%A8-java-%E4%B8%AD%E5%A6%82%E4%BD%95%E8%BF%9B%E8%A1%8C%E5%A4%9A%E7%BA%BF%E7%A8%8B%E6%B5%8B%E8%AF%95/:3:0","tags":["java"],"title":"在 java 中如何进行多线程测试","uri":"/ooooo-notes/%E5%9C%A8-java-%E4%B8%AD%E5%A6%82%E4%BD%95%E8%BF%9B%E8%A1%8C%E5%A4%9A%E7%BA%BF%E7%A8%8B%E6%B5%8B%E8%AF%95/"},{"categories":["微信文章"],"content":"1. 实现简单的 CAS 例子 CAS 相信大家都听过，就是 compareAndSet(V expectedValue, V newValue), 真正会用的人很少，这里的难点主要是无阻塞算法。 先实现一个简单 CAS 例子，只具有学习的意义。 getValue: 获取值 compareAndSet: 比较旧值，设置新值 public class SimulatedCAS { private int value; public SimulatedCAS(int value) { this.value = value; } public synchronized int getValue() { return value; } public synchronized boolean compareAndSet(int expectedValue, int newValue) { if (expectedValue == value) { this.value = newValue; return true; } return false; } } 重点：真正用 CAS 的时候，都是 while 循环 ","date":"2022-11-09 08:00:00","objectID":"/ooooo-notes/%E5%9C%A8-java-%E4%B8%AD%E4%BD%BF%E7%94%A8-cas-%E6%9D%A5%E5%AE%9E%E7%8E%B0%E6%A0%88/:1:0","tags":["java"],"title":"在 java 中使用 CAS 来实现栈","uri":"/ooooo-notes/%E5%9C%A8-java-%E4%B8%AD%E4%BD%BF%E7%94%A8-cas-%E6%9D%A5%E5%AE%9E%E7%8E%B0%E6%A0%88/"},{"categories":["微信文章"],"content":"2. 用 CAS 来实现一个栈 代码： 用链表来实现，当然用数组实现也可以，比较麻烦一点，后面我再写一个示例 每次操作都是先 get 来获取 top 对象，然后再 compareAndSet top public class ConcurrentStack\u003cE\u003e { private final AtomicReference\u003cNode\u003cE\u003e\u003e top = new AtomicReference\u003c\u003e(); public void push(E e) { Node\u003cE\u003e newHead = new Node\u003c\u003e(e); Node\u003cE\u003e oldHead; do { oldHead = top.get(); newHead.next = oldHead; } while (!top.compareAndSet(oldHead, newHead)); } public E pop() { Node\u003cE\u003e oldHead; Node\u003cE\u003e newHead; do { oldHead = top.get(); if (oldHead == null) { return null; } newHead = oldHead.next; } while (!top.compareAndSet(oldHead, newHead)); oldHead.next = null; return oldHead.e; } private static class Node\u003cE\u003e { private final E e; private Node\u003cE\u003e next; public Node(E e) { this.e = e; } } } ","date":"2022-11-09 08:00:00","objectID":"/ooooo-notes/%E5%9C%A8-java-%E4%B8%AD%E4%BD%BF%E7%94%A8-cas-%E6%9D%A5%E5%AE%9E%E7%8E%B0%E6%A0%88/:2:0","tags":["java"],"title":"在 java 中使用 CAS 来实现栈","uri":"/ooooo-notes/%E5%9C%A8-java-%E4%B8%AD%E4%BD%BF%E7%94%A8-cas-%E6%9D%A5%E5%AE%9E%E7%8E%B0%E6%A0%88/"},{"categories":["微信文章"],"content":"3. 代码实现位置 github 地址 ","date":"2022-11-09 08:00:00","objectID":"/ooooo-notes/%E5%9C%A8-java-%E4%B8%AD%E4%BD%BF%E7%94%A8-cas-%E6%9D%A5%E5%AE%9E%E7%8E%B0%E6%A0%88/:3:0","tags":["java"],"title":"在 java 中使用 CAS 来实现栈","uri":"/ooooo-notes/%E5%9C%A8-java-%E4%B8%AD%E4%BD%BF%E7%94%A8-cas-%E6%9D%A5%E5%AE%9E%E7%8E%B0%E6%A0%88/"},{"categories":["english"],"content":"lesson 1, 2022.11.1 Last Week I Went to the theatre, I had a very good seat, the play was very interesting.I did not enjoy it. The young man and young woman was sitting behind me.They were talking loudly. I got very angry. I could not hear actors. I turned round. I looked at the young man and young woman angrily. They did not pay any attention. In the end. I could not bear it. I turned round again. “I can’t hear a word” I said angrily. “it’s none of your business.” the young man said rudely. “This is a private conversation”. ","date":"2022-11-01 08:00:00","objectID":"/ooooo-notes/english/02/:1:0","tags":["english"],"title":"新概念英语二","uri":"/ooooo-notes/english/02/"},{"categories":["english"],"content":"lesson 2, 2022.11.5 It was Sunday, I never get up early on the Sundays. I sometimes stay in bed until lunchtime. Last Sunday. I got up very late. I looked out of the window, It was dark outside. “What a day”, I thought. “It’s raining again.” Just then, the telephone rang. It was my Aunt Lucy. “I’ve just arrived by train”, she said. “I’m coming to see you”. “But I’m still having breakfast” I said. “What are you doing?” she asked. “I’m having breakfast”. I repeated. “Dear me” she said. “Do you always get up so late”, “It’s one o’clock.”. ","date":"2022-11-01 08:00:00","objectID":"/ooooo-notes/english/02/:2:0","tags":["english"],"title":"新概念英语二","uri":"/ooooo-notes/english/02/"},{"categories":["english"],"content":"lesson 3, 2028.11.7 Postcards always spoil my holidays. Last Summer, I went to go Italy. I visited the museums and sat in public gardens. The friendly waiter taught me a few words of Italian, then he lent me a book. I read a few lines, but I did not understand a word. Every day I thought about postcards. My holidays passed quickly. I did not send cards to my friends. On the last day I made a big decision. I got up early and bought thirty-seven cards. I spent the whole day in my room. but I did not write a single card. ","date":"2022-11-01 08:00:00","objectID":"/ooooo-notes/english/02/:3:0","tags":["english"],"title":"新概念英语二","uri":"/ooooo-notes/english/02/"},{"categories":["english"],"content":"lesson 4, 2022.11.7 I’ve just received a letter from my brother. Tim. He is in Australia.He has been there for six months. He is an engineer. He is working for a big firm and has already visited a great number of different places in Australia. he has just bought an Australian car and has gone to Alice Springs, a small town in the centre of Australia. He will soon visit Darwin, From there, He will fly to Perth, My brother has never been aboard before. so he is finding this trip very exciting. ","date":"2022-11-01 08:00:00","objectID":"/ooooo-notes/english/02/:4:0","tags":["english"],"title":"新概念英语二","uri":"/ooooo-notes/english/02/"},{"categories":["english"],"content":"lesson 5, 2022.11.8 Mr. James Scott has a garage in Silbury and now he has just bought another garage in Pinhurst, Pinhurst is only five miles from Silbury, but Mr. Scott cannot get a telephone for his new garage. so he has just bought twelve pigeons. Yesterday, A pigeon carried the first message from Silbury to Pinhurst, the bird covered the distance in three minutes. Up to now, Mr. scott has sent a great many requests for spare parts and other urgent messages from one garage to the other, In this way, he has begun his own private ’telephone’ service. ","date":"2022-11-01 08:00:00","objectID":"/ooooo-notes/english/02/:5:0","tags":["english"],"title":"新概念英语二","uri":"/ooooo-notes/english/02/"},{"categories":["english"],"content":"lesson 6, 2022.11.9 I have just moved to a house in Bridge street. Yesterday, a beggar knocked at my door, he asked me for a meal and a glass of beer. In return for this, the beggar stood on his head and sang songs. I gave him a meal, he ate the food and drank the beer. Then he put a piece of cheese in his pocket and went away. Later a neighbour told me about him, Everybody knows him, his name is Percy Buttons. he calls at every house in the street once a month and always asks for a meal and a glass of beer. ","date":"2022-11-01 08:00:00","objectID":"/ooooo-notes/english/02/:6:0","tags":["english"],"title":"新概念英语二","uri":"/ooooo-notes/english/02/"},{"categories":["english"],"content":"lesson 7, 2022.11.10 The plane was late and detectives were waiting at the airport all morning. They were expecting a valuable parcel of diamonds from South Africa. A few hours earlier, someone had told the police that thieves would try to steal the diamonds. When the plane arrived, some of the detectives were waiting inside the main building while others were waiting on the airfield. Two men took the parcel off plane and carried it into Customs House. When two detectives were keeping guard at the door, two others opened the parcel. to their surprise, the precious parcel was full of stones and sand. ","date":"2022-11-01 08:00:00","objectID":"/ooooo-notes/english/02/:7:0","tags":["english"],"title":"新概念英语二","uri":"/ooooo-notes/english/02/"},{"categories":["english"],"content":"lesson 8, 2022.11.12 Joe has the most beautiful garden in our town. Nearly everybody enters for “the nicest garden competition” each year, but Joe wins every time. Bill’s garden is larger than Joe’s. Bill works harder than Joe and grows more flowers and vegetables, but Joe’s garden is more interesting. He has made neat paths and has built a wooden bridge over a pool. I like gardens too, but I do not like hard work. Every year I enter for the garden competition too, and I always win a little prize for the worst garden in the town. ","date":"2022-11-01 08:00:00","objectID":"/ooooo-notes/english/02/:8:0","tags":["english"],"title":"新概念英语二","uri":"/ooooo-notes/english/02/"},{"categories":["english"],"content":"lesson 9, 2022.11.14 On Wednesday evening, we went to the Town Hall. It was the last day of the year and a large crowd of people had gathered under the Town Hall clock. It would strike twelve in twenty minutes’ time. Fifteen minutes passed and then, at five to twelve, the clock stopped. The big minute hand did not move. We waited and waited, but nothing happened. Suddenly someone shouted, “It’s two minutes past twelve, the clock has stopped.” I looked at my watch, it was true. The big clock refused to welcome the New Year. At that moment, everyone began to laugh and sing. ","date":"2022-11-01 08:00:00","objectID":"/ooooo-notes/english/02/:9:0","tags":["english"],"title":"新概念英语二","uri":"/ooooo-notes/english/02/"},{"categories":["english"],"content":"lesson 10, 2022.11.16 We have an old musical instrument, it is called a clavichord. It was made in Germany in 1681. Our clavichord is kept in the living room. It has belonged to our family for a long time. The instrument was bought by my grandfather many years ago. Recently it was damaged by a visitor. She tried to play jazz on it. She struck the keys too hard and two of the strings were broken. My father was shocked. Now we are not allowed to touch it. It is being repaired by a friend of my father’s. ","date":"2022-11-01 08:00:00","objectID":"/ooooo-notes/english/02/:10:0","tags":["english"],"title":"新概念英语二","uri":"/ooooo-notes/english/02/"},{"categories":["english"],"content":"lesson 11, 2022.11.18 I was having dinner in a restaurant when Tony came in. Tony worked in a lawyer’s office years ago, but he is now working at a bank. He gets a good salary, but he always borrows money from his friends and never pays it back. Tony saw me and came and sat at the same table. He has never borrowed money from me. While he was eating, I asked him to lend me twenty pounds. To my surprise, he gave me the money immediately. “I have never borrowed any money from you”, Tony said, “so now you can pay for my dinner.” ","date":"2022-11-01 08:00:00","objectID":"/ooooo-notes/english/02/:11:0","tags":["english"],"title":"新概念英语二","uri":"/ooooo-notes/english/02/"},{"categories":["english"],"content":"lesson 12, 2022.11.20 Our neighbour, Captain Charles Alison, will sail from Portsmouth tomorrow. We’ll meet him at the harbour early in the morning. He will be in his small boat, Topsail. Topsail is a famous little boat. It has sailed across the Atlantic many times. Captain Alison will set out at eight o’clock, so we’ll have plenty of time. We’ll see his boat and then we’ll say goodbye to him. He will be away for two months. We are very proud of him. He will take part in an important race across the Atlantic. ","date":"2022-11-01 08:00:00","objectID":"/ooooo-notes/english/02/:12:0","tags":["english"],"title":"新概念英语二","uri":"/ooooo-notes/english/02/"},{"categories":["english"],"content":"lesson 13, 2022.11.21 The Greenwood boys are a group of pop singers. At present, they are visiting all parts of the country. They will be arriving here tomorrow. They will be coming by train and most of the young people in the town will be meeting them at the station. Tomorrow evening they will be singing at the workers’ club. The Greenwood boys will be staying for five days. During this time, they will give five performances. As usual, the police will have a difficult time. They will be trying to keep order. It is always same on these occasions. ","date":"2022-11-01 08:00:00","objectID":"/ooooo-notes/english/02/:13:0","tags":["english"],"title":"新概念英语二","uri":"/ooooo-notes/english/02/"},{"categories":["english"],"content":"lesson 14, 2022.11.22 I had an amusing experience last year. After I had left a small village in the south of French, I drove on to the next town. On the way, the young man waved to me. I stopped and he asked me for a lift. As soon as he had got into the car, I said good morning to him in French and he replied in the same language. Apart from a few words, I do not know any French at all. Neither of us spoke during the journey. I had nearly reached the town, when the young man suddenly said, very slowly, ‘Do you speck English ?’. As I soon learnt, he was English himself. ","date":"2022-11-01 08:00:00","objectID":"/ooooo-notes/english/02/:14:0","tags":["english"],"title":"新概念英语二","uri":"/ooooo-notes/english/02/"},{"categories":["english"],"content":"lesson 15, 2022.11.23 The secretary told me that Mr.xxx would see me. I felt very nervous when I went info his office. He did not look up from his desk when I entered. After I had sat down, he said that business was very bad. He told me that the firm could not afford to pay such large salaries. Twenty people had already left. I knew that my turn had come. ‘Mr. xxx’ I said in a weak voice. ‘Don’t interrupt’, he said. Then he smiled and he told me I would receive an extra thousand pounds a year. ","date":"2022-11-01 08:00:00","objectID":"/ooooo-notes/english/02/:15:0","tags":["english"],"title":"新概念英语二","uri":"/ooooo-notes/english/02/"},{"categories":["english"],"content":"lesson 16, 2022.11.27 If you park your car in the wrong place, the traffic policeman will soon find it. You will be very lucky if he lets you go without a ticket. However, this does not always happen. Traffic police are sometimes very polite. During a holidays in Sweden. I found this note on my car, ‘Sir, We welcome you to our city, This is a ‘No Parking’ area. You will enjoy your stay here if you pay attention to our street signs. This note is only a reminder’. If you receive a request like this, you cannot fail to obey it. ","date":"2022-11-01 08:00:00","objectID":"/ooooo-notes/english/02/:16:0","tags":["english"],"title":"新概念英语二","uri":"/ooooo-notes/english/02/"},{"categories":["english"],"content":"lesson 17, 2022.11.28 My aunt Jennifer is an actress. She must be at least thirty-five years old. In spite of this, she often appears on the stage as a young girl. Jennifer will have to take part in a new play soon. This time, she will be a girl of seventeen. In the play, she must appear in a bright red dress and long black stockings. Last year in another play, she had to wear short socks, and a bright orange-colored dress. If anyone ever asks her how old she is, she always answers, ‘Darling, it must be terrible to be grown up’. ","date":"2022-11-01 08:00:00","objectID":"/ooooo-notes/english/02/:17:0","tags":["english"],"title":"新概念英语二","uri":"/ooooo-notes/english/02/"},{"categories":["english"],"content":"lesson 18, 2022.11.30 After I had had lunch in a village pub, I looked for my bag. I had left it on a chair beside the door and now it wasn’t there. As I was looking for it, the landlord came in. ‘Did you have a good meal?’ he asked. ‘Yes, thank you’ I answered, ‘but I can’t pay the bill, I haven’t got my bag’. The landlord smiled and immediately went out. In a few minutes he returned with my bag and gave it back to me. ‘I’m very sorry’ he said, ‘My dog had taken it into garden’, He often does this. ","date":"2022-11-01 08:00:00","objectID":"/ooooo-notes/english/02/:18:0","tags":["english"],"title":"新概念英语二","uri":"/ooooo-notes/english/02/"},{"categories":["english"],"content":"lesson 19, 2022.12.3 ‘The play may begin at any moment’ I said. ‘It may have begun already’ Susan answered. I hurried to the ticket office, ‘May I have two tickets please?’ I asked. ‘I’m sorry, We’ve sold out’ the girl said. ‘What a pity!’ Susan exclaimed. Just then, a man hurried to the ticket office, ‘Can I returned these two ticket?’ he asked. ‘Certainly.’ the girl said. I went back to the ticket office at once. ‘Could I have those two tickets please?’ ‘Certainly, but they’re for next Wednesday’s performance. Do you still want them?’. ‘I might as well have them’ I said sadly. ","date":"2022-11-01 08:00:00","objectID":"/ooooo-notes/english/02/:19:0","tags":["english"],"title":"新概念英语二","uri":"/ooooo-notes/english/02/"},{"categories":["english"],"content":"lesson 20, 2022.12.5 Fishing is my favorite sport. I often fish for hours without catching anything. But this does not worry me. Some fishermen are unlucky. Instead of catching, they catch old boots and rubbish. I am even less unlucky. I never catch anything, not even old boots. After having spent whole mornings on the river, I always go home with an empty bag. ‘You must give up fishing.’ My friends say. ‘It’s a waste of time.’ But they don’t realize one important thing. I am not really interested in fishing. I am only interested in sitting in a boat and doing nothing at all. ","date":"2022-11-01 08:00:00","objectID":"/ooooo-notes/english/02/:20:0","tags":["english"],"title":"新概念英语二","uri":"/ooooo-notes/english/02/"},{"categories":["english"],"content":"lesson 21, 2022.12.8 Aeroplanes are slowly driving me mad. I live near an airport and passing planes can be heard night and day. The airport was built years ago, but for some seasons it could not be used then. Last year, however, it came into use. Over a hundred people must have been driven away from their homes by the noise. I am one of the few people left. Sometimes I think this house will be knocked down by a passing plane. I have been offered a large sum of money to go away. but I am determined to stay here. Everyone says I must be mad and they are probably right. ","date":"2022-11-01 08:00:00","objectID":"/ooooo-notes/english/02/:21:0","tags":["english"],"title":"新概念英语二","uri":"/ooooo-notes/english/02/"},{"categories":["english"],"content":"lesson 22, 2022.12.12 My daughter, Jane, never dreamed of receiving a letter from a girl of her own age in Holland. Last year, we were traveling across the Channel and Jane put a piece of paper with her name and address on it into a bottle. She threw the bottle into the sea. Jane never thought of it again, but ten months later, she received a letter from a girl in Holland. Both girls write to each other regularly now. However, they have decided to use the post office. Letters will cost a little more, but they will certainly travel faster. ","date":"2022-11-01 08:00:00","objectID":"/ooooo-notes/english/02/:22:0","tags":["english"],"title":"新概念英语二","uri":"/ooooo-notes/english/02/"},{"categories":["english"],"content":"lesson 23, 2022.12.13 I had a letter from my sister yesterday. She lives in Nigeria. In her letter, she said me that she would come to England next year. If she comes, she will get a surprise. We are now living in a beautiful new house in the country. Work on it had begun before my sister left. The house was completed five months ago. In my letter, I told her that she could stay with us. The house has many larger rooms and there is a lovely garden. It is a very modern house, so it looks strange to some people. It must be the only modern house in the district. ","date":"2022-11-01 08:00:00","objectID":"/ooooo-notes/english/02/:23:0","tags":["english"],"title":"新概念英语二","uri":"/ooooo-notes/english/02/"},{"categories":["english"],"content":"lesson 24, 2022.12.15 I entered a hotel manager’s office and sat down. I had just lost 50 and I felt very upset. “I left the money in my room”. I said, “and it’s not there now”. The manager was sympathetic, but he could do nothing. “Everyone’s losing money these days” he said. He started to complain about this wicked world, but was interrupted by a knock at the door. A girl came in and put an envelope on his desk. It contained $50. “I found this outside this gentleman’s room.” she said. “Well”, I said to the manager. “there is still some honesty in this world”. ","date":"2022-11-01 08:00:00","objectID":"/ooooo-notes/english/02/:24:0","tags":["english"],"title":"新概念英语二","uri":"/ooooo-notes/english/02/"},{"categories":["english"],"content":"lesson 25, 2022.12.16 I arrived in London at last. The railway station was big, black and dark. I didn’t know the way to my hotel, so I asked a porter. I not only spoke English very carefully, but very clearly as well. The porter, however, could not understand me. I repeated my question several times and at last he understood. He answered me, but he spoke neither slowly nor clearly. “I am a foreigner.” I said. Then he spoke slowly, but I could not understand him. My teacher never spoke English like that. The porter and I looked at each other and smiled. Then he said something and I understood it. “You’ll soon learn english” he said. I wonder. In England, each person speaks a different language. The English understand each other, but I don’t understand them. Do they speak English? ","date":"2022-11-01 08:00:00","objectID":"/ooooo-notes/english/02/:25:0","tags":["english"],"title":"新概念英语二","uri":"/ooooo-notes/english/02/"},{"categories":["english"],"content":"lesson 26, 2022.12.20 I am an art student and paint a lot of pictures. Many people pretend that they understand modern art. They always tell you what a picture is about. Of course, Many pictures are not about anything. They are just pretty patterns. We like them in the same way that we like pretty curtain material. I think that young children often appreciate modern pictures better than anyone else. The notice more. My sister is only seven，but she always tells me whether my pictures are good or not. She came into my room yesterday. “What are you doing?” she asked. “I’m hanging this picture on the wall” I answered. “It’s a new no, Do you like it?”. She looked at it critically for a moment. “It’s all right.” she said. “But isn’t it upside down?” I looked at it again. She was right！It was. ","date":"2022-11-01 08:00:00","objectID":"/ooooo-notes/english/02/:26:0","tags":["english"],"title":"新概念英语二","uri":"/ooooo-notes/english/02/"},{"categories":["english"],"content":"lesson 27, 2022.12.24 Late in the afternoon. The boys put up their tent in the middle of a field. As soon as this was done, they cooked a meal over an open fire. They were all hungry and the food smelled good. After a wonderful meal, they told stories and sang songs by the campfire. But some time later it began to rain. The boys felt tired, so they put out the fire and crept into their tent. Their sleeping bags were warm and comfortable, so they all slept soundly. In the middle of the night. two boys woke up and began shouting. The tent was full of water. They leapt out of their sleeping bags and hurried outside. It was raining heavily and they found that a stream had formed in the field. The stream wound its way across the field and then flowed right under their tent. ","date":"2022-11-01 08:00:00","objectID":"/ooooo-notes/english/02/:27:0","tags":["english"],"title":"新概念英语二","uri":"/ooooo-notes/english/02/"},{"categories":["english"],"content":"lesson 28, 2022.12.24 Jasper White is one of those rare people who believes in ancient myths. he has just bought a new house in the city. But ever since he moved in, he has had trouble with cars and their owners. When he returns home at night, he always finds that someone has parked a car outside his gate. Because of this, he has not been able to get his own car into his garage even once. Jasper has put up “No parking” signs outside his gate, but these have not had any effect. Now he has put an ugly stone head over the gate. It is one of the ugliest faces I have ever seen. I asked him what it was, he told me that it was Medusa, the Gorgon, Jasper hopes that she will turn cars and their owners to stone. But none of them has been turned to stone yet. ","date":"2022-11-01 08:00:00","objectID":"/ooooo-notes/english/02/:28:0","tags":["english"],"title":"新概念英语二","uri":"/ooooo-notes/english/02/"},{"categories":["english"],"content":"lesson 29, 2023.1.4 Captain Ben Fawcett has bought an unusual taxi and has begun a new service. The “taxi” is a small Swiss aeroplane called a “Pilatus Porter”. This wonderful plane can carry seven passengers. The most surprising thing about it, however, is that it can land anywhere, on snow, water or even on a ploughed field. Captain Fawcett’s first passenger was a doctor who flew from Birmingham to a lonely village in the Welsh mountains. Since then, Captain Fawcett has flown passengers to many unusual places. Once he landed on the roof of a block of flats and on another occasion he landed in a deserted car park. Captain Fawcett has just refused a strange request from a businessman. The man wanted to fly Rockall, a lonely island in the Atlantic Ocean, but Captain Fawcett did not take him, because the trip was too dangerous. ","date":"2022-11-01 08:00:00","objectID":"/ooooo-notes/english/02/:29:0","tags":["english"],"title":"新概念英语二","uri":"/ooooo-notes/english/02/"},{"categories":["english"],"content":"lesson 30. 2023.1.4 The Wayle is a small river that cuts across the park near my home. I like sitting by the Wayle on fine afternoons. It was warm last Sunday, so I went and sat on the river bank as usual. Some children were playing games on the bank and there were some people rowing on the river. Suddenly, one of the children kicked a ball very hard and it went towards a passing boat. Some people on the bank called out to the man in the boat, but he did not hear him. The ball struck him so hard that he nearly fell into the water. I turned to look at the children, but there weren’t any in sight. They had all run away. The man laughed when he realized what had happened, he called out to the children and threw the ball back to the bank. ","date":"2022-11-01 08:00:00","objectID":"/ooooo-notes/english/02/:30:0","tags":["english"],"title":"新概念英语二","uri":"/ooooo-notes/english/02/"},{"categories":["english"],"content":"lesson 31, 2023.1.6 Yesterday afternoon Frank was telling me about his experiences as a young man. Before he retired, he was the head of a large business company, but as a boy he used to work in a small shop. It is his job to repair bicycles and at that time he used to work fourteen hours a day. He saved money for years and in 1958 he bought a small workshop of his own. In his twenties Frank used to make spare parts for aeroplanes. At that time he had two helpers. In a few years the small workshop had become a large factory which employed seven hundred and twenty-eight people. Frank smiled when he remembered his hard early years and a long road to success. Frank was still smiling when the door opened and his wife came in. She wanted him to repair their grandson’s bicycle. ","date":"2022-11-01 08:00:00","objectID":"/ooooo-notes/english/02/:31:0","tags":["english"],"title":"新概念英语二","uri":"/ooooo-notes/english/02/"},{"categories":["english"],"content":"lesson 32, 2023.1.6 People are not so honest as they once were. The temptation to steal is greater than ever before, especially in large shops. The detective recently watched a well-dressed woman who always went into a large store on Monday mornings. One Monday, there were fewer people in the shop than usual when the woman came in, so it was easier for the detective to watch him. The woman first bought a few small articles. After a little time, she chose one of the most expensive dresses in the shop and handed it to an assistant who wrapped it up for her as quick as possible. Then the woman simply took the parcel and walked out of the shop without paying. When she was arrested, the detective found out that the shop assistant was her daughter. The girl “gave” her mother a free dress once a week. ","date":"2022-11-01 08:00:00","objectID":"/ooooo-notes/english/02/:32:0","tags":["english"],"title":"新概念英语二","uri":"/ooooo-notes/english/02/"},{"categories":["english"],"content":"lesson 33, 2023.1.10 Nearly a week passed before the girl was able to explain what had happened to her. One afternoon she set out from the coast in a small boat and was caught in a storm. Towards evening, the boat struck a rock and the girl jumped into the sea. Then she swam to the shore after spending the whole night in the water. During that time she covered a distance of eight miles. Early next morning, she saw a light ahead. She knew she was near the shore because the light was high up on the cliffs. On arriving at the shore, the girl struggled up the cliff towards the light she had seen. That was all she remembered. When she woke up a day later, she found herself in hospital. ","date":"2022-11-01 08:00:00","objectID":"/ooooo-notes/english/02/:33:0","tags":["english"],"title":"新概念英语二","uri":"/ooooo-notes/english/02/"},{"categories":["english"],"content":"lesson 34, 2023.1.10 Dan Robinson has been worried all week. Last Tuesday he received a letter from the local police. In the letter, he was asked to call at the station. Dan wondered why he was wanted by the police. but he went into the station yesterday and now he is not worried anymore. At the station, he was told by a smiling policeman that his bicycle had been found. Five days ago, the policeman told him, the bicycle was picked up in a small village four hundred miles away. It is now being sent to his home by train. Dan was most surprised when he heard the news. He was amused too, because he never expected the bicycle to be found. It was stolen twenty years ago when Dan was a boy of fifteen. ","date":"2022-11-01 08:00:00","objectID":"/ooooo-notes/english/02/:34:0","tags":["english"],"title":"新概念英语二","uri":"/ooooo-notes/english/02/"},{"categories":["english"],"content":"lesson 35, 2023.1.12 Roy used to drive a taxi. A short while ago, however, he became to a bus driver and he has not regretted it. He is finding his new work far more exciting. When he was driving along Carford street recently, he saw two thieves rush out of a shop and run towards a waiting car. One of them was carrying a bag full of money. Roy acted quickly and drove his bus straight at two thieves. The one with the money got such a fright that he dropped the bag. As two thieves were trying to get away in their car, Roy drove his bus into the back of it. While the battered car was moving away, Roy stopped his bus and telephoned the police. The thieves’ car was badly damaged and easy to recognize. Shortly afterwards, the police stopped the car and both men were arrested. ","date":"2022-11-01 08:00:00","objectID":"/ooooo-notes/english/02/:35:0","tags":["english"],"title":"新概念英语二","uri":"/ooooo-notes/english/02/"},{"categories":["微信文章"],"content":"grpc 使用方式 grpc 作为一个通信方式，现在可以说是非常流行。如果不会 grpc，你可能跟不上时代了, 这里我只是做一个很简单的例子，并说下如何进一步学习 grpc。 grpc 接口需要编写 .proto 文件，如下面的例子： 有一个接口类：Greeter. 有两个方法 SayHello, SayHi. syntax = \"proto3\"; option java_multiple_files = true; option java_package = \"com.ooooo.grpc.helloworld\"; option java_outer_classname = \"HelloWorldProto\"; package helloworld; // The greeting service definition. service Greeter { // Sends a greeting rpc SayHello (HelloRequest) returns (HelloReply) {} rpc SayHi (HelloRequest) returns (HelloReply) {} } // The request message containing the user's name. message HelloRequest { string name = 1; } // The response message containing the greetings message HelloReply { string message = 1; } 在这里，我很推荐大家看下，protobuf 是怎么编码的。 proto3, 官方地址： https://developers.google.com/protocol-buffers/docs/proto3 proto3 encoding, 官方地址： https://developers.google.com/protocol-buffers/docs/encoding 编写完 .proto 文件，执行 gradle 的 generateProto 任务, 就会生成相应的 java 代码。 然后编写入口程序 GrpcClient 和 GrpcServer。 ","date":"2022-10-22 09:00:00","objectID":"/ooooo-notes/%E5%9C%A8-java-%E4%B8%AD%E5%A6%82%E4%BD%95%E4%BD%BF%E7%94%A8-grpc/:1:0","tags":["java","grpc"],"title":"在 java 中如何使用 grpc","uri":"/ooooo-notes/%E5%9C%A8-java-%E4%B8%AD%E5%A6%82%E4%BD%95%E4%BD%BF%E7%94%A8-grpc/"},{"categories":["微信文章"],"content":"如何进一步学习 grpc 学习 grpc 如何使用，如何扩展，可以看 https://github.com/grpc/grpc-java/tree/master/examples. 在 spring-boot 中如何使用，有开源的 starter. grpc 是基于 http2 协议的，你必须熟悉 http2 协议。 更深入的学习，也就是学习源码，有时间给大家说下 grpc 的源码，也是比较简单的。 ","date":"2022-10-22 09:00:00","objectID":"/ooooo-notes/%E5%9C%A8-java-%E4%B8%AD%E5%A6%82%E4%BD%95%E4%BD%BF%E7%94%A8-grpc/:2:0","tags":["java","grpc"],"title":"在 java 中如何使用 grpc","uri":"/ooooo-notes/%E5%9C%A8-java-%E4%B8%AD%E5%A6%82%E4%BD%95%E4%BD%BF%E7%94%A8-grpc/"},{"categories":["微信文章"],"content":"2. 代码实现位置 github 地址 ","date":"2022-10-22 09:00:00","objectID":"/ooooo-notes/%E5%9C%A8-java-%E4%B8%AD%E5%A6%82%E4%BD%95%E4%BD%BF%E7%94%A8-grpc/:3:0","tags":["java","grpc"],"title":"在 java 中如何使用 grpc","uri":"/ooooo-notes/%E5%9C%A8-java-%E4%B8%AD%E5%A6%82%E4%BD%95%E4%BD%BF%E7%94%A8-grpc/"},{"categories":["微信文章"],"content":"1. 添加打印 SQL 的方式 打印 SQL 的方式有很多，比如有 idea 插件，有 mybatis 拦截器，有代理 datasource, 有代理 driver. 我比较认可的方式就是代理 driver. 这种无任何侵入性。 下面来介绍如何使用 p6spy Driver。 示例代码： 使用 BeanPostProcessor 来动态扩展 bean。 判断 bean 是否为 DataSource 类型，并判断开发配置 dev.sql-log.enabled。 根据现有的 driver 配置来创建新的 datasource，并设置 url。 实际开启，还需要 spy.properties 配置文件 @Slf4j @Configuration public class DevDataSourceConfiguration implements BeanPostProcessor, EnvironmentAware { @Setter private Environment environment; @Override public Object postProcessAfterInitialization(Object bean, String beanName) throws BeansException { if (bean instanceof DataSource \u0026\u0026 parseBoolean(environment.resolvePlaceholders(\"${dev.sql-log.enabled:false}\"))) { log.info(\"已开启日志打印，将使用[P6SpyDriver]\"); if (environment.acceptsProfiles(Profiles.of(\"run\"))) { log.warn(\"在生产环境一定要关闭配置[dev.sql-log.enabled]\"); } return proxyDataSource((DataSource) bean); } return bean; } public DataSource proxyDataSource(DataSource dataSource) { if (dataSource instanceof AbstractRoutingDataSource) { AbstractRoutingDataSource abstractRoutingDataSource = (AbstractRoutingDataSource) dataSource; // resolvedDataSources Field resolvedDataSourcesField = findField(AbstractRoutingDataSource.class, \"resolvedDataSources\"); makeAccessible(resolvedDataSourcesField); @SuppressWarnings(\"unchecked\") Map\u003cObject, DataSource\u003e resolvedDataSources = (Map\u003cObject, DataSource\u003e) getField(resolvedDataSourcesField, abstractRoutingDataSource); if (resolvedDataSources != null) { resolvedDataSources.forEach((k, v) -\u003e resolvedDataSources.put(k, convertToProxyDataSource(v))); } // resolvedDefaultDataSource Field resolvedDefaultDataSourceField = findField(AbstractRoutingDataSource.class, \"resolvedDefaultDataSource\"); makeAccessible(resolvedDefaultDataSourceField); DataSource resolvedDefaultDataSource = (DataSource) getField(resolvedDefaultDataSourceField, abstractRoutingDataSource); if (resolvedDefaultDataSource != null) { ReflectionUtils.setField(resolvedDefaultDataSourceField, abstractRoutingDataSource, convertToProxyDataSource(resolvedDefaultDataSource)); } return abstractRoutingDataSource; } return convertToProxyDataSource(dataSource); } public DataSource convertToProxyDataSource(DataSource dataSource) { if (dataSource instanceof HikariDataSource) { HikariConfig oldConfig = (HikariDataSource) dataSource; // jdbc:h2:mem:test to jdbc:p6spy:h2:mem:test String jdbcUrl = oldConfig.getJdbcUrl(); if (!jdbcUrl.contains(\"p6spy\")) { jdbcUrl = \"jdbc:p6spy\" + jdbcUrl.substring(4); } HikariConfig newConfig = new HikariConfig(); newConfig.setPoolName(\"proxy-P6SpyDriver\"); newConfig.setDriverClassName(\"com.p6spy.engine.spy.P6SpyDriver\"); newConfig.setJdbcUrl(jdbcUrl); newConfig.setUsername(oldConfig.getUsername()); newConfig.setPassword(oldConfig.getPassword()); return new HikariDataSource(newConfig); } return dataSource; } } ","date":"2022-10-22 09:00:00","objectID":"/ooooo-notes/%E5%9C%A8-java-%E4%B8%AD%E5%A6%82%E4%BD%95%E6%B7%BB%E5%8A%A0-sql-%E6%97%A5%E5%BF%97/:1:0","tags":["java"],"title":"在 java 中如何添加 SQL 日志","uri":"/ooooo-notes/%E5%9C%A8-java-%E4%B8%AD%E5%A6%82%E4%BD%95%E6%B7%BB%E5%8A%A0-sql-%E6%97%A5%E5%BF%97/"},{"categories":["微信文章"],"content":"2. 代码实现位置 github 地址 ","date":"2022-10-22 09:00:00","objectID":"/ooooo-notes/%E5%9C%A8-java-%E4%B8%AD%E5%A6%82%E4%BD%95%E6%B7%BB%E5%8A%A0-sql-%E6%97%A5%E5%BF%97/:2:0","tags":["java"],"title":"在 java 中如何添加 SQL 日志","uri":"/ooooo-notes/%E5%9C%A8-java-%E4%B8%AD%E5%A6%82%E4%BD%95%E6%B7%BB%E5%8A%A0-sql-%E6%97%A5%E5%BF%97/"},{"categories":["微信文章"],"content":"1. 自定义 classloader 有时候，我们在项目开发的时候，会遇到比较恶心的问题，存在两个不同 jar 包，但是类的全限定名是一样的，而这两个包都不能删除，这时候调用可能就会出问题。 如何解决上面的问题？ 我的答案就是自定义类加载器。 场景模拟 module-a: 表示 a.jar module-b: 表示 b.jar module-main: 表示 程序入口 由于在 module-main 项目中同时引入了 module-a 和 module-b 这两个 jar 包，但是存在冲突类 HelloService, 最终导致程序运行错误。 如何自定义 classloader，来解决问题？ 重新定义 loadClass 方法，打破父类委托机制 使用 getResources 方法来获取所有的 class 文件，然后判断 @SneakyThrows public String test1(String message) { // 查找类 ClassLoader classLoader = new ModuleAClassLoader(); Class\u003c?\u003e clazz = classLoader.loadClass(\"com.ooooo.HelloService\"); // 执行 Method test1 = ReflectionUtils.findMethod(clazz, \"test1\", String.class); Object result = test1.invoke(null, message); return (String) result; } @SneakyThrows public String test2(String message) { // 查找类 ClassLoader classLoader = new ModuleBClassLoader(); Class\u003c?\u003e clazz = classLoader.loadClass(\"com.ooooo.HelloService\"); // 执行 Method test2 = ReflectionUtils.findMethod(clazz, \"test2\", String.class); Object result = test2.invoke(null, message); return (String) result; } private static class ModuleAClassLoader extends ClassLoader { public ModuleAClassLoader() { super(ModuleAClassLoader.class.getClassLoader()); } @SneakyThrows @Override protected Class\u003c?\u003e loadClass(String name, boolean resolve) throws ClassNotFoundException { Class\u003c?\u003e c = findLoadedClass(name); if (c == null) { // 当前路径下去找 if (name.contains(\"com.ooooo\")) { String path = name.replace(\".\", \"/\") + \".class\"; Enumeration\u003cURL\u003e resources = getResources(path); URL targetUrl = null; while (resources.hasMoreElements()) { targetUrl = resources.nextElement(); if (targetUrl.toString().contains(\"module-a\")) { break; } } // 读取 class 文件 InputStream in = targetUrl.openStream(); byte[] bytes = StreamUtils.copyToByteArray(in); in.close(); c = defineClass(name, bytes, 0, bytes.length); } } if (c == null) { c = getParent().loadClass(name); } if (resolve) { resolveClass(c); } return c; } } private static class ModuleBClassLoader extends ClassLoader { public ModuleBClassLoader() { super(ModuleAClassLoader.class.getClassLoader()); } @SneakyThrows @Override protected Class\u003c?\u003e loadClass(String name, boolean resolve) throws ClassNotFoundException { Class\u003c?\u003e c = findLoadedClass(name); if (c == null) { // 当前路径下去找 if (name.contains(\"com.ooooo\")) { String path = name.replace(\".\", \"/\") + \".class\"; Enumeration\u003cURL\u003e resources = getResources(path); URL targetUrl = null; while (resources.hasMoreElements()) { targetUrl = resources.nextElement(); if (targetUrl.toString().contains(\"module-b\")) { break; } } // 读取 class 文件 InputStream in = targetUrl.openStream(); byte[] bytes = StreamUtils.copyToByteArray(in); in.close(); c = defineClass(name, bytes, 0, bytes.length); } } if (c == null) { c = getParent().loadClass(name); } if (resolve) { resolveClass(c); } return c; } } } ","date":"2022-10-18 09:00:00","objectID":"/ooooo-notes/%E5%9C%A8-java-%E4%B8%AD%E5%A6%82%E4%BD%95%E8%A7%A3%E5%86%B3%E7%B1%BB%E5%86%B2%E7%AA%81%E9%97%AE%E9%A2%98/:1:0","tags":["java"],"title":"在 java 中如何解决类冲突问题","uri":"/ooooo-notes/%E5%9C%A8-java-%E4%B8%AD%E5%A6%82%E4%BD%95%E8%A7%A3%E5%86%B3%E7%B1%BB%E5%86%B2%E7%AA%81%E9%97%AE%E9%A2%98/"},{"categories":["微信文章"],"content":"2. 代码实现位置 github 地址 ","date":"2022-10-18 09:00:00","objectID":"/ooooo-notes/%E5%9C%A8-java-%E4%B8%AD%E5%A6%82%E4%BD%95%E8%A7%A3%E5%86%B3%E7%B1%BB%E5%86%B2%E7%AA%81%E9%97%AE%E9%A2%98/:2:0","tags":["java"],"title":"在 java 中如何解决类冲突问题","uri":"/ooooo-notes/%E5%9C%A8-java-%E4%B8%AD%E5%A6%82%E4%BD%95%E8%A7%A3%E5%86%B3%E7%B1%BB%E5%86%B2%E7%AA%81%E9%97%AE%E9%A2%98/"},{"categories":["微信文章"],"content":"1. methodHandle 调用 在过去，我们调用一个类的方法，除了直接调用，再就是使用反射来调用了。而今天我要说说 jdk 新引入的方式来调用。 比如我们有一个很简单的 UserService 类。 public class UserService { public String getUsername(String id) { return \"username\" + id; } } 直接调用和反射调用的方式比较简单，我就不说明了。 下面来演示 invoke 包的使用。 使用 MethodHandles.lookup() 来查找对应的方法 使用 methodHandle 来调用，分为几种不同的方式 public class UserServiceTest { private MethodType methodType; private Lookup lookup; private MethodHandle methodHandle; @SneakyThrows @BeforeEach public void beforeEach() { methodType = MethodType.methodType(String.class, String.class); lookup = MethodHandles.lookup(); methodHandle = lookup.findVirtual(UserService.class, \"getUsername\", methodType); } @SneakyThrows @Test public void invokeWithArguments() { UserService userService = new UserService(); Object obj = methodHandle.bindTo(userService).invokeWithArguments(\"1\"); assertEquals(\"username1\", obj); } @SneakyThrows @Test public void invoke() { UserService userService = new UserService(); Object obj = methodHandle.invoke(userService, \"1\"); assertEquals(\"username1\", obj); } /** * 要求类型全匹配, 包括返回值类型 */ @SneakyThrows @Test public void invokeExact() { UserService userService = new UserService(); String s = (String) methodHandle.invokeExact(userService, \"1\"); assertEquals(\"username1\", s); } } ","date":"2022-10-17 09:00:00","objectID":"/ooooo-notes/%E5%A6%82%E4%BD%95%E4%BD%BF%E7%94%A8-java-%E4%B8%AD-invoke-%E5%8C%85/:1:0","tags":["java"],"title":"如何使用 java 中 invoke 包?","uri":"/ooooo-notes/%E5%A6%82%E4%BD%95%E4%BD%BF%E7%94%A8-java-%E4%B8%AD-invoke-%E5%8C%85/"},{"categories":["微信文章"],"content":"2. 代码实现位置 github 地址 ","date":"2022-10-17 09:00:00","objectID":"/ooooo-notes/%E5%A6%82%E4%BD%95%E4%BD%BF%E7%94%A8-java-%E4%B8%AD-invoke-%E5%8C%85/:2:0","tags":["java"],"title":"如何使用 java 中 invoke 包?","uri":"/ooooo-notes/%E5%A6%82%E4%BD%95%E4%BD%BF%E7%94%A8-java-%E4%B8%AD-invoke-%E5%8C%85/"},{"categories":["微信文章"],"content":"1. jmh 微基准测试 实际上，在 java 中进行微基椎测试并不容易，主要原因在于解释执行，编译执行，而编译执行又分为 C1编译, C2编译。即使是对同一个代码来说，不同的 jvm 参数也会导致测试不一样。 那是否应该了解微基准测试？ 我的答案，是必须掌握的。 jmh 就是我们应该学习的微基准测试的框架。 下面我以一个示例来说明如何快速上手测试，测试 StringBuilder， StringBuffer， String 连接字符的性能。 注意点： Fork 参数可以测试不同的 Jvm 参数。 输出结果的时间单位最好是纳秒。 测试模式可以自由选择，如果测试性能，最好选择平均时间。 如果是在 IDE 中，建议安装 jmh 插件来执行。 代码示例 // 预热的参数 @Warmup(time = 1) // 测试的参数 @Measurement(time = 1) // 可以添加 JVM 参数来测试 @Fork(value = 1) @State(Scope.Thread) @OutputTimeUnit(TimeUnit.NANOSECONDS) @BenchmarkMode(Mode.AverageTime) public class TestStringBenchmark { @Benchmark public String stringBuilder() { StringBuilder sb = new StringBuilder(); sb.append(\"hello\"); sb.append(\"world\"); return sb.toString(); } @Benchmark public String stringBuffer() { StringBuffer sb = new StringBuffer(); sb.append(\"hello\"); sb.append(\"world\"); return sb.toString(); } @Benchmark public String stringConcat() { return \"hello\" + \"world\"; } } ","date":"2022-10-16 09:00:00","objectID":"/ooooo-notes/%E5%9C%A8-java-%E4%B8%AD%E5%A6%82%E4%BD%95%E8%BF%9B%E8%A1%8C%E5%BE%AE%E5%9F%BA%E5%87%86%E6%B5%8B%E8%AF%95/:1:0","tags":["java"],"title":"在 java 中如何进行微基准测试 ?","uri":"/ooooo-notes/%E5%9C%A8-java-%E4%B8%AD%E5%A6%82%E4%BD%95%E8%BF%9B%E8%A1%8C%E5%BE%AE%E5%9F%BA%E5%87%86%E6%B5%8B%E8%AF%95/"},{"categories":["微信文章"],"content":"2. 代码实现位置 github 地址 ","date":"2022-10-16 09:00:00","objectID":"/ooooo-notes/%E5%9C%A8-java-%E4%B8%AD%E5%A6%82%E4%BD%95%E8%BF%9B%E8%A1%8C%E5%BE%AE%E5%9F%BA%E5%87%86%E6%B5%8B%E8%AF%95/:2:0","tags":["java"],"title":"在 java 中如何进行微基准测试 ?","uri":"/ooooo-notes/%E5%9C%A8-java-%E4%B8%AD%E5%A6%82%E4%BD%95%E8%BF%9B%E8%A1%8C%E5%BE%AE%E5%9F%BA%E5%87%86%E6%B5%8B%E8%AF%95/"},{"categories":["微信文章"],"content":"3. 参考 强烈建议大家看官方代码, 地址: https://github.com/openjdk/jmh ","date":"2022-10-16 09:00:00","objectID":"/ooooo-notes/%E5%9C%A8-java-%E4%B8%AD%E5%A6%82%E4%BD%95%E8%BF%9B%E8%A1%8C%E5%BE%AE%E5%9F%BA%E5%87%86%E6%B5%8B%E8%AF%95/:3:0","tags":["java"],"title":"在 java 中如何进行微基准测试 ?","uri":"/ooooo-notes/%E5%9C%A8-java-%E4%B8%AD%E5%A6%82%E4%BD%95%E8%BF%9B%E8%A1%8C%E5%BE%AE%E5%9F%BA%E5%87%86%E6%B5%8B%E8%AF%95/"},{"categories":["微信文章"],"content":"我们在开发过程中，经常会使用 lambda 函数式编程，这样会更加简单。 ","date":"2022-10-11 22:25:00","objectID":"/ooooo-notes/%E5%9C%A8-java-%E4%B8%AD%E5%A6%82%E4%BD%95%E8%8E%B7%E5%8F%96-lambda-%E7%9A%84%E6%96%B9%E6%B3%95%E5%90%8D%E7%A7%B0/:0:0","tags":["IDE"],"title":"在 java 中如何获取 lambda 的方法名称?","uri":"/ooooo-notes/%E5%9C%A8-java-%E4%B8%AD%E5%A6%82%E4%BD%95%E8%8E%B7%E5%8F%96-lambda-%E7%9A%84%E6%96%B9%E6%B3%95%E5%90%8D%E7%A7%B0/"},{"categories":["微信文章"],"content":"1. 使用方式 比如下面有一个很简单的 User 类。其中有一个属性 username @Data public class User { private String username; public String getPassword(String password) { return password; } } // 使用的方式 // user -\u003e user.getUsername() 等价于 User::getUsername Function\u003cUser, String\u003e getUsername1 = User::getUsername 比如我现在要使用 user -\u003e user.getUsername()， 这样的 lambda 表达式来获取一个 User 对象的 username 属性值。 我现在可以这样来获取 username 这个方法名称。 public class LambdaUtils { // 正常情况，要做缓存 public static \u003cT\u003e String resolveMethod(SFunction\u003cT, ?\u003e func) { SerializedLambda serializedLambda = resovle(func); String methodName = serializedLambda.getImplMethodName(); return methodName; } @SneakyThrows public static SerializedLambda resovle(SFunction\u003c?, ?\u003e func) { Method method = func.getClass().getDeclaredMethod(\"writeReplace\"); method.setAccessible(true); return (SerializedLambda) method.invoke(func); } } ","date":"2022-10-11 22:25:00","objectID":"/ooooo-notes/%E5%9C%A8-java-%E4%B8%AD%E5%A6%82%E4%BD%95%E8%8E%B7%E5%8F%96-lambda-%E7%9A%84%E6%96%B9%E6%B3%95%E5%90%8D%E7%A7%B0/:1:0","tags":["IDE"],"title":"在 java 中如何获取 lambda 的方法名称?","uri":"/ooooo-notes/%E5%9C%A8-java-%E4%B8%AD%E5%A6%82%E4%BD%95%E8%8E%B7%E5%8F%96-lambda-%E7%9A%84%E6%96%B9%E6%B3%95%E5%90%8D%E7%A7%B0/"},{"categories":["微信文章"],"content":"2. 代码实现位置 本节的内容，我叙述的不是很好，可能看的一脸懵, 使用过 mybatis-plus 的人，可能会有点印象 LambdaQueryWrapper， 推荐看下这个测试类 LambdaUtilsTests github 地址 ","date":"2022-10-11 22:25:00","objectID":"/ooooo-notes/%E5%9C%A8-java-%E4%B8%AD%E5%A6%82%E4%BD%95%E8%8E%B7%E5%8F%96-lambda-%E7%9A%84%E6%96%B9%E6%B3%95%E5%90%8D%E7%A7%B0/:2:0","tags":["IDE"],"title":"在 java 中如何获取 lambda 的方法名称?","uri":"/ooooo-notes/%E5%9C%A8-java-%E4%B8%AD%E5%A6%82%E4%BD%95%E8%8E%B7%E5%8F%96-lambda-%E7%9A%84%E6%96%B9%E6%B3%95%E5%90%8D%E7%A7%B0/"},{"categories":["微信文章"],"content":"如何设计一个连接池 ? ","date":"2022-09-13 10:20:00","objectID":"/ooooo-notes/%E5%A6%82%E4%BD%95%E8%AE%BE%E8%AE%A1%E4%B8%80%E4%B8%AA%E8%BF%9E%E6%8E%A5%E6%B1%A0/:0:0","tags":["java"],"title":"如何设计一个连接池","uri":"/ooooo-notes/%E5%A6%82%E4%BD%95%E8%AE%BE%E8%AE%A1%E4%B8%80%E4%B8%AA%E8%BF%9E%E6%8E%A5%E6%B1%A0/"},{"categories":["微信文章"],"content":"1. 需求 ","date":"2022-09-13 10:20:00","objectID":"/ooooo-notes/%E5%A6%82%E4%BD%95%E8%AE%BE%E8%AE%A1%E4%B8%80%E4%B8%AA%E8%BF%9E%E6%8E%A5%E6%B1%A0/:1:0","tags":["java"],"title":"如何设计一个连接池","uri":"/ooooo-notes/%E5%A6%82%E4%BD%95%E8%AE%BE%E8%AE%A1%E4%B8%80%E4%B8%AA%E8%BF%9E%E6%8E%A5%E6%B1%A0/"},{"categories":["微信文章"],"content":"2. 实现的关键点 ","date":"2022-09-13 10:20:00","objectID":"/ooooo-notes/%E5%A6%82%E4%BD%95%E8%AE%BE%E8%AE%A1%E4%B8%80%E4%B8%AA%E8%BF%9E%E6%8E%A5%E6%B1%A0/:2:0","tags":["java"],"title":"如何设计一个连接池","uri":"/ooooo-notes/%E5%A6%82%E4%BD%95%E8%AE%BE%E8%AE%A1%E4%B8%80%E4%B8%AA%E8%BF%9E%E6%8E%A5%E6%B1%A0/"},{"categories":["微信文章"],"content":"3. druid datasource ","date":"2022-09-13 10:20:00","objectID":"/ooooo-notes/%E5%A6%82%E4%BD%95%E8%AE%BE%E8%AE%A1%E4%B8%80%E4%B8%AA%E8%BF%9E%E6%8E%A5%E6%B1%A0/:3:0","tags":["java"],"title":"如何设计一个连接池","uri":"/ooooo-notes/%E5%A6%82%E4%BD%95%E8%AE%BE%E8%AE%A1%E4%B8%80%E4%B8%AA%E8%BF%9E%E6%8E%A5%E6%B1%A0/"},{"categories":["微信文章"],"content":"4. common-pool2 ","date":"2022-09-13 10:20:00","objectID":"/ooooo-notes/%E5%A6%82%E4%BD%95%E8%AE%BE%E8%AE%A1%E4%B8%80%E4%B8%AA%E8%BF%9E%E6%8E%A5%E6%B1%A0/:4:0","tags":["java"],"title":"如何设计一个连接池","uri":"/ooooo-notes/%E5%A6%82%E4%BD%95%E8%AE%BE%E8%AE%A1%E4%B8%80%E4%B8%AA%E8%BF%9E%E6%8E%A5%E6%B1%A0/"},{"categories":["微信文章"],"content":"如何设计一个对象池 ? ","date":"2022-09-12 10:20:00","objectID":"/ooooo-notes/%E5%A6%82%E4%BD%95%E8%AE%BE%E8%AE%A1%E4%B8%80%E4%B8%AA%E5%AF%B9%E8%B1%A1%E6%B1%A0/:0:0","tags":["java"],"title":"如何设计一个对象池 ?","uri":"/ooooo-notes/%E5%A6%82%E4%BD%95%E8%AE%BE%E8%AE%A1%E4%B8%80%E4%B8%AA%E5%AF%B9%E8%B1%A1%E6%B1%A0/"},{"categories":["微信文章"],"content":"1. 需求 ","date":"2022-09-12 10:20:00","objectID":"/ooooo-notes/%E5%A6%82%E4%BD%95%E8%AE%BE%E8%AE%A1%E4%B8%80%E4%B8%AA%E5%AF%B9%E8%B1%A1%E6%B1%A0/:1:0","tags":["java"],"title":"如何设计一个对象池 ?","uri":"/ooooo-notes/%E5%A6%82%E4%BD%95%E8%AE%BE%E8%AE%A1%E4%B8%80%E4%B8%AA%E5%AF%B9%E8%B1%A1%E6%B1%A0/"},{"categories":["微信文章"],"content":"2. 简单的实现 ","date":"2022-09-12 10:20:00","objectID":"/ooooo-notes/%E5%A6%82%E4%BD%95%E8%AE%BE%E8%AE%A1%E4%B8%80%E4%B8%AA%E5%AF%B9%E8%B1%A1%E6%B1%A0/:2:0","tags":["java"],"title":"如何设计一个对象池 ?","uri":"/ooooo-notes/%E5%A6%82%E4%BD%95%E8%AE%BE%E8%AE%A1%E4%B8%80%E4%B8%AA%E5%AF%B9%E8%B1%A1%E6%B1%A0/"},{"categories":["微信文章"],"content":"3. 实现垃圾回收 软引用，弱引用 ","date":"2022-09-12 10:20:00","objectID":"/ooooo-notes/%E5%A6%82%E4%BD%95%E8%AE%BE%E8%AE%A1%E4%B8%80%E4%B8%AA%E5%AF%B9%E8%B1%A1%E6%B1%A0/:3:0","tags":["java"],"title":"如何设计一个对象池 ?","uri":"/ooooo-notes/%E5%A6%82%E4%BD%95%E8%AE%BE%E8%AE%A1%E4%B8%80%E4%B8%AA%E5%AF%B9%E8%B1%A1%E6%B1%A0/"},{"categories":["微信文章"],"content":"在 idea 中实际有一个非常有用的功能，那就是远端构建和远端运行。 在我们实际开发项目中，自己的本地环境和服务器环境不太一样，例如 go 开发中的 build-tags, 还有 c/c++ 开发中的API 调用不一样，无法模拟相同的开发环境。 ","date":"2022-09-12 08:14:00","objectID":"/ooooo-notes/%E5%9C%A8-idea-%E4%B8%AD%E4%BD%BF%E7%94%A8%E8%BF%9C%E7%AB%AF-build-%E5%92%8C-run/:0:0","tags":["IDE"],"title":"在 idea 中使用远端 build 和 run","uri":"/ooooo-notes/%E5%9C%A8-idea-%E4%B8%AD%E4%BD%BF%E7%94%A8%E8%BF%9C%E7%AB%AF-build-%E5%92%8C-run/"},{"categories":["微信文章"],"content":"1. 问题说明 这里拿 运行 kubernetes 来说明这个问题。 我们都知道在 windows 系统 和 mac 系统中，你是很难在本地运行 kubernetes 的，因为需要涉及到很多的组件，很多功能也只有在 linux 系统中才会开启。 一般来说，任何程序都应该是有远程调试这个功能的，但是这个功能在本地学习源码时会有点不方便，主要体现在自己在本地改了源码，需要同步到你远端的机器上，然后 build 和 run， 整套过程不是自动的。 ","date":"2022-09-12 08:14:00","objectID":"/ooooo-notes/%E5%9C%A8-idea-%E4%B8%AD%E4%BD%BF%E7%94%A8%E8%BF%9C%E7%AB%AF-build-%E5%92%8C-run/:1:0","tags":["IDE"],"title":"在 idea 中使用远端 build 和 run","uri":"/ooooo-notes/%E5%9C%A8-idea-%E4%B8%AD%E4%BD%BF%E7%94%A8%E8%BF%9C%E7%AB%AF-build-%E5%92%8C-run/"},{"categories":["微信文章"],"content":"2. 使用教程 选择在远端机器上运行。 选择在远端机器上构建和编译完成后运行。 指定程序参数。 如下图： 在远端构建和运行 新建一个target配置。 新建或选择一个ssh配置, 对于 rsync 可以不用开启。 设置远端go的执行路径，这个必须要指定，如果 GOPATH 不指定，则使用远端默认配置*。 建议指定一个具体的路径，否则，每次都会生成新的目录，构建比较缓慢。 如下图： 在远端构建和运行 ","date":"2022-09-12 08:14:00","objectID":"/ooooo-notes/%E5%9C%A8-idea-%E4%B8%AD%E4%BD%BF%E7%94%A8%E8%BF%9C%E7%AB%AF-build-%E5%92%8C-run/:2:0","tags":["IDE"],"title":"在 idea 中使用远端 build 和 run","uri":"/ooooo-notes/%E5%9C%A8-idea-%E4%B8%AD%E4%BD%BF%E7%94%A8%E8%BF%9C%E7%AB%AF-build-%E5%92%8C-run/"},{"categories":["微信文章"],"content":"在 spring 中，我们常常会基于现有的代码来扩展之前的功能，或者换一个实现的方式。 在上一篇中，我使用 BeanPostProcessor 来进行扩展。 而在这一篇中，我使用 BeanDefinitionRegistryPostProcessor 来进行扩展。 由于已经实现过一次，我这里就不多说了。 ","date":"2022-09-05 21:09:01","objectID":"/ooooo-notes/%E5%9C%A8-spring-%E4%B8%AD%E5%A6%82%E4%BD%95%E6%89%A9%E5%B1%95%E7%8E%B0%E6%9C%89%E7%B1%BB%E7%9A%84%E5%8A%9F%E8%83%BD2/:0:0","tags":["java","spring","spring-extension"],"title":"在 spring 中如何扩展现有类的功能2 ?","uri":"/ooooo-notes/%E5%9C%A8-spring-%E4%B8%AD%E5%A6%82%E4%BD%95%E6%89%A9%E5%B1%95%E7%8E%B0%E6%9C%89%E7%B1%BB%E7%9A%84%E5%8A%9F%E8%83%BD2/"},{"categories":["微信文章"],"content":"1. 实现思路 判断 beanName 删除原有的 beanDefinition 注册新的 beanDefinition 注意点： 新实现的类，必须要是 CompositePropertySources 的子类，否则注入会有问题 所有方法都必须重新实现一遍，无法复用父类的方法 @Component public class CompositePropertySourcesBeanDefinitionRegistry implements BeanDefinitionRegistryPostProcessor { public static final String COMPOSITE_PROPERTY_SOURCES_BEAN_NAME = \"compositePropertySources\"; @Override public void postProcessBeanDefinitionRegistry(BeanDefinitionRegistry registry) throws BeansException { if (registry.containsBeanDefinition(COMPOSITE_PROPERTY_SOURCES_BEAN_NAME)) { registry.removeBeanDefinition(COMPOSITE_PROPERTY_SOURCES_BEAN_NAME); RootBeanDefinition definition = new RootBeanDefinition(ProxyCompositePropertySources.class); definition.setAutowireMode(AbstractBeanDefinition.AUTOWIRE_CONSTRUCTOR); registry.registerBeanDefinition(COMPOSITE_PROPERTY_SOURCES_BEAN_NAME, definition); } } @Override public void postProcessBeanFactory(ConfigurableListableBeanFactory beanFactory) throws BeansException { } } ","date":"2022-09-05 21:09:01","objectID":"/ooooo-notes/%E5%9C%A8-spring-%E4%B8%AD%E5%A6%82%E4%BD%95%E6%89%A9%E5%B1%95%E7%8E%B0%E6%9C%89%E7%B1%BB%E7%9A%84%E5%8A%9F%E8%83%BD2/:1:0","tags":["java","spring","spring-extension"],"title":"在 spring 中如何扩展现有类的功能2 ?","uri":"/ooooo-notes/%E5%9C%A8-spring-%E4%B8%AD%E5%A6%82%E4%BD%95%E6%89%A9%E5%B1%95%E7%8E%B0%E6%9C%89%E7%B1%BB%E7%9A%84%E5%8A%9F%E8%83%BD2/"},{"categories":["微信文章"],"content":"2. 如何选择 如果是改进之前的功能，就使用第一种方式, BeanPostProcessor 如果是重写之前的功能，就使用第二种方式, BeanDefinitionRegistryPostProcessor 如果不好选择，就啥用第二种方式，也是最强大的。 ","date":"2022-09-05 21:09:01","objectID":"/ooooo-notes/%E5%9C%A8-spring-%E4%B8%AD%E5%A6%82%E4%BD%95%E6%89%A9%E5%B1%95%E7%8E%B0%E6%9C%89%E7%B1%BB%E7%9A%84%E5%8A%9F%E8%83%BD2/:2:0","tags":["java","spring","spring-extension"],"title":"在 spring 中如何扩展现有类的功能2 ?","uri":"/ooooo-notes/%E5%9C%A8-spring-%E4%B8%AD%E5%A6%82%E4%BD%95%E6%89%A9%E5%B1%95%E7%8E%B0%E6%9C%89%E7%B1%BB%E7%9A%84%E5%8A%9F%E8%83%BD2/"},{"categories":["微信文章"],"content":"2. 完整代码实现 github 地址 ","date":"2022-09-05 21:09:01","objectID":"/ooooo-notes/%E5%9C%A8-spring-%E4%B8%AD%E5%A6%82%E4%BD%95%E6%89%A9%E5%B1%95%E7%8E%B0%E6%9C%89%E7%B1%BB%E7%9A%84%E5%8A%9F%E8%83%BD2/:3:0","tags":["java","spring","spring-extension"],"title":"在 spring 中如何扩展现有类的功能2 ?","uri":"/ooooo-notes/%E5%9C%A8-spring-%E4%B8%AD%E5%A6%82%E4%BD%95%E6%89%A9%E5%B1%95%E7%8E%B0%E6%9C%89%E7%B1%BB%E7%9A%84%E5%8A%9F%E8%83%BD2/"},{"categories":["微信文章"],"content":"在 spring 中，我们常常会基于现有的代码来扩展之前的功能，或者换一个实现的方式。 ","date":"2022-09-05 21:09:00","objectID":"/ooooo-notes/%E5%9C%A8-spring-%E4%B8%AD%E5%A6%82%E4%BD%95%E6%89%A9%E5%B1%95%E7%8E%B0%E6%9C%89%E7%B1%BB%E7%9A%84%E5%8A%9F%E8%83%BD/:0:0","tags":["java","spring","spring-extension"],"title":"在 spring 中如何扩展现有类的功能 ?","uri":"/ooooo-notes/%E5%9C%A8-spring-%E4%B8%AD%E5%A6%82%E4%BD%95%E6%89%A9%E5%B1%95%E7%8E%B0%E6%9C%89%E7%B1%BB%E7%9A%84%E5%8A%9F%E8%83%BD/"},{"categories":["微信文章"],"content":"1. 原有的功能 在这里基于之前的功能获取属性来继续深入。 大致代码如下 @Slf4j @Order public class CompositePropertySources implements PropertySources { private final MutablePropertySources mutablePropertySources = new MutablePropertySources(); public CompositePropertySources(List\u003cAbstractSimplePropertySource\u003e sources) { if (sources == null) return; AnnotationAwareOrderComparator.sort(sources); for (AbstractSimplePropertySource source : sources) { mutablePropertySources.addLast(source); } } public boolean containsProperty(String name) { return stream().anyMatch(p -\u003e p.containsProperty(name)); } public String getProperty(String name) { return isBlank(name) ? name : getProperty(name, null); } public String getProperty(String propertyName, String defaultValue) { String value = null; for (PropertySource\u003c?\u003e ps : mutablePropertySources) { value = (String) ps.getProperty(propertyName); if (value != null) { return value; } } return defaultValue; } public Map\u003cString, String\u003e getProperties(String... propertyNames) { if (propertyNames != null) { Map\u003cString, String\u003e map = new HashMap\u003c\u003e(); for (String key : propertyNames) { map.put(key, getProperty(key, null)); } return map; } return null; } } ","date":"2022-09-05 21:09:00","objectID":"/ooooo-notes/%E5%9C%A8-spring-%E4%B8%AD%E5%A6%82%E4%BD%95%E6%89%A9%E5%B1%95%E7%8E%B0%E6%9C%89%E7%B1%BB%E7%9A%84%E5%8A%9F%E8%83%BD/:1:0","tags":["java","spring","spring-extension"],"title":"在 spring 中如何扩展现有类的功能 ?","uri":"/ooooo-notes/%E5%9C%A8-spring-%E4%B8%AD%E5%A6%82%E4%BD%95%E6%89%A9%E5%B1%95%E7%8E%B0%E6%9C%89%E7%B1%BB%E7%9A%84%E5%8A%9F%E8%83%BD/"},{"categories":["微信文章"],"content":"2. 新的需求 由于之前的功能是根据 key 来获取 value的，而现在需要根据业务编号和 key 来获取 value。 先根据 businType.key 来获取 value 如果结果不是 null，则返回 如果结果是 null， 再根据 key 来获取 value 根据上面的描述，也就是优先取业务类型的配置 因为我们的功能实际上在上一篇就已经完成了，所以在这一节中，只需要扩展原有的功能就行了。 这里我使用 BeanPostProcessor 来进行扩展，选择这个类的原因是原有的 bean 已经生成了，无需更改 bean 定义. 实现如下： 判断 bean 是否为 CompositePropertySources 的实例 使用 ProxyCompositePropertySources 对象来代替原有的类 使用 propertyNamesFunction 来分隔 propertyName @Component public class CompositePropertySourcesBeanPostProcessor implements BeanPostProcessor { @Override public Object postProcessAfterInitialization(Object bean, String beanName) throws BeansException { if (bean instanceof CompositePropertySources) { return new ProxyCompositePropertySources((CompositePropertySources) bean); } return bean; } protected static class ProxyCompositePropertySources extends CompositePropertySources { private final CompositePropertySources compositePropertySources; public ProxyCompositePropertySources(CompositePropertySources compositePropertySources) { super(null); this.compositePropertySources = compositePropertySources; } @Override public String getProperty(String propertyName, String defaultValue) { String[] propertyNames = propertyNamesFunction.apply(propertyName); for (String p : propertyNames) { String v = compositePropertySources.getProperty(p); if (v != null) { return v; } } return defaultValue; } @Override public boolean containsProperty(String propertyName) { String[] propertyNames = propertyNamesFunction.apply(propertyName); for (String p : propertyNames) { boolean contains = compositePropertySources.containsProperty(p); if (contains) { return true; } } return false; } } private static final Function\u003cString, String[]\u003e propertyNamesFunction = (propertyName) -\u003e { if (propertyName == null) { return new String[0]; } propertyName = propertyName.trim(); if (propertyName.contains(\".\")) { return new String[]{propertyName, propertyName.substring(propertyName.lastIndexOf(\".\") + 1)}; } return new String[]{propertyName}; }; } ","date":"2022-09-05 21:09:00","objectID":"/ooooo-notes/%E5%9C%A8-spring-%E4%B8%AD%E5%A6%82%E4%BD%95%E6%89%A9%E5%B1%95%E7%8E%B0%E6%9C%89%E7%B1%BB%E7%9A%84%E5%8A%9F%E8%83%BD/:2:0","tags":["java","spring","spring-extension"],"title":"在 spring 中如何扩展现有类的功能 ?","uri":"/ooooo-notes/%E5%9C%A8-spring-%E4%B8%AD%E5%A6%82%E4%BD%95%E6%89%A9%E5%B1%95%E7%8E%B0%E6%9C%89%E7%B1%BB%E7%9A%84%E5%8A%9F%E8%83%BD/"},{"categories":["微信文章"],"content":"3. 完整代码实现 github 地址 ","date":"2022-09-05 21:09:00","objectID":"/ooooo-notes/%E5%9C%A8-spring-%E4%B8%AD%E5%A6%82%E4%BD%95%E6%89%A9%E5%B1%95%E7%8E%B0%E6%9C%89%E7%B1%BB%E7%9A%84%E5%8A%9F%E8%83%BD/:3:0","tags":["java","spring","spring-extension"],"title":"在 spring 中如何扩展现有类的功能 ?","uri":"/ooooo-notes/%E5%9C%A8-spring-%E4%B8%AD%E5%A6%82%E4%BD%95%E6%89%A9%E5%B1%95%E7%8E%B0%E6%9C%89%E7%B1%BB%E7%9A%84%E5%8A%9F%E8%83%BD/"},{"categories":["微信文章"],"content":"1. 需求 希望根据 propertyName 来获取相应的 propertyValue, 这个接口需要支持多种数据来源。 ","date":"2022-09-05 21:09:00","objectID":"/ooooo-notes/%E5%9C%A8-spring-%E4%B8%AD%E5%A6%82%E4%BD%95%E8%AE%BE%E8%AE%A1%E4%B8%80%E4%B8%AA%E8%8E%B7%E5%8F%96%E9%85%8D%E7%BD%AE%E7%9A%84%E6%8E%A5%E5%8F%A3/:1:0","tags":["java","spring","spring-extension"],"title":"在 spring 中如何设计一个获取配置的接口?","uri":"/ooooo-notes/%E5%9C%A8-spring-%E4%B8%AD%E5%A6%82%E4%BD%95%E8%AE%BE%E8%AE%A1%E4%B8%80%E4%B8%AA%E8%8E%B7%E5%8F%96%E9%85%8D%E7%BD%AE%E7%9A%84%E6%8E%A5%E5%8F%A3/"},{"categories":["微信文章"],"content":"2. 设计接口 很明显这个接口应该设计为这样, 有一个方法为 Object getProperty(String name) 来获取属性。 因为是在 spring 中，所以我就直接复用了 org.springframework.core.env.PropertySource, 但这个类需要泛型，所以我就随便实现了一个 Map\u003cString, String\u003e 的泛型，也不会用到这个。 抽象类的设计如下： public abstract class AbstractSimplePropertySource extends PropertySource\u003cMap\u003cString, String\u003e\u003e implements EnvironmentAware { protected Environment environment; public AbstractSimplePropertySource(String name) { super(name, Collections.emptyMap()); } public void setEnvironment(@NonNull Environment environment) { this.environment = environment; } public Environment getEnvironment() { return environment; } } ","date":"2022-09-05 21:09:00","objectID":"/ooooo-notes/%E5%9C%A8-spring-%E4%B8%AD%E5%A6%82%E4%BD%95%E8%AE%BE%E8%AE%A1%E4%B8%80%E4%B8%AA%E8%8E%B7%E5%8F%96%E9%85%8D%E7%BD%AE%E7%9A%84%E6%8E%A5%E5%8F%A3/:2:0","tags":["java","spring","spring-extension"],"title":"在 spring 中如何设计一个获取配置的接口?","uri":"/ooooo-notes/%E5%9C%A8-spring-%E4%B8%AD%E5%A6%82%E4%BD%95%E8%AE%BE%E8%AE%A1%E4%B8%80%E4%B8%AA%E8%8E%B7%E5%8F%96%E9%85%8D%E7%BD%AE%E7%9A%84%E6%8E%A5%E5%8F%A3/"},{"categories":["微信文章"],"content":"3. 具体类的实现 ","date":"2022-09-05 21:09:00","objectID":"/ooooo-notes/%E5%9C%A8-spring-%E4%B8%AD%E5%A6%82%E4%BD%95%E8%AE%BE%E8%AE%A1%E4%B8%80%E4%B8%AA%E8%8E%B7%E5%8F%96%E9%85%8D%E7%BD%AE%E7%9A%84%E6%8E%A5%E5%8F%A3/:3:0","tags":["java","spring","spring-extension"],"title":"在 spring 中如何设计一个获取配置的接口?","uri":"/ooooo-notes/%E5%9C%A8-spring-%E4%B8%AD%E5%A6%82%E4%BD%95%E8%AE%BE%E8%AE%A1%E4%B8%80%E4%B8%AA%E8%8E%B7%E5%8F%96%E9%85%8D%E7%BD%AE%E7%9A%84%E6%8E%A5%E5%8F%A3/"},{"categories":["微信文章"],"content":"3.1 根据请求参数来获取配置 先从请求参数中获取 再从请求头中获取 代码逻辑是比较简单的，我就不解释了。 这种形式，解决了前端可以传入相应的配置，来改变后端的执行逻辑。 @Order(0) public class RequestParamsPropertySource extends AbstractSimplePropertySource { public RequestParamsPropertySource() { super(ENV_PREFIX + \"request_params\"); } @Override public Object getProperty(String name) { String propertyKey = ENV_PREFIX + name.replace(\".\", \"_\"); String propertyValue = null; try { // 先请求参数中获取 ServletRequestAttributes attr = (ServletRequestAttributes) RequestContextHolder.currentRequestAttributes(); HttpServletRequest request = attr.getRequest(); propertyValue = request.getParameter(propertyKey); if (StringUtils.isBlank(propertyValue)) { // 再从请求头中获取 propertyValue = request.getHeader(propertyKey); } } catch (Throwable ignored) { } // 空字符也当做null propertyValue = defaultIfBlank(propertyValue, null); return propertyValue; } } ","date":"2022-09-05 21:09:00","objectID":"/ooooo-notes/%E5%9C%A8-spring-%E4%B8%AD%E5%A6%82%E4%BD%95%E8%AE%BE%E8%AE%A1%E4%B8%80%E4%B8%AA%E8%8E%B7%E5%8F%96%E9%85%8D%E7%BD%AE%E7%9A%84%E6%8E%A5%E5%8F%A3/:3:1","tags":["java","spring","spring-extension"],"title":"在 spring 中如何设计一个获取配置的接口?","uri":"/ooooo-notes/%E5%9C%A8-spring-%E4%B8%AD%E5%A6%82%E4%BD%95%E8%AE%BE%E8%AE%A1%E4%B8%80%E4%B8%AA%E8%8E%B7%E5%8F%96%E9%85%8D%E7%BD%AE%E7%9A%84%E6%8E%A5%E5%8F%A3/"},{"categories":["微信文章"],"content":"3.2 根据环境变量来获取配置 代码逻辑是比较简单的，我就不解释了。 @Order(2) public class EnvironmentPropertySource extends AbstractSimplePropertySource { public EnvironmentPropertySource() { super(ENV_PREFIX + \"environment\"); } @Override public Object getProperty(String name) { String env_property_key = ENV_PREFIX + name.replace(\".\", \"_\"); return System.getenv(env_property_key.toUpperCase()); } } ","date":"2022-09-05 21:09:00","objectID":"/ooooo-notes/%E5%9C%A8-spring-%E4%B8%AD%E5%A6%82%E4%BD%95%E8%AE%BE%E8%AE%A1%E4%B8%80%E4%B8%AA%E8%8E%B7%E5%8F%96%E9%85%8D%E7%BD%AE%E7%9A%84%E6%8E%A5%E5%8F%A3/:3:2","tags":["java","spring","spring-extension"],"title":"在 spring 中如何设计一个获取配置的接口?","uri":"/ooooo-notes/%E5%9C%A8-spring-%E4%B8%AD%E5%A6%82%E4%BD%95%E8%AE%BE%E8%AE%A1%E4%B8%80%E4%B8%AA%E8%8E%B7%E5%8F%96%E9%85%8D%E7%BD%AE%E7%9A%84%E6%8E%A5%E5%8F%A3/"},{"categories":["微信文章"],"content":"3.3 根据本地配置文件来获取参数 在实际开发过程，大家可能是共用一套数据库环境，在这个情况下，如果某一个人改了数据库配置，这样会对其他人造成影响，所以必须要设计出一个用于开发的配置类。 使用 apache 的 configuration 包，来实现配置文件的动态刷新 从 builder.getConfiguration() 对象中获取配置 大致代码如下： @Order(1) public class LocalPropertiesPropertySource extends AbstractSimplePropertySource implements InitializingBean { @Autowired private ApplicationEventPublisher publisher; private ReloadingFileBasedConfigurationBuilder\u003cPropertiesConfiguration\u003e builder; private static final String DEFAULT_LOCAL_PROPERTIES_PATH = \"sysoptions.properties\"; public LocalPropertiesPropertySource() { super(ENV_PREFIX + \"local_properties\"); log.debug(\"开发环境，启用[{}]配置\", getClass()); } @Override public void afterPropertiesSet() { Integer sysOptionsLoadInterval = getEnvironment().getProperty(\"dev.localPropertiesLoadInterval\", Integer.class, 1); String sysOptionsPath = getEnvironment().getProperty(\"dev.localPropertiesPath\", DEFAULT_LOCAL_PROPERTIES_PATH); File propertiesFile = new File(sysOptionsPath); if (!propertiesFile.exists()) { return; } // server boot will publish event publisher.publishEvent(new LocalProperitesReReloadingEvent(new Object(), propertiesFile.getAbsolutePath())); builder = new ReloadingFileBasedConfigurationBuilder\u003c\u003e(PropertiesConfiguration.class).configure(new Parameters().fileBased().setFile(propertiesFile)); ReloadingController reloadingController = builder.getReloadingController(); reloadingController.addEventListener(ReloadingEvent.ANY, e -\u003e publisher.publishEvent(new LocalProperitesReReloadingEvent(e, propertiesFile.getAbsolutePath()))); PeriodicReloadingTrigger trigger = new PeriodicReloadingTrigger(reloadingController, null, sysOptionsLoadInterval, SECONDS); trigger.start(); } @Override public Object getProperty(@NonNull String name) { if (builder == null) return null; Configuration configuration = null; try { configuration = builder.getConfiguration(); String config_value = configuration.getString(name); if (config_value != null) { return config_value; } } catch (ConfigurationException e) { log.error(e.getMessage(), e); } return null; } } ","date":"2022-09-05 21:09:00","objectID":"/ooooo-notes/%E5%9C%A8-spring-%E4%B8%AD%E5%A6%82%E4%BD%95%E8%AE%BE%E8%AE%A1%E4%B8%80%E4%B8%AA%E8%8E%B7%E5%8F%96%E9%85%8D%E7%BD%AE%E7%9A%84%E6%8E%A5%E5%8F%A3/:3:3","tags":["java","spring","spring-extension"],"title":"在 spring 中如何设计一个获取配置的接口?","uri":"/ooooo-notes/%E5%9C%A8-spring-%E4%B8%AD%E5%A6%82%E4%BD%95%E8%AE%BE%E8%AE%A1%E4%B8%80%E4%B8%AA%E8%8E%B7%E5%8F%96%E9%85%8D%E7%BD%AE%E7%9A%84%E6%8E%A5%E5%8F%A3/"},{"categories":["微信文章"],"content":"3.4 其他的扩展 到这里，你就应该很清楚的知道怎么去扩展其他的类了，比如数据库的实现， redis 的实现 ","date":"2022-09-05 21:09:00","objectID":"/ooooo-notes/%E5%9C%A8-spring-%E4%B8%AD%E5%A6%82%E4%BD%95%E8%AE%BE%E8%AE%A1%E4%B8%80%E4%B8%AA%E8%8E%B7%E5%8F%96%E9%85%8D%E7%BD%AE%E7%9A%84%E6%8E%A5%E5%8F%A3/:3:4","tags":["java","spring","spring-extension"],"title":"在 spring 中如何设计一个获取配置的接口?","uri":"/ooooo-notes/%E5%9C%A8-spring-%E4%B8%AD%E5%A6%82%E4%BD%95%E8%AE%BE%E8%AE%A1%E4%B8%80%E4%B8%AA%E8%8E%B7%E5%8F%96%E9%85%8D%E7%BD%AE%E7%9A%84%E6%8E%A5%E5%8F%A3/"},{"categories":["微信文章"],"content":"4. 使用的入口 上面只是定义了一个接口和几个实现类，统一的入口类，实际上还没有。 使用构造函数的方式来注入所有的 AbstractSimplePropertySource 。 对于每个获取配置的类，肯定有优先级，所以要排序。 使用 MutablePropertySources 这个类做辅助。 大致代码如下： @Slf4j @Order public class CompositePropertySources implements PropertySources { private final MutablePropertySources mutablePropertySources = new MutablePropertySources(); public CompositePropertySources(List\u003cAbstractSimplePropertySource\u003e sources) { if (sources == null) return; AnnotationAwareOrderComparator.sort(sources); for (AbstractSimplePropertySource source : sources) { mutablePropertySources.addLast(source); } } public boolean containsProperty(String name) { return stream().anyMatch(p -\u003e p.containsProperty(name)); } public String getProperty(String name) { return isBlank(name) ? name : getProperty(name, null); } public String getProperty(String propertyName, String defaultValue) { String value = null; for (PropertySource\u003c?\u003e ps : mutablePropertySources) { value = (String) ps.getProperty(propertyName); if (value != null) { return value; } } return defaultValue; } public Map\u003cString, String\u003e getProperties(String... propertyNames) { if (propertyNames != null) { Map\u003cString, String\u003e map = new HashMap\u003c\u003e(); for (String key : propertyNames) { map.put(key, getProperty(key, null)); } return map; } return null; } } ","date":"2022-09-05 21:09:00","objectID":"/ooooo-notes/%E5%9C%A8-spring-%E4%B8%AD%E5%A6%82%E4%BD%95%E8%AE%BE%E8%AE%A1%E4%B8%80%E4%B8%AA%E8%8E%B7%E5%8F%96%E9%85%8D%E7%BD%AE%E7%9A%84%E6%8E%A5%E5%8F%A3/:4:0","tags":["java","spring","spring-extension"],"title":"在 spring 中如何设计一个获取配置的接口?","uri":"/ooooo-notes/%E5%9C%A8-spring-%E4%B8%AD%E5%A6%82%E4%BD%95%E8%AE%BE%E8%AE%A1%E4%B8%80%E4%B8%AA%E8%8E%B7%E5%8F%96%E9%85%8D%E7%BD%AE%E7%9A%84%E6%8E%A5%E5%8F%A3/"},{"categories":["微信文章"],"content":"5. 完整代码实现 github 地址 ","date":"2022-09-05 21:09:00","objectID":"/ooooo-notes/%E5%9C%A8-spring-%E4%B8%AD%E5%A6%82%E4%BD%95%E8%AE%BE%E8%AE%A1%E4%B8%80%E4%B8%AA%E8%8E%B7%E5%8F%96%E9%85%8D%E7%BD%AE%E7%9A%84%E6%8E%A5%E5%8F%A3/:5:0","tags":["java","spring","spring-extension"],"title":"在 spring 中如何设计一个获取配置的接口?","uri":"/ooooo-notes/%E5%9C%A8-spring-%E4%B8%AD%E5%A6%82%E4%BD%95%E8%AE%BE%E8%AE%A1%E4%B8%80%E4%B8%AA%E8%8E%B7%E5%8F%96%E9%85%8D%E7%BD%AE%E7%9A%84%E6%8E%A5%E5%8F%A3/"},{"categories":["微信文章"],"content":"作为一个 Java 开发，Spring 的技术可以说是必须要掌握的，不仅仅是会使用，而且要掌握原理，学会扩展。 今天我就说说，哪些核心类和扩展类是必须要掌握的，同时我也说明这些扩展可以干什么，后面 Spring 文章，我会用到这些扩展类，让你学懂这些类。 ","date":"2022-09-05 18:00:00","objectID":"/ooooo-notes/%E5%9C%A8-spring-%E4%B8%AD%E6%9C%89%E5%93%AA%E4%BA%9B%E6%A0%B8%E5%BF%83%E7%B1%BB%E5%92%8C%E6%89%A9%E5%B1%95%E7%B1%BB/:0:0","tags":["java","spring"],"title":"在 spring 中有哪些核心类和扩展类?","uri":"/ooooo-notes/%E5%9C%A8-spring-%E4%B8%AD%E6%9C%89%E5%93%AA%E4%BA%9B%E6%A0%B8%E5%BF%83%E7%B1%BB%E5%92%8C%E6%89%A9%E5%B1%95%E7%B1%BB/"},{"categories":["微信文章"],"content":"核心类： IOC容器: org.springframework.context.ApplicationContext 配置类: org.springframework.core.env.Environment Bean工厂：org.springframework.beans.factory.BeanFactory 事件发布器： org.springframework.context.ApplicationEventPublisher 资源加载器： org.springframework.core.io.ResourceLoader 上面这几个类，是我们经常会用到的。它们都有相应的 Aware 接口, 如 org.springframework.context.ApplicationContextAware, 可以设置 applicationContext 对象到我们自己定义的 bean 对象中. 注意这样 setApplicationContext 的方式比 @Autowired 注解注入 applicationContext 的方式的时机要早很多，所以一般推荐用 setApplicationContext 的方式。 相应的源码 ApplicationContextAwareProcessor ","date":"2022-09-05 18:00:00","objectID":"/ooooo-notes/%E5%9C%A8-spring-%E4%B8%AD%E6%9C%89%E5%93%AA%E4%BA%9B%E6%A0%B8%E5%BF%83%E7%B1%BB%E5%92%8C%E6%89%A9%E5%B1%95%E7%B1%BB/:1:0","tags":["java","spring"],"title":"在 spring 中有哪些核心类和扩展类?","uri":"/ooooo-notes/%E5%9C%A8-spring-%E4%B8%AD%E6%9C%89%E5%93%AA%E4%BA%9B%E6%A0%B8%E5%BF%83%E7%B1%BB%E5%92%8C%E6%89%A9%E5%B1%95%E7%B1%BB/"},{"categories":["微信文章"],"content":"扩展类 beanFactory的后置处理器： org.springframework.beans.factory.config.BeanFactoryPostProcessor bean的后置处理器： org.springframework.beans.factory.config.BeanPostProcessor 上面的两个类非常重要，如果你现在还不会熟练使用它们，说明 spring 掌握的很一般。 ","date":"2022-09-05 18:00:00","objectID":"/ooooo-notes/%E5%9C%A8-spring-%E4%B8%AD%E6%9C%89%E5%93%AA%E4%BA%9B%E6%A0%B8%E5%BF%83%E7%B1%BB%E5%92%8C%E6%89%A9%E5%B1%95%E7%B1%BB/:2:0","tags":["java","spring"],"title":"在 spring 中有哪些核心类和扩展类?","uri":"/ooooo-notes/%E5%9C%A8-spring-%E4%B8%AD%E6%9C%89%E5%93%AA%E4%BA%9B%E6%A0%B8%E5%BF%83%E7%B1%BB%E5%92%8C%E6%89%A9%E5%B1%95%E7%B1%BB/"},{"categories":["微信文章"],"content":"非常有用的类 代理工厂： org.springframework.aop.framework.ProxyFactory bean工厂： org.springframework.beans.factory.ObjectFactory 属性绑定： org.springframework.boot.context.properties.bind.Binder 选择性导入bean: org.springframework.context.annotation.ImportSelector 上面这几个类，一般在扩展功能时，都会用到。 ","date":"2022-09-05 18:00:00","objectID":"/ooooo-notes/%E5%9C%A8-spring-%E4%B8%AD%E6%9C%89%E5%93%AA%E4%BA%9B%E6%A0%B8%E5%BF%83%E7%B1%BB%E5%92%8C%E6%89%A9%E5%B1%95%E7%B1%BB/:3:0","tags":["java","spring"],"title":"在 spring 中有哪些核心类和扩展类?","uri":"/ooooo-notes/%E5%9C%A8-spring-%E4%B8%AD%E6%9C%89%E5%93%AA%E4%BA%9B%E6%A0%B8%E5%BF%83%E7%B1%BB%E5%92%8C%E6%89%A9%E5%B1%95%E7%B1%BB/"},{"categories":["随笔"],"content":"1. 下载代码 git clone git@github.com:apache/tomcat.git ","date":"2022-08-10 18:32:22","objectID":"/ooooo-notes/%E6%90%AD%E5%BB%BA-tomcat-%E6%BA%90%E7%A0%81%E8%B0%83%E8%AF%95%E7%8E%AF%E5%A2%83/:1:0","tags":["tomcat","source code"],"title":"搭建 tomcat 源码调试环境","uri":"/ooooo-notes/%E6%90%AD%E5%BB%BA-tomcat-%E6%BA%90%E7%A0%81%E8%B0%83%E8%AF%95%E7%8E%AF%E5%A2%83/"},{"categories":["随笔"],"content":"2. 安装ant 我本地安装的是 1.10.12 版本, ant 下载地址 配置环境变量 ANT_HOME, 加入到 PATH 环境变量中 执行命令验证 ant -version ","date":"2022-08-10 18:32:22","objectID":"/ooooo-notes/%E6%90%AD%E5%BB%BA-tomcat-%E6%BA%90%E7%A0%81%E8%B0%83%E8%AF%95%E7%8E%AF%E5%A2%83/:2:0","tags":["tomcat","source code"],"title":"搭建 tomcat 源码调试环境","uri":"/ooooo-notes/%E6%90%AD%E5%BB%BA-tomcat-%E6%BA%90%E7%A0%81%E8%B0%83%E8%AF%95%E7%8E%AF%E5%A2%83/"},{"categories":["随笔"],"content":"3. 导入到 idea 中 # 进入 tomcat 根目前 cd tomcat # 复制配置文件 build.properties cp build.properties.default build.properties # 更改 build.properties 中的配置 base.path=第三方jar的下载目录 # 设置 idea ant ide-intellij # 执行编译命令, 会生成 output 目录 ant deploy 然后用 idea 打开项目，idea 会弹出让你配置下面的变量 ANT_HOME = ${ant.home} TOMCAT_BUILD_LIBS = ${base.path} ","date":"2022-08-10 18:32:22","objectID":"/ooooo-notes/%E6%90%AD%E5%BB%BA-tomcat-%E6%BA%90%E7%A0%81%E8%B0%83%E8%AF%95%E7%8E%AF%E5%A2%83/:3:0","tags":["tomcat","source code"],"title":"搭建 tomcat 源码调试环境","uri":"/ooooo-notes/%E6%90%AD%E5%BB%BA-tomcat-%E6%BA%90%E7%A0%81%E8%B0%83%E8%AF%95%E7%8E%AF%E5%A2%83/"},{"categories":["随笔"],"content":"4. idea 中 配置 检查你的项目依赖有没有问题 项目依赖配置 上面的三个依赖，其实就是 ServletContainerInitializer 的实现, 比如 res/META-INF/jasper.jar/services/jakarta.servlet.ServletContainerInitializer. 更改配置文件 conf/server.xml # 改为编译输出目录 appBase=\"output/build/webapps\" 运行程序 org.apache.catalina.startup.Bootstrap#main ","date":"2022-08-10 18:32:22","objectID":"/ooooo-notes/%E6%90%AD%E5%BB%BA-tomcat-%E6%BA%90%E7%A0%81%E8%B0%83%E8%AF%95%E7%8E%AF%E5%A2%83/:4:0","tags":["tomcat","source code"],"title":"搭建 tomcat 源码调试环境","uri":"/ooooo-notes/%E6%90%AD%E5%BB%BA-tomcat-%E6%BA%90%E7%A0%81%E8%B0%83%E8%AF%95%E7%8E%AF%E5%A2%83/"},{"categories":["随笔"],"content":" deployment 资源是我们经常需要使用的，也是我们最应该熟悉的源码. 对于调试源码，我使用是 deployment_controller_test.go 测试类， TestSyncDeploymentCreatesReplicaSet 方法. ","date":"2022-07-15 18:32:22","objectID":"/ooooo-notes/%E8%B0%83%E8%AF%95-deployment-controller-%E7%9A%84%E6%BA%90%E7%A0%81/:0:0","tags":["k8s","cloud native","source code"],"title":"调试 deployment-controller 的源码","uri":"/ooooo-notes/%E8%B0%83%E8%AF%95-deployment-controller-%E7%9A%84%E6%BA%90%E7%A0%81/"},{"categories":["随笔"],"content":"TestSyncDeploymentCreatesReplicaSet 测试方法的结构 源码路径：kubernetes\\pkg\\controller\\deployment\\deployment_controller_test.go 测试配置对象 f := newFixture(t) 创建一个 fixture 对象， 里面有 objects 属性，这个用来模拟 clientSet, 也就是请求 etcd 的接口，后面将会详细描述。 创建一个 Deployment 对象， 标签为 “foo”: “bar” d := newDeployment(\"foo\", 1, nil, nil, nil, map[string]string{\"foo\": \"bar\"}) 添加缓存对象，用于后续的List接口 f.dLister = append(f.dLister, d) f.objects = append(f.objects, d) 创建一个 ReplicaSet 对象 rs := newReplicaSet(d, \"deploymentrs-4186632231\", 1) 希望的测试结果 f.expectCreateRSAction(rs) f.expectUpdateDeploymentStatusAction(d) f.expectUpdateDeploymentStatusAction(d) 从上面的语句就可以发现，kubernetes 的测试类，意图非常明确。 也就是说 我创建一个 Deployment ** 对象，肯定会产生一个 ReplicaSet 对象，并且 DeploymentStatus 会被更新两次, 接下来，我们来看看是kubernetes如何做到的。 ","date":"2022-07-15 18:32:22","objectID":"/ooooo-notes/%E8%B0%83%E8%AF%95-deployment-controller-%E7%9A%84%E6%BA%90%E7%A0%81/:1:0","tags":["k8s","cloud native","source code"],"title":"调试 deployment-controller 的源码","uri":"/ooooo-notes/%E8%B0%83%E8%AF%95-deployment-controller-%E7%9A%84%E6%BA%90%E7%A0%81/"},{"categories":["随笔"],"content":"测试方法的执行 f.run(testutil.GetKey(d, t)) 获取 Deployment 的 key 属性 代码 testutil.GetKey(d, t) func GetKey(obj interface{}, t *testing.T) string { // 每个删除的对象都是这个类型, 这里取出了真实的对象 tombstone, ok := obj.(cache.DeletedFinalStateUnknown) if ok { // if tombstone , try getting the value from tombstone.Obj obj = tombstone.Obj } // 取出指针类型中 value，获取 Name 属性 val := reflect.ValueOf(obj).Elem() name := val.FieldByName(\"Name\").String() if len(name) == 0 { t.Errorf(\"Unexpected object %v\", obj) } // 获取key, 结果就是 {namespace}/{name} key, err := keyFunc(obj) if err != nil { t.Errorf(\"Unexpected error getting key for %T %v: %v\", val.Interface(), name, err) return \"\" } return key } 调用 fixture.run_() 方法 这个函数有三个入参：run_(deploymentName string, startInformers bool, expectError bool). 当前的测试方法 TestSyncDeploymentCreatesReplicaSet 的 startInformers 参数为 false, 表示不启动 Informer, 后续会用另外一个测试类来说明 Informer 的启动过程. 接下来详细看看 fixture.run_() 方法都干了啥。 创建一个 controller c, informers, err := f.newController() func (f *fixture) newController() (*DeploymentController, informers.SharedInformerFactory, error) { // 这个也就是之前说的，objects 会用来构建 模拟的 clientSet f.client = fake.NewSimpleClientset(f.objects...) // 创建了 informer 和 deploymentController informers := informers.NewSharedInformerFactory(f.client, controller.NoResyncPeriodFunc()) c, err := NewDeploymentController(informers.Apps().V1().Deployments(), informers.Apps().V1().ReplicaSets(), informers.Core().V1().Pods(), f.client) if err != nil { return nil, nil, err } // 模拟一个 recorder c.eventRecorder = \u0026record.FakeRecorder{} // 所有状态默认为 synced c.dListerSynced = alwaysReady c.rsListerSynced = alwaysReady c.podListerSynced = alwaysReady // 下面这个代码很关键 // 先前在 fixture 对象中加入了相应的 Lister，在这里遍历这些 Lister, 就是为了模拟 Informer 的本地缓存 // kube_controller_manager 程序启动之后，会请求 kube_apiserver 来获取相应的资源，从而更新到自己的缓存中 for _, d := range f.dLister { informers.Apps().V1().Deployments().Informer().GetIndexer().Add(d) } for _, rs := range f.rsLister { informers.Apps().V1().ReplicaSets().Informer().GetIndexer().Add(rs) } for _, pod := range f.podLister { informers.Core().V1().Pods().Informer().GetIndexer().Add(pod) } return c, informers, nil } 详细分析是怎么模拟 clientSet 的？ f.client = fake.NewSimpleClientset(f.objects...) func NewSimpleClientset(objects ...runtime.Object) *Clientset { // 这个是模拟的最终实现对象，所有操作都是依赖它来完成的 o := testing.NewObjectTracker(scheme, codecs.UniversalDecoder()) // 遍历对象，依次添加 for _, obj := range objects { if err := o.Add(obj); err != nil { panic(err) } } // 创建一个 clientSet cs := \u0026Clientset{tracker: o} // 下面三个都是依赖 tracker 来实现的， 通过不同的 Action, 比如 ListActionImpl、GetActionImpl 等 cs.discovery = \u0026fakediscovery.FakeDiscovery{Fake: \u0026cs.Fake} cs.AddReactor(\"*\", \"*\", testing.ObjectReaction(o)) cs.AddWatchReactor(\"*\", func(action testing.Action) (handled bool, ret watch.Interface, err error) { gvr := action.GetResource() ns := action.GetNamespace() watch, err := o.Watch(gvr, ns) if err != nil { return false, nil, err } return true, watch, nil }) return cs } ObjectTracker 如果添加对象的？ testing.NewObjectTracker(scheme, codecs.UniversalDecoder()) func (t *tracker) Add(obj runtime.Object) error { // 添加 List if meta.IsListType(obj) { return t.addList(obj, false) } // 用来获取 namespace objMeta, err := meta.Accessor(obj) // 获取 gvk gvks, _, err := t.scheme.ObjectKinds(obj) for _, gvk := range gvks { // NOTE: UnsafeGuessKindToResource is a heuristic and default match. The // actual registration in apiserver can specify arbitrary route for a // gvk. If a test uses such objects, it cannot preset the tracker with // objects via Add(). Instead, it should trigger the Create() function // of the tracker, where an arbitrary gvr can be specified. gvr, _ := meta.UnsafeGuessKindToResource(gvk) // Resource doesn't have the concept of \"__internal\" version, just set it to \"\". if gvr.Version == runtime.APIVersionInternal { gvr.Version = \"\" } // 添加这个 err := t.add(gvr, obj, objMeta.GetNamespace(), false) if err != nil { return err } } return nil } func (t *tracker) add(gvr schema.GroupVersionResource, obj runtime.Object, ns st","date":"2022-07-15 18:32:22","objectID":"/ooooo-notes/%E8%B0%83%E8%AF%95-deployment-controller-%E7%9A%84%E6%BA%90%E7%A0%81/:2:0","tags":["k8s","cloud native","source code"],"title":"调试 deployment-controller 的源码","uri":"/ooooo-notes/%E8%B0%83%E8%AF%95-deployment-controller-%E7%9A%84%E6%BA%90%E7%A0%81/"},{"categories":["随笔"],"content":"1. GVK 定义 GVK(group version kind): 资源组、资源版本、资源类型 表示 apps 组下 v1 版本 Deployment 类型的资源。 apiVersion: apps/v1 kind: Deployment 表示 core 组下 v1 版本 Pod 类型的资源。(没有组信息表示核心组) apiVersion: v1 kind: Pod ","date":"2022-07-02 18:32:22","objectID":"/ooooo-notes/%E5%AD%A6%E4%B9%A0-k8s-%E6%BA%90%E7%A0%81%E7%9A%84%E5%89%8D%E7%BD%AE%E7%9F%A5%E8%AF%86/:1:0","tags":["k8s","cloud native","source code"],"title":"学习 k8s 源码的前置知识","uri":"/ooooo-notes/%E5%AD%A6%E4%B9%A0-k8s-%E6%BA%90%E7%A0%81%E7%9A%84%E5%89%8D%E7%BD%AE%E7%9F%A5%E8%AF%86/"},{"categories":["随笔"],"content":"2. kubernetes 对象结构 每个对象都可以分为四个部分。 例如 Deployment 资源： TypeMeta: GVK 信息 ObjectMeta: 对象元数据，比如有属性 name、namespace DeploymentSpec: 对象定义规范，比如有属性 replicas(控制副本数量)、template(定义 Pod 的模板)、selector(标签选择器，与 Pod 标签一样)、strategy(Pod 升级策略) DeploymentStatus: 对象运行时状态， 比如有属性 replicas(总的副本数) 代码路径：kubernetes\\vendor\\k8s.io\\api\\apps\\v1\\types.go type Deployment struct { metav1.TypeMeta `json:\",inline\"` // Standard object's metadata. // More info: https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata // +optional metav1.ObjectMeta `json:\"metadata,omitempty\" protobuf:\"bytes,1,opt,name=metadata\"` // Specification of the desired behavior of the Deployment. // +optional Spec DeploymentSpec `json:\"spec,omitempty\" protobuf:\"bytes,2,opt,name=spec\"` // Most recently observed status of the Deployment. // +optional Status DeploymentStatus `json:\"status,omitempty\" protobuf:\"bytes,3,opt,name=status\"` } ","date":"2022-07-02 18:32:22","objectID":"/ooooo-notes/%E5%AD%A6%E4%B9%A0-k8s-%E6%BA%90%E7%A0%81%E7%9A%84%E5%89%8D%E7%BD%AE%E7%9F%A5%E8%AF%86/:2:0","tags":["k8s","cloud native","source code"],"title":"学习 k8s 源码的前置知识","uri":"/ooooo-notes/%E5%AD%A6%E4%B9%A0-k8s-%E6%BA%90%E7%A0%81%E7%9A%84%E5%89%8D%E7%BD%AE%E7%9F%A5%E8%AF%86/"},{"categories":["随笔"],"content":"3. kubernetes 源码目录结构 cmd: 可执行程序包， 例如 kubelet 的入口为 kubernetes\\cmd\\kubelet\\kubelet.go pkg: kubernetes 包路径, 有些子目录与 cmd 目录一样，就是入口文件依赖的包 vendor: 第三方包，其中也有 kubernetes 的包 plugin: 准入插件和认证插件 hack: 脚本路径，非常有用 api: OpenAPI 定义 上述,只是简单的描述了，目前只需要知道 cmd, pkg, vendor 是非常重要的，也是我们经常看的。 ","date":"2022-07-02 18:32:22","objectID":"/ooooo-notes/%E5%AD%A6%E4%B9%A0-k8s-%E6%BA%90%E7%A0%81%E7%9A%84%E5%89%8D%E7%BD%AE%E7%9F%A5%E8%AF%86/:3:0","tags":["k8s","cloud native","source code"],"title":"学习 k8s 源码的前置知识","uri":"/ooooo-notes/%E5%AD%A6%E4%B9%A0-k8s-%E6%BA%90%E7%A0%81%E7%9A%84%E5%89%8D%E7%BD%AE%E7%9F%A5%E8%AF%86/"},{"categories":["随笔"],"content":"4. 如何去阅读源码，真的需要把整个项目都运行起来吗？ 我个人认为是完全不需要。 一般来说看源码，只需要了解主线代码，知道哪些类是怎么配合的，一起完成了什么样的功能，即使你把整个程序都运行起来了，有些分支条件的代码，需要特殊的输入数据，在你不熟悉代码的情况下，你也很难去模拟，这时候我们只能看代码的测试类 ，来了解代码是怎样处理这个特殊数据的。 特意说明一下： 以后的阅读代码的部分，我基本以测试类来带领大家阅读。 ","date":"2022-07-02 18:32:22","objectID":"/ooooo-notes/%E5%AD%A6%E4%B9%A0-k8s-%E6%BA%90%E7%A0%81%E7%9A%84%E5%89%8D%E7%BD%AE%E7%9F%A5%E8%AF%86/:4:0","tags":["k8s","cloud native","source code"],"title":"学习 k8s 源码的前置知识","uri":"/ooooo-notes/%E5%AD%A6%E4%B9%A0-k8s-%E6%BA%90%E7%A0%81%E7%9A%84%E5%89%8D%E7%BD%AE%E7%9F%A5%E8%AF%86/"},{"categories":["随笔"],"content":"1. 方法1，在本机的 IDE 来调试源码 如果你是 linux 系统，可以在 linux 中搭建一个 kubernetes 单机的集群，在此系统中安装 IDE(Goland) 来调试. 具体步骤如下： ","date":"2022-06-28 18:32:22","objectID":"/ooooo-notes/%E6%90%AD%E5%BB%BA-k8s-1.24-%E7%89%88%E6%9C%AC%E7%9A%84%E6%BA%90%E7%A0%81%E8%B0%83%E8%AF%95%E7%8E%AF%E5%A2%83/:1:0","tags":["k8s","cloud native","source code"],"title":"搭建 k8s 1.24 版本的源码调试环境","uri":"/ooooo-notes/%E6%90%AD%E5%BB%BA-k8s-1.24-%E7%89%88%E6%9C%AC%E7%9A%84%E6%BA%90%E7%A0%81%E8%B0%83%E8%AF%95%E7%8E%AF%E5%A2%83/"},{"categories":["随笔"],"content":"1. 下载源码 (go 的版本要求 1.18.x) git clone git@github.com:kubernetes/kubernetes.git cd kubernetes git checkout -b origin/release-1.24 go mod download ","date":"2022-06-28 18:32:22","objectID":"/ooooo-notes/%E6%90%AD%E5%BB%BA-k8s-1.24-%E7%89%88%E6%9C%AC%E7%9A%84%E6%BA%90%E7%A0%81%E8%B0%83%E8%AF%95%E7%8E%AF%E5%A2%83/:1:1","tags":["k8s","cloud native","source code"],"title":"搭建 k8s 1.24 版本的源码调试环境","uri":"/ooooo-notes/%E6%90%AD%E5%BB%BA-k8s-1.24-%E7%89%88%E6%9C%AC%E7%9A%84%E6%BA%90%E7%A0%81%E8%B0%83%E8%AF%95%E7%8E%AF%E5%A2%83/"},{"categories":["随笔"],"content":"2. 用 IDE 打开 kubernetes 源码 ","date":"2022-06-28 18:32:22","objectID":"/ooooo-notes/%E6%90%AD%E5%BB%BA-k8s-1.24-%E7%89%88%E6%9C%AC%E7%9A%84%E6%BA%90%E7%A0%81%E8%B0%83%E8%AF%95%E7%8E%AF%E5%A2%83/:1:2","tags":["k8s","cloud native","source code"],"title":"搭建 k8s 1.24 版本的源码调试环境","uri":"/ooooo-notes/%E6%90%AD%E5%BB%BA-k8s-1.24-%E7%89%88%E6%9C%AC%E7%9A%84%E6%BA%90%E7%A0%81%E8%B0%83%E8%AF%95%E7%8E%AF%E5%A2%83/"},{"categories":["随笔"],"content":"3. 找到服务的启动参数（比如 kube-controller-manager） # 执行命令 ps aux | grep kube-controller-manager | grep -v grep # 命令执行之后，输出如下, kube-controller-manager 后面的就是程序的参数 root 1584 4.0 0.8 820020 110072 ? Ssl 23:28 0:02 kube-controller-manager --allocate-node-cidrs=true --authentication-kubeconfig=/etc/kubernetes/controller-manager.conf --authorization-kubeconfig=/etc/kubernetes/controller-manager.conf --bind-address=127.0.0.1 --client-ca-file=/etc/kubernetes/pki/ca.crt --cluster-cidr=10.244.0.0/16 --cluster-name=kubernetes --cluster-signing-cert-file=/etc/kubernetes/pki/ca.crt --cluster-signing-key-file=/etc/kubernetes/pki/ca.key --controllers=*,bootstrapsigner,tokencleaner --kubeconfig=/etc/kubernetes/controller-manager.conf --leader-elect=true --requestheader-client-ca-file=/etc/kubernetes/pki/front-proxy-ca.crt --root-ca-file=/etc/kubernetes/pki/ca.crt --service-account-private-key-file=/etc/kubernetes/pki/sa.key --service-cluster-ip-range=10.96.0.0/12 --use-service-account-credentials=true ","date":"2022-06-28 18:32:22","objectID":"/ooooo-notes/%E6%90%AD%E5%BB%BA-k8s-1.24-%E7%89%88%E6%9C%AC%E7%9A%84%E6%BA%90%E7%A0%81%E8%B0%83%E8%AF%95%E7%8E%AF%E5%A2%83/:1:3","tags":["k8s","cloud native","source code"],"title":"搭建 k8s 1.24 版本的源码调试环境","uri":"/ooooo-notes/%E6%90%AD%E5%BB%BA-k8s-1.24-%E7%89%88%E6%9C%AC%E7%9A%84%E6%BA%90%E7%A0%81%E8%B0%83%E8%AF%95%E7%8E%AF%E5%A2%83/"},{"categories":["随笔"],"content":"4. 移动 kubernetes 的静态 pod （比如 kube-controller-manager） cd /etc/kubernetes mv manifests/kube-controller-manager.yaml ./ ","date":"2022-06-28 18:32:22","objectID":"/ooooo-notes/%E6%90%AD%E5%BB%BA-k8s-1.24-%E7%89%88%E6%9C%AC%E7%9A%84%E6%BA%90%E7%A0%81%E8%B0%83%E8%AF%95%E7%8E%AF%E5%A2%83/:1:4","tags":["k8s","cloud native","source code"],"title":"搭建 k8s 1.24 版本的源码调试环境","uri":"/ooooo-notes/%E6%90%AD%E5%BB%BA-k8s-1.24-%E7%89%88%E6%9C%AC%E7%9A%84%E6%BA%90%E7%A0%81%E8%B0%83%E8%AF%95%E7%8E%AF%E5%A2%83/"},{"categories":["随笔"],"content":"5. 用 IDE 启动服务（比如 kube-controller-manager） 程序的入口： cmd/kube-controller-manager/controller-manager.go (其他的服务也是类似的路径) 点击，配置启动参数，如下图 配置启动参数 现在基本就配置好了 ","date":"2022-06-28 18:32:22","objectID":"/ooooo-notes/%E6%90%AD%E5%BB%BA-k8s-1.24-%E7%89%88%E6%9C%AC%E7%9A%84%E6%BA%90%E7%A0%81%E8%B0%83%E8%AF%95%E7%8E%AF%E5%A2%83/:1:5","tags":["k8s","cloud native","source code"],"title":"搭建 k8s 1.24 版本的源码调试环境","uri":"/ooooo-notes/%E6%90%AD%E5%BB%BA-k8s-1.24-%E7%89%88%E6%9C%AC%E7%9A%84%E6%BA%90%E7%A0%81%E8%B0%83%E8%AF%95%E7%8E%AF%E5%A2%83/"},{"categories":["随笔"],"content":"6. 检查服务是否正常启动 （比如 kube-controller-manager） # 执行命令看没有 kube-controller-manager kubectl get pods -A ","date":"2022-06-28 18:32:22","objectID":"/ooooo-notes/%E6%90%AD%E5%BB%BA-k8s-1.24-%E7%89%88%E6%9C%AC%E7%9A%84%E6%BA%90%E7%A0%81%E8%B0%83%E8%AF%95%E7%8E%AF%E5%A2%83/:1:6","tags":["k8s","cloud native","source code"],"title":"搭建 k8s 1.24 版本的源码调试环境","uri":"/ooooo-notes/%E6%90%AD%E5%BB%BA-k8s-1.24-%E7%89%88%E6%9C%AC%E7%9A%84%E6%BA%90%E7%A0%81%E8%B0%83%E8%AF%95%E7%8E%AF%E5%A2%83/"},{"categories":["随笔"],"content":"2. 方法2. 借助 dlv 来调试源码 如果你是 mac/window 系统，可以借助 dlv 来调试源码。 具体步骤如下： ","date":"2022-06-28 18:32:22","objectID":"/ooooo-notes/%E6%90%AD%E5%BB%BA-k8s-1.24-%E7%89%88%E6%9C%AC%E7%9A%84%E6%BA%90%E7%A0%81%E8%B0%83%E8%AF%95%E7%8E%AF%E5%A2%83/:2:0","tags":["k8s","cloud native","source code"],"title":"搭建 k8s 1.24 版本的源码调试环境","uri":"/ooooo-notes/%E6%90%AD%E5%BB%BA-k8s-1.24-%E7%89%88%E6%9C%AC%E7%9A%84%E6%BA%90%E7%A0%81%E8%B0%83%E8%AF%95%E7%8E%AF%E5%A2%83/"},{"categories":["随笔"],"content":"1. 下载源码 (go 的版本要求 1.18.x) git clone git@github.com:kubernetes/kubernetes.git cd kubernetes git checkout -b origin/release-1.24 go mod download 上述下载源码，需要在 本地window 和 k8s节点 上都下载。 注意：由于是远端调试，所以需要在 k8s master 节点上，重新编译源码，去掉 -N -l. ","date":"2022-06-28 18:32:22","objectID":"/ooooo-notes/%E6%90%AD%E5%BB%BA-k8s-1.24-%E7%89%88%E6%9C%AC%E7%9A%84%E6%BA%90%E7%A0%81%E8%B0%83%E8%AF%95%E7%8E%AF%E5%A2%83/:2:1","tags":["k8s","cloud native","source code"],"title":"搭建 k8s 1.24 版本的源码调试环境","uri":"/ooooo-notes/%E6%90%AD%E5%BB%BA-k8s-1.24-%E7%89%88%E6%9C%AC%E7%9A%84%E6%BA%90%E7%A0%81%E8%B0%83%E8%AF%95%E7%8E%AF%E5%A2%83/"},{"categories":["随笔"],"content":"2. 在 k8s master 节点上，重新编译源码 cd kubernetes make DBG=1 # 在 hack/lib/golang.sh 中 ","date":"2022-06-28 18:32:22","objectID":"/ooooo-notes/%E6%90%AD%E5%BB%BA-k8s-1.24-%E7%89%88%E6%9C%AC%E7%9A%84%E6%BA%90%E7%A0%81%E8%B0%83%E8%AF%95%E7%8E%AF%E5%A2%83/:2:2","tags":["k8s","cloud native","source code"],"title":"搭建 k8s 1.24 版本的源码调试环境","uri":"/ooooo-notes/%E6%90%AD%E5%BB%BA-k8s-1.24-%E7%89%88%E6%9C%AC%E7%9A%84%E6%BA%90%E7%A0%81%E8%B0%83%E8%AF%95%E7%8E%AF%E5%A2%83/"},{"categories":["随笔"],"content":"3. 下载可能用到的工具，如 dlv (你可能需要提前设置 GOPATH 环境变量) # 可能非常慢，需要设置代理 go get -u github.com/cloudflare/cfssl/cmd/cfssl go get -u github.com/cloudflare/cfssl/cmd/cfssljson go get -u github.com/go-delve/delve/cmd/dlv # 配置环境变量 PATH=$PATH:$GOPATH/bin ","date":"2022-06-28 18:32:22","objectID":"/ooooo-notes/%E6%90%AD%E5%BB%BA-k8s-1.24-%E7%89%88%E6%9C%AC%E7%9A%84%E6%BA%90%E7%A0%81%E8%B0%83%E8%AF%95%E7%8E%AF%E5%A2%83/:2:3","tags":["k8s","cloud native","source code"],"title":"搭建 k8s 1.24 版本的源码调试环境","uri":"/ooooo-notes/%E6%90%AD%E5%BB%BA-k8s-1.24-%E7%89%88%E6%9C%AC%E7%9A%84%E6%BA%90%E7%A0%81%E8%B0%83%E8%AF%95%E7%8E%AF%E5%A2%83/"},{"categories":["随笔"],"content":"4. 找到服务的启动参数（比如 kube-controller-manager） # 执行命令 ps aux | grep kube-controller-manager | grep -v grep # 命令执行之后，输出如下, kube-controller-manager 后面的就是程序的参数 root 1584 4.0 0.8 820020 110072 ? Ssl 23:28 0:02 kube-controller-manager --allocate-node-cidrs=true --authentication-kubeconfig=/etc/kubernetes/controller-manager.conf --authorization-kubeconfig=/etc/kubernetes/controller-manager.conf --bind-address=127.0.0.1 --client-ca-file=/etc/kubernetes/pki/ca.crt --cluster-cidr=10.244.0.0/16 --cluster-name=kubernetes --cluster-signing-cert-file=/etc/kubernetes/pki/ca.crt --cluster-signing-key-file=/etc/kubernetes/pki/ca.key --controllers=*,bootstrapsigner,tokencleaner --kubeconfig=/etc/kubernetes/controller-manager.conf --leader-elect=true --requestheader-client-ca-file=/etc/kubernetes/pki/front-proxy-ca.crt --root-ca-file=/etc/kubernetes/pki/ca.crt --service-account-private-key-file=/etc/kubernetes/pki/sa.key --service-cluster-ip-range=10.96.0.0/12 --use-service-account-credentials=true ","date":"2022-06-28 18:32:22","objectID":"/ooooo-notes/%E6%90%AD%E5%BB%BA-k8s-1.24-%E7%89%88%E6%9C%AC%E7%9A%84%E6%BA%90%E7%A0%81%E8%B0%83%E8%AF%95%E7%8E%AF%E5%A2%83/:2:4","tags":["k8s","cloud native","source code"],"title":"搭建 k8s 1.24 版本的源码调试环境","uri":"/ooooo-notes/%E6%90%AD%E5%BB%BA-k8s-1.24-%E7%89%88%E6%9C%AC%E7%9A%84%E6%BA%90%E7%A0%81%E8%B0%83%E8%AF%95%E7%8E%AF%E5%A2%83/"},{"categories":["随笔"],"content":"5. 移动 kubernetes 的静态 pod （比如 kube-controller-manager） cd /etc/kubernetes mv manifests/kube-controller-manager.yaml ./ ","date":"2022-06-28 18:32:22","objectID":"/ooooo-notes/%E6%90%AD%E5%BB%BA-k8s-1.24-%E7%89%88%E6%9C%AC%E7%9A%84%E6%BA%90%E7%A0%81%E8%B0%83%E8%AF%95%E7%8E%AF%E5%A2%83/:2:5","tags":["k8s","cloud native","source code"],"title":"搭建 k8s 1.24 版本的源码调试环境","uri":"/ooooo-notes/%E6%90%AD%E5%BB%BA-k8s-1.24-%E7%89%88%E6%9C%AC%E7%9A%84%E6%BA%90%E7%A0%81%E8%B0%83%E8%AF%95%E7%8E%AF%E5%A2%83/"},{"categories":["随笔"],"content":"6. 用 dlv 启动服务 注意: 启动参数和程序路径，配置成你自己的，监听的端口是 2346 dlv 在配置程序参数时，有 --， 如果后面参数有特殊符号，用 --key=\"value\" 形式 dlv 启动之后，必须要触发(IDE go remote)，才能启动, 否则会一直等着。 dlv --listen=:2346 --headless=true --api-version=2 --accept-multiclient exec /root/kubernetes/_output/bin/kube-controller-manager -- --allocate-node-cidrs=true --authentication-kubeconfig=/etc/kubernetes/controller-manager.conf --authorization-kubeconfig=/etc/kubernetes/controller-manager.conf --bind-address=127.0.0.1 --client-ca-file=/etc/kubernetes/pki/ca.crt --cluster-cidr=10.244.0.0/16 --cluster-name=kubernetes --cluster-signing-cert-file=/etc/kubernetes/pki/ca.crt --cluster-signing-key-file=/etc/kubernetes/pki/ca.key --controllers=*,bootstrapsigner,tokencleaner --kubeconfig=/etc/kubernetes/controller-manager.conf --leader-elect=true --requestheader-client-ca-file=/etc/kubernetes/pki/front-proxy-ca.crt --root-ca-file=/etc/kubernetes/pki/ca.crt --service-account-private-key-file=/etc/kubernetes/pki/sa.key --service-cluster-ip-range=10.96.0.0/12 --use-service-account-credentials=true ","date":"2022-06-28 18:32:22","objectID":"/ooooo-notes/%E6%90%AD%E5%BB%BA-k8s-1.24-%E7%89%88%E6%9C%AC%E7%9A%84%E6%BA%90%E7%A0%81%E8%B0%83%E8%AF%95%E7%8E%AF%E5%A2%83/:2:6","tags":["k8s","cloud native","source code"],"title":"搭建 k8s 1.24 版本的源码调试环境","uri":"/ooooo-notes/%E6%90%AD%E5%BB%BA-k8s-1.24-%E7%89%88%E6%9C%AC%E7%9A%84%E6%BA%90%E7%A0%81%E8%B0%83%E8%AF%95%E7%8E%AF%E5%A2%83/"},{"categories":["随笔"],"content":"7. 用 IDE 连接 dlv 服务（比如 kube-controller-manager） 程序的入口： cmd/kube-controller-manager/controller-manager.go (其他的服务也是类似的路径) 添加 Go Remote， 配置 host 和 port。 点击 ok，然后启动服务。 连接dlv 现在基本就配置好了 ","date":"2022-06-28 18:32:22","objectID":"/ooooo-notes/%E6%90%AD%E5%BB%BA-k8s-1.24-%E7%89%88%E6%9C%AC%E7%9A%84%E6%BA%90%E7%A0%81%E8%B0%83%E8%AF%95%E7%8E%AF%E5%A2%83/:2:7","tags":["k8s","cloud native","source code"],"title":"搭建 k8s 1.24 版本的源码调试环境","uri":"/ooooo-notes/%E6%90%AD%E5%BB%BA-k8s-1.24-%E7%89%88%E6%9C%AC%E7%9A%84%E6%BA%90%E7%A0%81%E8%B0%83%E8%AF%95%E7%8E%AF%E5%A2%83/"},{"categories":["随笔"],"content":"8. 提供一个调试的脚本 （可选） 你现在会发现，如果想要调试，就必须要把 manifests/kube-controller-manager.yaml 移出去，等不需要调试了，再把这个文件移回来，这样非常麻烦。所以使用一个脚本来实现。 脚本内容如下(注意更改你的路径)： cleanup() { mv /etc/kubernetes/kube-controller-manager.yaml /etc/kubernetes/manifests } trap cleanup EXIT mv /etc/kubernetes/manifests/kube-controller-manager.yaml /etc/kubernetes dlv --listen=:2346 --headless=true --api-version=2 --accept-multiclient exec /root/kubernetes/_output/bin/kube-controller-manager -- --allocate-node-cidrs=true --authentication-kubeconfig=/etc/kubernetes/controller-manager.conf --authorization-kubeconfig=/etc/kubernetes/controller-manager.conf --bind-address=127.0.0.1 --client-ca-file=/etc/kubernetes/pki/ca.crt --cluster-cidr=10.244.0.0/16 --cluster-name=kubernetes --cluster-signing-cert-file=/etc/kubernetes/pki/ca.crt --cluster-signing-key-file=/etc/kubernetes/pki/ca.key --controllers=*,bootstrapsigner,tokencleaner --kubeconfig=/etc/kubernetes/controller-manager.conf --leader-elect=true --requestheader-client-ca-file=/etc/kubernetes/pki/front-proxy-ca.crt --root-ca-file=/etc/kubernetes/pki/ca.crt --service-account-private-key-file=/etc/kubernetes/pki/sa.key --service-cluster-ip-range=10.96.0.0/12 --use-service-account-credentials=true ","date":"2022-06-28 18:32:22","objectID":"/ooooo-notes/%E6%90%AD%E5%BB%BA-k8s-1.24-%E7%89%88%E6%9C%AC%E7%9A%84%E6%BA%90%E7%A0%81%E8%B0%83%E8%AF%95%E7%8E%AF%E5%A2%83/:2:8","tags":["k8s","cloud native","source code"],"title":"搭建 k8s 1.24 版本的源码调试环境","uri":"/ooooo-notes/%E6%90%AD%E5%BB%BA-k8s-1.24-%E7%89%88%E6%9C%AC%E7%9A%84%E6%BA%90%E7%A0%81%E8%B0%83%E8%AF%95%E7%8E%AF%E5%A2%83/"},{"categories":["随笔"],"content":"1. install logstash refer to logstash document ","date":"2022-06-01 18:32:22","objectID":"/ooooo-notes/logstash-%E7%9A%84%E7%AE%80%E5%8D%95%E4%BD%BF%E7%94%A8/:1:0","tags":["logstash"],"title":"logstash 的简单使用","uri":"/ooooo-notes/logstash-%E7%9A%84%E7%AE%80%E5%8D%95%E4%BD%BF%E7%94%A8/"},{"categories":["随笔"],"content":"2. logstash example config file input { tcp { port =\u003e 12345 codec =\u003e \"json_lines\" } } filter{ grok { match =\u003e [\"message\", \"%{TIMESTAMP_ISO8601:logdate}\"] } date { match =\u003e [\"logdate\", \"yyyy-MM-dd HH:mm:ss.SSS\"] target =\u003e \"@timestamp\" } mutate { remove_field =\u003e [\"logdate\"] } ruby { code =\u003e \"event.set('timestamp', event.get('@timestamp').time.localtime + 8*60*60)\" } ruby { code =\u003e \"event.set('@timestamp',event.get('timestamp'))\" } mutate { remove_field =\u003e [\"timestamp\"] } } output { stdout { codec =\u003e rubydebug { metadata =\u003e true } } file { path =\u003e \"./logs/%{+YYYY-MM-dd-HH}.log\" codec =\u003e line { format =\u003e \"%{message}\"} } } notes: it will serve TCP connection on localhost:12345. uses codec named json_lines, json data format such as { \"message\" : \"xxxx\" }. matche date pattern in the log, then use it as its time. reset the field @timestamp, output to the local file. ","date":"2022-06-01 18:32:22","objectID":"/ooooo-notes/logstash-%E7%9A%84%E7%AE%80%E5%8D%95%E4%BD%BF%E7%94%A8/:1:1","tags":["logstash"],"title":"logstash 的简单使用","uri":"/ooooo-notes/logstash-%E7%9A%84%E7%AE%80%E5%8D%95%E4%BD%BF%E7%94%A8/"},{"categories":["随笔"],"content":"1. install wsl open Microsoft Store, then search ubuntu and click to install it. open terminal # list all linux subsystem wsl --list # set default linux subsystem, then you can input 'wsl' to inter system wsl --set-default ubuntu # enter default linux subsystem wsl # install cmake, g++, gcc，gdb cd /usr/local wget https://cmake.org/files/v3.22/cmake-3.22.0-linux-x86_64.tar.gz tar xf cmake-3.22.0-linux-x86_64.tar.gz ln -s /usr/local/cmake-3.22.0-linux-x86_64/bin/cmake /usr/bin sudo apt install build-essential ","date":"2022-06-01 18:32:22","objectID":"/ooooo-notes/%E5%9C%A8-windows-%E4%B8%8A%E8%B0%83%E8%AF%95-redis/:1:0","tags":["redis"],"title":"在 windows 上调试 redis","uri":"/ooooo-notes/%E5%9C%A8-windows-%E4%B8%8A%E8%B0%83%E8%AF%95-redis/"},{"categories":["随笔"],"content":"2. setting clion open File | Settings | Build, Execution, Deployment | Toolchains menu. add new toolchains and select wsl. 在 clion 中创建工具链 setting wsl configuration, you maybe install cmake, gcc, g++, gdb. you must execute command git config core.autocrlf input in your terminal, because windows is CRLF, then git clone . select wsl in File | Settings | Build, Execution, Deployment | Makefile, because building redis by using makefile. login wsl and enter redis directory, for example: cd /mnt/c/Users/ooooo/Development/code/Demo/redis execute command make, you maybe need to execute cd src \u0026\u0026 ls | grep .sh | xargs chmod a+x 选择可执行文件 ","date":"2022-06-01 18:32:22","objectID":"/ooooo-notes/%E5%9C%A8-windows-%E4%B8%8A%E8%B0%83%E8%AF%95-redis/:2:0","tags":["redis"],"title":"在 windows 上调试 redis","uri":"/ooooo-notes/%E5%9C%A8-windows-%E4%B8%8A%E8%B0%83%E8%AF%95-redis/"},{"categories":["随笔"],"content":"1. 机器初始化设置 ","date":"2022-04-01 18:32:22","objectID":"/ooooo-notes/%E4%BD%BF%E7%94%A8-ubuntu-%E6%9D%A5%E5%AE%89%E8%A3%85-kubernetes-1.24-%E7%89%88%E6%9C%AC/:1:0","tags":["k8s","cloud native"],"title":"使用 ubuntu 来安装 kubernetes 1.24 版本","uri":"/ooooo-notes/%E4%BD%BF%E7%94%A8-ubuntu-%E6%9D%A5%E5%AE%89%E8%A3%85-kubernetes-1.24-%E7%89%88%E6%9C%AC/"},{"categories":["随笔"],"content":"hostname 设置 hostnamectl ## 查看当前的hostname hostnamectl set-hostname node1 ## 设置主机名为node1 ","date":"2022-04-01 18:32:22","objectID":"/ooooo-notes/%E4%BD%BF%E7%94%A8-ubuntu-%E6%9D%A5%E5%AE%89%E8%A3%85-kubernetes-1.24-%E7%89%88%E6%9C%AC/:1:1","tags":["k8s","cloud native"],"title":"使用 ubuntu 来安装 kubernetes 1.24 版本","uri":"/ooooo-notes/%E4%BD%BF%E7%94%A8-ubuntu-%E6%9D%A5%E5%AE%89%E8%A3%85-kubernetes-1.24-%E7%89%88%E6%9C%AC/"},{"categories":["随笔"],"content":"/etc/hosts 文件 和 DNS 配置 # k8s master 192.168.130.131 node1 # 更改dns配置 vim /etc/systemd/resolved.conf # 更改下面内容 [Resolve] DNS=8.8.8.8 8.8.4.4 # 重启dns systemctl restart systemd-resolved.service refer: ubuntu dns resolver ","date":"2022-04-01 18:32:22","objectID":"/ooooo-notes/%E4%BD%BF%E7%94%A8-ubuntu-%E6%9D%A5%E5%AE%89%E8%A3%85-kubernetes-1.24-%E7%89%88%E6%9C%AC/:1:2","tags":["k8s","cloud native"],"title":"使用 ubuntu 来安装 kubernetes 1.24 版本","uri":"/ooooo-notes/%E4%BD%BF%E7%94%A8-ubuntu-%E6%9D%A5%E5%AE%89%E8%A3%85-kubernetes-1.24-%E7%89%88%E6%9C%AC/"},{"categories":["随笔"],"content":"创建非 root 用户(可选) # 添加用户 useradd ooooo -g ooooo # 修改用户密码 passwd ooooo ","date":"2022-04-01 18:32:22","objectID":"/ooooo-notes/%E4%BD%BF%E7%94%A8-ubuntu-%E6%9D%A5%E5%AE%89%E8%A3%85-kubernetes-1.24-%E7%89%88%E6%9C%AC/:1:3","tags":["k8s","cloud native"],"title":"使用 ubuntu 来安装 kubernetes 1.24 版本","uri":"/ooooo-notes/%E4%BD%BF%E7%94%A8-ubuntu-%E6%9D%A5%E5%AE%89%E8%A3%85-kubernetes-1.24-%E7%89%88%E6%9C%AC/"},{"categories":["随笔"],"content":"安装 containerd 和 runc 安装 containerd wget https://github.com/containerd/containerd/releases/download/v1.6.6/containerd-1.6.6-linux-amd64.tar.gz tar Cxzvf /usr/local containerd-1.6.6-linux-amd64.tar.gz mkdir -p /usr/local/lib/systemd/system/ 通过 systemd 来启动 containerd 将下面的内容写入 /usr/local/lib/systemd/system/containerd.service # Copyright The containerd Authors. # # Licensed under the Apache License, Version 2.0 (the \"License\"); # you may not use this file except in compliance with the License. # You may obtain a copy of the License at # # http://www.apache.org/licenses/LICENSE-2.0 # # Unless required by applicable law or agreed to in writing, software # distributed under the License is distributed on an \"AS IS\" BASIS, # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. # See the License for the specific language governing permissions and # limitations under the License. [Unit] Description=containerd container runtime Documentation=https://containerd.io After=network.target local-fs.target [Service] ExecStartPre=-/sbin/modprobe overlay ExecStart=/usr/local/bin/containerd Type=notify Delegate=yes KillMode=process Restart=always RestartSec=5 # Having non-zero Limit*s causes performance problems due to accounting overhead # in the kernel. We recommend using cgroups to do container-local accounting. LimitNPROC=infinity LimitCORE=infinity LimitNOFILE=infinity # Comment TasksMax if your systemd version does not supports it. # Only systemd 226 and above support this version. TasksMax=infinity OOMScoreAdjust=-999 [Install] WantedBy=multi-user.target 启动 containerd systemctl daemon-reload systemctl enable --now containerd 配置 containerd mkdir -p /etc/containerd # 生成默认配置文件 containerd config default | tee /etc/containerd/config.toml # 修改 /etc/containerd/config.toml 配置 # image 使用阿里云的地址， SystemdCgroup 更改为 true sandbox_image = \"registry.aliyuncs.com/google_containers/pause:3.6\" ... [plugins.\"io.containerd.grpc.v1.cri\".containerd.runtimes.runc.options] ... SystemdCgroup = true # 修改完成后 systemctl restart containerd 安装 runc wget https://github.com/opencontainers/runc/releases/download/v1.1.3/runc.amd64 install -m 755 runc.amd64 /usr/local/sbin/runc 安装 cni 插件 wget https://github.com/containernetworking/plugins/releases/download/v1.1.1/cni-plugins-linux-amd64-v1.1.1.tgz mkdir -p /opt/cni/bin tar Cxzvf /opt/cni/bin cni-plugins-linux-amd64-v1.1.1.tgz ","date":"2022-04-01 18:32:22","objectID":"/ooooo-notes/%E4%BD%BF%E7%94%A8-ubuntu-%E6%9D%A5%E5%AE%89%E8%A3%85-kubernetes-1.24-%E7%89%88%E6%9C%AC/:1:4","tags":["k8s","cloud native"],"title":"使用 ubuntu 来安装 kubernetes 1.24 版本","uri":"/ooooo-notes/%E4%BD%BF%E7%94%A8-ubuntu-%E6%9D%A5%E5%AE%89%E8%A3%85-kubernetes-1.24-%E7%89%88%E6%9C%AC/"},{"categories":["随笔"],"content":"2. k8s 安装 官方 k8s 安装文档 # 参考文档检查服务器的状态是否可以安装 k8s 服务 # 临时关闭 swap 分区 swapoff -a # 查看 swap 分区是否关闭，显示 0 表示已关闭 free -h # 永久关闭 swap 分区 编辑 /etc/fstab 文件, 注释最后一行 # 检查 br_netfilter 是否被加载，没有任何输出，表示没有加载 lsmod | grep br_netfilter # 加载 br_netfilter 模块 sudo modprobe br_netfilter ## 配置网络 cat \u003c\u003cEOF | sudo tee /etc/modules-load.d/k8s.conf br_netfilter EOF cat \u003c\u003cEOF | sudo tee /etc/sysctl.d/k8s.conf net.bridge.bridge-nf-call-ip6tables = 1 net.bridge.bridge-nf-call-iptables = 1 net.ipv4.ip_forward = 1 EOF sudo sysctl --system # 安装软件 sudo apt-get update sudo apt-get install -y apt-transport-https ca-certificates curl sudo curl -fsSLo /usr/share/keyrings/kubernetes-archive-keyring.gpg https://mirrors.aliyun.com/kubernetes/apt/doc/apt-key.gpg echo \"deb [signed-by=/usr/share/keyrings/kubernetes-archive-keyring.gpg] https://mirrors.aliyun.com/kubernetes/apt/ kubernetes-xenial main\" | sudo tee /etc/apt/sources.list.d/kubernetes.list sudo apt-get update # 默认安装最新版本 sudo apt-get install -y kubelet kubeadm kubectl # 不自动更新 sudo apt-mark hold kubelet kubeadm kubectl # 查看镜像列表， 报错需要添加配置, crictl 是官方提供的 crictl images # vim /etc/crictl.yaml 添加以下内容 runtime-endpoint: unix:///run/containerd/containerd.sock image-endpoint: unix:///run/containerd/containerd.sock timeout: 10 debug: false # 设置 kubelet 开机启动，并且现在启动 # 启动之后可能会报错，如果原因是 没有读取到 kubelet 的配置文件，这里可以不用管，稍后会重启这个服务 sudo systemctl enable --now kubelet # 查看 kubelet 的状态 sudo systemctl status kubelet # 查看 kubelet 的日志 journalctl -xeu kubelet ","date":"2022-04-01 18:32:22","objectID":"/ooooo-notes/%E4%BD%BF%E7%94%A8-ubuntu-%E6%9D%A5%E5%AE%89%E8%A3%85-kubernetes-1.24-%E7%89%88%E6%9C%AC/:2:0","tags":["k8s","cloud native"],"title":"使用 ubuntu 来安装 kubernetes 1.24 版本","uri":"/ooooo-notes/%E4%BD%BF%E7%94%A8-ubuntu-%E6%9D%A5%E5%AE%89%E8%A3%85-kubernetes-1.24-%E7%89%88%E6%9C%AC/"},{"categories":["随笔"],"content":"3. 创建 k8s 集群 创建 k8s 集群官方文档 k8s pod network 插件文档 # 执行 kubeadm init 命令， 在 k8s master 机器上执行，默认情况下， k8s 创建 pod 不会在 master 机器上 # 重点注意: --pod-network-cidr=10.244.0.0/16 这个参数必须要有，没有的话安装 cni 会报错 # 注意 preflight 的前置检查输出，如果有问题，百度自行解决 # 替换为你自己的 ip 和 hostname sudo kubeadm init --image-repository registry.aliyuncs.com/google_containers --apiserver-advertise-address=192.168.130.128 --pod-network-cidr=10.244.0.0/16 --control-plane-endpoint=node1 # 执行命令之后，会有 kubeadm join 输出行 # （分为 master-token 和 worker-token）， 类似于下面的命令，可以在另一个节点上执行 worker-join-token 的命令 sudo kubeadm join 192.168.130.128:6443 --token 8auvt0.zfw0ayr45d80q8pb \\ --discovery-token-ca-cert-hash sha256:efe854739efef5fbaf3f6e28c899481c8d7797c1997fc8315b921a9ede400ca8 # 去掉污点，让单个节点也可以运行, (我这里只有一个节点) kubectl taint nodes --all node-role.kubernetes.io/control-plane- node-role.kubernetes.io/master- ## 在机器上执行 kubeadm join 或者 kubeadm init 命令之后，重启 kubelet 服务 sudo systemctl restart kubelet sudo systemctl status kubelet # 设置 kubectl 的配置文件， 为 $HOME/.kube/config mkdir -p $HOME/.kube sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config sudo chown $(id -u):$(id -g) $HOME/.kube/config # 安装 pod network 插件, 这里使用 calico 插件 curl -o calico-operator.yaml https://projectcalico.docs.tigera.io/manifests/tigera-operator.yaml curl -o calico-custom-resources.yaml https://projectcalico.docs.tigera.io/manifests/custom-resources.yaml # 重点 # 更改 calico-custom-resources.yaml 的 cidr 配置, 值为 --pod-network-cidr （在 kubeadm init 指定了） # 多个网卡，也可以更改，否则可能会报错，搜索 interface kubectl create -f calico-operator.yaml kubectl create -f calico-custom-resources.yaml # 查看 calico 是否已经启动完成, cni 也启动成功 kubectl get pods -A # 成功之后会有下面的服务， 都是 running 状态 calico-apiserver calico-apiserver-78c5f69667-gbxbv 1/1 Running 0 88s calico-apiserver calico-apiserver-78c5f69667-h64wk 1/1 Running 0 88s calico-system calico-kube-controllers-68884f975d-q4l8s 1/1 Running 0 40m calico-system calico-node-4d7hs 1/1 Running 0 40m calico-system calico-typha-854c6b9b4b-s8ls7 1/1 Running 0 40m kube-system coredns-74586cf9b6-4pkxf 1/1 Running 0 76m kube-system coredns-74586cf9b6-9hxwl 1/1 Running 0 76m kube-system etcd-node1 1/1 Running 0 76m kube-system kube-apiserver-node1 1/1 Running 0 76m kube-system kube-controller-manager-node1 1/1 Running 0 76m kube-system kube-proxy-mn6fr 1/1 Running 0 76m kube-system kube-scheduler-node1 1/1 Running 0 76m tigera-operator tigera-operator-5fb55776df-gjs7s 1/1 Running 0 64m ","date":"2022-04-01 18:32:22","objectID":"/ooooo-notes/%E4%BD%BF%E7%94%A8-ubuntu-%E6%9D%A5%E5%AE%89%E8%A3%85-kubernetes-1.24-%E7%89%88%E6%9C%AC/:3:0","tags":["k8s","cloud native"],"title":"使用 ubuntu 来安装 kubernetes 1.24 版本","uri":"/ooooo-notes/%E4%BD%BF%E7%94%A8-ubuntu-%E6%9D%A5%E5%AE%89%E8%A3%85-kubernetes-1.24-%E7%89%88%E6%9C%AC/"},{"categories":["随笔"],"content":"install nfs in docker ","date":"2022-03-01 18:32:22","objectID":"/ooooo-notes/%E5%9C%A8-docker-%E4%B8%8A%E5%AE%89%E8%A3%85-nfs/:1:0","tags":["docker","nfs"],"title":"在 docker 上安装 nfs","uri":"/ooooo-notes/%E5%9C%A8-docker-%E4%B8%8A%E5%AE%89%E8%A3%85-nfs/"},{"categories":["随笔"],"content":"1. create share directory used by nfs mkdir -p /home/ooooo/shared/nfs ","date":"2022-03-01 18:32:22","objectID":"/ooooo-notes/%E5%9C%A8-docker-%E4%B8%8A%E5%AE%89%E8%A3%85-nfs/:1:1","tags":["docker","nfs"],"title":"在 docker 上安装 nfs","uri":"/ooooo-notes/%E5%9C%A8-docker-%E4%B8%8A%E5%AE%89%E8%A3%85-nfs/"},{"categories":["随笔"],"content":"2. create exports.txt used by nfs This the exports.txt mainly used to mount dir (path in the container ) and permission. for example: It indicates read only for all ip. vim /home/ooooo/exports.txt /home/ooooo/shared/nfs *(ro,no_subtree_check) ","date":"2022-03-01 18:32:22","objectID":"/ooooo-notes/%E5%9C%A8-docker-%E4%B8%8A%E5%AE%89%E8%A3%85-nfs/:1:2","tags":["docker","nfs"],"title":"在 docker 上安装 nfs","uri":"/ooooo-notes/%E5%9C%A8-docker-%E4%B8%8A%E5%AE%89%E8%A3%85-nfs/"},{"categories":["随笔"],"content":"3. execute docker command docker run -d \\ -v /home/ooooo/shared/nfs:/home/ooooo/shared/nfs \\ -v /home/ooooo/exports.txt:/etc/exports:ro \\ --cap-add SYS_ADMIN \\ -p 2049:2049 \\ erichough/nfs-server # check nfs server netstat -nla | grep 2049 # mount nfs dir (check mount.nfs whether is exist ) # 172.17.0.2 is container ip mount 172.17.0.2:/home/ooooo/shared/nfs /home/ooooo/nfs-mount ","date":"2022-03-01 18:32:22","objectID":"/ooooo-notes/%E5%9C%A8-docker-%E4%B8%8A%E5%AE%89%E8%A3%85-nfs/:1:3","tags":["docker","nfs"],"title":"在 docker 上安装 nfs","uri":"/ooooo-notes/%E5%9C%A8-docker-%E4%B8%8A%E5%AE%89%E8%A3%85-nfs/"},{"categories":["随笔"],"content":"5. 参考 docker images ","date":"2022-03-01 18:32:22","objectID":"/ooooo-notes/%E5%9C%A8-docker-%E4%B8%8A%E5%AE%89%E8%A3%85-nfs/:1:4","tags":["docker","nfs"],"title":"在 docker 上安装 nfs","uri":"/ooooo-notes/%E5%9C%A8-docker-%E4%B8%8A%E5%AE%89%E8%A3%85-nfs/"},{"categories":["随笔"],"content":" cygwin 的环境变量要放在第一个，这样 rsync 和 ssh 都是 cygwin 的. window 是执行命令 where ssh， 看看 ssh 有几个实现 （比如 openssh 和 cygwin 的 ssh ） refer: window install cygwin ","date":"2022-01-03 08:00:00","objectID":"/ooooo-notes/%E5%9C%A8-windows-%E4%B8%AD-rsync-%E9%97%AE%E9%A2%98/:0:0","tags":["resolution"],"title":"在 windows 中 rsync 问题","uri":"/ooooo-notes/%E5%9C%A8-windows-%E4%B8%AD-rsync-%E9%97%AE%E9%A2%98/"},{"categories":["随笔"],"content":" win10安装wireshark经常报“KB2999226 和 KB3118401” install wireshark open installation directory, manually install vc_redist.x64.exe by double click reinstall wireshark ","date":"2022-01-02 08:00:00","objectID":"/ooooo-notes/%E5%9C%A8-windows-%E4%B8%AD%E5%AE%89%E8%A3%85-wireshark-%E6%8A%A5%E9%94%99/:0:0","tags":["resolution"],"title":"在 windows 中安装 wireshark 报错","uri":"/ooooo-notes/%E5%9C%A8-windows-%E4%B8%AD%E5%AE%89%E8%A3%85-wireshark-%E6%8A%A5%E9%94%99/"},{"categories":["计划"],"content":"0、持续学习者 Talk is cheap. Show me the code. 英语比编程简单。 学习和实践要平衡。 学会和时间做朋友。 学会投资，学会理财。 学会先做减法，再做加法。 学英语很重要，学英语很重要，学英语很重要。 说明： ⭕ 进行中 ✅ 已完成 ❌ 已废弃 ❓ 有必要 ❗ 重要性 📝 记笔记 🖊 写代码 ","date":"2022-01-01 09:00:00","objectID":"/ooooo-notes/2022%E5%B9%B4%E8%AE%A1%E5%88%92/:1:0","tags":["learning"],"title":"2022年学习计划","uri":"/ooooo-notes/2022%E5%B9%B4%E8%AE%A1%E5%88%92/"},{"categories":["计划"],"content":"1、关于英语 《新概念英语》 ⭕ 《每日英语听力 ~ EnglishPod》 ","date":"2022-01-01 09:00:00","objectID":"/ooooo-notes/2022%E5%B9%B4%E8%AE%A1%E5%88%92/:2:0","tags":["learning"],"title":"2022年学习计划","uri":"/ooooo-notes/2022%E5%B9%B4%E8%AE%A1%E5%88%92/"},{"categories":["计划"],"content":"2、关于技术 计划 🎉： 只记录自己认为有用的笔记。 ","date":"2022-01-01 09:00:00","objectID":"/ooooo-notes/2022%E5%B9%B4%E8%AE%A1%E5%88%92/:3:0","tags":["learning"],"title":"2022年学习计划","uri":"/ooooo-notes/2022%E5%B9%B4%E8%AE%A1%E5%88%92/"},{"categories":["计划"],"content":"1、书籍 0️⃣1️⃣. 《Java 性能优化权威指南》 ✅ 0️⃣2️⃣. 《深入理解 Java 虚拟机（第3版）》 ✅ 0️⃣3️⃣. 《Spring Cloud 微服务：入门、实战与进阶》 ❌ 0️⃣4️⃣. 《arthas》 0️⃣5️⃣. 《Java 并发编程实战》 ✅ 0️⃣6️⃣. 《深入理解 Kafka：核心设计与实践原理》 0️⃣7️⃣. 《从零开始学架构》 ✅ 0️⃣8️⃣. 《高可用可伸缩微服务架构》 ❌ 0️⃣9️⃣. 《分布式一致性算法开发实战》 1️⃣0️⃣. 《Go Web 编程》 ⭕ 1️⃣2️⃣. 《Effective C++》 1️⃣3️⃣. 《More Effective C++》 1️⃣4️⃣. 《深度探索C++对象模型》 1️⃣5️⃣. 《Go语言设计与实现》 1️⃣6️⃣. 《wireshark网络分析的艺术》 ✅ 1️⃣7️⃣. 《Vim实用技巧（第2版）》 ⭕ 1️⃣8️⃣. 《RocketMQ技术内幕 第二版》 1️⃣9️⃣. 《Kubernetes in Action中文版》 ✅ 2️⃣1️⃣. 《rocketmq 源码》 ⭕ 2️⃣2️⃣. 《kubernetes 源码》 ⭕ 2️⃣3️⃣. 《istio 源码》 2️⃣4️⃣. 《etcd 源码》 2️⃣5️⃣. 《dubbo 源码》 ⭕ 2️⃣6️⃣. 《pulsar 源码》 ❌ 2️⃣7️⃣. 《nsq 源码》 2️⃣8️⃣. 《eventing 源码》 2️⃣9️⃣. 《serving 源码》 3️⃣0️⃣. 《深入浅出Istio：Service Mesh快速入门与实践》 ✅ 3️⃣1️⃣. 《Istio服务网格技术解析与实践》 ✅ 3️⃣2️⃣. 《云原生服务网格Istio：原理、实践、架构与源码解析》 3️⃣3️⃣. 《gRPC与云原生应用开发》 ✅ 3️⃣4️⃣. 《Quarkus 实战》 ✅ 3️⃣4️⃣. 《gin 源码》 ✅ 3️⃣4️⃣. 《grpc-go 源码》 ⭕ 3️⃣5️⃣. 《Go语言精进之路 I》 ✅ 3️⃣6️⃣. 《Go语言精进之路 II》 ✅ 3️⃣7️⃣. 《Wireshark网络分析就这么简单》 ✅ 3️⃣8️⃣. 《MySQL技术内幕》 3️⃣9️⃣. 《深入解析Java虚拟机HotSpot》 4️⃣0️⃣. 《Rust权威指南》 ⭕ 4️⃣1️⃣. 《深入剖析Kubernetes》 4️⃣2️⃣. 《Kubernetes编程》 ⭕ 4️⃣3️⃣. 《Kubernetes源码剖析》 ✅ 4️⃣4️⃣. 《Kubernetes设计模式》 4️⃣5️⃣. 《Helm学习指南：Kubernetes上的应用程序管理》 ✅ 4️⃣6️⃣. 《Knative实战:基于Kubernetes的无服务器架构实践》 ✅ 4️⃣7️⃣. 《深入剖析Java虚拟机》 4️⃣8️⃣. 《Groovy程序设计》 ✅ ","date":"2022-01-01 09:00:00","objectID":"/ooooo-notes/2022%E5%B9%B4%E8%AE%A1%E5%88%92/:3:1","tags":["learning"],"title":"2022年学习计划","uri":"/ooooo-notes/2022%E5%B9%B4%E8%AE%A1%E5%88%92/"},{"categories":["计划"],"content":"2、文档 0️⃣1️⃣. 《深入拆解 Java 虚拟机》 0️⃣2️⃣. 《How to write Go code》 0️⃣3️⃣. 《Effective Go》 0️⃣4️⃣. 从 0 开始带你成为JVM实战高手 ⭕ 0️⃣4️⃣. Go 语言项目开发实战 ⭕ 0️⃣5️⃣. Redis 核心技术与实战 ✅ 0️⃣6️⃣. Redis 源码剖析与实战 ⭕ 0️⃣7️⃣. 深入拆解 Tomcat \u0026 Jetty ✅ 0️⃣8️⃣. 深入 C 语言和程序运行原理 0️⃣9️⃣. 罗剑锋的 C++ 实战笔记 ✅ ","date":"2022-01-01 09:00:00","objectID":"/ooooo-notes/2022%E5%B9%B4%E8%AE%A1%E5%88%92/:3:2","tags":["learning"],"title":"2022年学习计划","uri":"/ooooo-notes/2022%E5%B9%B4%E8%AE%A1%E5%88%92/"},{"categories":["计划"],"content":"3、视频 0️⃣1️⃣. 《玩转算法系列–图论精讲》 0️⃣2️⃣. 《玩转算法面试》 ⭕ 0️⃣3️⃣. 《看的见的算法》 0️⃣4️⃣. 《极客时间– 算法进阶训练营》 ✅ 0️⃣5️⃣. 《Dubbo 3 深度剖析 - 透过源码认识你》 ✅ 0️⃣5️⃣. 《Go 微服务实战 38 讲》 ✅ 0️⃣6️⃣. 《Netty 源码剖析与实战》 ✅ ","date":"2022-01-01 09:00:00","objectID":"/ooooo-notes/2022%E5%B9%B4%E8%AE%A1%E5%88%92/:3:3","tags":["learning"],"title":"2022年学习计划","uri":"/ooooo-notes/2022%E5%B9%B4%E8%AE%A1%E5%88%92/"},{"categories":["计划"],"content":"4、算法 0️⃣1️⃣. 每周至少 5 道 Leetcode。 ⭕ 0️⃣2️⃣. leetcode 全站排名1000以内。 ⭕ 0️⃣3️⃣. leetcode 周赛全国排名2000以内。 ⭕ 0️⃣4️⃣. leetcode 周赛全球排名10000以内。 ⭕ ","date":"2022-01-01 09:00:00","objectID":"/ooooo-notes/2022%E5%B9%B4%E8%AE%A1%E5%88%92/:3:4","tags":["learning"],"title":"2022年学习计划","uri":"/ooooo-notes/2022%E5%B9%B4%E8%AE%A1%E5%88%92/"},{"categories":["计划"],"content":"4、关于其他 ","date":"2022-01-01 09:00:00","objectID":"/ooooo-notes/2022%E5%B9%B4%E8%AE%A1%E5%88%92/:4:0","tags":["learning"],"title":"2022年学习计划","uri":"/ooooo-notes/2022%E5%B9%B4%E8%AE%A1%E5%88%92/"},{"categories":["计划"],"content":"1、书籍 🎉 0️⃣1️⃣. 《聪明的投资者》 0️⃣2️⃣. 《一万小时天才理论》 ❌ 0️⃣3️⃣. 《番茄工作法图解》 ❌ 0️⃣4️⃣. 《三体》 ✅ 0️⃣5️⃣. 《三体Ⅱ》 0️⃣6️⃣. 《三体Ⅲ》 0️⃣7️⃣. 《非暴力沟通》 ⭕ 0️⃣8️⃣. 《管理你的每一天》 ❌ 0️⃣9️⃣. 《原则》 1️⃣0️⃣. 《思考，快与慢》 1️⃣1️⃣. 《关键对话》 1️⃣2️⃣. 《当下的启蒙》 1️⃣3️⃣. 《把时间当作朋友》 ❌ 1️⃣4️⃣. 《白夜行》 ❌ 1️⃣5️⃣. 《亲密关系：通往灵魂的桥梁》 ⭕ 1️⃣6️⃣. 《蛤蟆先生去看心理医生》 ✅ 1️⃣7️⃣. 《刻意练习》 ⭕ 1️⃣7️⃣. 《卓有成效的工程师》 ⭕ ","date":"2022-01-01 09:00:00","objectID":"/ooooo-notes/2022%E5%B9%B4%E8%AE%A1%E5%88%92/:4:1","tags":["learning"],"title":"2022年学习计划","uri":"/ooooo-notes/2022%E5%B9%B4%E8%AE%A1%E5%88%92/"},{"categories":["计划"],"content":"2、选读 🎉 0️⃣1️⃣. 《人性的弱点》 0️⃣2️⃣. 《算法 4》 0️⃣3️⃣. 《数据密集型应用系统设计》 0️⃣4️⃣. 《当我谈跑步时，我谈些什么》 0️⃣5️⃣. 《Kafka 官网》 ❓ 0️⃣6️⃣. 《MIT 高级数据课程》 ❓ ","date":"2022-01-01 09:00:00","objectID":"/ooooo-notes/2022%E5%B9%B4%E8%AE%A1%E5%88%92/:4:2","tags":["learning"],"title":"2022年学习计划","uri":"/ooooo-notes/2022%E5%B9%B4%E8%AE%A1%E5%88%92/"},{"categories":["计划"],"content":"3、尝试 🎉 0️⃣1️⃣. 学会使用尤克里里弹奏 ","date":"2022-01-01 09:00:00","objectID":"/ooooo-notes/2022%E5%B9%B4%E8%AE%A1%E5%88%92/:4:3","tags":["learning"],"title":"2022年学习计划","uri":"/ooooo-notes/2022%E5%B9%B4%E8%AE%A1%E5%88%92/"},{"categories":["计划"],"content":"4、了解 🎉 0️⃣1️⃣. 暂无 ","date":"2022-01-01 09:00:00","objectID":"/ooooo-notes/2022%E5%B9%B4%E8%AE%A1%E5%88%92/:4:4","tags":["learning"],"title":"2022年学习计划","uri":"/ooooo-notes/2022%E5%B9%B4%E8%AE%A1%E5%88%92/"},{"categories":["计划"],"content":"5、娱乐 0️⃣1️⃣. 《荒岛余生》 ✅ 0️⃣2️⃣. 《星际穿越》 ✅ 0️⃣3️⃣. 《这个杀手不太冷》 ✅ 0️⃣4️⃣. 《美丽人生》 ✅ 0️⃣5️⃣. 《阿甘正传》 ✅ 0️⃣6️⃣. 《奇遇人生 第一季》 0️⃣7️⃣. 《一本好书 1》 0️⃣8️⃣. 《一本好书 2》 0️⃣9️⃣. 《楚门的世界》 ✅ 1️⃣0️⃣. 《穿越时空的少女》 ✅ 1️⃣1️⃣. 《五等分的新娘 剧场版》 ✅ 1️⃣2️⃣. 《你的名字》 ✅ 1️⃣3️⃣. 《工作细胞》 ✅ ","date":"2022-01-01 09:00:00","objectID":"/ooooo-notes/2022%E5%B9%B4%E8%AE%A1%E5%88%92/:4:5","tags":["learning"],"title":"2022年学习计划","uri":"/ooooo-notes/2022%E5%B9%B4%E8%AE%A1%E5%88%92/"},{"categories":["随笔"],"content":" idea gradle project show duplicated file tree（project view and packages view） you must select idea as run test and building idea gradle 项目结构显示错误 ","date":"2022-01-01 08:00:00","objectID":"/ooooo-notes/idea-%E4%B8%AD-gradle-%E9%A1%B9%E7%9B%AE%E7%BB%93%E6%9E%84%E6%98%BE%E7%A4%BA%E9%94%99%E8%AF%AF/:0:0","tags":["resolution"],"title":"idea 中 gradle 项目结构显示错误","uri":"/ooooo-notes/idea-%E4%B8%AD-gradle-%E9%A1%B9%E7%9B%AE%E7%BB%93%E6%9E%84%E6%98%BE%E7%A4%BA%E9%94%99%E8%AF%AF/"},{"categories":["随笔"],"content":"1. 两台机器初始化设置 ","date":"2021-12-01 18:32:22","objectID":"/ooooo-notes/%E5%9C%A8-centos-%E7%B3%BB%E7%BB%9F%E4%B8%AD%E5%AE%89%E8%A3%85-k8s/:1:0","tags":["k8s","cloud native"],"title":"在 centos 上安装 k8s","uri":"/ooooo-notes/%E5%9C%A8-centos-%E7%B3%BB%E7%BB%9F%E4%B8%AD%E5%AE%89%E8%A3%85-k8s/"},{"categories":["随笔"],"content":"1.1 hostname 设置 hostnamectl ## 查看当前的hostname hostnamectl set-hostname centos1 ## 设置主机名为centos1, 在 192.168.130.131 上执行 hostnamectl set-hostname centos2 ## 设置主机名为centos1, 在 192.168.130.132 上执行 ","date":"2021-12-01 18:32:22","objectID":"/ooooo-notes/%E5%9C%A8-centos-%E7%B3%BB%E7%BB%9F%E4%B8%AD%E5%AE%89%E8%A3%85-k8s/:1:1","tags":["k8s","cloud native"],"title":"在 centos 上安装 k8s","uri":"/ooooo-notes/%E5%9C%A8-centos-%E7%B3%BB%E7%BB%9F%E4%B8%AD%E5%AE%89%E8%A3%85-k8s/"},{"categories":["随笔"],"content":"1.2 /etc/hosts 文件 (两个机器都需要) 192.168.1.8 ooooo 192.168.130.131 centos1 ## k8s master 192.168.130.132 centos2 ## k8s worker ","date":"2021-12-01 18:32:22","objectID":"/ooooo-notes/%E5%9C%A8-centos-%E7%B3%BB%E7%BB%9F%E4%B8%AD%E5%AE%89%E8%A3%85-k8s/:1:2","tags":["k8s","cloud native"],"title":"在 centos 上安装 k8s","uri":"/ooooo-notes/%E5%9C%A8-centos-%E7%B3%BB%E7%BB%9F%E4%B8%AD%E5%AE%89%E8%A3%85-k8s/"},{"categories":["随笔"],"content":"1.3 创建非 root 用户 (两个机器都需要) useradd ooooo -g ooooo ## 添加用户，两个机器都执行 passwd ooooo ## 修改用户密码，两个机器都执行 ","date":"2021-12-01 18:32:22","objectID":"/ooooo-notes/%E5%9C%A8-centos-%E7%B3%BB%E7%BB%9F%E4%B8%AD%E5%AE%89%E8%A3%85-k8s/:1:3","tags":["k8s","cloud native"],"title":"在 centos 上安装 k8s","uri":"/ooooo-notes/%E5%9C%A8-centos-%E7%B3%BB%E7%BB%9F%E4%B8%AD%E5%AE%89%E8%A3%85-k8s/"},{"categories":["随笔"],"content":"1.4 添加 yum 代理 (两个机器都需要) sudo vim /etc/yum.conf ## 编辑 yum 配置文件 proxy=http://ooooo:10800 ## 在文件中添加一行 ","date":"2021-12-01 18:32:22","objectID":"/ooooo-notes/%E5%9C%A8-centos-%E7%B3%BB%E7%BB%9F%E4%B8%AD%E5%AE%89%E8%A3%85-k8s/:1:4","tags":["k8s","cloud native"],"title":"在 centos 上安装 k8s","uri":"/ooooo-notes/%E5%9C%A8-centos-%E7%B3%BB%E7%BB%9F%E4%B8%AD%E5%AE%89%E8%A3%85-k8s/"},{"categories":["随笔"],"content":"1.5 安装 docker 服务 (两个机器都需要) 官方 docker 安装文档 参考文档安装 docker sudo vim /etc/docker/daemon.json ## 编辑 docker 配置文件， 添加下面 json 配置，这是因为 k8s 默认使用的 cgroup driver 是 systemd { \"exec-opts\": [\"native.cgroupdriver=systemd\"] } sudo systemctl enable --now docker.service ## 设置 docker 服务开机启动，并且现在启动 sudo systemctl status docker ## 查看 docker 服务的状态， 失败了，使用下一条命令查看日志 journalctl -xeu docker ## 查看 docker 日志服务 ","date":"2021-12-01 18:32:22","objectID":"/ooooo-notes/%E5%9C%A8-centos-%E7%B3%BB%E7%BB%9F%E4%B8%AD%E5%AE%89%E8%A3%85-k8s/:1:5","tags":["k8s","cloud native"],"title":"在 centos 上安装 k8s","uri":"/ooooo-notes/%E5%9C%A8-centos-%E7%B3%BB%E7%BB%9F%E4%B8%AD%E5%AE%89%E8%A3%85-k8s/"},{"categories":["随笔"],"content":"2. k8s 的 kubeadm 安装 (两台都需要) 官方 k8s 安装文档 参考文档检查服务器的状态是否可以安装 k8s 服务 ## 关闭 swap 分区 swapoff -a sudo echo vm.swappiness=0 \u003e\u003e /etc/sysctl.con ## 永久关闭 swap 分区， k8s 不能运行在有 swap 分区的机器上 free -h ## 查看 swap 分区是否关闭，显示 0 表示已关闭 ## 检查 br_netfilter 是否被加载，没有任何输出，表示没有加载 lsmod | grep br_netfilter sudo modprobe br_netfilter ## 加载 br_netfilter 模块 ## 配置网络 cat \u003c\u003cEOF | sudo tee /etc/modules-load.d/k8s.conf br_netfilter EOF cat \u003c\u003cEOF | sudo tee /etc/sysctl.d/k8s.conf net.bridge.bridge-nf-call-ip6tables = 1 net.bridge.bridge-nf-call-iptables = 1 EOF sudo sysctl --system 安装容器运行时(runtime),k8s 高版本采用自动检查方式,不用做任何处理 ## 添加 k8s 镜像仓库，在前面中，设置了 yum 代理 ## 在官方文档中多了 exclude=kubelet kubeadm kubectl ，这里去掉, 直接安装最新版本的 cat \u003c\u003cEOF | sudo tee /etc/yum.repos.d/kubernetes.repo [kubernetes] name=Kubernetes baseurl=https://packages.cloud.google.com/yum/repos/kubernetes-el7-\\$basearch enabled=1 gpgcheck=1 repo_gpgcheck=1 gpgkey=https://packages.cloud.google.com/yum/doc/yum-key.gpg https://packages.cloud.google.com/yum/doc/rpm-package-key.gpg EOF ## 关闭 selinux sudo setenforce 0 sudo sed -i 's/^SELINUX=enforcing$/SELINUX=permissive/' /etc/selinux/config ## 安装 k8s 服务, --disableexcludes=kubernetes 表示排除 kubernetes 之外的镜像源 sudo yum install -y kubelet kubeadm kubectl --disableexcludes=kubernetes ## 设置 kubelet 开机启动，并且现在启动 ## 启动之后可能会报错，如果原因是 没有读取到 kubelet 的配置文件，这里可以不用管，稍后会重启这个服务 sudo systemctl enable --now kubelet sudo systemctl status kubelet ## 查看 kubelet 的状态 journalctl -xeu kubelet ## 查看 kubelet 的日志 ","date":"2021-12-01 18:32:22","objectID":"/ooooo-notes/%E5%9C%A8-centos-%E7%B3%BB%E7%BB%9F%E4%B8%AD%E5%AE%89%E8%A3%85-k8s/:2:0","tags":["k8s","cloud native"],"title":"在 centos 上安装 k8s","uri":"/ooooo-notes/%E5%9C%A8-centos-%E7%B3%BB%E7%BB%9F%E4%B8%AD%E5%AE%89%E8%A3%85-k8s/"},{"categories":["随笔"],"content":"3. 创建 k8s 集群 (两台都需要) 创建 k8s 集群官方文档 k8s pod network 插件文档 ## 执行 kubeadm init 命令， 在 k8s master 机器上执行，默认情况下， k8s 创建 pod 不会在 master 机器上 ## 重点注意: --pod-network-cidr=10.244.0.0/16 这个参数必须要有，没有的话安装 cni 会报错 ## 注意 preflight 的前置检查输出，可能需要添加 docker group，这个会输出有提示的命令 sudo kubeadm init --image-repository registry.aliyuncs.com/google_containers --apiserver-advertise-address=192.168.130.131 --pod-network-cidr=10.244.0.0/16 ## 执行命令之后，会有 kubeadm join 输出行 ## （分为 master-token 和 worker-token）， 类似于下面的命令，在 centos2 上执行 worker-join-token 的命令 sudo kubeadm join 192.168.130.131:6443 --token 8auvt0.zfw0ayr45d80q8pb \\ --discovery-token-ca-cert-hash sha256:efe854739efef5fbaf3f6e28c899481c8d7797c1997fc8315b921a9ede400ca8 ## 在机器上执行 kubeadm join 或者 kubeadm init 命令之后，重启 kubelet 服务 sudo systemctl restart kubelet sudo systemctl status kubelet ## 设置 kubectl 的配置文件， 为 $HOME/.kube/config mkdir -p $HOME/.kube sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config sudo chown $(id -u):$(id -g) $HOME/.kube/config ## 安装 pod network 插件, 这里使用 flannel 插件 kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml ## 查看 flannel 是否已经启动完成, cni 也启动成功 kubectl get pods -A ## 成功之后会有下面的服务， 都是 running 状态 kube-system coredns-7f6cbbb7b8-5hqt5 1/1 Running 15 (76m ago) 26h kube-system coredns-7f6cbbb7b8-lwdrv 1/1 Running 15 (76m ago) 26h kube-system etcd-centos1 1/1 Running 18 (76m ago) 26h kube-system kube-apiserver-centos1 1/1 Running 25 (76m ago) 26h kube-system kube-controller-manager-centos1 1/1 Running 12 (76m ago) 26h kube-system kube-flannel-ds-6lx7s 1/1 Running 6 (76m ago) 21h kube-system kube-flannel-ds-n5tfn 1/1 Running 6 (76m ago) 21h kube-system kube-proxy-78jrm 1/1 Running 8 (76m ago) 26h kube-system kube-proxy-wl5jg 1/1 Running 8 (76m ago) 26h kube-system kube-scheduler-centos1 1/1 Running 16 (76m ago) 26h ","date":"2021-12-01 18:32:22","objectID":"/ooooo-notes/%E5%9C%A8-centos-%E7%B3%BB%E7%BB%9F%E4%B8%AD%E5%AE%89%E8%A3%85-k8s/:3:0","tags":["k8s","cloud native"],"title":"在 centos 上安装 k8s","uri":"/ooooo-notes/%E5%9C%A8-centos-%E7%B3%BB%E7%BB%9F%E4%B8%AD%E5%AE%89%E8%A3%85-k8s/"},{"categories":["随笔"],"content":"在 ~\\.gradle 目录下新建文件 init.gradle, 内容如下 allprojects { repositories { mavenLocal() maven { name \"Alibaba\" ; url \"https://maven.aliyun.com/repository/public\" } maven { name \"Bstek\" ; url \"http://nexus.bsdn.org/content/groups/public/\" } } buildscript { repositories { maven { name \"Alibaba\" ; url 'https://maven.aliyun.com/repository/public' } maven { name \"Bstek\" ; url 'https://nexus.bsdn.org/content/groups/public/' } maven { name \"M2\" ; url 'https://plugins.gradle.org/m2/' } } } } ","date":"2021-01-02 08:00:00","objectID":"/ooooo-notes/gradle-%E5%85%A8%E5%B1%80%E8%AE%BE%E7%BD%AE%E4%BB%93%E5%BA%93%E9%95%9C%E5%83%8F/:0:0","tags":["resolution"],"title":"gradle 全局设置仓库镜像","uri":"/ooooo-notes/gradle-%E5%85%A8%E5%B1%80%E8%AE%BE%E7%BD%AE%E4%BB%93%E5%BA%93%E9%95%9C%E5%83%8F/"},{"categories":["计划"],"content":"0、持续学习者 Talk is cheap. Show me the code. 英语比编程简单。 学习和实践要平衡。 学会和时间做朋友。 学会投资，学会理财。 学会先做减法，再做加法。 学英语很重要，学英语很重要，学英语很重要。 说明： ⭕ 进行中 ✅ 已完成 ❌ 已废弃 ❓ 有必要 ❗ 重要性 📝 记笔记 🖊 写代码 ","date":"2021-01-01 09:00:00","objectID":"/ooooo-notes/2021%E5%B9%B4%E8%AE%A1%E5%88%92/:1:0","tags":["learning"],"title":"2021年学习计划","uri":"/ooooo-notes/2021%E5%B9%B4%E8%AE%A1%E5%88%92/"},{"categories":["计划"],"content":"1、关于英语 听说读写，目前的学习重点是日常沟通，所以放弃背单词。 计划 🎉： 目前我已经背单词 518 多天，我将会继续背单词（墨墨背单词）。 ❌ 学习《新概念英语一》 ✅ 看 Spring Framework。 学习《赖世雄美语音标》 学习《新概念英语二》 目前状态: 《新概念英语二》。 ","date":"2021-01-01 09:00:00","objectID":"/ooooo-notes/2021%E5%B9%B4%E8%AE%A1%E5%88%92/:2:0","tags":["learning"],"title":"2021年学习计划","uri":"/ooooo-notes/2021%E5%B9%B4%E8%AE%A1%E5%88%92/"},{"categories":["计划"],"content":"2、关于技术 计划 🎉： 只记录自己认为有用的笔记。 ","date":"2021-01-01 09:00:00","objectID":"/ooooo-notes/2021%E5%B9%B4%E8%AE%A1%E5%88%92/:3:0","tags":["learning"],"title":"2021年学习计划","uri":"/ooooo-notes/2021%E5%B9%B4%E8%AE%A1%E5%88%92/"},{"categories":["计划"],"content":"1、书籍 0️⃣1️⃣. 《Java 性能优化权威指南》 0️⃣2️⃣. 《Netty 实战》 ✅ 0️⃣3️⃣. 《程序员面试金典（第6版）》 ⭕ 0️⃣4️⃣. 《图解TCP/IP》 ✅ 0️⃣5️⃣. 《Spring 源码深度解析》 ✅ 0️⃣6️⃣. 《深入理解 Java 虚拟机（第3版）》 ⭕ 0️⃣7️⃣. 《Spring Cloud 微服务：入门、实战与进阶》 0️⃣8️⃣. 《Spring Cloud Alibaba 微服务原理与实战》 ✅ 0️⃣9️⃣. 《深入理解 Apache Dubbo 与实战》 ✅ 1️⃣0️⃣. 《arthas》 1️⃣1️⃣. 《Java 并发编程实战》 1️⃣2️⃣. 《深入理解 Kafka：核心设计与实践原理》 1️⃣3️⃣. 《Spring 5核心原理与30个类手写实战》 ❌ 1️⃣4️⃣. 《Netty 4核心原理与手写RPC框架实战》 ❌ 1️⃣5️⃣. 《从零开始学架构》 1️⃣6️⃣. 《高可用可伸缩微服务架构》 1️⃣7️⃣. 《实战Java虚拟机》 ❓ 1️⃣9️⃣. 《分布式一致性算法开发实战》 2️⃣0️⃣. 《Go Web 编程》 2️⃣1️⃣. 《consul》 ❌ 2️⃣2️⃣. 《Java 异步编程实战》 ❓ 2️⃣3️⃣. 《Effective C++》 2️⃣4️⃣. More Effective C++ 2️⃣5️⃣. 深度探索C++对象模型 2️⃣6️⃣. 《深入浅出 Docker》 ✅ 2️⃣7️⃣. 《码出高效：Java开发手册》 2️⃣8️⃣. 《Go 专家编程》 2️⃣9️⃣. 《流畅的 Python》 3️⃣0️⃣. 《wireshark网络分析的艺术》 3️⃣2️⃣. 《RocketMQ技术内幕》 ✅ 3️⃣3️⃣. 《RocketMQ分布式消息中间件：核心原理与最佳实践》 ✅ 3️⃣4️⃣. 《RocketMQ实战与原理解析》 ✅ 3️⃣5️⃣. 《Vim实用技巧（第2版）》 ⭕ 3️⃣6️⃣. 《RocketMQ技术内幕 第二版》 3️⃣7️⃣. 《Kubernetes in Action中文版》 ⭕ 3️⃣8️⃣. 《rocketmq》 ⭕ 3️⃣9️⃣. 《spring cloud stream》 ✅ 3️⃣9️⃣. 《dubbo》 ⭕ 4️⃣0️⃣. 《pulsar》 ⭕ 4️⃣0️⃣. 《nsq》 4️⃣0️⃣. 《eventing》 4️⃣0️⃣. 《serving》 4️⃣1️⃣. 《Activiti》 ✅ ","date":"2021-01-01 09:00:00","objectID":"/ooooo-notes/2021%E5%B9%B4%E8%AE%A1%E5%88%92/:3:1","tags":["learning"],"title":"2021年学习计划","uri":"/ooooo-notes/2021%E5%B9%B4%E8%AE%A1%E5%88%92/"},{"categories":["计划"],"content":"2、文档 0️⃣1️⃣. 《深入拆解 Java 虚拟机》 0️⃣2️⃣. 通读 Spring 官网，+实践+代码+笔记。 0️⃣1️⃣ spring-cloud-netflix-eureka-clients 0️⃣2️⃣ spring-cloud-netflix-eureka-server 0️⃣3️⃣ spring-cloud-task ✅ 0️⃣3️⃣. 学习 Go 语言，通读 Go 官网，+实践+代码+笔记。 0️⃣1️⃣. 《A Tour of Go》 ✅ 0️⃣2️⃣. 《Tutorial: Create a module》 ✅ 0️⃣3️⃣. 《Writing Web Applications》 ✅ 0️⃣4️⃣. 《How to write Go code》 0️⃣5️⃣. 《Effective Go》 0️⃣4️⃣. Protobuf javaTutorial ✅ 0️⃣5️⃣. Go语言核心36讲 ✅ 0️⃣6️⃣. 消息队列高手课 ✅ 0️⃣7️⃣. 从 0 开始带你成为消息中间件实战高手 ✅ 0️⃣8️⃣. 从 0 开始带你成为JVM实战高手 ⭕ ","date":"2021-01-01 09:00:00","objectID":"/ooooo-notes/2021%E5%B9%B4%E8%AE%A1%E5%88%92/:3:2","tags":["learning"],"title":"2021年学习计划","uri":"/ooooo-notes/2021%E5%B9%B4%E8%AE%A1%E5%88%92/"},{"categories":["计划"],"content":"3、视频 0️⃣1️⃣. 《玩转算法系列–图论精讲》 0️⃣2️⃣. 《玩转算法面试》 ⭕ 0️⃣3️⃣. 《利用Go优越的性能设计与实现高性能企业级微服务网关》 ⭕ 0️⃣3️⃣. 《看的见的算法》 0️⃣4️⃣. 《极客时间– Java 进阶训练营》 ❌ 0️⃣5️⃣. 《极客时间– go 进阶训练营》 ❌ 0️⃣5️⃣. 《极客时间– 算法进阶训练营》 ⭕ ","date":"2021-01-01 09:00:00","objectID":"/ooooo-notes/2021%E5%B9%B4%E8%AE%A1%E5%88%92/:3:3","tags":["learning"],"title":"2021年学习计划","uri":"/ooooo-notes/2021%E5%B9%B4%E8%AE%A1%E5%88%92/"},{"categories":["计划"],"content":"4、算法 0️⃣1️⃣. 每周至少 5 道 Leetcode。 ⭕ 0️⃣2️⃣. leetcode 全站排名1000以内。 ⭕ 0️⃣3️⃣. leetcode 周赛全国排名2000以内。 ⭕ 0️⃣4️⃣. leetcode 周赛全球排名10000以内。 ⭕ ","date":"2021-01-01 09:00:00","objectID":"/ooooo-notes/2021%E5%B9%B4%E8%AE%A1%E5%88%92/:3:4","tags":["learning"],"title":"2021年学习计划","uri":"/ooooo-notes/2021%E5%B9%B4%E8%AE%A1%E5%88%92/"},{"categories":["计划"],"content":"4、关于其他 ","date":"2021-01-01 09:00:00","objectID":"/ooooo-notes/2021%E5%B9%B4%E8%AE%A1%E5%88%92/:4:0","tags":["learning"],"title":"2021年学习计划","uri":"/ooooo-notes/2021%E5%B9%B4%E8%AE%A1%E5%88%92/"},{"categories":["计划"],"content":"1、书籍 🎉 0️⃣1️⃣. 《聪明的投资者》 0️⃣2️⃣. 《一万小时天才理论》 ⭕ 0️⃣3️⃣. 《番茄工作法图解》 ⭕ 0️⃣4️⃣. 《三体》 ✅ 0️⃣5️⃣. 《三体Ⅱ》 0️⃣6️⃣. 《三体Ⅲ》 0️⃣7️⃣. 《非暴力沟通》 ⭕ 0️⃣8️⃣. 《管理你的每一天》 ⭕ 0️⃣9️⃣. 《原则》 1️⃣0️⃣. 《思考，快与慢》 1️⃣1️⃣. 《关键对话》 1️⃣2️⃣. 《当下的启蒙》 1️⃣3️⃣. 《把时间当作朋友》 ⭕ 1️⃣3️⃣. 《活着》 ✅ 1️⃣4️⃣. 《白夜行》 ⭕ 1️⃣5️⃣. 《亲密关系：通往灵魂的桥梁》 ⭕ ","date":"2021-01-01 09:00:00","objectID":"/ooooo-notes/2021%E5%B9%B4%E8%AE%A1%E5%88%92/:4:1","tags":["learning"],"title":"2021年学习计划","uri":"/ooooo-notes/2021%E5%B9%B4%E8%AE%A1%E5%88%92/"},{"categories":["计划"],"content":"2、选读 🎉 0️⃣1️⃣. 《C++ Prime（第5版）》 ❌ 0️⃣2️⃣. 《算法 4》 0️⃣3️⃣. 《JMC 工具(Java Mission Control)》 0️⃣4️⃣. 《ZooKeeper》 0️⃣5️⃣. 《MIT 高级数据课程》 0️⃣6️⃣. 《Resilience4j》 ❓ 0️⃣7️⃣. 《Google Guava》 ❓ 0️⃣8️⃣. 《Kafka 官网》 0️⃣9️⃣. 《Spring Security 实战》 ❌ 1️⃣0️⃣. 《Jvisualvm》 1️⃣1️⃣. 《深入理解 Nginx（第 2 版）》 ❓ 1️⃣2️⃣. 《分布式服务框架：原理与实践》 1️⃣3️⃣. 《chrome-devtools》 ❌ 1️⃣4️⃣. 《人性的弱点》 1️⃣5️⃣. 《深入剖析 Tomcat》 1️⃣6️⃣. 《Java 编程方法论：响应式Spring Reactor 3设计与实现》 1️⃣7️⃣. 《数据密集型应用系统设计》 1️⃣8️⃣. 《Go 程序设计语言》 ❌ 1️⃣9️⃣. 《机器学习实战：基于Scikit-Learn、Keras和TensorFlow》 2️⃣0️⃣. 《Python深度学习》 2️⃣1️⃣. 《当我谈跑步时，我谈些什么》 ","date":"2021-01-01 09:00:00","objectID":"/ooooo-notes/2021%E5%B9%B4%E8%AE%A1%E5%88%92/:4:2","tags":["learning"],"title":"2021年学习计划","uri":"/ooooo-notes/2021%E5%B9%B4%E8%AE%A1%E5%88%92/"},{"categories":["计划"],"content":"3、尝试 🎉 0️⃣1️⃣. 学会使用尤克里里弹奏 ","date":"2021-01-01 09:00:00","objectID":"/ooooo-notes/2021%E5%B9%B4%E8%AE%A1%E5%88%92/:4:3","tags":["learning"],"title":"2021年学习计划","uri":"/ooooo-notes/2021%E5%B9%B4%E8%AE%A1%E5%88%92/"},{"categories":["计划"],"content":"4、了解 🎉 0️⃣1️⃣. 《hugo》 ✅ ","date":"2021-01-01 09:00:00","objectID":"/ooooo-notes/2021%E5%B9%B4%E8%AE%A1%E5%88%92/:4:4","tags":["learning"],"title":"2021年学习计划","uri":"/ooooo-notes/2021%E5%B9%B4%E8%AE%A1%E5%88%92/"},{"categories":["计划"],"content":"5、娱乐 0️⃣1️⃣. 《傲慢与偏见》 ✅ 0️⃣2️⃣. 《肖申克的救赎》 ✅ 0️⃣3️⃣. 《志明与春娇》 ✅ 0️⃣4️⃣. 《春娇与志明》 ✅ 0️⃣4️⃣. 《春娇救志明》 ✅ ","date":"2021-01-01 09:00:00","objectID":"/ooooo-notes/2021%E5%B9%B4%E8%AE%A1%E5%88%92/:4:5","tags":["learning"],"title":"2021年学习计划","uri":"/ooooo-notes/2021%E5%B9%B4%E8%AE%A1%E5%88%92/"},{"categories":["计划"],"content":"6. 总结 今年大部分的时间都放在阅读源码上，导致很多的书籍没有看完，也放弃了英语学习。 年初定的计划在实际执行过程中，两次改变了学习重点， 1. mq 源码 2. k8s 源码。 认真思考指定 2022 的计划 ","date":"2021-01-01 09:00:00","objectID":"/ooooo-notes/2021%E5%B9%B4%E8%AE%A1%E5%88%92/:4:6","tags":["learning"],"title":"2021年学习计划","uri":"/ooooo-notes/2021%E5%B9%B4%E8%AE%A1%E5%88%92/"},{"categories":["随笔"],"content":"在 conf/server.xml 中的 Host标签添加 \u003cHost name=\"localhost\" appBase=\"webapps\" unpackWARs=\"true\" autoDeploy=\"true\"\u003e \u003cValve className=\"org.apache.catalina.valves.AccessLogValve\" directory=\"logs\" prefix=\"localhost_access_log\" suffix=\".txt\" pattern=\"%h %l %u %t \u0026quot;%r\u0026quot; %s %b\" /\u003e // 这是新加的 \u003cValve className=\"org.apache.catalina.valves.ErrorReportValve\" errorCode.400=\"webapps/ROOT/error.jsp\" errorCode.0=\"webapps/ROOT/error.jsp\" showReport=\"false\" showServerInfo=\"false\" /\u003e // 这是新加的 \u003c/Host\u003e 上面的 error.jsp 放在 webapps/ROOT/ 参考 https://stackoverflow.com/questions/52814582/tomcat-is-not-redirecting-to-400-bad-request-custom-error-page 参考 https://tomcat.apache.org/tomcat-9.0-doc/config/valve.html#Error_Report_Valve ","date":"2021-01-01 08:00:00","objectID":"/ooooo-notes/tomcat-%E8%87%AA%E5%AE%9A%E4%B9%89%E9%94%99%E8%AF%AF%E9%A1%B5/:0:0","tags":["resolution"],"title":"tomcat 自定义错误页","uri":"/ooooo-notes/tomcat-%E8%87%AA%E5%AE%9A%E4%B9%89%E9%94%99%E8%AF%AF%E9%A1%B5/"},{"categories":["随笔"],"content":" 重写方法XMLHttpRequest.prototype.send XMLHttpRequest.prototype._send = XMLHttpRequest.prototype.send XMLHttpRequest.prototype.send = function (params) { var attached_params = mdcUtil.MDC_DEVICE_ID + \"=\" + mdcUtil.getMdcDeviceId(); if (params) { params += \"\u0026\" + attached_params; } else { params = attached_params; } return this._send(params) } ","date":"2020-01-05 08:00:00","objectID":"/ooooo-notes/%E5%9C%A8-js-%E4%B8%AD%E7%BB%9F%E4%B8%80%E8%AE%BE%E7%BD%AE%E8%AF%B7%E6%B1%82%E5%8F%82%E6%95%B0%E7%9A%84%E5%8F%A6%E4%B8%80%E7%A7%8D%E6%96%B9%E6%B3%95/:0:0","tags":["resolution"],"title":"在 js 中统一设置请求参数的另一种方法","uri":"/ooooo-notes/%E5%9C%A8-js-%E4%B8%AD%E7%BB%9F%E4%B8%80%E8%AE%BE%E7%BD%AE%E8%AF%B7%E6%B1%82%E5%8F%82%E6%95%B0%E7%9A%84%E5%8F%A6%E4%B8%80%E7%A7%8D%E6%96%B9%E6%B3%95/"},{"categories":["随笔"],"content":" 编辑运行配置，设置环境变量中的工作目录为当前模块目录。 ","date":"2020-01-04 08:00:00","objectID":"/ooooo-notes/idea-%E5%A4%9A%E6%A8%A1%E5%9D%97%E9%A1%B9%E7%9B%AE%E5%90%AF%E5%8A%A8webapp-%E7%9B%AE%E5%BD%95%E4%B8%8B%E6%96%87%E4%BB%B6%E8%AE%BF%E9%97%AE%E4%B8%8D%E5%88%B0/:0:0","tags":["resolution"],"title":"idea 多模块项目启动，webapp 目录下文件访问不到","uri":"/ooooo-notes/idea-%E5%A4%9A%E6%A8%A1%E5%9D%97%E9%A1%B9%E7%9B%AE%E5%90%AF%E5%8A%A8webapp-%E7%9B%AE%E5%BD%95%E4%B8%8B%E6%96%87%E4%BB%B6%E8%AE%BF%E9%97%AE%E4%B8%8D%E5%88%B0/"},{"categories":["随笔"],"content":" 去掉属性 required，添加 rules 规则 { required: true, message: '请输入姓名', trigger: 'blur' } ","date":"2020-01-03 08:00:00","objectID":"/ooooo-notes/element-ui-%E4%B8%AD-el-form-item-%E6%A0%A1%E9%AA%8C%E5%87%BA%E7%8E%B0%E8%8B%B1%E6%96%87/:0:0","tags":["resolution"],"title":"element-ui 中 el-form-item 校验出现英文","uri":"/ooooo-notes/element-ui-%E4%B8%AD-el-form-item-%E6%A0%A1%E9%AA%8C%E5%87%BA%E7%8E%B0%E8%8B%B1%E6%96%87/"},{"categories":["随笔"],"content":"问题 If you see that the storm process is getting crashed even though you have enough memory (swap/free) available then you should also check the “/proc/sys/vm/overcommit_memory” This switch knows 3 different settings: =\u003e 0: The Linux kernel is free to over commit memory(this is the default), a heuristic algorithm is applied to figure out if enough memory is available. =\u003e 1: The Linux kernel will always over commit memory, and never check if enough memory is available. This increases the risk of out-of-memory situations, but also improves memory-intensive workloads. =\u003e 2: The Linux kernel will not over commit memory, and only allocate as much memory as defined in over commit_ratio. As sometimes OS kills /crashes a process due to a system OS setting, the system OS memory overcommit setting was 2 (when it should have been set to 0) - ","date":"2020-01-02 08:00:00","objectID":"/ooooo-notes/java-%E5%87%BA%E7%8E%B0%E5%86%85%E5%AD%98%E5%88%86%E9%85%8D%E5%A4%B1%E8%B4%A5/:0:0","tags":["resolution"],"title":"java 出现内存分配失败","uri":"/ooooo-notes/java-%E5%87%BA%E7%8E%B0%E5%86%85%E5%AD%98%E5%88%86%E9%85%8D%E5%A4%B1%E8%B4%A5/"},{"categories":["计划"],"content":"0、持续学习者 Talk is cheap. Show me the code. 英语比编程简单。 学习和实践要平衡。 学会和时间做朋友。 学会投资，学会理财。 学会先做减法，再做加法。 学英语很重要，学英语很重要，学英语很重要。 说明： ⭕ 进行中 ✅ 已完成 ❌ 已废弃 ❓ 有必要 ❗ 重要 📝 记笔记* 🖊 写代码 ","date":"2020-01-01 09:00:00","objectID":"/ooooo-notes/2020%E5%B9%B4%E8%AE%A1%E5%88%92/:1:0","tags":["learning"],"title":"2020年学习计划","uri":"/ooooo-notes/2020%E5%B9%B4%E8%AE%A1%E5%88%92/"},{"categories":["计划"],"content":"1、关于英语 听说读写，其中最容易的应该是读，然后再是写，我有很大的信心能在两三年之内（2022）正常读写。剩余就是听和说了，目前对我真的太难了。 计划 🎉： 目前我已经背单词 430 多天，我将会继续背单词（墨墨背单词）😄 。 看 YouTube - English with Lucy。 ❓ 看 Spring。 看 Friends。 ❓ 目前状态: 扇贝阅读。 ❗ ","date":"2020-01-01 09:00:00","objectID":"/ooooo-notes/2020%E5%B9%B4%E8%AE%A1%E5%88%92/:2:0","tags":["learning"],"title":"2020年学习计划","uri":"/ooooo-notes/2020%E5%B9%B4%E8%AE%A1%E5%88%92/"},{"categories":["计划"],"content":"2、关于技术 只记录自己认为有用的笔记。 计划 🎉： 阅读书籍，+笔记。 0️⃣1️⃣. 《Java 并发编程的艺术》 ✅ 0️⃣2️⃣. 《Redis 开发与运维》 ✅ 0️⃣3️⃣. 《剑指 Offer》 ✅ 0️⃣4️⃣. 《Effective Java中文版（第3版）》 ✅ 0️⃣5️⃣. 《Java 8 函数式编程 》 ✅ 0️⃣6️⃣. 《算法图解》 ✅ 0️⃣7️⃣. 《设计模式》 ❗ ✅ 0️⃣8️⃣. 《图解 HTTP 》 ✅ 0️⃣9️⃣. 《Spring Boot编程思想（核心篇）》 ✅ 阅读文档，+笔记。 0️⃣1️⃣. 《MySQL 实战 45 讲》 ✅ 0️⃣2️⃣. 《Kafka 核心技术与实战》 ✅ 0️⃣3️⃣. 《Java 核心技术 36 讲》 ✅ 0️⃣4️⃣. 《Java 并发编程实战》 ✅ 观看视频，+代码 0️⃣1️⃣. 《算法面试通关 40 讲》 ✅ 0️⃣2️⃣. 《玩转算法系列–玩转数据结构 Java 版》 ✅ 0️⃣3️⃣. 《算法与数据结构-综合提升 C++ 版》 ✅ 0️⃣4️⃣. 《玩转 Spring 全家桶》 ✅ 0️⃣5️⃣. 《Go 语言从入门到实战》 ✅ 通读 Spring 官网，+实践+代码+笔记。 0️⃣1️⃣ spring-cloud-stream ✅ 0️⃣2️⃣ spring-cloud-netflix-zuul ✅ ","date":"2020-01-01 09:00:00","objectID":"/ooooo-notes/2020%E5%B9%B4%E8%AE%A1%E5%88%92/:3:0","tags":["learning"],"title":"2020年学习计划","uri":"/ooooo-notes/2020%E5%B9%B4%E8%AE%A1%E5%88%92/"},{"categories":["计划"],"content":"4、关于其他 阅读，+笔记 0️⃣1️⃣. 《指数基金投资指南》 ✅ 0️⃣2️⃣. 《富爸爸穷爸爸》 ✅ 0️⃣3️⃣. 《解读基金》 ✅ 0️⃣4️⃣. 《富爸爸财务自由之路》 ✅ 0️⃣5️⃣. 《克莱因壶》 ✅ ","date":"2020-01-01 09:00:00","objectID":"/ooooo-notes/2020%E5%B9%B4%E8%AE%A1%E5%88%92/:4:0","tags":["learning"],"title":"2020年学习计划","uri":"/ooooo-notes/2020%E5%B9%B4%E8%AE%A1%E5%88%92/"},{"categories":["计划"],"content":"5、选读 🎉 0️⃣1️⃣. 《阿里巴巴 Java 开发手册》 ✅ 0️⃣2️⃣. 《Spring 实战（第5版 ）》 ✅ 0️⃣3️⃣. 《Cloud Native Java》 ✅ 0️⃣4️⃣. 《MyBatis 技术内幕》 ✅ 0️⃣5️⃣. 《看透 Spring MVC》 ✅ 0️⃣6️⃣. 《Redis 深度历险：核心原理与应用实践》 ✅ 0️⃣7️⃣. 《Spring Boot 编程思想》 ✅ 0️⃣8️⃣. 《Offer来了：Java面试核心知识点精讲（原理篇）》 ✅ 0️⃣9️⃣. 《Offer来了：Java面试核心知识点精讲（框架篇）》 ✅ ","date":"2020-01-01 09:00:00","objectID":"/ooooo-notes/2020%E5%B9%B4%E8%AE%A1%E5%88%92/:5:0","tags":["learning"],"title":"2020年学习计划","uri":"/ooooo-notes/2020%E5%B9%B4%E8%AE%A1%E5%88%92/"},{"categories":["随笔"],"content":" 执行命令 rm -rf ~/.zcompdump* ","date":"2020-01-01 08:00:00","objectID":"/ooooo-notes/zsh-%E6%B7%BB%E5%8A%A0%E6%8F%92%E4%BB%B6%E5%90%8E%E4%B8%8D%E7%94%9F%E6%95%88/:0:0","tags":["resolution"],"title":"zsh 添加插件后，不生效","uri":"/ooooo-notes/zsh-%E6%B7%BB%E5%8A%A0%E6%8F%92%E4%BB%B6%E5%90%8E%E4%B8%8D%E7%94%9F%E6%95%88/"},{"categories":null,"content":"1、Redis 特性 ","date":"0001-01-01 00:00:00","objectID":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/01/:1:0","tags":null,"title":"","uri":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/01/"},{"categories":null,"content":"1、速度快 Redis 的所有数据都是放在内存中的。 Redis 是 C 语言实现的。 Redis 使用单线程架构，避免多线程环境上下文切换。 ","date":"0001-01-01 00:00:00","objectID":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/01/:1:1","tags":null,"title":"","uri":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/01/"},{"categories":null,"content":"2、基于键值对的数据结构服务器 Redis 主要提供五种基本数据结构：string（字符串）、hash（哈希）、list（列表）、set（集合）、zset（有序集合），还提供 Bitmaps（位图）、HyperLogLog（基数统计算法）、GEO（地理位置）高级数据结构。 ","date":"0001-01-01 00:00:00","objectID":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/01/:1:2","tags":null,"title":"","uri":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/01/"},{"categories":null,"content":"3、丰富的功能 提供了键过期功能，可以用来实现缓存。 提供了发布订阅功能，可以用来实现消息系统。 支持 Lua 脚本功能，可以利用 Lua 创造新的 Redis 命令。 提供了简单事务功能。 提供了 Pipeline（流水线）功能。 ","date":"0001-01-01 00:00:00","objectID":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/01/:1:3","tags":null,"title":"","uri":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/01/"},{"categories":null,"content":"4、简单稳定 早期的 Redis 源码只有两万行，3.0 版本后添加了集群特性，代码增到 5 万行左右，Redis 自己实现了事件处理功能。 ","date":"0001-01-01 00:00:00","objectID":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/01/:1:4","tags":null,"title":"","uri":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/01/"},{"categories":null,"content":"5、客户端语言多 Redis 提供了简单的 TCP 通信协议，主流的编程语言都有其客户端实现。 ","date":"0001-01-01 00:00:00","objectID":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/01/:1:5","tags":null,"title":"","uri":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/01/"},{"categories":null,"content":"6、持久化 Redis 提供了 AOF 、 RDB 两种持久化方式。 ","date":"0001-01-01 00:00:00","objectID":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/01/:1:6","tags":null,"title":"","uri":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/01/"},{"categories":null,"content":"7、主从复制、高可用和分布式 Redis Sentinel 、Redis Cluster ","date":"0001-01-01 00:00:00","objectID":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/01/:1:7","tags":null,"title":"","uri":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/01/"},{"categories":null,"content":"2、Redis 使用场景 ","date":"0001-01-01 00:00:00","objectID":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/01/:2:0","tags":null,"title":"","uri":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/01/"},{"categories":null,"content":"Redis 可以做什么 缓存。（ Redis 提供了键过期时间设置） 排行榜系统。（ Redis 提供了列表 list 和有序集合 set 数据结构） 计数器应用。（ Redis 提供了计数功能 incr、decr ） 社交网络。（赞/踩、粉丝、共同好友/喜好） 消息队列。（ Redis 提供了列表 list 的 rpush, blpop 命令 ） ","date":"0001-01-01 00:00:00","objectID":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/01/:2:1","tags":null,"title":"","uri":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/01/"},{"categories":null,"content":"Redis 不可以做什么 Redis 基于内存，不能做存储。 避免用 Redis 来缓存冷数据。 ","date":"0001-01-01 00:00:00","objectID":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/01/:2:2","tags":null,"title":"","uri":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/01/"},{"categories":null,"content":"1、预备 ","date":"0001-01-01 00:00:00","objectID":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/02/:1:0","tags":null,"title":"","uri":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/02/"},{"categories":null,"content":"1、全局命令 help HELP command 查看所有的key KEYS * 键总数 DBSIZE 检查键是否存在 EXISTS key 删除键 DEL key 键过期 EXPIRE key sencond ttl 返回过期时间 TTL key \u003e0: 剩余过期时间 -1: 没有设置过期时间 -2: 键不存在 key的数据结构类型 TYPE key ","date":"0001-01-01 00:00:00","objectID":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/02/:1:1","tags":null,"title":"","uri":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/02/"},{"categories":null,"content":"2、数据结构与内部编码 每种数据结构都有自己底层的内部编码实现，通过命令 OBJECT ENCODING key 来查看。 string 内部编码： raw、int、embstr hash 内部编码： ziplist、hashtable list 内部编码： ziplist、linkedlist、quicklist set 内部编码： intset、hashtable zset 内部编码： ziplist、skiplist ","date":"0001-01-01 00:00:00","objectID":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/02/:1:2","tags":null,"title":"","uri":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/02/"},{"categories":null,"content":"2、string ","date":"0001-01-01 00:00:00","objectID":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/02/:2:0","tags":null,"title":"","uri":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/02/"},{"categories":null,"content":"1、常用命令 设置值、获取值 SET key value [expiration EX seconds|PX milliseconds] [NX|XX] GET key 说明： NX：不存在 key，才设置成功。同命令 SETNX key value XX：存在 key，才设置成功。 批量设置、批量获取 MSET key value [key value ...] MGET key [key ...] 说明： 批量操作可以减少网络时间。 计数 INCR key INCRBY key increment INCRBYFLOAT key increment DECR key DECRBY key decrement ","date":"0001-01-01 00:00:00","objectID":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/02/:2:1","tags":null,"title":"","uri":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/02/"},{"categories":null,"content":"2、不常用的命令 追加值 APPEND key value 字符串长度 STRLEN key 设置并返回原值 GETSET key value 设置指定位置的字符 SETRANGE key offset value 获取部分字符串 GETRANGE key start end ","date":"0001-01-01 00:00:00","objectID":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/02/:2:2","tags":null,"title":"","uri":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/02/"},{"categories":null,"content":"3、内部编码 字符串内部编码有三种： int：8 个字节的长整型 embstr：小于等于 39 个字节的字符串 raw：大于 39 个字节的字符串 ","date":"0001-01-01 00:00:00","objectID":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/02/:2:3","tags":null,"title":"","uri":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/02/"},{"categories":null,"content":"4、使用场景 缓存 （网站请求数据缓存） 计数 （网站的浏览数和播放数） 共享 Session （用户登录信息） 限速 （验证码接口） ","date":"0001-01-01 00:00:00","objectID":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/02/:2:4","tags":null,"title":"","uri":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/02/"},{"categories":null,"content":"3、hash ","date":"0001-01-01 00:00:00","objectID":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/02/:3:0","tags":null,"title":"","uri":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/02/"},{"categories":null,"content":"1、命令 设置值、获取值 HSET key field value HGET key field 删除 field HDEL key field [field ...] 计算 field 的个数 HLEN key 批量设置、批量获取 HMSET key field value [field value ...] HMGET key field [field ...] 是否存在 field HEXISTS key field 获取所有的 field HKEYS key 获取所有的 value HVALS key 获取所有的 field-value HGETALL key 说明： field 个数比较多时，会阻塞 redis。 计数 HINCRBY key field increment HINCRBYFLOAT key field increment 获取 value 长度 HSTRLEN key field ","date":"0001-01-01 00:00:00","objectID":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/02/:3:1","tags":null,"title":"","uri":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/02/"},{"categories":null,"content":"2、内部编码 ziplist（压缩表）：元素个数小于 hash-max-ziplist-entries = 512 ，同时 value 小于 hash-max-ziplist-value = 64，就使用 ziplist， 配置参数在 redis.conf 中。 hashtable（哈希表）：无法满足 ziplist 的条件，会使用 hashtable。 ","date":"0001-01-01 00:00:00","objectID":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/02/:3:2","tags":null,"title":"","uri":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/02/"},{"categories":null,"content":"4、list ","date":"0001-01-01 00:00:00","objectID":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/02/:4:0","tags":null,"title":"","uri":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/02/"},{"categories":null,"content":"1、命令 添加 RPUSH key value [value ...] # 右边添加 LPUSH key value [value ...] # 左边添加 LINSERT key BEFORE|AFTER pivot value # 指定位置插入 查询 LRANGE key start stop # 范围为[start, stop], 查询所有是 start = 0, stop = -1 LLEN key # 列表长度 删除 LPOP key # 左边弹出 RPOP key # 右边弹出 LREM key count value ## 删除 count 个 value 值, count \u003e 0,从左边删除；count \u003c 0,从右边删除；count = 0, 删除所有 LTRIM key start stop # 只保留[start, stop]的元素 修改 LSET key index value # 设置指定索引的值 阻塞 BLPOP key [key ...] timeout # 从左边弹出元素，如果为空，则阻塞 BRPOP key [key ...] timeout # 从右边弹出元素，如果为空，则阻塞 # timeout：阻塞时间。多个 key, 从左扫描。 ","date":"0001-01-01 00:00:00","objectID":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/02/:4:1","tags":null,"title":"","uri":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/02/"},{"categories":null,"content":"2、内部编码 ziplist（压缩表）：元素个数小于 list-max-ziplist-entries = 512 ，同时 value 小于 list-max-ziplist-value = 64，就使用 ziplist， 配置参数在 redis.conf 中。 linkedlist（链表）：无法满足 ziplist 的条件，会使用 linkedlist。 ","date":"0001-01-01 00:00:00","objectID":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/02/:4:2","tags":null,"title":"","uri":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/02/"},{"categories":null,"content":"5、set ","date":"0001-01-01 00:00:00","objectID":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/02/:5:0","tags":null,"title":"","uri":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/02/"},{"categories":null,"content":"1、集合内操作 添加 SADD key member [member ...] 删除 SREM key member [member ...] 计算元素个数 SCARD key 是否在集合中 SISMEMBER key member # 随机返回 count 个元素，不会删除元素 SRANDMEMBER key [count] 随机弹出 count 个元素，会删除元素 SPOP key [count] 获取所有元素 SMEMBERS key ","date":"0001-01-01 00:00:00","objectID":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/02/:5:1","tags":null,"title":"","uri":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/02/"},{"categories":null,"content":"2、集合间操作 多个集合的交集 SINTER key [key ...] 多个集合的并集 SUNION key [key ...] 多个集合的差集 SDIFF key [key ...] 将交集、并集、差集的结果保存 SINTERSTORE destination key [key ...] SUNIONSTORE destination key [key ...] SDIFFSTORE destination key [key ...] 说明： destination 表示目标 key。 key 表示需要操作的 key。 ","date":"0001-01-01 00:00:00","objectID":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/02/:5:2","tags":null,"title":"","uri":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/02/"},{"categories":null,"content":"3、内部编码 intset（整数集合）：value 值为整型，个数小于 set-max-intset-entries = 512 时，使用 intset。配置参数在 redis.conf 中。 hashtable（哈希表）：不满足 intset 条件时，使用 hashtable。 ","date":"0001-01-01 00:00:00","objectID":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/02/:5:3","tags":null,"title":"","uri":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/02/"},{"categories":null,"content":"4、使用场景 标签系统：计算不同人相同喜好的标签 (SINTER命令)。 SADD user1:tags tag1 tag2 SADD user2:tags tag2 tag3 SINTER user1:tags user2:tags ","date":"0001-01-01 00:00:00","objectID":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/02/:5:4","tags":null,"title":"","uri":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/02/"},{"categories":null,"content":"6、zset ","date":"0001-01-01 00:00:00","objectID":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/02/:6:0","tags":null,"title":"","uri":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/02/"},{"categories":null,"content":"1、集合内操作 添加 ZADD key [NX|XX] [CH] [INCR] score member [score member ...] # NX: 不存在 key，才设置成功 # XX: 存在 可以，才设置成功 计算成员个数 ZCARD key 获取某个成员的分数 ZSCORE key member 获取某个成员的排名 ZRANK key member # 从低到高 ZREVRANK key member # 从高到低 删除 ZREM key member [member ...] # 删除成员 ZREMRANGEBYRANK key start stop # 删除指定排名范围的成员 ZREMRANGEBYSCORE key min max # 删除指定分数范围的成员 增加成员的分数 ZINCRBY key increment member 获取指定排名范围的成员 ZRANGE key start stop [WITHSCORES] # 从低到高 ZREVRANGE key start stop [WITHSCORES] # 从高到底 获取指定分数范围的成员 ZRANGEBYSCORE key min max [WITHSCORES] [LIMIT offset count] # WITHSCORES：结果返回分数 # LIMIT offset count：限制返回个数 获取指定分数范围的成员个数 ZCOUNT key min max ","date":"0001-01-01 00:00:00","objectID":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/02/:6:1","tags":null,"title":"","uri":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/02/"},{"categories":null,"content":"2、集合间操作 交集 ZINTERSTORE destination numkeys key [key ...] [WEIGHTS weight] [AGGREGATE SUM|MIN|MAX] # destination：计算结果保存的键 # numkeys：参与的键，也就是 key 的总数 # weight：每一个 key 参与的权重，默认为 1 # AGGREGATE：聚合操作，默认为 sum 并集 ZUNIONSTORE destination numkeys key [key ...] [WEIGHTS weight] [AGGREGATE SUM|MIN|MAX] # 参数同 ZINTERSTORE ","date":"0001-01-01 00:00:00","objectID":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/02/:6:2","tags":null,"title":"","uri":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/02/"},{"categories":null,"content":"3、内部编码 ziplist（压缩列表）：个数小于 zset-max-ziplist-entries = 128 时，value 小于 zet-max-ziplist-value = 64 使用 ziplist。配置参数在 redis.conf 中。 skiplist（跳跃表）：不满足 ziplist 条件时，使用 skiplist。 ","date":"0001-01-01 00:00:00","objectID":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/02/:6:3","tags":null,"title":"","uri":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/02/"},{"categories":null,"content":"6、键管理 ","date":"0001-01-01 00:00:00","objectID":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/02/:7:0","tags":null,"title":"","uri":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/02/"},{"categories":null,"content":"1、单个键管理 键重命名 RENAME key newkey # 存在 key，会覆盖 RENAMENX key newkey # 不存在 key，才重命名成功 随机返回一个键 RANDOMKEY 键过期 EXPIRE key seconds # \u003e0: 剩余过期时间 # -1: 没有设置过期时间； # -2: 键不存在 Redis 不支持二级数据结构（哈希表、列表）过期 setex 原子命令设置 value 和 expire 迁移键 MOVE key db # 迁移到另一个db, 不建议使用，因为集群环境只能使用一个数据库 DUMP key; RESTORE key ttl serialized-value [REPLACE] # 操作麻烦，不建议使用 MIGRATE host port key| destination-db timeout [COPY] [REPLACE] [KEYS key] # 可以使用 # COPY: 迁移后不会删除源键 # REPLACE: 迁移后会覆盖目标库的键 示例： 迁移到 localhost:6380 的 db0 库上，timeout为 1000ms，命令为 MIGRATE localhost 6380 hello 0 1000。 迁移多个键 k1, k2, k3，命令为 MIGRATE localhost 6380 \"\" 0 1000 KEYS k1 k2 k3。 ","date":"0001-01-01 00:00:00","objectID":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/02/:7:1","tags":null,"title":"","uri":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/02/"},{"categories":null,"content":"2、遍历键 全量遍历键 KEYS pattern # 键很多时，会阻塞 Redis 渐进式遍历建 SCAN cursor [MATCH pattern] [COUNT count] # count: 每次查询 key 的个数。 # pattern: 同命令 scan。 说明： 第一次查询设置 cursor 为 0，结果会返回 cursor，如果 cursor 为 0，表示遍历结束，否则设置 cursor 为当前返回值，再次查询。 ","date":"0001-01-01 00:00:00","objectID":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/02/:7:2","tags":null,"title":"","uri":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/02/"},{"categories":null,"content":"3、数据库管理 无，因为集群模式下，只能使用一个数据库，生产环境也是如此。 ","date":"0001-01-01 00:00:00","objectID":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/02/:7:3","tags":null,"title":"","uri":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/02/"},{"categories":null,"content":"1、慢查询 ","date":"0001-01-01 00:00:00","objectID":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/03/:1:0","tags":null,"title":"","uri":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/03/"},{"categories":null,"content":"1、慢查询配置 slowlog-log-slower-than: 10000 (默认值，单位微秒)，超过 10 毫秒的语句就会被记录下来。 slowlog-max-len: 128（默认值），Redis 内部使用列表来保存慢查询日志。 lowlog-log-slower-than = 0, 会记录所有命令。 lowlog-log-slower-than \u003c 0, 不会记录任何命令。 配置方式： 修改配置文件 redis.conf。 动态修改 config set lowlog-log-slower-than 20000 # 设置慢查询时间 config set slowlog-max-len 1000 # 设置慢查询日志大小 config rewrite # 持久化到配置文件 慢查询命令 SLOWLOG GET 10 # 获取最近 10 条日志 SLOWLOG LEN # 获取日志条数 SLOWLOG RESET # 慢查询日志重置 ","date":"0001-01-01 00:00:00","objectID":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/03/:1:1","tags":null,"title":"","uri":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/03/"},{"categories":null,"content":"2、最佳实践 参数 slowlog-max-len，建议调大日志列表，比如 1000以上；参数 slowlog-log-slower-than，默认超过 10ms 就判断为慢查询，如果每条命令执行时间在 1ms 以上，则 1s 的并发量不足 1000，所以对于高 OPS 场景设置为 1ms。 慢查询只记录命令执行时间，不包括命令排队和网络传输时间。 慢查询日志只是一个先进先出的队列，如果查询较多，可能会丢失日志数据，可以利用 SLOWLOG GET 命令将日志存入 mysql 中，也可以利用开源工具 CacheCloud。 ","date":"0001-01-01 00:00:00","objectID":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/03/:1:2","tags":null,"title":"","uri":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/03/"},{"categories":null,"content":"2、Redis shell ","date":"0001-01-01 00:00:00","objectID":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/03/:2:0","tags":null,"title":"","uri":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/03/"},{"categories":null,"content":"1、redis-cli 命令 -x 参数 echo \"world\" | redis-cli -x set hello # 设置key为 hello， value为 world -c 参数 集群参数，防止 moved 和 asked 异常。 –rdb 参数 请求 Redis 实例生成 RDB 文件，保存在本地。 –bigkeys 参数 选出大 key，这些 key 可能是系统瓶颈。 –eval 参数 运行 lua 脚本。 latency 参数 –latency: 客户端与主机延迟 。 –latency-history: 分时段展示延迟，用 -i 参数来指定，默认为 15s。 –latency-dist: 统计图表形式展示延迟。 –stat 参数 实时获取 Redis 重要统计信息，信息比 info 命令少。 ","date":"0001-01-01 00:00:00","objectID":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/03/:2:1","tags":null,"title":"","uri":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/03/"},{"categories":null,"content":"2、redis-server 命令 –test-memory 参数 redis-server --test-memory 1024 # 检测是否可以给 Redis 分配 1G 内存 ","date":"0001-01-01 00:00:00","objectID":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/03/:2:2","tags":null,"title":"","uri":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/03/"},{"categories":null,"content":"3、redis-benchmark 命令 用来做基准性能测试。 redis-benchmark -c 100 -n 20000 -q -r 10000 -t get,set --csv # -c 客户端并发数 # -n 客户端请求总数 # -q 仅仅显示 requests per second 信息 # -r 随机键的范围（0-9999），不是个数 # -t 指定命令 # --csv 结果按照 csv 格式输出 ","date":"0001-01-01 00:00:00","objectID":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/03/:2:3","tags":null,"title":"","uri":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/03/"},{"categories":null,"content":"3、Pipeline ","date":"0001-01-01 00:00:00","objectID":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/03/:3:0","tags":null,"title":"","uri":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/03/"},{"categories":null,"content":"1、pipeline 概念 redis 执行一条命令可以分为四个过程： 发送命令 命令排队 执行命令 返回结果 其中 1. 和 4. 称为 RTT (往返时间)。 pipeline 可以将一组 redis 命令通过一次 RTT 发给 Redis，再按照执行结果返回给客户端。 redis-cli 脚本的 –pipe 选项就是使用 pipeline 机制。 ","date":"0001-01-01 00:00:00","objectID":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/03/:3:1","tags":null,"title":"","uri":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/03/"},{"categories":null,"content":"2、性能测试 pipeline 执行速度一般比逐条执行快，客户端与服务端网路延时越大，效果越明显。 ","date":"0001-01-01 00:00:00","objectID":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/03/:3:2","tags":null,"title":"","uri":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/03/"},{"categories":null,"content":"3、原生批量和 pipeline 原生批量命令是原子的，pipeline 不是原子的（中间可以执行其他命令）。 原生批量命令是一个命令对应多个 key, pipeline 支持多个命令。 原生批零命令是 Redis 服务端实现的，pipeline 是客户端和服务端共同实现的。 ","date":"0001-01-01 00:00:00","objectID":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/03/:3:3","tags":null,"title":"","uri":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/03/"},{"categories":null,"content":"4、最佳实践 pipeline 封装的数据不能过多，即大数据可以拆分为批量的小 pipeline 命令。 pipeline 只能操作一个 Redis 实例。 ","date":"0001-01-01 00:00:00","objectID":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/03/:3:4","tags":null,"title":"","uri":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/03/"},{"categories":null,"content":"4、事务与Lua 为了保证多个命令组合的原子性，Redis 提供了简单事务功能和 lua 脚本。 ","date":"0001-01-01 00:00:00","objectID":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/03/:4:0","tags":null,"title":"","uri":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/03/"},{"categories":null,"content":"1、事务 MULTI # 开启事务 set a 1 # 执行命令，实际上把命令放到队列中 set b 1 # 执行命令，实际上把命令放到队列中 EXEC # 真正的执行命令 命令错误，会导致事务执行失败，比如 set a 1 写成了 sett a 1。 运行时错误，redis 不支持回滚，比如 sadd a 1 写成了 zadd a 1 b，假设 a 这个 key 已经存在，就会抛出错误。 事务简单主要原因就是，redis 不支持回滚。 ","date":"0001-01-01 00:00:00","objectID":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/03/:4:1","tags":null,"title":"","uri":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/03/"},{"categories":null,"content":"2、Lua 在 Redis 中使用 Lua，有两种方式 eval 和 evalsha。 eval EVAL script numkeys key [key ...] arg [arg ...] eval 'return \"hello \" .. KEYS[1] .. ARGV[1]' 1 world redis # 例子 # 输出 \"hello worldredis\" 如果 Lua 脚本较长，可以使用 redis-cli –eval 选项来执行。 evalsha 使用 eval 命令，每次都需要将脚本发送到服务端，使用 evalsha 命令就避免了开销。 redis-cli script load hello.lua # 加载 lua 脚本到服务端，会返回 sha1 值。 EVALSHA sha1 numkeys key [key ...] arg [arg ...] # 执行 lua 脚本，参数 sha1 就是返回的 sha1 值，其他参数同 eval 命令。 lua 中使用 redis API redis.call(\"set\", \"a\" , 1) redis.call(\"get\", \"a\" ) 也可以使用 redis.pcall 命令，两者差别在于 pcall 命令会忽略错误继续执行，call 遇到错误停止。 lua 脚本执行是原子性的，中间不会插入别的命令。 管理 lua 脚本命令 SCRIPT LOAD [script] # 加载 lua 脚本，返回 sha1 值 SCRIPT EXISTS sha1 [sha1] # 是否存在 sha1 的脚本 SCRIPT FLUSH # 清空 lua 脚本 SCRIPT KILL # 杀掉 lua 脚本 ","date":"0001-01-01 00:00:00","objectID":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/03/:4:2","tags":null,"title":"","uri":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/03/"},{"categories":null,"content":"5、Bitmaps ","date":"0001-01-01 00:00:00","objectID":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/03/:5:0","tags":null,"title":"","uri":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/03/"},{"categories":null,"content":"1、数据结构模型 Bitmaps 不是一种数据结构，实际上它是字符串，但它可以对字符串的位进行操作，你可以想象一个以位为单位的数组，每个单元只能存储 0 和1。 ","date":"0001-01-01 00:00:00","objectID":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/03/:5:1","tags":null,"title":"","uri":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/03/"},{"categories":null,"content":"2、命令 设置值 SETBIT key offset value # offset 从 0 开始 获取值 GETBIT key offset # 结果只有 0 或者 1 BITCOUNT key [start end] # 对[start, end]范围获取值为 1 的个数 Bitmaps 间的运算 BITOP operation destkey key [key ...] # operation 可以是 and(交集)、or(并集)、not(非)、xor(异或) 获取第一个为 bit 的 offset 值 BITPOS key bit [start] [end] # [start,end]范围中第一个出现 bit 的 offset ","date":"0001-01-01 00:00:00","objectID":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/03/:5:2","tags":null,"title":"","uri":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/03/"},{"categories":null,"content":"3、分析 利用 Bitmaps 来统计网站访问用户： SETBIT users:2020-03-22 1 1 # 2020-03-22 这一天 1 号访问了。 SETBIT users:2020-03-23 2 1 # 2020-03-23 这一天 2 号访问了。 BITCOUNT users:2020-03-23 # 2020-03-23 这一天 访问用户量 BITOP and users:2020-03-22_23 users:2020-03-23 users:2020-03-22 # 两天都访问的用户量 set 和 bitmaps 对比： 数据类型 每个用户 id 占用空间 需要存储用户量 全部内存量 set 64 位 5 千万 64 位 * 5 千万 = 400 MB bitmaps 1 位 1 亿 1 位 * 1 亿 = 12.5 MB 从表格可以看出 bitmaps 节省内存。 但如果每天的活跃用户很少，set 可能比 bitmaps 好，因为 set 需要内存 64 位 * 10 万 = 800 KB，而 bitmap 还是需要 12.5 MB 内存。 ","date":"0001-01-01 00:00:00","objectID":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/03/:5:3","tags":null,"title":"","uri":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/03/"},{"categories":null,"content":"6、HyperLogLog HyperLoglog 不是一种新的数据结构，而是一种基数算法，可以利用极小的内存空间完成独立总数统计，数据集可以 ID、Email、IP。 命令 PFADD key element [element ...] # 添加元素 PFCOUNT key [key ...] # 计数 PFMERGE destkey sourcekey [sourcekey ...] # merge 注意： 只是计算独立总数，不需要获取单条数据 HyperLogLog 有误差 ","date":"0001-01-01 00:00:00","objectID":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/03/:6:0","tags":null,"title":"","uri":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/03/"},{"categories":null,"content":"7、发布订阅 Redis 提供发布/订阅模式的消息机制。 命令 PUBLISH channel message # 向指定的 channel 发布消息 SUBSCRIBE channel [channel ...] # 向指定的 channel 订阅消息 PSUBSCRIBE pattern [pattern ...] # 模式订阅消息 UNSUBSCRIBE [channel [channel ...]] # 取消订阅 PUNSUBSCRIBE [pattern [pattern ...]] # 模式取消订阅 PUBSUB subcommand [argument [argument ...]] # 查看订阅 # PUBSUB channels [pattern] # 频道 # PUBSUB numsub [channel ...] # channel 订阅数 # PUBSUB numpat # 模式订阅数 注意： Redis 提供的消息机制，无法实现消息堆积、回溯 消息队列的优点：异步、解耦、削峰，缺点：复杂度提高。 ","date":"0001-01-01 00:00:00","objectID":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/03/:7:0","tags":null,"title":"","uri":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/03/"},{"categories":null,"content":"8、GEO Redis 提供了 GEO（地址位置）功能，支持存储地理位置信息。 添加位置信息 GEOADD key longitude latitude member [longitude latitude member ...] 获取位置信息 GEOPOS key member [member ...] 获取两个地理位置的距离 GEODIST key member1 member2 [unit] # unit: m(米)；（km）公里；（mi）英里；（fl）尺 获取指定范围内的地理位置集合 GEORADIUS key longitude latitude radius m|km|ft|mi [WITHCOORD] [WITHDIST] [WITHHASH] [COUNT count] [ASC|DESC] [STORE key] [STOREDIST key] # 根据具体的经纬度来获取 GEORADIUSBYMEMBER key member radius m|km|ft|mi [WITHCOORD] [WITHDIST] [WITHHASH] [COUNT count] [ASC|DESC] [STORE key] [STOREDIST key] # 根据某一个成员来获取 获取 geohash GEOHASH key member [member ...] # Redis 将二维的经纬度转化为一维字符串 删除地理位置 ZREM key member [member ...] # Redis 没有提供专门的删除命令，可以借助 ZREM 命令来删除 # GEO 的数据类型为 zset ","date":"0001-01-01 00:00:00","objectID":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/03/:8:0","tags":null,"title":"","uri":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/03/"},{"categories":null,"content":"1、客户端通信协议 Redis 制定了 RESP（redis序列化协议）实现客户端和服务端的正常交互。 ","date":"0001-01-01 00:00:00","objectID":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/04/:1:0","tags":null,"title":"","uri":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/04/"},{"categories":null,"content":"1、发送命令格式 CRLF 为 ‘\\r\\n’ *\u003c参数数量\u003e CRLF $\u003c参数 1 的字节数量\u003e CRLF \u003c参数 1\u003e CRLF ... $\u003c参数 N 的字节数量\u003e CRLF \u003c参数 N\u003e CRLF 以 set hello world 命令为例： *3 $3 set $5 hello $5 world ","date":"0001-01-01 00:00:00","objectID":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/04/:1:1","tags":null,"title":"","uri":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/04/"},{"categories":null,"content":"2、返回结果格式 状态回复，第一个字节为 “+\"。如 set 错误回复，第一个字节为 “-\"。如 错误命令 整数回复，第一个字节为 “:\"。如 incr 字符串回复，第一个字节为 “$\"。如 get 多条字符串回复，第一个字节为 “*\"。如 mget ","date":"0001-01-01 00:00:00","objectID":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/04/:1:2","tags":null,"title":"","uri":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/04/"},{"categories":null,"content":"2、Java 客户端 Jedis jedis 用的很少了，请参考 lettuce、redisson ","date":"0001-01-01 00:00:00","objectID":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/04/:2:0","tags":null,"title":"","uri":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/04/"},{"categories":null,"content":"3、Python 客户端 redis-py 略 ","date":"0001-01-01 00:00:00","objectID":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/04/:3:0","tags":null,"title":"","uri":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/04/"},{"categories":null,"content":"4、客户端管理 ","date":"0001-01-01 00:00:00","objectID":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/04/:4:0","tags":null,"title":"","uri":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/04/"},{"categories":null,"content":"1、客户端 API 1、client list 与 Redis 服务端相连的所有客户端连接信息 说明： id: 客户端连接唯一标识，递增，但 Redis 重启后重置为 0。 addr: 客户端 IP 和 PORT。 fd: socket 文件描述符，与 lsof 命令中 fd 是同一个。 name: 客户端名字，与 client setName 和 client getName 有关 Redis 为每个客户端分配了输入缓冲区，它的作用将客户端发送的命令临时保存，Redis 会从输入缓冲区中拉取命令并执行。不受 maxmemory 参数影响。 qbuf: 客户端的输入缓冲区总容量 qbuf-free: 客户端的输入缓冲区剩余容量 输入缓冲区过大的原因： Redis 处理速度跟不上输入缓冲区的输入速度，可能存在 bigKey。 Redis 发生了阻塞。 Redis 为每个客户端分配了输出缓冲区，它的作用是保存命令执行的结果返回给客户端。通过配置文件中的 client-output-buffer-limit \u003cclass\u003e \u003chard limit\u003e \u003csoft limit\u003e \u003csoft seconds\u003e 来配置。不受 maxmemory 参数影响。 obl: 固定输出缓冲区大小 oll: 动态输出缓冲区大小，当固定缓冲区满了，就会使用动态缓冲区 omem: 输出缓冲区总计的字节数 其他信息： age: 已连接的时间 idle: 最近一次空闲时间 flag: S 表示 slave 客户端，N 表示普通客户端，O 表示执行 monitor 命令的客户端 db: 数据库索引下标 sub/psub: 当前客户端订阅的频道 multi: 当前事务已执行命令个数 客户端限制 maxclients (默认为 1000) 和 timeout，通过 config set maxclients 10000 命令和 config set timeout 30 命令来设置。 监控缓冲区方法： 定期执行 client list 命令，收集 qbuf 和 qbuf-free。 执行 info clients 命令，找到最大的输入缓冲区 client_recent_max_input_buffer 2、client getName / setName 给当前客户端设置名字 3、client kill 杀掉指定 ip 和 port 的客户端 client kill ip:port 4、client pause 阻塞客户端 timeout 毫秒 client pause timeout(毫秒) 5、monitor 监控 Redis 正在执行的命令，如果并发量过大，会造成输出缓冲区暴涨。 monitor ","date":"0001-01-01 00:00:00","objectID":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/04/:4:1","tags":null,"title":"","uri":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/04/"},{"categories":null,"content":"2、客户端相关配置 客户端的配置如下： timeout： 空闲连接超时时间 tcp-keepalive： 检查死的连接 tcp-backlog: TCP 连接过后，会将接受的连接放入队列中，tcp-backlog 就是这个队列的大小。 ","date":"0001-01-01 00:00:00","objectID":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/04/:4:2","tags":null,"title":"","uri":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/04/"},{"categories":null,"content":"3、客户端统计信息 1、info clients 运行命令如下： 2、info stats 运行命令如下： 客户端相关的指标 total_connections_received： 总共接受的连接数 rejected_connections： 拒绝的连接数 ","date":"0001-01-01 00:00:00","objectID":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/04/:4:3","tags":null,"title":"","uri":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/04/"},{"categories":null,"content":"5、客户端常见异常 ","date":"0001-01-01 00:00:00","objectID":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/04/:5:0","tags":null,"title":"","uri":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/04/"},{"categories":null,"content":"1、无法从连接池中获取连接 可能的原因： 连接池设置过小。 没有正确使用连接池，用过后没有释放。 具体还是要看选用的客户端，没有连接了是怎么处理的？（是等待还是直接拒接抛出异常） ","date":"0001-01-01 00:00:00","objectID":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/04/:5:1","tags":null,"title":"","uri":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/04/"},{"categories":null,"content":"2、客户端读写超时 可能的原因： 读写超时时间设置短。 命令本身就很慢。 网络不正常。 Redis 阻塞。 ","date":"0001-01-01 00:00:00","objectID":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/04/:5:2","tags":null,"title":"","uri":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/04/"},{"categories":null,"content":"3、客户端连接超时 可能的原因： 连接超时时间设置短。 网络不正常。 Redis 发生阻塞，导致 tcp-backlog 已满。 ","date":"0001-01-01 00:00:00","objectID":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/04/:5:3","tags":null,"title":"","uri":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/04/"},{"categories":null,"content":"4、客户端缓冲区异常 可能的原因： 输出缓冲区满，比如用 get 命令来获取一个bigKey。 长时间空闲连接被服务端主动断开。 不正常的并发读写，Redis 对象被多个线程并发操作。 ","date":"0001-01-01 00:00:00","objectID":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/04/:5:4","tags":null,"title":"","uri":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/04/"},{"categories":null,"content":"5、Lua 脚本执行 可能的原因： lua 脚本执行时间超过参数 lua-time-limit。 ","date":"0001-01-01 00:00:00","objectID":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/04/:5:5","tags":null,"title":"","uri":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/04/"},{"categories":null,"content":"6、客户端连接数过大 客户端连接数超过 maxclients，新的连接就会被拒绝。 从两个方面来解决： 客户端：通过下线部分应用节点，使 Redis 的连接数降下来，从而继续找其根本原因，或者调整 maxclients 参数。 服务端：如果 Redis 是高可用模式，可以把当前的节点故障转移。 ","date":"0001-01-01 00:00:00","objectID":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/04/:5:6","tags":null,"title":"","uri":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/04/"},{"categories":null,"content":"6、客户端案例分析 ","date":"0001-01-01 00:00:00","objectID":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/04/:6:0","tags":null,"title":"","uri":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/04/"},{"categories":null,"content":"1、Redis 内存陡增 现象： 服务端：Redis 主节点内存陡增，从节点内存无变化。 客户端：产生 OOM 异常。 可能的原因： 确实有大量的写入，通过执行命令 dbsize 来获取主从节点的键个数。 排查是否由客户端缓冲区应引发的问题，通过执行命令 info clients来查看。 处理方法： 通过命令 redis-cli info list | grep -v \"omemo=0\"， 找到非零的客户端连接，然后 kill 掉。 可能就是运行命令 monitor 造成的，一般都建议在生厂环境中禁用 monitor。 ","date":"0001-01-01 00:00:00","objectID":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/04/:6:1","tags":null,"title":"","uri":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/04/"},{"categories":null,"content":"2、客户端周期性超时 现象： 客户端：客户端周期性超时 服务端：无明显现象，只是一些慢查询。 可能的原因： 网络不正常。 执行命令造成慢查询导致的周期性超时。 处理方法： 运维层面，监控慢查询，一旦超多阈值，就发出报警。 避免不正确使用命令，如 KEYS *、HGETALL key 等。 ","date":"0001-01-01 00:00:00","objectID":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/04/:6:2","tags":null,"title":"","uri":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/04/"},{"categories":null,"content":"1、RDB ","date":"0001-01-01 00:00:00","objectID":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/05/:1:0","tags":null,"title":"","uri":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/05/"},{"categories":null,"content":"1、触发机制 手动触发，分别有 save 和 bgsave 两个命令。 save： 会阻塞当前 Redis 服务器，直到 RDB 过程完成为止，不建议使用。 bgsave： Redis 进程会 fork 出子进程，子进程进行 RDB 持久化，阻塞只会发生在 fork 阶段。 自动触发的场景： save m n 配置，表示在 m 秒中存在 n 次数据改变，才会触发 bgsave。 从节点全量复制过程中，主节点会执行 bgsave 生成 RDB 文件，发送子节点。 默认关闭情况下，如果没有开启 AOF，也会执行 bgsave。 ","date":"0001-01-01 00:00:00","objectID":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/05/:1:1","tags":null,"title":"","uri":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/05/"},{"categories":null,"content":"2、触发流程 bgsave 命令的运行流程如下图： 说明； 执行 bgsave 命令， 判断是否有 AOF/RDB 进程。 执行 info stats 命令，选项 latest_fork_usec 表示最后一次 fork 使用的秒数。 bgsave 命令执行完成后，会出现 Background saving started 提示。 子进程创建 RDB 文件成功后，对原有的文件进行原子替换, 执行 lastsave 命令获取最后一次生成 RDB 文件的时间，对应 info Persistence 命令中的选项 rdb_last_save_time。 ","date":"0001-01-01 00:00:00","objectID":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/05/:1:2","tags":null,"title":"","uri":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/05/"},{"categories":null,"content":"3、RDB 文件的处理 RDB 文件通过配置文件参数 dbfilename 和 dir来配置，也可以通过命令 config set dir {dir} 和 config set dbfilename {dbfilename} 来动态配置。 RDB 文件默认采用 LZF 压缩，通常建议开启，因为主从复制时，需要发送 RDB 文件到从节点，这样可以节省带宽。 RDB 默认也开启校验，可以通过脚本 redis-check-rdb 来校验生成相应的错误报告。 ","date":"0001-01-01 00:00:00","objectID":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/05/:1:3","tags":null,"title":"","uri":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/05/"},{"categories":null,"content":"4、RDB 的优缺点 优点： RDB 非常适合备份、全量复制等场景，比如每 6 小时定时执行 bgsave，可用于灾难恢复。 RDB 的恢复数据远远快于 AOF 方式。 缺点： RDB 无法做到秒级持久化，fork 创建子进程也属于重量级操作。 RDB 用特定的二进制格式保存，可能有版本不兼容问题。 ","date":"0001-01-01 00:00:00","objectID":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/05/:1:4","tags":null,"title":"","uri":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/05/"},{"categories":null,"content":"2、AOF 以独立的日志记录每次写命令，重启时再重新执行 AOF 文件中的命令达到恢复数据的目的。 AOF 解决了数据持久化的实时性。 ","date":"0001-01-01 00:00:00","objectID":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/05/:2:0","tags":null,"title":"","uri":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/05/"},{"categories":null,"content":"1、AOF 工作流程 开启 AOF 需要设置参数 appendonly yes。 通过参数 appendfilename 来设置文件名。 工作流程如下图： 说明： 所有的写入命令会追加到 aof_buf (缓冲区)中。 AOF 缓冲区会根据同步策略（参数默认设置 appendfsync everysec）来做同步操作。 会定期对 AOF 文件进行 rewrite，达到压缩的目的，因为可能有些 key 过期了。 机器重启时，如果开启了 AOF，则使用 AOF 加载数据。 ","date":"0001-01-01 00:00:00","objectID":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/05/:2:1","tags":null,"title":"","uri":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/05/"},{"categories":null,"content":"2、命令写入 AOF 采用文本协议格式，也就是说 AOF 文件中存储就是写入的命令，这样具有阅读性、便于修改。 AOF 把命令先写入 aof_buf 中，根据不同的同步策略可以在性能和安全上做出平衡，没有特殊要求，就设置为 everysec。 三种策略； no: don’t fsync, just let the OS flush the data when it wants. Faster. always: fsync after every write to the append only log. Slow, Safest. everysec: fsync only one time every second. Compromise. ","date":"0001-01-01 00:00:00","objectID":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/05/:2:2","tags":null,"title":"","uri":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/05/"},{"categories":null,"content":"3、重写机制 AOF 文件可以变小的原因： 超时的数据，可以不用再写入文件中。 key 过期了，可能含有无效命令，如 del key1。 多个命令可以合并成一个，如 lpush list a 和 lpush list b 可以合并为 lpush list a b。 触发 AOF 重写方式： 手动执行命令 bgrewriteaof 自动触发，根据配置参数 auto-aof-rewrite-percentage 100 和 auto-aof-rewrite-min-size 64mb。 参数说明： This is how it works: Redis remembers the size of the AOF file after the latest rewrite (if no rewrite has happened since the restart, the size of the AOF at startup is used). This base size is compared to the current size. If the current size is bigger than the specified percentage, the rewrite is triggered. Also you need to specify a minimal size for the AOF file to be rewritten, this is useful to avoid rewriting the AOF file even if the percentage increase is reached but it is still pretty small 自动触发时机: aof_current_size \u003e auto-aof-rewrite-min-size \u0026\u0026 (aof_current_size - aof_base_size) / aof_base_size \u003e auto-aof-rewrite-percentage AOF 重写流程图如下： 说明： 执行 AOF 重写请求，如果有子进程在执行 bgsave 则等待完成之后再操作。 fork 子进程进行重写，父进程接受请求，修改命令写入 aof_buf 中根据策略同步到磁盘。 fork 操作运用写时复制技术，所以子进程只能共享操作 fork 时的内存，这时父进程可能还在响应请求，所以把重写后的新命令放入 aof_rewrite_buf 缓冲区中。 把 aof_rewrite_buf 中数据写入新的 AOF 文件中，根据开启参数 aof-rewrite-incremental-fsync yes，每 32MB 同步到磁盘。 ","date":"0001-01-01 00:00:00","objectID":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/05/:2:3","tags":null,"title":"","uri":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/05/"},{"categories":null,"content":"4、重启加载 重启加载图： ","date":"0001-01-01 00:00:00","objectID":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/05/:2:4","tags":null,"title":"","uri":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/05/"},{"categories":null,"content":"5、文件校验 加载损坏的 AOF 文件会拒绝启动，可以先备份文件，然后再执行命令 redis-check-aof [--fix] \u003cfile.aof\u003e 来进行修复。 ","date":"0001-01-01 00:00:00","objectID":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/05/:2:5","tags":null,"title":"","uri":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/05/"},{"categories":null,"content":"3、问题定位与优化 ","date":"0001-01-01 00:00:00","objectID":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/05/:3:0","tags":null,"title":"","uri":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/05/"},{"categories":null,"content":"1、fork 操作 Redis 做 RDB 或者 AOF 重写时，必不可少的操作就是 fork。fork 用的写时复制技术，会复制父进程的内存页表。 改善 fork操作的耗时： 优先使用物理机或者高效支持 fork 操作的虚拟化技术。 fork 耗时和内存量成正比，单个 Redis 实例建议不超过 10G。 linux 内存分配策略，避免物理内存不足导致 fork 失败。 降低 fork 操作频率，比如避免不必要的全量复制，适当放宽 AOF 自动触发时机。 ","date":"0001-01-01 00:00:00","objectID":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/05/:3:1","tags":null,"title":"","uri":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/05/"},{"categories":null,"content":"2、子进程开销监控和优化 CPU，子进程负责把内存中的数据写入文件中，属于 IO 密集型操作，不要和其他 IO 密集型服务部署在一起。 内存，写时复制技术，避免在大量写入时做子进程重写操作，导致父进程维护大量页副本，造成内存消耗，可以关闭 THP。 磁盘，AOF 重写会消耗大量磁盘 IO，可以关闭，参数设置为 no-appendfsync-on-rewrite yes，默认是关闭的，但是开启后，可能丢失数据。 ","date":"0001-01-01 00:00:00","objectID":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/05/:3:2","tags":null,"title":"","uri":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/05/"},{"categories":null,"content":"3、AOF 追加阻塞 AOF 持久化，常用的同步策略是 everysec，用于平衡性能和安全性，对于这种方式，Redis 使用另一个线程每秒执行 fsync 同步磁盘，当系统磁盘繁忙时，可能造成 Redis 主进程阻塞。 ","date":"0001-01-01 00:00:00","objectID":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/05/:3:3","tags":null,"title":"","uri":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/05/"},{"categories":null,"content":"4、多实例部署 略 ","date":"0001-01-01 00:00:00","objectID":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/05/:4:0","tags":null,"title":"","uri":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/05/"},{"categories":null,"content":"1、配置 ","date":"0001-01-01 00:00:00","objectID":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/06/:1:0","tags":null,"title":"","uri":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/06/"},{"categories":null,"content":"1、建立复制 建立复制前会删除全部数据。 配置复制的方式有三种； 配置文件中加入 slaveof {masterHost} {masterPort}。 redis 启动命令后加入 slaveof {masterHost} {masterPort}。 直接执行命令 slaveof {masterHost} {masterPort}。 ","date":"0001-01-01 00:00:00","objectID":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/06/:1:1","tags":null,"title":"","uri":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/06/"},{"categories":null,"content":"2、断开复制 在从节点执行命令 slaveof no one 来断开复制。所谓切主操作，就是先断开复制，然后再建立复制。 ","date":"0001-01-01 00:00:00","objectID":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/06/:1:2","tags":null,"title":"","uri":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/06/"},{"categories":null,"content":"3、安全性 为了安全性，一般都会在主节点上设置 requirepass 123456，所有客户端访问必须使用 auth 123456 验证。因此从节点开启复制时，也要设置 masterauth 123456。 ","date":"0001-01-01 00:00:00","objectID":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/06/:1:3","tags":null,"title":"","uri":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/06/"},{"categories":null,"content":"4、只读 默认情况下，从节点使用 slave-read-only=yes 配置为只读模式。由于已经开启了复制，建议从节点保持只读模式。 ","date":"0001-01-01 00:00:00","objectID":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/06/:1:4","tags":null,"title":"","uri":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/06/"},{"categories":null,"content":"5、传输延迟 主从节点之间复制数据，肯定会有延迟。 redis 提供了 repl-disable-tcp-nodelay 参数用于关闭 TCP_NODELAY，默认关闭。 当关闭时，主节点的数据无论大小都会发送到从节点，这样就降低了延迟，但增加了带宽。 当开启时，主节点会合并较小的 TCP 数据包，从而节省带宽。 ","date":"0001-01-01 00:00:00","objectID":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/06/:1:5","tags":null,"title":"","uri":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/06/"},{"categories":null,"content":"2、拓扑 主要有三种；一主一从，一主多从，树状主从。 一主一从： 最简单的结构，一般只在 从节点上开启 AOF 操作。 一主多从： 用于读多写少、读写分离的场景 树状主从： 从节点不但可以复制主节点的数据，还可以作为其他的节点的主节点。 ","date":"0001-01-01 00:00:00","objectID":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/06/:2:0","tags":null,"title":"","uri":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/06/"},{"categories":null,"content":"3、原理 ","date":"0001-01-01 00:00:00","objectID":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/06/:3:0","tags":null,"title":"","uri":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/06/"},{"categories":null,"content":"1、复制过程 执行 slaveof {masterHost} {masterPort} 命令后，保存主节点信息。 从节点每秒运行定时任务维护复制逻辑，当发现新的主节点后，建立连接。 发送 ping 命令，主要是检查网络是否可用和是否可以处理命令（可能主节点阻塞了）。 权限验证，requirepass 和 masterauth 是否匹配。 同步数据集，分为全量同步和部分同步。 命令持续复制，新的命令持续发给从节点。 ","date":"0001-01-01 00:00:00","objectID":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/06/:3:1","tags":null,"title":"","uri":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/06/"},{"categories":null,"content":"2、数据同步 同步过程分为全量复制和部分复制。 参与主从复制的节点都会维护自身复制偏移量。命令 info 中 master_repl_offset 和 slave_repl_offset 。 复制积压缓冲区，主节点把命令发送从节点，还会把命令写入复制积压缓冲区，这个用于部分复制和命令丢失的场景。命令 info 中 repl_backlog_*。 主节点运行 ID，用来唯一识别 Redis 节点，当运行 ID 变化了，从节点将做全量复制了。节点重启了，运行 ID 也会变化。可以执行命令 debug reload 来重新加载并保持运行 ID 不变，但是会阻塞当前 Redis。 命令 info 中 run_id。 ","date":"0001-01-01 00:00:00","objectID":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/06/:3:2","tags":null,"title":"","uri":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/06/"},{"categories":null,"content":"3、全量复制 ","date":"0001-01-01 00:00:00","objectID":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/06/:3:3","tags":null,"title":"","uri":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/06/"},{"categories":null,"content":"4、部分复制 ","date":"0001-01-01 00:00:00","objectID":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/06/:3:4","tags":null,"title":"","uri":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/06/"},{"categories":null,"content":"5、心跳 ","date":"0001-01-01 00:00:00","objectID":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/06/:3:5","tags":null,"title":"","uri":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/06/"},{"categories":null,"content":"6、异步复制 ","date":"0001-01-01 00:00:00","objectID":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/06/:3:6","tags":null,"title":"","uri":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/06/"},{"categories":null,"content":"4、开发与运维中的问题 ","date":"0001-01-01 00:00:00","objectID":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/06/:4:0","tags":null,"title":"","uri":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/06/"},{"categories":null,"content":"Redis 开发与运维 第一章 初识 Redis 第二章 API 的理解和使用 第三章 小功能大用处 第四章 客户端 第五章 持久化 第六章 复制 ","date":"0001-01-01 00:00:00","objectID":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/readme/:1:0","tags":null,"title":"","uri":"/ooooo-notes/old-notes/books/redis-development-and-operation-and-maintenance/readme/"},{"categories":null,"content":"1、上下文切换 CPU 通过时间片分配算法来循环执行任务，当前任务执行完一个时间片后会切换到下一个任务。但是在切换前会保存上一个任务的状态，以便下次切换回这个任务时，再次加载该任务状态。这就是上下文切换。 创建过多的线程，会使上下文切换频繁，执行效率也可能不如单线程。 上图的 cs (context switch) 表示上下文切换次数。 减少上下文切换的方法： 无锁并发编程，多线程处理数据时，可以用一个方法来避免锁。如将数据的 ID 按照 Hash 算法取余分段，不同的线程处理不同段的数据。 CAS 算法，Java 的 Atomic 包。 使用最少线程，避免创建不需要的线程，比如任务很少，创建的线程较多。 协程，单线程实现多任务的调度，并维持多任务状态切换。 减少上下文切换示例 jstack 命令来 dump 线程 jstack 31177 \u003e /home/xxx/dump-31177 统计线程都处于什么状态 grep java.lang.Thread.State dump-31177 | awk '{print $2$3$4$5}' | sort | uniq -c 查看这些 waiting 的线程，根据需要合理配置线程数。 ","date":"0001-01-01 00:00:00","objectID":"/ooooo-notes/old-notes/books/the-art-of-java-concurrent-programming/01/:1:0","tags":null,"title":"","uri":"/ooooo-notes/old-notes/books/the-art-of-java-concurrent-programming/01/"},{"categories":null,"content":"2、死锁 死锁示例： public static void main(String[] args) { Object lockA = new Object(); Object lockB = new Object(); Thread t1 = new Thread(() -\u003e { synchronized (lockA) { System.out.println(\"get lockA\"); timeSleep(2); synchronized (lockB) { System.out.println(\"get lockB\"); } } }); Thread t2 = new Thread(() -\u003e { synchronized (lockB) { System.out.println(\"get lockB\"); synchronized (lockA) { System.out.println(\"get lockA\"); } } }); t1.start(); t2.start(); } 避免死锁的方法： 避免一个线程同时获取多个锁，也就是同时申请所有的资源。 尝试使用定时锁，如 lock.tryLock(timeout) 来替换内部锁。 对于数据库锁，加锁和加锁必须在一个数据库连接里。 ","date":"0001-01-01 00:00:00","objectID":"/ooooo-notes/old-notes/books/the-art-of-java-concurrent-programming/01/:2:0","tags":null,"title":"","uri":"/ooooo-notes/old-notes/books/the-art-of-java-concurrent-programming/01/"},{"categories":null,"content":"3、资源限制的挑战 带宽，比如带宽只有 20M, 一个线程最多只能使用 10M，也就是说线程数最大只能是 2，多余的线程没有资源可以使用。 磁盘 ","date":"0001-01-01 00:00:00","objectID":"/ooooo-notes/old-notes/books/the-art-of-java-concurrent-programming/01/:3:0","tags":null,"title":"","uri":"/ooooo-notes/old-notes/books/the-art-of-java-concurrent-programming/01/"},{"categories":null,"content":"4、总结 强烈建议使用 JDK 并发包提供的并发容器和工具类来解决并发问题。 ","date":"0001-01-01 00:00:00","objectID":"/ooooo-notes/old-notes/books/the-art-of-java-concurrent-programming/01/:4:0","tags":null,"title":"","uri":"/ooooo-notes/old-notes/books/the-art-of-java-concurrent-programming/01/"},{"categories":null,"content":"Java 并发编程的艺术 第一章 并发编程的挑战 第二章 Java 并发机制的底层实现原理 第三章 Java 内存模型 ","date":"0001-01-01 00:00:00","objectID":"/ooooo-notes/old-notes/books/the-art-of-java-concurrent-programming/readme/:1:0","tags":null,"title":"","uri":"/ooooo-notes/old-notes/books/the-art-of-java-concurrent-programming/readme/"},{"categories":null,"content":"Java 并发编程实战 ","date":"0001-01-01 00:00:00","objectID":"/ooooo-notes/old-notes/geektime/java-con-practice/readme/:1:0","tags":null,"title":"","uri":"/ooooo-notes/old-notes/geektime/java-con-practice/readme/"},{"categories":null,"content":"1、Java本身有两个显著的特性 JRE 就是 Java 运行环境， JDK 就是 Java 开发工具包 跨平台运行（一次编写，到处运行） 垃圾回收器（程序员不用手动回收内存，但仍然可能存在内存泄漏） ","date":"0001-01-01 00:00:00","objectID":"/ooooo-notes/old-notes/geektime/java-core-36/01/:1:0","tags":null,"title":"","uri":"/ooooo-notes/old-notes/geektime/java-core-36/01/"},{"categories":null,"content":"2、Java是解析执行？（不太正确） 我们开发的 java 源代码，经过 javac 编译成为字节码，在运行时，通过 JVM 内置的解析器将字节码装换为机器码。 常见的 JVM， 比如 Oracle 的 Hotspot JVM，提供了 JIT（Just-In-Time）动态编译器。 在主流的 Java 版本中，Java 8 采用混合模式-Xmixed进行。 Oracle Hotspot JVM 提供了两种不同的 JIT 编译器，C1 对应 client 模式，适用于启动敏感的应用，C2 对应 server 模式，适用于长时间运行的服务器。默认采用的是分层编译。 JVM 启动时，可以通过指定不同的参数对运行模式选择。 -Xint JVM 只进行解释执行。 -Xcomp JVM 只进行编译执行。 除了上面的编译方式，还有一种新的编译方式（AOT），就是直接把字节码编译为机器码。 利用下面的命令把某个类或者某个模块编译成为AOT库 jaotc --output libHelloWorld.so HelloWorld.class jaotc --output libjava.base.so --module java.base 然后在启动时直接指定 java -XX:AOTLibrary=./libHelloWorld.so,./libjava.base.so HelloWorld ","date":"0001-01-01 00:00:00","objectID":"/ooooo-notes/old-notes/geektime/java-core-36/01/:2:0","tags":null,"title":"","uri":"/ooooo-notes/old-notes/geektime/java-core-36/01/"},{"categories":null,"content":"1、Exception 和 Error Exception 和 Error 都继承 Throwable 类，只有 Throwable 类的实例才可以抛出。 Exception 是可以预料的意外情况，可以被捕获进行相应的处理。而 Error 是不太可能出现的情况，可能会造成程序终止，如 OutOfMemoryError（内存溢出）。 Exception 分为可检查（checked）异常和不检查（unchecked）异常，可检查异常必须显式捕获处理，不检查异常就是运行时异常。如 NullPointerException 。 常见的 Exception NullPointerException （空指针异常） ArrayIndexOutOfBoundsException （数组越界异常） NoSuchFileException （文件没有找到异常） InterruptedException （线程被打断异常） ClassCastException （类型转换异常） 常见的 Error NoClassDefFoundError （类没有被找到错误） OutOfMemoryError （堆内存溢出错误） StackOverflowError （栈内存溢出错误） ","date":"0001-01-01 00:00:00","objectID":"/ooooo-notes/old-notes/geektime/java-core-36/02/:1:0","tags":null,"title":"","uri":"/ooooo-notes/old-notes/geektime/java-core-36/02/"},{"categories":null,"content":"2、try-catch-finally try (BuferedReader br = new BuferedReader(...); BuferedWriter writer = new BuferedWriter(...)) { // do something catch ( IOException | XEception e) { // Multiple catch // Handle it } finally { // do something } 注意 尽量不要捕获 Exception 类型的异常，具体异常具体处理。 不要生吞（swallow）异常，避免错误后出现难以诊断的情况，可以输出到日志中。 Java 的异常处理机制会有额外的开销 try-catch 的代码段会影响 JVM 的优化，尽量只捕获有必要的代码段。 Java 每实例化一个 Exception，就会对当前栈进行快照。 ","date":"0001-01-01 00:00:00","objectID":"/ooooo-notes/old-notes/geektime/java-core-36/02/:2:0","tags":null,"title":"","uri":"/ooooo-notes/old-notes/geektime/java-core-36/02/"},{"categories":null,"content":"1、final final 修饰的类，不能被继承。 final 修饰的变量，不能被修改。 final 修饰的方法，不能被重写。 final 不是 immutable，对象的属性还是可以改变的。 ","date":"0001-01-01 00:00:00","objectID":"/ooooo-notes/old-notes/geektime/java-core-36/03/:1:0","tags":null,"title":"","uri":"/ooooo-notes/old-notes/geektime/java-core-36/03/"},{"categories":null,"content":"2、finally finally 是 Java 保证代码一定会被执行的机制，可以使用 try-catch-finally、try-finally 来关闭数据库连接，unlock()等。 如果是利用 finally 机制来关闭资源，最好是用 try-with-resources。 特例 try { // do something Sysem.exit(1); } finally{ Sysem.out.println(“Print from fnally”); } 上面的 finally 语句不会被执行。 ","date":"0001-01-01 00:00:00","objectID":"/ooooo-notes/old-notes/geektime/java-core-36/03/:2:0","tags":null,"title":"","uri":"/ooooo-notes/old-notes/geektime/java-core-36/03/"},{"categories":null,"content":"3、finalize finalize 方法是 Object 中一个方法，它的设计目的是保证对象在垃圾收集前完成资源的回收，现在已经不推荐使用，在 Java 9 中已被标记为 @Deprecated。 使用 finalize 可能会使程序性能降低，因为 JVM 会做额外处理。 Java 目前使用 Cleaner 来替换 finalize，Cleaner的实现利用了幻象引用（虚引用）和引用队列，比如 mysql-connector-java 就是利用幻象引用来回收资源。 ","date":"0001-01-01 00:00:00","objectID":"/ooooo-notes/old-notes/geektime/java-core-36/03/:3:0","tags":null,"title":"","uri":"/ooooo-notes/old-notes/geektime/java-core-36/03/"},{"categories":null,"content":"1、kafka概念 ​ Apache Kafka是一款开源的消息引擎系统，也是一个分布式流处理平台；消息引擎系统是一组规范，企业利用这组规范在不同系统之间传递语义准确的消息，实现松耦合的异步式数据传输。 ","date":"0001-01-01 00:00:00","objectID":"/ooooo-notes/old-notes/geektime/java-core-36/04/:1:0","tags":null,"title":"","uri":"/ooooo-notes/old-notes/geektime/java-core-36/04/"},{"categories":null,"content":"2、kafka特点 使用纯二进制的字节序列 同时支持两种消息引擎模型（点对点、发布/订阅） ","date":"0001-01-01 00:00:00","objectID":"/ooooo-notes/old-notes/geektime/java-core-36/04/:2:0","tags":null,"title":"","uri":"/ooooo-notes/old-notes/geektime/java-core-36/04/"},{"categories":null,"content":"3、kafka的架构 Topic：主题，承载消息的逻辑容器，用来区分业务； Producer：生产者，向主题发布新消息的应用程序； Consumer：消费者，向主题订阅新消息的应用程序； Partition：分区，每个Topic可以设置多个分区； Replica：副本，同一个消息以提供数据冗余可以有多个副本，分为领导者副本（可对外提供服务）和追随者副本（不可以对外提供服务）；对于分区实现高可用； Consumer Group：消费者组，多个消费者可组成一个消费者组，同时消费多个分区实现高吞吐；同一个消费者组内的消费者不可重复消费同一分区的消息； Rebalance：重平衡，消费者组内的某个消费者挂掉后，会重新分配订阅主题分区； Offset：位移，有分区位移和消费者位移两个概念；分区位移是消息的位置标记（从0开始），是固定的；消费者位移是消费者在订阅消息时的消费进度，是动态的； 图解partition和replication的概念 ","date":"0001-01-01 00:00:00","objectID":"/ooooo-notes/old-notes/geektime/java-core-36/04/:3:0","tags":null,"title":"","uri":"/ooooo-notes/old-notes/geektime/java-core-36/04/"},{"categories":null,"content":"4、多种Kafka对比 Apache Kafka：社区版kafka；迭代速度快，社区响应快，但是仅提供核心功能，缺少高级特性； Confluent Kafka：Confluent公司提供的Kafka；集成了很多高级特性，分免费版和收费版，但是相关资料不全，普及率低； CDH/HDP Kafka：大数据平台内嵌的Apache Kafka，操作简单，节省运维成本，但是把控度低，演进速度慢； ","date":"0001-01-01 00:00:00","objectID":"/ooooo-notes/old-notes/geektime/java-core-36/04/:4:0","tags":null,"title":"","uri":"/ooooo-notes/old-notes/geektime/java-core-36/04/"},{"categories":null,"content":"5、Kafka版本演变 ​ kafka版本命名规范：例kafka_2.11-2.1.1.tgz（2.11表示scala版本，2.1.1表示kafka版本） kafka版本号规范：大版本号 - 小版本号 - Patch 号（修订号） 0.7版本：只提供最基础的消息队列功能； 0.8版本：引入了副本机制， 成为了一个真正意义上完备的分布式高可靠消息队列解决方案；（但是生产和消费的客户端还是老版本的，应指定zk地址而不是broker地址）； 0.8.2.0版本：引入了新版本Producer API（但是bug还有点多，不建议使用）； 0.8.2.2版本：老版本的Consumer API比较稳定了； 0.9版本：增加了基础的安全认证/权限功能，同时使用java重写了Consumer API，还引入了Kafka Connect组件用于实现高性能的数据抽取；另外新版本的Producer API比较稳定了，不建议使用新版本的Consumer API； 0.10版本：引入了Kafka Streams，正式升级为分布式流处理平台； 0.10.2.2版本：新版本的Consumer API比较稳定了，该版本也修复了一个可能导致Producer性能降低的bug； 0.11版本：提供了幂等性Producer API（幂等性就是消息去重，默认不开启）和事务API（实现流处理结果正确性的基石），还对Kafka消息格式做了重构；（该版本也是主流版本）； 0.11.0.3版本：消息引擎功能非常完善了； 1.1版本：实现故障转移（即Failover）； 1.0版本和2.0版本：只要是是对Kafka Streams的改进； ","date":"0001-01-01 00:00:00","objectID":"/ooooo-notes/old-notes/geektime/java-core-36/04/:5:0","tags":null,"title":"","uri":"/ooooo-notes/old-notes/geektime/java-core-36/04/"},{"categories":null,"content":"6、Kafka的核心参数 ","date":"0001-01-01 00:00:00","objectID":"/ooooo-notes/old-notes/geektime/java-core-36/04/:6:0","tags":null,"title":"","uri":"/ooooo-notes/old-notes/geektime/java-core-36/04/"},{"categories":null,"content":"1、配置文件参数 log.dirs：指定broker需要使用的若干个文件目录路径，无默认值；生产环境必须配置，CSV格式（例如：/home/kafka1,/home/kafka2） zookeeper.connect：指定zk的地址和端口（例hadoop01:2181,hadoop02:2181,hadoop03:2181），zk保存了topic、分区的信息等等，如果多个kafka集群共有一个zk集群，加上chroot即可，chroot是别名，则指定格式为hadoop01:2181,hadoop02:2181,hadoop03:2181/kafka1或hadoop01:2181,hadoop02:2181,hadoop03:2181/kafka2; listeners：监听器，指定协议、主机名、端口； advertised.listeners：指该监听器是broker对外发布的； host.name/port：过期参数，可以不指定； auto.create.topics.enable：是否允许自动创建topic； unclean.leader.election.enable：是否允许unclean leader选举，原本数据多的分区才有资格选举leader，该参数设置为true后，数据少的也可以参与选举，会造成数据丢失，建议设置为false； auto.leader.rebalance.enable：是否允许定期选举leader，不建议开启； log.retention.{hour|minutes|ms}：控制一条消息被保存多长时间，ms优先级最高； log.retention.bytes：指定broker为消息保存的总磁盘容量大小，默认值为-1，表示没有上限； message.max.bytes：控制broker能够接收的最大消息大小，默认值为1000012，太小，建议重设置； ","date":"0001-01-01 00:00:00","objectID":"/ooooo-notes/old-notes/geektime/java-core-36/04/:6:1","tags":null,"title":"","uri":"/ooooo-notes/old-notes/geektime/java-core-36/04/"},{"categories":null,"content":"2、Topic级别参数 retention.ms：规定了该topic消息被保存的时长； retention.bytes：规定了要为该topic预留多大的磁盘空间（默认-1，表示没有上限）； max.message.bytes：Broker能够接收的该topic的最大消息大小； 以上参数可以通过两种方式设置 创建topic时进行设置：例bin/kafka-topics.sh –zookeeper hadoop01:2181,hadoop02:2181,hadoop03:2181 –create –topic my-topic –partitions 1 –replication-factor 1 –config max.message.bytes=64000 –config flush.messages=1 修改topic时设置：例bin/kafka-topics.sh –zookeeper hadoop01:2181,hadoop02:2181,hadoop03:2181 –alter –topic my-topic –config max.message.bytes=128000 ","date":"0001-01-01 00:00:00","objectID":"/ooooo-notes/old-notes/geektime/java-core-36/04/:6:2","tags":null,"title":"","uri":"/ooooo-notes/old-notes/geektime/java-core-36/04/"},{"categories":null,"content":"3、JVM参数 KAFKA_HEAP_OPTS：指定堆大小； KAFKA_JVM_PERFORMANCE_OPTS：指定GC参数； 在启动kafka前设置这两个环境变量。 ","date":"0001-01-01 00:00:00","objectID":"/ooooo-notes/old-notes/geektime/java-core-36/04/:6:3","tags":null,"title":"","uri":"/ooooo-notes/old-notes/geektime/java-core-36/04/"},{"categories":null,"content":"4、操作系统参数 ulimit -n：打开文件描述符最大值（例ulimit -n 100000）； 文件系统类型：建议选择XFS； swappniess：swap空间大小，建议设置略大于0的值； 提交时间：即flush落盘时间，kafka的数据会先写到操作系统的页缓存上，然后会根据LRU算法定期将页缓存的数据落盘到磁盘，默认为5秒，可适当增大时间间隔； ","date":"0001-01-01 00:00:00","objectID":"/ooooo-notes/old-notes/geektime/java-core-36/04/:6:4","tags":null,"title":"","uri":"/ooooo-notes/old-notes/geektime/java-core-36/04/"},{"categories":null,"content":"7、分区策略 ","date":"0001-01-01 00:00:00","objectID":"/ooooo-notes/old-notes/geektime/java-core-36/04/:7:0","tags":null,"title":"","uri":"/ooooo-notes/old-notes/geektime/java-core-36/04/"},{"categories":null,"content":"1、生产者分区策略 轮询策略（Round-robin）：kafka生产者API默认的分区策略，最大限度负载均衡； 随机策略（Randomness）：可自定义实现该策略； 按消息键保存策略（Key-ordering）：key可以是业务上的字段信息，按业务场景自定义分区； ","date":"0001-01-01 00:00:00","objectID":"/ooooo-notes/old-notes/geektime/java-core-36/04/:7:1","tags":null,"title":"","uri":"/ooooo-notes/old-notes/geektime/java-core-36/04/"},{"categories":null,"content":"2、消费者分区策略 按范围分配：指定分区消费； 轮询分配：按顺序分配给消费者； 自定义：设置partition.assignment.strategy为自定义的类； ","date":"0001-01-01 00:00:00","objectID":"/ooooo-notes/old-notes/geektime/java-core-36/04/:7:2","tags":null,"title":"","uri":"/ooooo-notes/old-notes/geektime/java-core-36/04/"},{"categories":null,"content":"8、压缩 ​ kafka的消息层次分为消息集合和消息。一个消息集合包含若干条日志项，日志项就是装消息的地方；kafka底层的消息日志由一系列消息集合日志项组成，kafka是在消息集合层面上进行写入操作； ","date":"0001-01-01 00:00:00","objectID":"/ooooo-notes/old-notes/geektime/java-core-36/04/:8:0","tags":null,"title":"","uri":"/ooooo-notes/old-notes/geektime/java-core-36/04/"},{"categories":null,"content":"1、kafka消息格式 ​ kafka有两大消息格式：V1和V2（0.11.0.0版本后引入的）； ​ V1中每条消息需要执行CRC校验，但是在某些情况下CRC值是会变化的，会浪费空间和耽误CPU时间；在保存压缩消息上，是把多条消息进行压缩然后保存到外层消息的消息体字段中。 ​ V2对V1改进了很多，CRC校验工作移到了消息集合这一层，而且在保存压缩消息上，是对整个消息集合进行压缩。 ","date":"0001-01-01 00:00:00","objectID":"/ooooo-notes/old-notes/geektime/java-core-36/04/:8:1","tags":null,"title":"","uri":"/ooooo-notes/old-notes/geektime/java-core-36/04/"},{"categories":null,"content":"2、何时压缩和解压缩 生产者端：生产消息时指定压缩方法。 broker端：默认的压缩方式是producer，如果指定了跟producer不同的压缩方式时，会先解压缩再按新指定的方式压缩；或者broker端发生了消息格式转换也会重压缩。 consumer端：解压缩。 ","date":"0001-01-01 00:00:00","objectID":"/ooooo-notes/old-notes/geektime/java-core-36/04/:8:2","tags":null,"title":"","uri":"/ooooo-notes/old-notes/geektime/java-core-36/04/"},{"categories":null,"content":"3、压缩算法 （kafka2.1.0版本前支持的算法：GZIP、Snappy、LZ4；该版本开始后支持Zstandard算法） 压缩算法的优劣有两个指标：压缩比和压缩/解压缩吞吐量； 压缩比：zstd \u003e LZ4 \u003e GZIP \u003e Snappy 吞吐量：LZ4 \u003e Snappy \u003e zstd / GZIP ","date":"0001-01-01 00:00:00","objectID":"/ooooo-notes/old-notes/geektime/java-core-36/04/:8:3","tags":null,"title":"","uri":"/ooooo-notes/old-notes/geektime/java-core-36/04/"},{"categories":null,"content":"9、无消息丢失配置 kafka只对已提交的消息做有限度的持久化保证。 不要使用producer.send(msg),而要使用producer.send(msg,callback); 设置acks = all；表明所有副本broker都要接收到消息，保证消息”已提交“； 设置retries为一个较大的值； 设置unclean.leader.election.enable = false；表示禁止落后的broker被选为leader。 设置replication.factor \u003e= 3； 设置min.insync.replicas \u003e 1；控制消息至少被写入多少个副本才算“已提交”； 确保replication.factor \u003e min.insync.replicas；推荐replication.factor = min.insync.replicas + 1； 设置enable.auto.commit = false；确保消息消费完成再提交； ","date":"0001-01-01 00:00:00","objectID":"/ooooo-notes/old-notes/geektime/java-core-36/04/:9:0","tags":null,"title":"","uri":"/ooooo-notes/old-notes/geektime/java-core-36/04/"},{"categories":null,"content":"10、幂等性和事务性 ​ 幂等性：指某些操作执行一次或多次的结果是一样的，在kafka中是对重复消息去重。 ​ 引入事务的作用：1、 生产者多次发送消息可以封装成一个原子操作，要么都成功，要么失败；2、 consumer-transform-producer模式下，因为消费者提交偏移量出现问题，导致在重复消费消息时，生产者重复生产消息。需要将这个模式下消费者提交偏移量操作和生产者一系列生成消息的操作封装成一个原子操作。 ​ kafka事务一般为两种：1、 只有Producer生产消息 ；2、生产消费并存（consumer-transform-producer）；3、只有Consumer消费消息。 幂等性Producer：只能保证单分区、单会话上的消息幂等性（设置幂等性：props.put(“enable.idempotence”, true)或props.put(ProducerConfig.ENABLE_IDEMPOTENCE_CONFIG, true)） 事务提供的ACID特性：原子性（Atomicity）、一致性（Consistency）、隔离性（Isolation）、持久性（Durability） 隔离性：表示并发执行的事务彼此相互隔离，互不影响。 事务型Producer：能够保证跨分区、跨会话间的幂等性(设置事务型Producer：开启enable.idempotence = true，然后设置Producer端参数transctional.id,还要调用一些事务API，如下列代码；表示record1和record2要么全部写入成功要么失败。在consumer端要设置isolation.level，read_uncommitted是默认值，表示可以读取到kafka任何消息，不管事务型Producer是提交事务还是终止事务；建议使用read_committed，表示只读取事务型成功提交的消息以及非事务型Producer写入的消息。) producer.initTransactions(); try{ producer.beginTransaction(); producer.send(record1); producer.send(record2); producer.commitTransaction(); }catch(KafkaException e){ producer.abortTransaction(); } ","date":"0001-01-01 00:00:00","objectID":"/ooooo-notes/old-notes/geektime/java-core-36/04/:10:0","tags":null,"title":"","uri":"/ooooo-notes/old-notes/geektime/java-core-36/04/"},{"categories":null,"content":"11、Consumer Group ","date":"0001-01-01 00:00:00","objectID":"/ooooo-notes/old-notes/geektime/java-core-36/04/:11:0","tags":null,"title":"","uri":"/ooooo-notes/old-notes/geektime/java-core-36/04/"},{"categories":null,"content":"1、特性 一个Consumer Group可以有一个或多个Consumer； Droup ID是一个字符串，标识这一个唯一的Consumer Group； 同一个Group中的Consumer不能重复订阅一个分区； （老版本的Consumer Group把消费者位移保存在zk中，由于频繁读写会导致zk集群性能降低，新版本把消费者位移保存在kafka的_consumer_offsets的topic中。） ","date":"0001-01-01 00:00:00","objectID":"/ooooo-notes/old-notes/geektime/java-core-36/04/:11:1","tags":null,"title":"","uri":"/ooooo-notes/old-notes/geektime/java-core-36/04/"},{"categories":null,"content":"2、rebalance的触发条件 Consumer Group中的成员数变更； 订阅的topic数变更（比如订阅了用正则匹配的topic，新增了一个符合的topic）； 订阅的topic的分区数变更； ","date":"0001-01-01 00:00:00","objectID":"/ooooo-notes/old-notes/geektime/java-core-36/04/:11:2","tags":null,"title":"","uri":"/ooooo-notes/old-notes/geektime/java-core-36/04/"},{"categories":null,"content":"3、rebalance的弊端 rebalance影响Consumer端TPS；（rebalance期间，consumer会停止工作） rebalance过程很慢； rebalance效率不高； ","date":"0001-01-01 00:00:00","objectID":"/ooooo-notes/old-notes/geektime/java-core-36/04/:11:3","tags":null,"title":"","uri":"/ooooo-notes/old-notes/geektime/java-core-36/04/"},{"categories":null,"content":"4、非必要rebalance consumer未能及时给coordinator发送心跳，导致consumer被踢出Consumer Group；需设置合理的session.timeout.ms（默认值是10s，表示coordinator在10s内没收到consumer的心跳消息，该consumer被判定为dead）和heartbeat.interval.ms（表示consumer发送心跳请求的频率）的值 （推荐session.timeout.ms=2s，heartbeat.interval.ms=6s） consumer消费时间过长；需设置max.poll.interval.ms的值，表示下游处理数据的时间； ","date":"0001-01-01 00:00:00","objectID":"/ooooo-notes/old-notes/geektime/java-core-36/04/:11:4","tags":null,"title":"","uri":"/ooooo-notes/old-notes/geektime/java-core-36/04/"},{"categories":null,"content":"12、位移主题（_consumer_offsets） ​ _consumer_offsets的主要作用就是保存Kafka消费者的位移消息。消息格式是KV对，Key保存的是\u003cGroup ID,主题名，分区号\u003e；另外还有两种消息格式：1.用于保存Consumer Group信息的消息；2.用于删除Group过期位移甚至是删除Group的消息。 ​ 当有第一个Consumer消费数据时，Kafka就会自动创建_consumer_offsets这个主题，默认有50个分区，3个副本。 ","date":"0001-01-01 00:00:00","objectID":"/ooooo-notes/old-notes/geektime/java-core-36/04/:12:0","tags":null,"title":"","uri":"/ooooo-notes/old-notes/geektime/java-core-36/04/"},{"categories":null,"content":"13、提交位移 ​ 从用户角度，分为自动提交和手动提交；从Consumer角度分为同步提交和异步提交。 org.apache.kafka.common.serialization.StringSerializer 自动提交 设置为自动提交后，调用poll方法时，会提交上次poll返回的所有消息，poll方法的逻辑是先提交上一批消息的位移，再处理下一批消息，可以保证不出现消费丢失的情况，缺点是可能出现重复消费。 Properties props = new Properties(); props.put(\"bootstrap.servers\",\"localhost:9092\"); props.put(\"group.id\",\"test\"); props.put(\"enable.auto.commit\",\"true\"); props.put(\"auto.commit.interval.ms\",\"2000\"); props.put(\"key.deserializer\",\"org.apache.kafka.common.serialization.String*Serializer\"); props.put(\"value.deserializer\",\"org.apache.kafka.common.serialization.String*Serializer\"); KafkaConsumer\u003cString,String\u003e consumer = new KafkaConsumer\u003c\u003e(props); consumer.subscribe(Arrays.asList(\"foo\",\"bar\")); while(true){ ConsumerRecords\u003cString,String\u003e records = consumer.poll(100); for(ConsumerRecord\u003cString,String\u003e record : records){ System.out.printf(\"offset = %d, key = %s, value = %s%n\", record.offset(), record.key(), resord.value()); } } 同步提交commitSync() 手动提交在调用commitSync()时，Consumer会处于阻塞状态，直到Broker端返回结果，影响整个程序的TPS。 while(true){ ConsumerRecords\u003cString,String\u003e records = consumer.poll(Duration.ofSeconds(1)); process(records); //处理消息 try{ consumer.commitSync(); }catch (CommitFailedException e){ handle(e); } } 异步提交commitAsync() 在调用commitAsync()时，会立即返回结果，不会阻塞；缺点是出现问题时不能自动重试。 while(true){ ConsumerRecords\u003cString,String\u003e records = consumer.poll(Duration.ofSeconds(1)); process(records); //处理消息 consumer.commitAsync((offsets,exception) -\u003e { if(exception != null) handle(exception); }); } 同步+异步提交 手动提交中，commitSync()和commitAsync()结合使用会有很好的效果，利用commitSync()的自动重试避免瞬时错误，利用commitAsync()不会阻塞。 try { while (true) { ConsumerRecords\u003cString, String\u003e records = consumer.poll(Duration.ofSeconds(1)); process(records); // 处理消息 commitAysnc(); // 使用异步提交规避阻塞 } } catch (Exception e) { handle(e); // 处理异常 } finally { try { consumer.commitSync(); // 最后一次提交使用同步阻塞式提交 } finally { consumer.close(); } } 同步/异步的细粒度提交 通常poll的数据全部处理完后再提交位移，如果poll的总数很大，而处理过程中出现差错了，下一次会重复消费，就需要设置细粒度的提交位移。 private Map\u003cTopicPartition, OffsetAndMetadata\u003e offsets = new HashMap\u003c\u003e(); int count = 0; …… while (true) { ConsumerRecords\u003cString, String\u003e records = consumer.poll(Duration.ofSeconds(1)); for (ConsumerRecord\u003cString, String\u003e record: records) { process(record); // 处理消息 offsets.put(new TopicPartition(record.topic(), record.partition()), new OffsetAndMetadata(record.offset() + 1)); if（count % 100 == 0） consumer.commitAsync(offsets, null); // 回调处理逻辑是 count++; } } ","date":"0001-01-01 00:00:00","objectID":"/ooooo-notes/old-notes/geektime/java-core-36/04/:13:0","tags":null,"title":"","uri":"/ooooo-notes/old-notes/geektime/java-core-36/04/"},{"categories":null,"content":"14、CommitFailedException ​ CommitFailedException是指Consumer客户端在提交位移时出现了错误或异常，而且不可恢复。 ","date":"0001-01-01 00:00:00","objectID":"/ooooo-notes/old-notes/geektime/java-core-36/04/:14:0","tags":null,"title":"","uri":"/ooooo-notes/old-notes/geektime/java-core-36/04/"},{"categories":null,"content":"1、场景一 ​ 当消息处理的总时间超过预设的 max.poll.interval.ms 参数值时，Kafka Consumer 端会抛出 CommitFailedException 异常。 解决方法如代码： … Properties props = new Properties(); … props.put(\"max.poll.interval.ms\", 5000); consumer.subscribe(Arrays.asList(\"test-topic\")); while (true) { ConsumerRecords\u003cString, String\u003e records = consumer.poll(Duration.ofSeconds(1)); // 使用 Thread.sleep 模拟真实的消息处理逻辑 Thread.sleep(6000L); consumer.commitSync(); } 防止出现该异常的办法： 缩短单条消息处理时间； 增加Comsumer端允许下游系统消费一批消息的最大时长（设置max.poll.interval.ms的值，在0.10.1.0版本后才有该参数）； 减少下游系统一次性消费的消息总数（设置max.poll.records的值）； 下游系统使用多线程加速消费； ","date":"0001-01-01 00:00:00","objectID":"/ooooo-notes/old-notes/geektime/java-core-36/04/:14:1","tags":null,"title":"","uri":"/ooooo-notes/old-notes/geektime/java-core-36/04/"},{"categories":null,"content":"2、场景二 ​ Standalone Consumer在消费时也需要指定groud.id，如果出现了一个相同group.id的Consumer Group，kafka也会抛出异常；这种情况很少见，目前没有解决办法，要尽量去避免。 ","date":"0001-01-01 00:00:00","objectID":"/ooooo-notes/old-notes/geektime/java-core-36/04/:14:2","tags":null,"title":"","uri":"/ooooo-notes/old-notes/geektime/java-core-36/04/"},{"categories":null,"content":"15、多线程 ​ kafka 0.10.1.0版本开始，KafkaConsumer就是双线程设计，即用户主线程和心跳线程；用户主线程就是启动Consumer应用程序main方法的那个程序，心跳线程只负责定期给对应的的Broker机器发送心跳请求，以标识消费者应用的存活性。所以在消费层面上，Consumer依然是单线程设计。 多线程方案： 方案一： ​ 消费者程序启动多个线程，每个线程维护专属的KafkaConsumer实例，负责完整的消息获取和消息处理流程； 优点： 实现起来简单； 线程之间没有交互，省去保障线程安全的开销； 由于每个线程使用专属的Consumer实例执行消息获取和消息处理，可以保证分区内的消费顺序； 缺点： 由于每个线程要维护自己的Consumer实例，会占用很多系统资源； 线程数受限于Consumer订阅主题的总分区数； 如有某个线程处理较慢，会造成rebalance； 实现代码如下： public class KafkaConsumerRunner implements Runnable { private final AtomicBoolean closed = new AtomicBoolean(false); private final KafkaConsumer consumer; public void run() { try { consumer.subscribe(Arrays.asList(\"topic\")); while (!closed.get()) { ConsumerRecords records = consumer.poll(Duration.ofMillis(10000)); // 执行消息处理逻辑 ... } } catch (WakeupException e) { // Ignore exception if closing if (!closed.get()) throw e; } finally { consumer.close(); } } // Shutdown hook which can be called from a separate thread public void shutdown() { closed.set(true); consumer.wakeup(); } } 方案二： ​ 消费者程序使用单或多线程获取消息，同时创建多个消费线程执行消息处理逻辑； 优点： 高伸缩性，自由调节消息获取和消息处理的线程数； 缺点： 实现难度大； 由于消息获取和消息处理解耦，会破坏消息在分区中的顺序； 会使得整个消息消费链路被拉长，位移提交可能会出错，导致重复消费； private final KafkaConsumer\u003cString, String\u003e consumer; private ExecutorService executors; ... private int workerNum = ...; executors = new ThreadPoolExecutor( workerNum, workerNum, 0L, TimeUnit.MILLISECONDS, new ArrayBlockingQueue\u003c\u003e(1000), new ThreadPoolExecutor.CallerRunsPolicy() ); ... while (true) { ConsumerRecords\u003cString, String\u003e records = consumer.poll(Duration.ofSeconds(1)); for (final ConsumerRecord record : records) { executors.submit(new Worker(record)); } } .. ","date":"0001-01-01 00:00:00","objectID":"/ooooo-notes/old-notes/geektime/java-core-36/04/:15:0","tags":null,"title":"","uri":"/ooooo-notes/old-notes/geektime/java-core-36/04/"},{"categories":null,"content":"Java 核心技术 36讲 01、谈谈你对 Java 的理解 02、Exception 和 Error 的区别 03、final、finally、finalize 的区别 ","date":"0001-01-01 00:00:00","objectID":"/ooooo-notes/old-notes/geektime/java-core-36/readme/:1:0","tags":null,"title":"","uri":"/ooooo-notes/old-notes/geektime/java-core-36/readme/"},{"categories":null,"content":"1、Kafka 是一款消息引擎系统 消息引擎系统是一组规范。企业利用这组规范在不同系统之间传递语义准确的消息，实现松耦合的异步式数据传递。 Kafka 的消息编码格式是 二进制的字节序列 Kafka 支持的两种消息模型 点对点模型，系统 A 发送的消息只能被 B 系统消费，其他系统不能读取 A 系统发送的消息，一对一的关系。 发布 / 订阅模型，有 Topic（主题）、Producer（生产者）、Consumer（消费者）的概念，可能会存在多个 Producer 向 Topic 发送消息，也可能存在多个 Consumer 来消费消息，多对多的关系。 消息队列的优点：解耦、异步、削峰 ","date":"0001-01-01 00:00:00","objectID":"/ooooo-notes/old-notes/geektime/kafka-core-tech/01/:1:0","tags":null,"title":"","uri":"/ooooo-notes/old-notes/geektime/kafka-core-tech/01/"},{"categories":null,"content":"1、Kafka 术语 ","date":"0001-01-01 00:00:00","objectID":"/ooooo-notes/old-notes/geektime/kafka-core-tech/02/:1:0","tags":null,"title":"","uri":"/ooooo-notes/old-notes/geektime/kafka-core-tech/02/"},{"categories":null,"content":"Record（消息） Kafka 处理的主要对象。 ","date":"0001-01-01 00:00:00","objectID":"/ooooo-notes/old-notes/geektime/kafka-core-tech/02/:1:1","tags":null,"title":"","uri":"/ooooo-notes/old-notes/geektime/kafka-core-tech/02/"},{"categories":null,"content":"Topic（主题） 主题是承载消息的逻辑容器，在实际使用中多用来区分具体的业务。 你可以为每个业务、每个应用甚至是每类数据都创建专属的主题。 ","date":"0001-01-01 00:00:00","objectID":"/ooooo-notes/old-notes/geektime/kafka-core-tech/02/:1:2","tags":null,"title":"","uri":"/ooooo-notes/old-notes/geektime/kafka-core-tech/02/"},{"categories":null,"content":"Partition（分区） 一个有序不变的消息序列。每个主题下可以有多个分区。 ","date":"0001-01-01 00:00:00","objectID":"/ooooo-notes/old-notes/geektime/kafka-core-tech/02/:1:3","tags":null,"title":"","uri":"/ooooo-notes/old-notes/geektime/kafka-core-tech/02/"},{"categories":null,"content":"Offset（消息位移） 分区中每条消息的位置信息，是一个单调递增且不变的值。 ","date":"0001-01-01 00:00:00","objectID":"/ooooo-notes/old-notes/geektime/kafka-core-tech/02/:1:4","tags":null,"title":"","uri":"/ooooo-notes/old-notes/geektime/kafka-core-tech/02/"},{"categories":null,"content":"Producer（生产者） 向主题发布新消息的应用程序。 ","date":"0001-01-01 00:00:00","objectID":"/ooooo-notes/old-notes/geektime/kafka-core-tech/02/:1:5","tags":null,"title":"","uri":"/ooooo-notes/old-notes/geektime/kafka-core-tech/02/"},{"categories":null,"content":"Consumer（消费者） 从主题订阅新消息的应用程序。 ","date":"0001-01-01 00:00:00","objectID":"/ooooo-notes/old-notes/geektime/kafka-core-tech/02/:1:6","tags":null,"title":"","uri":"/ooooo-notes/old-notes/geektime/kafka-core-tech/02/"},{"categories":null,"content":"Consumer Offset（消费者位移） 消费者消费进度，每个消费者都有自己的消费者位移。 ","date":"0001-01-01 00:00:00","objectID":"/ooooo-notes/old-notes/geektime/kafka-core-tech/02/:1:7","tags":null,"title":"","uri":"/ooooo-notes/old-notes/geektime/kafka-core-tech/02/"},{"categories":null,"content":"Consumer Group（消费者组） 多个消费者实例共同组成的一个组，同时消费多个分区以实现高吞吐。 ","date":"0001-01-01 00:00:00","objectID":"/ooooo-notes/old-notes/geektime/kafka-core-tech/02/:1:8","tags":null,"title":"","uri":"/ooooo-notes/old-notes/geektime/kafka-core-tech/02/"},{"categories":null,"content":"Rebalance （重平衡） 消费者组内某个消费者实例挂掉后，其他消费者实例自动重新分配订阅主题分区的过程。 Rebalance 是 Kafka 消费者端实现高可用的重要手段。 ","date":"0001-01-01 00:00:00","objectID":"/ooooo-notes/old-notes/geektime/kafka-core-tech/02/:1:9","tags":null,"title":"","uri":"/ooooo-notes/old-notes/geektime/kafka-core-tech/02/"},{"categories":null,"content":"2、Kafka 三层消息架构 第一层是主题层，每个主题可以配置 M 个分区，而每个分区又可以配置 N 个副本。 第二层是分区层，每个分区的 N 个副本中只能有一个充当领导者角色，对外提供服务；其他 N-1 个副本是追随者副本，只是提供数据冗余之用。 第三层是消息层，分区中包含若干条消息，每条消息的位移从 0 开始，依次递增。 最后，客户端程序只能与分区的领导者副本进行交互。 ","date":"0001-01-01 00:00:00","objectID":"/ooooo-notes/old-notes/geektime/kafka-core-tech/02/:2:0","tags":null,"title":"","uri":"/ooooo-notes/old-notes/geektime/kafka-core-tech/02/"},{"categories":null,"content":"3、Kafka 持久化数据 Kafka 使用消息日志（Log）来保存数据，消息日志只能追加写入，所以避免了随机 I/O 操作，改为性能较好的顺序 I/O 写操作。 在 Kafka 底层，一个日志会细分成多个日志段，消息被追加写到当前最新的日志段中，当写满了一个日志段后，Kafka 会自动切分出一个新的日志段，并将老的日志段封存起来。Kafka 在后台还有定时任务会定期地检查老的日志段是否能够被删除，从而实现回收磁盘空间的目的。 ","date":"0001-01-01 00:00:00","objectID":"/ooooo-notes/old-notes/geektime/kafka-core-tech/02/:3:0","tags":null,"title":"","uri":"/ooooo-notes/old-notes/geektime/kafka-core-tech/02/"},{"categories":null,"content":"1、Kafka 是分布式流处理平台 Apache Kafka 是消息引擎系统，也是一个分布式流处理平台。 Kafka 的特性： 提供一套 API 实现生产者和消费者。 降低网络传输和磁盘存储开销。 实现高伸缩性架构。 可以实现端到端的精确一次处理语义。 ","date":"0001-01-01 00:00:00","objectID":"/ooooo-notes/old-notes/geektime/kafka-core-tech/03/:1:0","tags":null,"title":"","uri":"/ooooo-notes/old-notes/geektime/kafka-core-tech/03/"},{"categories":null,"content":"1、Kafka 不同的\"发行版\" ","date":"0001-01-01 00:00:00","objectID":"/ooooo-notes/old-notes/geektime/kafka-core-tech/04/:1:0","tags":null,"title":"","uri":"/ooooo-notes/old-notes/geektime/kafka-core-tech/04/"},{"categories":null,"content":"1、Apache Kafka 只提供最基础的组件，没有任何监控框架或工具。 如果你仅仅需要一个消息引擎系统亦或是简单的流处理应用场景，同时需要对系统有较大把控度，那么我推荐你使用 Apache Kafka。 优势在于迭代速度快，社区响应度高，使用它可以让你有更高的把控度；缺陷在于仅提供基础核心组件，缺失一些高级的特性。 ","date":"0001-01-01 00:00:00","objectID":"/ooooo-notes/old-notes/geektime/kafka-core-tech/04/:1:1","tags":null,"title":"","uri":"/ooooo-notes/old-notes/geektime/kafka-core-tech/04/"},{"categories":null,"content":"2、Confluent Kafka 分为免费版和企业版。 免费版包含 Schema 注册中心和 REST proxy 两大功能。前者是帮助你集中管理 Kafka 消息格式以实现数据前向 / 后向兼容；后者用开放 HTTP 接口的方式允许你通过网络访问 Kafka 的各种功能，这两个都是 Apache Kafka 所没有的。 企业版包含跨数据中心备份和集群监控两大功能。 优势在于集成了很多高级特性且由 Kafka 原班人马打造，质量上有保证；缺陷在于相关文档资料不全，普及率较低，没有太多可供参考的范例。 ","date":"0001-01-01 00:00:00","objectID":"/ooooo-notes/old-notes/geektime/kafka-core-tech/04/:1:2","tags":null,"title":"","uri":"/ooooo-notes/old-notes/geektime/kafka-core-tech/04/"},{"categories":null,"content":"3、Cloudera/Hortonworks Kafka 大数据云公司发布的 Kafka（CDH/HDP Kafka）。这些大数据平台天然集成了 Apache Kafka，通过便捷化的界面操作将 Kafka 的安装、运维、管理、监控全部统一在控制台中。 如果你需要快速地搭建消息引擎系统，或者你需要搭建的是多框架构成的数据平台且 Kafka 只是其中一个组件，那么我推荐你使用这些大数据云公司提供的 Kafka。 优势在于操作简单，节省运维成本；缺陷在于把控度低，演进速度较慢。 ","date":"0001-01-01 00:00:00","objectID":"/ooooo-notes/old-notes/geektime/kafka-core-tech/04/:1:3","tags":null,"title":"","uri":"/ooooo-notes/old-notes/geektime/kafka-core-tech/04/"},{"categories":null,"content":"1、Kafka 的版本 [scala-version] - [kafka-version].tar.gz 0.10.0.0 引进 Kafka Stream 0.11.0.0 添加幂等性 Producer 和事务 API，并对 Kafka 消息格式进行重构 总结： 如果只使用 Kafka 的消息引擎功能， 最少 0.10.2.2， 可以使用新版的 Consumer API。 如果使用 Kafka Stream，最少 2.0.0。 ","date":"0001-01-01 00:00:00","objectID":"/ooooo-notes/old-notes/geektime/kafka-core-tech/05/:1:0","tags":null,"title":"","uri":"/ooooo-notes/old-notes/geektime/kafka-core-tech/05/"},{"categories":null,"content":"1、操作系统 三个方面选择： I/O 模型的使用 数据网络传输效率 社区支持度 五种 I/O 模型：阻塞式 I/O、非阻塞式 I/O、I/O 多路复用、信号驱动 I/O 和异步 I/O。 Linux 中的系统调用 select 函数就属于 I/O 多路复用模型；epoll 系统调用则介于第三种和第四种模型之间。 Linux 还可以使用 zero copy。 总结； 高性能只能选择 linux。 ","date":"0001-01-01 00:00:00","objectID":"/ooooo-notes/old-notes/geektime/kafka-core-tech/06/:1:0","tags":null,"title":"","uri":"/ooooo-notes/old-notes/geektime/kafka-core-tech/06/"},{"categories":null,"content":"2、磁盘 总结： 追求性价比的公司可以不搭建 RAID。 使用机械磁盘完全能够胜任 Kafka 线上环境。 ","date":"0001-01-01 00:00:00","objectID":"/ooooo-notes/old-notes/geektime/kafka-core-tech/06/:2:0","tags":null,"title":"","uri":"/ooooo-notes/old-notes/geektime/kafka-core-tech/06/"},{"categories":null,"content":"3、磁盘容量 规划磁盘容量的思考因素： 新增消息数 消息留存时间 平均消息大小 备份数 是否启用压缩 例如： 每一天 1 亿条 1KB 大小的消息，保存两份且存留两周的时间。 一天数据大小： 1 亿 * 1 KB * 2 份 / 1000 / 1000 = 200 GB 预留 10% 的磁盘空间： 200 GB * 1.1 = 220 GB 两周时间： 220 GB * 14 = 3 TB 开启了压缩比，比如 0.75： 3 TB * 0.75 = 2.25 TB ","date":"0001-01-01 00:00:00","objectID":"/ooooo-notes/old-notes/geektime/kafka-core-tech/06/:3:0","tags":null,"title":"","uri":"/ooooo-notes/old-notes/geektime/kafka-core-tech/06/"},{"categories":null,"content":"4、带宽 以 1Gbps 的千兆网络为例， 在 1 小时内处理 1TB 的业务数据，需要机器数？ 带宽是 1Gbps，即每秒处理 1Gb 的数据，通常假设 Kafka 只能用到 70% 的带宽资源，毕竟其他进程也需要一些资源，也就是Kafka 最大只能使用 700MB 的带宽资源，常规性使用要预留 2/3，所以单台 Kafka 使用的带宽为 700 Mb / 3 = 240 Mbps。 1 个小时处理 1TB 数据， 1024 * 1024 / 3600 / (240 / 8) = 9.7 (注意 240 Mbps 是带宽，变为 MB 单位时，要除以 8)。 如果消息的副本数为 3，大约就是 30 台机器。 ","date":"0001-01-01 00:00:00","objectID":"/ooooo-notes/old-notes/geektime/kafka-core-tech/06/:4:0","tags":null,"title":"","uri":"/ooooo-notes/old-notes/geektime/kafka-core-tech/06/"},{"categories":null,"content":"1、Broker 端参数 ","date":"0001-01-01 00:00:00","objectID":"/ooooo-notes/old-notes/geektime/kafka-core-tech/07/:1:0","tags":null,"title":"","uri":"/ooooo-notes/old-notes/geektime/kafka-core-tech/07/"},{"categories":null,"content":"1、broker 存储信息 log.dirs: 日志目录，例如 /home/kafka1,/home/kafka2,/home/kafka3。 log.dir: 只需要设置参数log.dirs，此参数不需要。 ","date":"0001-01-01 00:00:00","objectID":"/ooooo-notes/old-notes/geektime/kafka-core-tech/07/:1:1","tags":null,"title":"","uri":"/ooooo-notes/old-notes/geektime/kafka-core-tech/07/"},{"categories":null,"content":"2、zk 信息 zookeeper.connect: 连接 zk 的参数，例如 zk1:2181,zk2:2181,zk3:2181。 如果让多个 Kafka 集群使用同一套 zk 集群，利用 zk 的 chroot 设置，例如 zookeeper.connect 可以设置为 zk1:2181,zk2:2181,zk3:2181/kafka1 和 zk1:2181,zk2:2181,zk3:2181/kafka2。 ","date":"0001-01-01 00:00:00","objectID":"/ooooo-notes/old-notes/geektime/kafka-core-tech/07/:1:2","tags":null,"title":"","uri":"/ooooo-notes/old-notes/geektime/kafka-core-tech/07/"},{"categories":null,"content":"3、broker 连接信息 listeners: 监听器，也就是通过什么协议访问指定主机名和端口开放的 Kafka 服务。 advertised.listeners: Broker 用于对外发布的监听器。 监听器配置，由三元组 \u003c协议名称，主机名，端口号\u003e 构成，例如你自己定义的协议名字 CONTROLLER://localhost:9092。 一旦你自己定义了协议名称，你必须还要指定 listener.security.protocol.map 参数告诉这个协议底层使用了哪种安全协议，比如指定 listener.security.protocol.map=CONTROLLER:PLAINTEXT 表示 CONTROLLER 这个自定义协议底层使用明文不加密传输数据。 host.name/port: 过期。 ","date":"0001-01-01 00:00:00","objectID":"/ooooo-notes/old-notes/geektime/kafka-core-tech/07/:1:3","tags":null,"title":"","uri":"/ooooo-notes/old-notes/geektime/kafka-core-tech/07/"},{"categories":null,"content":"4、topic 管理 auto.create.topics.enable: 是否允许自动创建 Topic, 建议为 false。 unclean.leader.election.enable： 是否允许 Unclean Leader 选举， 建议为 false。 Unclean Leader 选举，指的是落后太多的副本参与选举，可能会使数据丢失。 auto.leader.rebalance.enable: 是否允许定期进行 Leader 选举，建议为 false。 ","date":"0001-01-01 00:00:00","objectID":"/ooooo-notes/old-notes/geektime/kafka-core-tech/07/:1:4","tags":null,"title":"","uri":"/ooooo-notes/old-notes/geektime/kafka-core-tech/07/"},{"categories":null,"content":"5、数据留存 log.retention.{hour|minutes|ms}: 日志保留时间。例如 log.retention.hour=168 表示默认保存 7 天的数据。 log.retention.bytes: 消息保存的总磁盘容量大小。默认为 -1，表示容量无限制，在云上的多租户才用到此参数。 message.max.bytes: 最大消息大小。实际上，1MB 的消息很常见。 注意：上述的参数都不能使用默认值。 ","date":"0001-01-01 00:00:00","objectID":"/ooooo-notes/old-notes/geektime/kafka-core-tech/07/:1:5","tags":null,"title":"","uri":"/ooooo-notes/old-notes/geektime/kafka-core-tech/07/"},{"categories":null,"content":"1、Topic 级别参数 如果同时设置了 Topic 级别参数和全局 Broker 参数，Topic 级别参数会覆盖全局 Broker 参数的值，而每个 Topic 都能设置自己的参数值，这就是所谓的 Topic 级别参数。 retention.ms: 该 Topic 消息被保存的时长，会覆盖掉 Broker 端的全局参数值。 retention.bytes: 该 Topic 预留多大的磁盘空间，当前默认值是 -1，表示可以无限使用磁盘空间，在多租户的 Kafka 集群中用到。 max.message.bytes: Topic 的最大消息大小。 Topic 设置方式（🎉Kafka官方文档🎉）： 创建 Topic 时设置 –config 后面指定了想要设置的 Topic 级别参数。 bin/kafka-topics.sh --bootstrap-server localhost:9092 --create --topic my-topic --partitions 1 --replication-factor 1 --config retention.ms=15552000000 --config max.message.bytes=5242880 修改 Topic 时设置 bin/kafka-configs.sh --zookeeper localhost:2181 --entity-type topics --entity-name my-topic --alter --add-config max.message.bytes=10485760 ","date":"0001-01-01 00:00:00","objectID":"/ooooo-notes/old-notes/geektime/kafka-core-tech/08/:1:0","tags":null,"title":"","uri":"/ooooo-notes/old-notes/geektime/kafka-core-tech/08/"},{"categories":null,"content":"2、JVM 参数 Java7 中，如果 Broker 所在机器的 CPU 资源非常充裕，则建议开启 CMS 垃圾回收器, -XX:+UseCurrentMarkSweepGC。否则，使用吞吐量收集器。开启方法是指定 -XX:+UseParallelGC。 Java8 中，建议使用 G1 垃圾回收器。 建议使用 Java8。 KAFKA_HEAP_OPTS: 堆大小, 建议为 6GB，这是比较公认的合理值。 KAFKA_JVM_PERFORMANCE_OPTS: 指定 GC 参数。 比如你可以这样启动 Kafka： export KAFKA_HEAP_OPTS=--Xms6g --Xmx6g export KAFKA_JVM_PERFORMANCE_OPTS= -server -XX:+UseG1GC -XX:MaxGCPauseMillis=20 -XX:InitiatingHeapOccupancyPercent=35 -XX:+ExplicitGCInvokesConcurrent -Djava.awt.headless=true bin/kafka-server-start.sh config/server.properties ","date":"0001-01-01 00:00:00","objectID":"/ooooo-notes/old-notes/geektime/kafka-core-tech/08/:2:0","tags":null,"title":"","uri":"/ooooo-notes/old-notes/geektime/kafka-core-tech/08/"},{"categories":null,"content":"3、操作系统参数 主要系统参数： 文件描述符限制 执行命令 ulimit -n 1000000 来设置。 文件系统类型 XFS 的性能要强于 ext4。 Swappiness 将 swap 交换内存配置成一个接近 0 但不为 0 的值，比如 1。 提交时间 向 Kafka 发送数据并不是真要等数据被写入磁盘才会认为成功，而是只要数据被写入到操作系统的页缓存（Page Cache）上就可以了，随后操作系统根据 LRU 算法会定期将页缓存上的\"脏\"数据落盘到物理磁盘上。这个定期就是由提交时间来确定的，默认是 5 秒。由于 Kafka 的多副本的冗余机制，可以稍微拉大提交间隔来提高性能。 ","date":"0001-01-01 00:00:00","objectID":"/ooooo-notes/old-notes/geektime/kafka-core-tech/08/:2:1","tags":null,"title":"","uri":"/ooooo-notes/old-notes/geektime/kafka-core-tech/08/"},{"categories":null,"content":"1、为什么分区 Kafka 的消息组织方式实际上是三级结构：主题 - 分区 - 消息，主题下的每条消息只会保存在某一个分区中。 Kafka 的三级结构： 分区是为了实现系统的高伸缩性（Scalability），可以通过添加新的节点来增加整体系统的吞吐量。 ","date":"0001-01-01 00:00:00","objectID":"/ooooo-notes/old-notes/geektime/kafka-core-tech/09/:1:0","tags":null,"title":"","uri":"/ooooo-notes/old-notes/geektime/kafka-core-tech/09/"},{"categories":null,"content":"2、分区策略 所谓分区策略是决定生产者将消息发送到哪个分区的算法。 ","date":"0001-01-01 00:00:00","objectID":"/ooooo-notes/old-notes/geektime/kafka-core-tech/09/:2:0","tags":null,"title":"","uri":"/ooooo-notes/old-notes/geektime/kafka-core-tech/09/"},{"categories":null,"content":"1、自定义分区策略 你需要显式地配置生产者端的参数 partitioner.class，在编写生产者程序时，你可以编写一个具体的类实现 org.apache.kafka.clients.producer.Partitioner 接口，实现 partition() 和 close() 接口。 方法签名： int partition(String topic, Object key, byte[] keyBytes, Object value, byte[] valueBytes, Cluster cluster); ","date":"0001-01-01 00:00:00","objectID":"/ooooo-notes/old-notes/geektime/kafka-core-tech/09/:2:1","tags":null,"title":"","uri":"/ooooo-notes/old-notes/geektime/kafka-core-tech/09/"},{"categories":null,"content":"2、轮询策略 Kafka 默认的分区策略就是轮询策略，轮询策略有非常优秀的负载均衡表现，它总是能保证消息最大限度地被平均分配到所有分区上。 ","date":"0001-01-01 00:00:00","objectID":"/ooooo-notes/old-notes/geektime/kafka-core-tech/09/:2:2","tags":null,"title":"","uri":"/ooooo-notes/old-notes/geektime/kafka-core-tech/09/"},{"categories":null,"content":"3、随机策略 要实现随机策略版的 partition 方法(自定义分区策略)，如下： List\u003cPartitionInfo\u003e partitions = cluster.partitionsForTopic(topic); return ThreadLocalRandom.current().nextInt(partitions.size()); ","date":"0001-01-01 00:00:00","objectID":"/ooooo-notes/old-notes/geektime/kafka-core-tech/09/:2:3","tags":null,"title":"","uri":"/ooooo-notes/old-notes/geektime/kafka-core-tech/09/"},{"categories":null,"content":"4、按消息键保序策略 实现这个策略的 partition 方法(自定义分区策略)，如下： List\u003cPartitionInfo\u003e partitions = cluster.partitionsForTopic(topic); return Math.abs(key.hashCode()) % partitions.size(); Kafka 默认分区策略实际上同时实现了两种策略：如果指定了 Key，那么默认实现按消息键保序策略；如果没有指定 Key，则使用轮询策略。 ","date":"0001-01-01 00:00:00","objectID":"/ooooo-notes/old-notes/geektime/kafka-core-tech/09/:2:4","tags":null,"title":"","uri":"/ooooo-notes/old-notes/geektime/kafka-core-tech/09/"},{"categories":null,"content":"5、其他分区策略 比较常见的一种就是基于地理位置的分区策略。 比如根据 Broker 所在的 IP 地址实现定制化的分区策略，如下： List\u003cPartitionInfo\u003e partitions = cluster.partitionsForTopic(topic); return partitions.stream().filter(p -\u003e isSouth(p.leader().host())).map(PartitionInfo::partition).findAny().get(); 我们可以从所有分区中找出那些 Leader 副本在南方的所有分区，然后随机挑选一个进行消息发送。 ","date":"0001-01-01 00:00:00","objectID":"/ooooo-notes/old-notes/geektime/kafka-core-tech/09/:2:5","tags":null,"title":"","uri":"/ooooo-notes/old-notes/geektime/kafka-core-tech/09/"},{"categories":null,"content":"1、消息格式 Kafka 的消息格式目前有两种：V1 和 V2。 不论是哪个版本，Kafka 的消息层次都分为两层：消息集合(message set) 和 消息(message)。一个消息集合包含若干条日志项(record item)，日志项中包含多条消息。 在 V1 版本中，每条消息都要执行 CRC 校验，但有些情况下消息的 CRC 值是会发生变化的。比如在 Broker 端可能会对消息时间戳字段进行更新，或者在执行消息格式转换（兼容老版本客户端程序）。对于这些情况，每条消息都执行 CRC 校验就有点没必要了。在 V2 版本中，消息的 CRC 校验工作就被移到了消息集合这一层。 在 V1 版本中，保存压缩消息的方法是把多条消息进行压缩然后保存到外层消息的消息体字段中；而在 V2 版本中，是对整个消息集合进行压缩。显然后者应该比前者有更好的压缩效果。 在相同条件下，不论是否启用压缩，V2 版本都比 V1 版本节省磁盘空间。当启用压缩时，这种节省空间的效果更加明显。 ","date":"0001-01-01 00:00:00","objectID":"/ooooo-notes/old-notes/geektime/kafka-core-tech/10/:1:0","tags":null,"title":"","uri":"/ooooo-notes/old-notes/geektime/kafka-core-tech/10/"},{"categories":null,"content":"2、何时压缩 在 Kafka 中，压缩可能发生在两个地方：生产者端和 Broker 端。 Properties props = new Properties(); props.put(\"bootstrap.servers\", \"localhost:9092\"); props.put(\"compression.type\", \"gzip\"); Producer\u003cString, String\u003e producer = new KafkaProducer\u003c\u003e(props); 上述代码，表明该 Producer 的压缩算法使用的是 GZIP。 但有两种例外情况让 Broker 重新压缩消息： Broker 端指定了和 Producer 端不同的压缩算法。 Broker 端发生了消息格式转换（兼容老的客户端程序）。 ","date":"0001-01-01 00:00:00","objectID":"/ooooo-notes/old-notes/geektime/kafka-core-tech/10/:2:0","tags":null,"title":"","uri":"/ooooo-notes/old-notes/geektime/kafka-core-tech/10/"},{"categories":null,"content":"3、何时解压缩 一句话：Producer 端压缩、Broker 端保持、Consumer 端解压缩。 除了在 Consumer 端解压缩，Broker 端也会进行解压缩，目的是为了对消息执行各种验证。 ","date":"0001-01-01 00:00:00","objectID":"/ooooo-notes/old-notes/geektime/kafka-core-tech/10/:3:0","tags":null,"title":"","uri":"/ooooo-notes/old-notes/geektime/kafka-core-tech/10/"},{"categories":null,"content":"4、压缩算法对比 在 Kafka 2.1.0 版本之前，Kafka 支持 3 种压缩算法：GZIP、Snappy 和 LZ4。从 2.1.0 开始，Kafka 正式支持 Zstandard 算法（zstd）。 压缩算法有两个重要指标：压缩比、压缩/解压缩吞吐量。 吞吐量方面: LZ4 \u003e Snappy \u003e zstd 和 GZIP。 压缩比方面：zstd \u003e LZ4 \u003e GZIP \u003e Snappy 总结： 机器的 CPU 资源不充足，不建议开启压缩，因为压缩需要消耗大量 CPU。 机器的 CPU 资源充足，强烈建议你开启 zstd 压缩，这样能极大地节省网络资源消耗。 ","date":"0001-01-01 00:00:00","objectID":"/ooooo-notes/old-notes/geektime/kafka-core-tech/10/:4:0","tags":null,"title":"","uri":"/ooooo-notes/old-notes/geektime/kafka-core-tech/10/"},{"categories":null,"content":"Kafka 核心技术与实战 01、消息引擎系统 02、Kafka 术语 ","date":"0001-01-01 00:00:00","objectID":"/ooooo-notes/old-notes/geektime/kafka-core-tech/readme/:1:0","tags":null,"title":"","uri":"/ooooo-notes/old-notes/geektime/kafka-core-tech/readme/"},{"categories":null,"content":"1、MySQL 的基本架构 主要分为Server层和存储引擎层。 Server层包括连接器、查询缓存、分析器、优化器、执行器等，涵盖MySQL的大多数核心服务功能，以及所有的内置函数（如日期、时间、数学和加密函数等），所有跨存储引擎的功能都在这一层实现，比如存储过程、触发器、视图等。 存储引擎层负责数据的存储和提取。其架构模式是插件式的，支持 InnoDB 、 MyISAM 、 Memory 等多个存储引擎。现在最常用的存储引擎是 InnoDB ，它从 MySQL 5.5.5版本开始成为了默认存储引擎。 ","date":"0001-01-01 00:00:00","objectID":"/ooooo-notes/old-notes/geektime/mysql-45/01/:1:0","tags":null,"title":"","uri":"/ooooo-notes/old-notes/geektime/mysql-45/01/"},{"categories":null,"content":"2、查询语句的执行流程 ","date":"0001-01-01 00:00:00","objectID":"/ooooo-notes/old-notes/geektime/mysql-45/01/:2:0","tags":null,"title":"","uri":"/ooooo-notes/old-notes/geektime/mysql-45/01/"},{"categories":null,"content":"1、连接器 连接器负责跟客户端建立连接、获取权限、维持和管理连接。 连接命令： mysql -h$ip -P$port -u$user -p 如果用户名密码认证通过，连接器会到权限表里面查出你拥有的权限。 连接完成后，如果你没有后续的动作，这个连接就处于空闲状态（Sleep），通过 show processlist 命令查看如下： 客户端长时间处于 Sleep 状态，连接器就会自动将它断开。由参数 wait_timeout 控制的，默认值是8小时。 数据库里面，长连接是指连接成功后，如果客户端持续有请求，则一直使用同一个连接。短连接则是指每次执行完很少的几次查询就断开连接，下次查询再重新建立一个。建立连接的过程通常是比较复杂的，所以尽量使用长连接。 但全部使用长连接后，有些时候 MySQL 占用内存涨得特别快，这是因为 MySQL 在执行过程中临时使用的内存是管理在连接对象里面的，这些资源会在连接断开的时候才释放。所以如果长连接累积下来，可能导致内存占用太大，被系统强行杀掉（OOM），从现象看就是 MySQL 异常重启了。 长连接解决方案： 定期断开长连接. 在 MySQL 5.7 版本后，通过执行 mysql_reset_connection来重新初始化连接资源， 无需重新连接和权限认证。 ","date":"0001-01-01 00:00:00","objectID":"/ooooo-notes/old-notes/geektime/mysql-45/01/:2:1","tags":null,"title":"","uri":"/ooooo-notes/old-notes/geektime/mysql-45/01/"},{"categories":null,"content":"2、查询缓存 只要对一个表更新，这个表上所有的查询缓存都会被清空。 MySQL 8.0 缓存功能已经被删除。 ","date":"0001-01-01 00:00:00","objectID":"/ooooo-notes/old-notes/geektime/mysql-45/01/:2:2","tags":null,"title":"","uri":"/ooooo-notes/old-notes/geektime/mysql-45/01/"},{"categories":null,"content":"3、分析器 对SQL语句做解析，词法分析（select、insert、delete、update）、语法分析（语法是否正确）。 ","date":"0001-01-01 00:00:00","objectID":"/ooooo-notes/old-notes/geektime/mysql-45/01/:2:3","tags":null,"title":"","uri":"/ooooo-notes/old-notes/geektime/mysql-45/01/"},{"categories":null,"content":"4、优化器 优化器选择索引。 例如，对于下面的语句，有两种查询逻辑： mysql\u003e select * from t1 join t2 using(ID) where t1.c=10 and t2.d=20; 先从 t1 里面取出 c = 10 的记录的ID值，再根据 ID 值关联到 t2，再判断 t2 里面 d 的值是否等于 20。 先从 t2 里面取出 d = 20 的记录的ID值，再根据 ID 值关联到 t1，再判断 t1 里面 c 的值是否等于 10。 ","date":"0001-01-01 00:00:00","objectID":"/ooooo-notes/old-notes/geektime/mysql-45/01/:2:4","tags":null,"title":"","uri":"/ooooo-notes/old-notes/geektime/mysql-45/01/"},{"categories":null,"content":"5、执行器 先判断查询权限，如果有权限，执行器就会调用存储引擎提供的接口。 ","date":"0001-01-01 00:00:00","objectID":"/ooooo-notes/old-notes/geektime/mysql-45/01/:2:5","tags":null,"title":"","uri":"/ooooo-notes/old-notes/geektime/mysql-45/01/"},{"categories":null,"content":"3、问题 如果你执行 select * from T where k=1，报不存在 K 这一列。是在哪个阶段报出来的？ 分析器阶段 ","date":"0001-01-01 00:00:00","objectID":"/ooooo-notes/old-notes/geektime/mysql-45/01/:3:0","tags":null,"title":"","uri":"/ooooo-notes/old-notes/geektime/mysql-45/01/"},{"categories":null,"content":"1、更新语句的执行流程 创建表 T: mysql\u003e create table T(ID int primary key, c int); 将 ID = 2 这一行的值加 1 的 SQL 语句: mysql\u003e update T set c=c+1 where ID=2; 查询语句的那一套流程，更新语句也是同样会走一遍。 经过连接器和查询缓存之后，分析器通过词法和语法分析知道这是一条更新语句，优化器决定要使用 ID 这个索引，执行器负责具体执行，找到这一行，然后更新。 与查询流程不一样的是，更新流程还涉及两个重要的日志模块： **redo log（重做日志）**和 binlog（归档日志）。 ","date":"0001-01-01 00:00:00","objectID":"/ooooo-notes/old-notes/geektime/mysql-45/02/:1:0","tags":null,"title":"","uri":"/ooooo-notes/old-notes/geektime/mysql-45/02/"},{"categories":null,"content":"2、redo log redo log 是 InnoDB 引擎特有的日志模块。 在 MySQL 中，如果每一次的更新操作都需要写进磁盘，然后磁盘也要找到对应的那条记录，然后再更新，整个过程 IO 成本、查找成本都很高。为了提高更新效率，MySQL 使用 WAL（Write-Ahead Logging）技术，它的关键点就是先写日志，再写磁盘。 当有记录需要更新时，InnoDB 引擎就会先把记录写到 redo log 里面，并更新内存，这时更新就算完成了。同时，InnoDB引擎会在适当的时候（系统比较空闲），将这个记录更新到磁盘里面。 InnoDB的 redo log 是固定大小的，比如可以配置为一组 4 个文件，每个文件的大小是 1GB，那么总共可以记录 4GB 的操作。从头开始写，写到末尾又回到开头循环写。 write pos 是当前记录的位置，一边写一边后移。 checkpoint 是当前要擦除的位置，擦除记录前要把记录更新到数据文件。 有了 redo log，InnoDB 就可以保证即使数据库发生异常重启，之前提交的记录都不会丢失，这个能力称为 crash-safe。 ","date":"0001-01-01 00:00:00","objectID":"/ooooo-notes/old-notes/geektime/mysql-45/02/:2:0","tags":null,"title":"","uri":"/ooooo-notes/old-notes/geektime/mysql-45/02/"},{"categories":null,"content":"3、binlog binlog 是 Sever 层的日志模块，只能用于归档，比如主从复制。 两种日志不同： redo log 是InnoDB引擎特有的；binlog是MySQL的Server层实现的，所有引擎都可以使用。 redo log 是物理日志，记录的是“在某个数据页上做了什么修改”；binlog 是逻辑日志，记录的是这个语句的原始逻辑，比如“给ID=2这一行的c字段加1”。 redo log 是循环写，写到末尾又回到开头写；binlog是追加写入，写到一定大小后会切换到下一个，并不会覆盖以前的日志。 ","date":"0001-01-01 00:00:00","objectID":"/ooooo-notes/old-notes/geektime/mysql-45/02/:3:0","tags":null,"title":"","uri":"/ooooo-notes/old-notes/geektime/mysql-45/02/"},{"categories":null,"content":"4、两阶段提交 上面简单的 updata 语句的执行流程。 执行器先找引擎取 ID = 2 这一行。ID 是主键，引擎直接用树搜索找到这一行。如果ID=2 这一行所在的数据页本来就在内存中，就直接返回给执行器；否则，需要先从磁盘读入内存，然后再返回。 执行器拿到引擎给的行数据，把这个值加上1，比如原来是N，现在就是N+1，得到新的一行数据，再调用引擎接口写入这行新数据。 引擎将这行新数据更新到内存中，同时将这个更新操作记录到 redo log 里面，此时 redo log 处于 prepare 状态。然后告知执行器执行完成了，随时可以提交事务。 执行器生成这个操作的 binlog，并把 binlog 写入磁盘。 执行器调用引擎的提交事务接口，引擎把刚刚写入的 redo log 改成 commit 状态，更新完成。 如果不使用两阶段提交，会有什么问题？ 先写 redo log 后写 binlog。假设在 redo log 写完，binlog 还没有写完的时候，MySQL进程异常重启，binlog 中没有更新的数据。 先写 binlog 后写 redo log。如果在 binlog 写完之后 crash，由于 redo log 没有写，崩溃恢复以后这个事务无效，所以这一行 c 的值是 0，但 binlog 中 c = 1 了。 ","date":"0001-01-01 00:00:00","objectID":"/ooooo-notes/old-notes/geektime/mysql-45/02/:4:0","tags":null,"title":"","uri":"/ooooo-notes/old-notes/geektime/mysql-45/02/"},{"categories":null,"content":"5、配置 双1设置： innodb_flush_log_at_trx_commit = 1 表示每次事务的 redo log 都直接持久化到磁盘。这样可以保证 MySQL 异常重启之后数据不丢失。 sync_binlog = 1 表示每次事务的 binlog 都持久化到磁盘。这样可以保证 MySQL 异常重启之后 binlog 不丢失。 ","date":"0001-01-01 00:00:00","objectID":"/ooooo-notes/old-notes/geektime/mysql-45/02/:5:0","tags":null,"title":"","uri":"/ooooo-notes/old-notes/geektime/mysql-45/02/"},{"categories":null,"content":"6、问题 在什么场景下，一天一备会比一周一备更有优势呢？ 好处是“最长恢复时间”更短。 在一天一备的模式里，最坏情况下需要应用一天的 binlog。比如，你每天 0 点做一次全量备份，而要恢复出一个到昨天晚上 23 点的备份。 一周一备最坏情况就要应用一周的 binlog 了。 ","date":"0001-01-01 00:00:00","objectID":"/ooooo-notes/old-notes/geektime/mysql-45/02/:6:0","tags":null,"title":"","uri":"/ooooo-notes/old-notes/geektime/mysql-45/02/"},{"categories":null,"content":"1、事务隔离级别 事务的 ACID 中的 I 指的就是隔离性（Isolation）。 ","date":"0001-01-01 00:00:00","objectID":"/ooooo-notes/old-notes/geektime/mysql-45/03/:1:0","tags":null,"title":"","uri":"/ooooo-notes/old-notes/geektime/mysql-45/03/"},{"categories":null,"content":"1、隔离级别 读未提交（read-uncommitted）：一个事务还没提交时，它做的变更就能被别的事务看到。 读提交（read-committed）：一个事务提交之后，它做的变更才会被其他事务看到。 可重复读（repeatable-read）：一个事务执行过程中看到的数据，总是跟这个事务在启动时看到的数据是一致的。 串行化（serializable）：当出现读写锁冲突的时候，后执行事务必须等前一个事务执行完成，才能继续执行。 MySQL 的隔离级别设置为\"读提交\"。 ","date":"0001-01-01 00:00:00","objectID":"/ooooo-notes/old-notes/geektime/mysql-45/03/:1:1","tags":null,"title":"","uri":"/ooooo-notes/old-notes/geektime/mysql-45/03/"},{"categories":null,"content":"2、事务隔离例子 mysql\u003e create table T(c int) engine=InnoDB; insert into T(c) values(1); 在不同的隔离级别下的结果： 隔离级别为读未提交，v1 = 2, v2 = 2, v3 = 2。 隔离级别为读提交，v1 = 1, v2 = 2, v3 = 2。 隔离级别为可重复读，v1 = 1, v2 = 1, v3 = 2。 隔离级别为串行化，v1 = 1, v2 = 1, v3 = 2。 在实现上，数据库里面会创建一个视图，访问的时候以视图的逻辑结果为准。 在可重复读隔离级别下，这个视图是在事务启动时创建的，整个事务存在期间都用这个视图。 在读提交隔离级别下，这个视图是在每个SQL语句开始执行的时候创建的。 在读未提交隔离级别下，直接返回记录上的最新值，没有视图概念。 在串行化隔离级别下，直接用加锁的方式来避免并行访问。 ","date":"0001-01-01 00:00:00","objectID":"/ooooo-notes/old-notes/geektime/mysql-45/03/:1:2","tags":null,"title":"","uri":"/ooooo-notes/old-notes/geektime/mysql-45/03/"},{"categories":null,"content":"3、事务配置方式 通过命令 show variables like 'transaction_isolation'; 来查看当前隔离级别。通过命令 set transaction_isolation = 'read-committed'; 来设置隔离级别。 ","date":"0001-01-01 00:00:00","objectID":"/ooooo-notes/old-notes/geektime/mysql-45/03/:1:3","tags":null,"title":"","uri":"/ooooo-notes/old-notes/geektime/mysql-45/03/"},{"categories":null,"content":"2、事务隔离的实现 在 MySQL 中，实际上每条记录在更新的时候都会同时记录一条回滚操作。记录上的最新值，通过回滚操作，都可以得到前一个状态的值。 假设一个值从 1 被按顺序改成了 2、3、4，在回滚日志里面就会有类似下面的记录。 长事务意味着系统里面会存在很老的事务视图。由于这些事务随时可能访问数据库里面的任何数据，所以这个事务提交之前，数据库里面它可能用到的回滚记录都必须保留，这就会导致大量占用存储空间。长事务还占用锁资源，也可能拖垮整个库。 ","date":"0001-01-01 00:00:00","objectID":"/ooooo-notes/old-notes/geektime/mysql-45/03/:2:0","tags":null,"title":"","uri":"/ooooo-notes/old-notes/geektime/mysql-45/03/"},{"categories":null,"content":"3、事务的启动方式 建议你总是使用 set autocommit = 1, 通过显式语句的方式来启动事务。 在 autocommit = 1 的情况下，用 begin 显式启动的事务，如果执行 commit 则提交事务。如果执行 commit work and chain，则是提交事务并自动启动下一个事务。 你可以在 information_schema 库的 innodb_trx 这个表中查询长事务，比如下面这个语句，用于查找持续时间超过 60s 的事务。 mysql\u003e select * from information_schema.innodb_trx where TIME_TO_SEC(timediff(now(),trx_started))\u003e60 ","date":"0001-01-01 00:00:00","objectID":"/ooooo-notes/old-notes/geektime/mysql-45/03/:3:0","tags":null,"title":"","uri":"/ooooo-notes/old-notes/geektime/mysql-45/03/"},{"categories":null,"content":"4、问题 怎样避免长事务？ 在开发端： 设置set autocommit = 0。 确认是否有不必要的只读事务。 通过 SET MAX_EXECUTION_TIME 命令，来控制每个语句执行的最长时间，避免单个语句意外执行太长时间。 在数据库端： 控 information_schema.Innodb_trx 表，设置长事务阈值，超过就报警/或者 kill。 ercona 的 pt-kill 这个工具不错，推荐使用。 在业务功能测试阶段要求输出所有的 general_log，分析日志行为提前发现问题。 如果使用的是MySQL 5.6或者更新版本，把 innodb_undo_tablespaces 设置成2（或更大的值）。如果真的出现大事务导致回滚段过大，这样设置后清理起来更方便。 ","date":"0001-01-01 00:00:00","objectID":"/ooooo-notes/old-notes/geektime/mysql-45/03/:4:0","tags":null,"title":"","uri":"/ooooo-notes/old-notes/geektime/mysql-45/03/"},{"categories":null,"content":"1、索引的常见模型 ","date":"0001-01-01 00:00:00","objectID":"/ooooo-notes/old-notes/geektime/mysql-45/04/:1:0","tags":null,"title":"","uri":"/ooooo-notes/old-notes/geektime/mysql-45/04/"},{"categories":null,"content":"1、哈希表 以键-值（key-value）存储数据的结构。只适用于等值查询的场景。 ","date":"0001-01-01 00:00:00","objectID":"/ooooo-notes/old-notes/geektime/mysql-45/04/:1:1","tags":null,"title":"","uri":"/ooooo-notes/old-notes/geektime/mysql-45/04/"},{"categories":null,"content":"2、有序数组 查询效率高，但更新数据，成本高。只适用静态存储引擎。 上面数组的按照 ID_card 升序排列，如果查询条件是 where ID_card = '?'，可以用二分法查询。 ","date":"0001-01-01 00:00:00","objectID":"/ooooo-notes/old-notes/geektime/mysql-45/04/:1:2","tags":null,"title":"","uri":"/ooooo-notes/old-notes/geektime/mysql-45/04/"},{"categories":null,"content":"3、搜索树 InnoDB 引擎中使用 B+ 树（ N 叉树）。可以减少磁盘 IO。 ","date":"0001-01-01 00:00:00","objectID":"/ooooo-notes/old-notes/geektime/mysql-45/04/:1:3","tags":null,"title":"","uri":"/ooooo-notes/old-notes/geektime/mysql-45/04/"},{"categories":null,"content":"2、InnoDB的索引模型 我们有一个主键列为 ID 的表，表中有字段 k，并且在 k 上有索引，建表语句如下： mysql\u003e create table T( id int primary key, k int not null, name varchar(16), index (k))engine=InnoDB; 表中 R1 ~ R5 的 (ID,k) 值分别为 (100,1)、(200,2)、(300,3)、(500,5) 和 (600,6)。 在 InnoDB 引擎中，每个索引就是一颗 B+ 树。两颗索引树如下。 根据叶子节点的内容，索引类型分为主键索引和非主键索引。 主键索引的叶子节点存放的是整行数据。 非主键索引的叶子节点存放的是主键的值。 基于主键索引和普通索引的查询有什么区别？ 如果语句是 select * from T where ID=500，即主键查询方式，则只需要搜索 ID 这棵B+树； 如果语句是 select * from T where k=5，即普通索引查询方式，则需要先搜索 k 索引树，得到 ID 的值为 500，再到 ID 索引树搜索一次。这个过程称为回表。 ","date":"0001-01-01 00:00:00","objectID":"/ooooo-notes/old-notes/geektime/mysql-45/04/:2:0","tags":null,"title":"","uri":"/ooooo-notes/old-notes/geektime/mysql-45/04/"},{"categories":null,"content":"3、索引维护 B+ 树为了维护索引有序性，在插入新值的时候需要做必要的维护。以上面这个图为例，如果插入新的行 ID 值为 700，则只需要在 R5 的记录后面插入一个新记录。如果新插入的 ID 值为 400，就相对麻烦了，需要逻辑上挪动后面的数据，空出位置。 而更糟的情况是，如果 R5 所在的数据页已经满了，根据 B+ 树的算法，这时候需要申请一个新的数据页，然后挪动部分数据过去。这个过程称为页分裂。在这种情况下，性能自然会受影响。 除了性能外，页分裂操作还影响数据页的利用率。原本放在一个页的数据，现在分到两个页中，整体空间利用率降低大约50%。当相邻两个页由于删除了数据，利用率很低之后，会将数据页做合并。 总结： 如果主键是自增的，每次插入一条新的数据，就是追加操作，就不会触发页分裂。 主键长度越小，普通索引的叶子节点就越小，占用的空间也就越小。 ","date":"0001-01-01 00:00:00","objectID":"/ooooo-notes/old-notes/geektime/mysql-45/04/:3:0","tags":null,"title":"","uri":"/ooooo-notes/old-notes/geektime/mysql-45/04/"},{"categories":null,"content":"4、问题 对于上面例子中的 InnoDB 表 T，如果你要重建索引 k，你的两个 SQL 语句可以这么写： alter table T drop index k; alter table T add index(k); 如果你要重建主键索引，也可以这么写： alter table T drop primary key; alter table T add primary key(id); 可以执行 alter table T engine=InnoDB; 来重建索引。 ","date":"0001-01-01 00:00:00","objectID":"/ooooo-notes/old-notes/geektime/mysql-45/04/:4:0","tags":null,"title":"","uri":"/ooooo-notes/old-notes/geektime/mysql-45/04/"},{"categories":null,"content":"1、覆盖索引 创建表 T : mysql\u003e create table T ( ID int primary key, k int NOT NULL DEFAULT 0, s varchar(16) NOT NULL DEFAULT '', index k(k)) engine=InnoDB; insert into T values(100,1, 'aa'),(200,2,'bb'),(300,3,'cc'),(500,5,'ee'),(600,6,'ff'),(700,7,'gg'); 执行语句 select ID from T where k between 3 and 5，只需要扫描 k 索引树。因为结果只需要查询 ID，而 ID 在 k 索引树上，减少了回表操作。 索引树已经覆盖了查询结果，称之为覆盖索引。覆盖索引可以减少树的搜索次数，显著提升查询性能。 ","date":"0001-01-01 00:00:00","objectID":"/ooooo-notes/old-notes/geektime/mysql-45/05/:1:0","tags":null,"title":"","uri":"/ooooo-notes/old-notes/geektime/mysql-45/05/"},{"categories":null,"content":"2、最左前缀匹配 B+ 树这种索引结构，可以利用索引的最左前缀，来定位记录。 创建表 tuser ： CREATE TABLE `tuser` ( `id` int(11) NOT NULL, `id_card` varchar(32) DEFAULT NULL, `name` varchar(32) DEFAULT NULL, `age` int(11) DEFAULT NULL, `ismale` tinyint(1) DEFAULT NULL, PRIMARY KEY (`id`), KEY `id_card` (`id_card`), KEY `name_age` (`name`,`age`) ) ENGINE=InnoDB 当你的查询条件是 where name like ‘张%’，也是可以用到索引（name,age）的。 联合索引建立规则： 通过调整顺序，可以少维护一个索引，这是优先考虑的。 索引的空间。 ","date":"0001-01-01 00:00:00","objectID":"/ooooo-notes/old-notes/geektime/mysql-45/05/:2:0","tags":null,"title":"","uri":"/ooooo-notes/old-notes/geektime/mysql-45/05/"},{"categories":null,"content":"3、索引下推 可以对索引中存在的字段先做判断，减少回表次数。 当查询条件是 where name like '张%' and age=10 and ismale=1;，也是可以用到索引树（name,age）的。 ","date":"0001-01-01 00:00:00","objectID":"/ooooo-notes/old-notes/geektime/mysql-45/05/:3:0","tags":null,"title":"","uri":"/ooooo-notes/old-notes/geektime/mysql-45/05/"},{"categories":null,"content":"4、问题 有如下表： CREATE TABLE `geek` ( `a` int(11) NOT NULL, `b` int(11) NOT NULL, `c` int(11) NOT NULL, `d` int(11) NOT NULL, PRIMARY KEY (`a`,`b`), KEY `c` (`c`), KEY `ca` (`c`,`a`), KEY `cb` (`c`,`b`) ) ENGINE=InnoDB; 由于历史原因，这个表需要a、b做联合主键。 查询语句如下： select * from geek where c=N order by a limit 1; select * from geek where c=N order by b limit 1; 索引（c,a）、（c,b）是否都是必须的? 索引 (c,a) 不需要。 ","date":"0001-01-01 00:00:00","objectID":"/ooooo-notes/old-notes/geektime/mysql-45/05/:4:0","tags":null,"title":"","uri":"/ooooo-notes/old-notes/geektime/mysql-45/05/"},{"categories":null,"content":"1、全局表 根据加锁的范围，MySQL 里面的锁大致可以分成全局锁、表级锁和行锁三类。 MySQL 加全局读锁的命令：flush tables with read lock;（FTWRL）, 解锁命令：unlock tables;。让整个库处于只读状态。任何增删改语句都会被阻塞。 全局锁的典型使用场景是，做全库逻辑备份，也就是 select 所有的数据。 对于 MyISAM 引擎来说，备份只能加全局锁。 对于 Innodb 引擎来说，可以使用脚本 mysqldump，参数为 –-single-transaction，来启动一个事务，确保拿到一致性视图来备份数据。MyISAM 不支持事务，所以无法用此脚本。 全库只读，为什么不使用 set global readonly=true 的方式，主要原因有两点： 在有些系统中，readonly 会被用来做其他逻辑，比如判断一个库是主库还是备库。 异常处理机制有差异。 执行 FTWRL 命令后，客户端发生异常断开，MySQL 会自动释放这个全局锁。如果整库处理 readonly 状态，客户端发生异常，数据库会一直处于 readonly 状态。 ","date":"0001-01-01 00:00:00","objectID":"/ooooo-notes/old-notes/geektime/mysql-45/06/:1:0","tags":null,"title":"","uri":"/ooooo-notes/old-notes/geektime/mysql-45/06/"},{"categories":null,"content":"2、表级锁 MySQL 里面表级别的锁有两种：一种是表锁，一种是元数据锁（meta data lock，MDL)。 例如：如果在某个线程 A 中执行 lock tables t1 read, t2 write; 这个语句，则其他线程写 t1 、读写 t2 的语句都会被阻塞。线程 A 执行 unlock tables; 语句来解锁。 MDL 不需要显式使用，在访问一个表的时候会被自动加上。MDL 的作用是，保证读写的正确性。 在 MySQL 5.5 版本中引入了 MDL，当对一个表做增删改查操作的时候，加MDL读锁；当要对表做结构变更操作的时候，加MDL写锁。 给一个小表加个字段，导致整个库挂了。 session A 和 session B 都会加上 MDL读锁。 session C 要变更表结构，必要要加上 MDL写锁，此时只能阻塞。 session D 申请 MDL读锁 就会被 session C 阻塞。 MDL 锁必须要等整个事务提交后再释放。 如何安全地给小表加字段？ 首先要解决长事务，事务不提交，就会一直占着 MDL锁。在 MySQL 的 information_schema 库的 innodb_trx 表中，你可以查到当前执行中的事务。如果你要做 DDL 变更的表刚好有长事务在执行，要考虑先暂停 DDL，或者 kill 掉这个长事务。 如果要变更的表是一个热点表，虽然数据量不大，但是上面的请求很频繁，而你不得不加个字段, kill 可能未必管用，比较理想的机制是，在 alter table 语句里面设定等待时间，如果在这个等待时间里面能够拿到 MDL写锁 最好，拿不到也不要阻塞后面的业务语句，先放弃。之后再通过重试命令重复这个过程。 ALTER TABLE tbl_name NOWAIT add column ... ALTER TABLE tbl_name WAIT N add column ... ","date":"0001-01-01 00:00:00","objectID":"/ooooo-notes/old-notes/geektime/mysql-45/06/:2:0","tags":null,"title":"","uri":"/ooooo-notes/old-notes/geektime/mysql-45/06/"},{"categories":null,"content":"3、问题 备份一般都会在备库上执行，你在用 –-single-transaction 方法做逻辑备份的过程中，如果主库上的一个小表做了一个DDL，比如给一个表上加了一列。这时候，从备库上会看到什么现象呢？ 假设这个 DDL 是针对表 t1 的， 这里把备份过程中几个关键的语句列出来： Q1:SET SESSION TRANSACTION ISOLATION LEVEL REPEATABLE READ; Q2:START TRANSACTION WITH CONSISTENT SNAPSHOT; /* other tables */ Q3:SAVEPOINT sp; /* 时刻 1 */ Q4:show create table `t1`; /* 时刻 2 */ Q5:SELECT * FROM `t1`; /* 时刻 3 */ Q6:ROLLBACK TO SAVEPOINT sp; /* 时刻 4 */ /* other tables */ 在备份开始的时候，为了确保RR（可重复读）隔离级别，再设置一次RR隔离级别(Q1); 启动事务(Q2); 设置一个保存点，这个很重要(Q3); show create 是为了拿到表结构(Q4)，然后正式导数据 （Q5），回滚到SAVEPOINT sp，在这里的作用是释放 t1 的 MDL锁。 答案如下： 如果在 Q4 语句执行之前到达，现象：没有影响，备份拿到的是DDL后的表结构。 如果在\"时刻 2\"到达，则 Q5 执行的时候表结构被改过，报 Table definition has changed, please retry transaction，现象：mysqldump 终止； 如果在\"时刻2\"和\"时刻3\"之间到达，mysqldump 占着 t1 的 MDL读锁，binlog 被阻塞，现象：主从延迟，直到 Q6 执行完成。 从\"时刻4\"开始，mysqldump 释放了 MDL读锁，现象：没有影响，备份拿到的是 DDL 前的表结构。 ","date":"0001-01-01 00:00:00","objectID":"/ooooo-notes/old-notes/geektime/mysql-45/06/:3:0","tags":null,"title":"","uri":"/ooooo-notes/old-notes/geektime/mysql-45/06/"},{"categories":null,"content":"1、行锁 MySQL 的行锁是在引擎层由各个引擎自己实现的，并不是所有的引擎都支持行锁，比如 MyISAM 引擎就不支持，InnoDB 引擎支持行锁。 两阶段锁： 实际上事务B的 update 语句会被阻塞，直到事务A执行 commit 之后，事务B才能继续执行。 在 InnoDB 事务中，行锁是在需要的时候才加上的，但并不是不需要了就立刻释放，而是要等到事务结束时才释放。这个就是两阶段锁协议。 如果你的事务中需要锁多个行，要把最可能造成锁冲突、最可能影响并发度的锁尽量往后放。 比如，电影票在线交易业务： 从顾客A账户余额中扣除电影票价。 给影院B的账户余额增加这张电影票价。 记录一条交易日志。 要保证交易的原子性，就要把这三个操作放在一个事务中，按照 3 -\u003e 1 -\u003e 2 的顺序执行，就减少了事务之间的锁等待，提升了并发度。 ","date":"0001-01-01 00:00:00","objectID":"/ooooo-notes/old-notes/geektime/mysql-45/07/:1:0","tags":null,"title":"","uri":"/ooooo-notes/old-notes/geektime/mysql-45/07/"},{"categories":null,"content":"2、死锁和死锁检测 死锁例子： 事务A在等待事务B释放 id=2 的行锁，而事务B在等待事务A释放 id=1 的行锁。 事务A和事务B在互相等待对方的资源释放，就是进入了死锁状态。 当出现死锁以后，有两种策略 直接进入等待，直到超时，这个超时时间通过参数 innodb_lock_wait_timeout 来设置。 发起死锁检测，发现死锁后，主动回滚死锁链条中的某一个事务，让其他事务得以继续执行。通过参数 innodb_deadlock_detect 设置为 on。 在 InnoDB 中，innodb_lock_wait_timeout 的默认值是 50s，当出现死锁以后，第一个被锁住的线程要过 50s 才会超时退出，这个等待时间肯定是无法接受的，但如果设置为 1s，可能出现不是死锁的情况（大事务），造成误伤。 正常情况下，我们要采用第二种策略：主动死锁检测。innodb_deadlock_detect 的默认值为 on。 当更新同一行时，每个线程检查的时间复杂度为 O(n), 也就是说 1000 个线程，要操作 100w 次，所以死锁检测要耗费大量的 CPU 资源。 怎么解决由这种热点行更新导致的性能问题呢？ 一种头痛医头的方法，就是如果你能确保这个业务一定不会出现死锁，可以临时把死锁检测关掉。业务有损。 控制并发度。比如同一行同时最多只有 10 个线程在更新，那么死锁检测的成本很低，就不会出现这个问题。 并发控制要做在数据库服务端，如果你有中间件，可以考虑在中间件实现；如果你的团队有能修改 MySQL 源码的人，也可以做在 MySQL 里面。基本思路就是，对于相同行的更新，在进入引擎之前排队。这样在 InnoDB 内部就不会有大量的死锁检测工作了。 控制并发度也可以从业务设计上优化。考虑将一行改成逻辑上的多行来减少锁冲突。 还是以影院账户为例，可以考虑放在多条记录上，比如 10 个记录，影院的账户总额等于这 10 个记录的值的总和。这样每次要给影院账户加金额的时候，随机选其中一条记录来加。这样每次冲突概率变成原来的 1/10，可以减少锁等待个数，也就减少了死锁检测的 CPU 消耗。 如果账户余额可能会减少，比如退票逻辑，那么这时候就需要考虑当一部分行记录变成 0 的时候，代码要有特殊处理。 ","date":"0001-01-01 00:00:00","objectID":"/ooooo-notes/old-notes/geektime/mysql-45/07/:2:0","tags":null,"title":"","uri":"/ooooo-notes/old-notes/geektime/mysql-45/07/"},{"categories":null,"content":"3、问题 如果你要删除一个表里面的前 10000 行数据，有以下三种方法可以做到： 第一种，直接执行 delete from T limit 10000; 第二种，在一个连接中循环执行20次 delete from T limit 500; 第三种，在 20 个连接中同时执行 delete from T limit 500。 你会选择哪一种方法呢？为什么呢？ 第二种方式是相对较好的。 第一种方式单个语句占用时间长，锁的时间也比较长；而且大事务还会导致主从延迟。 第三种方式会人为造成锁冲突。 ","date":"0001-01-01 00:00:00","objectID":"/ooooo-notes/old-notes/geektime/mysql-45/07/:3:0","tags":null,"title":"","uri":"/ooooo-notes/old-notes/geektime/mysql-45/07/"},{"categories":null,"content":"1、事务 在第 3 篇文章和你讲事务隔离级别的时候提到过，如果是可重复读隔离级别，事务 T 启动的时候会创建一个视图 read-view，之后事务 T 执行期间，即使有其他事务修改了数据，事务 T 看到的仍然跟在启动时看到的一样。 例子： mysql\u003e CREATE TABLE `t` ( `id` int(11) NOT NULL, `k` int(11) DEFAULT NULL, PRIMARY KEY (`id`) ) ENGINE=InnoDB; insert into t(id, k) values(1,1),(2,2); 在可重复读隔离级别下，begin/start transaction 并不是事务的起点，只有执行到第一个语句时才会真正启动事务。如果你想要马上启动一个事务，可以使用 start transaction with consistent snapshot 这个命令。 在这个例子中，事务 C 没有显式地使用 begin/commit ，表示这个 update 语句本身就是一个事务，语句完成的时候会自动提交。 结果是事务 B 的 k 值为 3，事务A的 k 值为 1。 在 MySQL 里，有两个\"视图\"的概念: 一个是 view，它是一个用查询语句定义的虚拟表。 另一个是 InnoDB 在实现 MVCC 时用到的一致性读视图，即 consistent read view。 ","date":"0001-01-01 00:00:00","objectID":"/ooooo-notes/old-notes/geektime/mysql-45/08/:1:0","tags":null,"title":"","uri":"/ooooo-notes/old-notes/geektime/mysql-45/08/"},{"categories":null,"content":"2、“快照\"怎么工作的 在可重复读隔离级别下，事务在启动的时候就\"拍了个快照”。这个快照是基于整库的。 InnoDB 里面每个事务有一个唯一的事务 ID，叫作 transaction id 。它是在事务开始的时候向 InnoDB 的事务系统申请的，是按申请顺序严格递增的。 而每行数据也都是有多个版本的。每次事务更新数据的时候，都会生成一个新的数据版本，并且把transaction id赋值给这个数据版本的事务 ID，记为 row trx_id。也就是说，表中的一行记录，其实可能有多个版本(row)，每个版本有自己的 row trx_id。 一个记录被多个事务连续更新后的状态: 图中虚线框里是同一行数据的4个版本，当前最新版本是 V4，k 的值是 22，它是被transaction id 为 25 的事务更新的，因此它的 row trx_id 也是 25。 图中三个虚线箭头，就是 undo log，而 V1、V2、V3 并不是物理上真实存在的，而是每次需要的时候根据当前版本和 undo log 计算出来的。 一个事务只需要在启动的时候声明说，“以我启动的时刻为准，如果一个数据版本是在我启动之前生成的，就认；如果是我启动以后才生成的，我就不认，我必须要找到它的上一个版本”。如果\"上一个版本\"也不可见，那就得继续往前找。 在实现上， InnoDB 为每个事务构造了一个数组，用来保存这个事务启动瞬间，当前正在 活跃 (启动了但还没提交)的所有事务 ID。 数组里面事务 ID 的最小值记为低水位，当前系统里面已经创建过的事务 ID 的最大值加1记为高水位。 对于一个数据版本的 row trx_id, 有以下几种可能： 如果落在绿色部分，表示这个版本是已提交的事务或者是当前事务自己生成的，这个数据是可见的； 如果落在红色部分，表示这个版本是由将来启动的事务生成的，是肯定不可见的； 如果落在黄色部分，那就包括两种情况 a. 若 row trx_id 在数组中，表示这个版本是由还没提交的事务生成的，不可见； b. 若 row trx_id 不在数组中，表示这个版本是已经提交了的事务生成的，可见。 分析下图 1 中的三个事务，事务 A 为什么是 k=1 ？ 假设： 事务A开始前，系统里面只有一个活跃事务 ID 是 99； 事务 A、B、C 的版本号分别是 100、101、102，且当前系统里只有这四个事务； 三个事务开始前，(1,1）这一行数据的 row trx_id是 90。 这样，事务 A 的视图数组就是 [99,100] , 事务B的视图数组是 [99,100,101], 事务C的视图数组是 [99,100,101,102]。 事务A查询逻辑有关的操作: 总结： 一个数据版本，对于一个事务视图来说，除了自己的更新总是可见以外，有三种情况： 版本未提交，不可见； 版本已提交，但是是在视图创建后提交的，不可见； 版本已提交，而且是在视图创建前提交的，可见。 ","date":"0001-01-01 00:00:00","objectID":"/ooooo-notes/old-notes/geektime/mysql-45/08/:2:0","tags":null,"title":"","uri":"/ooooo-notes/old-notes/geektime/mysql-45/08/"},{"categories":null,"content":"3、更新逻辑 更新数据都是先读后写的，而这个读，只能读当前的值，称为\"当前读\"（current read）。 在执行事务 B 语句的时候，update 语句是当前读，这是 row trx_id 为 101，所以 select 语句能读到 k=3。 除了 update 语句外，select 语句如果加锁，也是当前读。 mysql\u003e select k from t where id=1 lock in share mode; # 读锁（S锁，共享锁） mysql\u003e select k from t where id=1 for update; # 写锁（X锁，排他锁） 假设事务 C 不是马上提交的，而是变成了下面的事务 C’，会怎么样呢？ 事务 C’ 没提交，也就是说 (1,2) 这个版本上的写锁还没释放。而事务B 是当前读，必须要读最新版本，而且必须加锁，因此就被锁住了，必须等到事务 C’ 释放这个锁，才能继续它的当前读。 总结： 可重复读的核心就是一致性读（consistent read）；而事务更新数据的时候，只能用当前读。如果当前的记录的行锁被其他事务占用的话，就需要进入锁等待。 读提交的逻辑和可重复读的逻辑类似，它们最主要的区别是： 在可重复读隔离级别下，只需要在事务开始的时候创建一致性视图，之后事务里的其他查询都共用这个一致性视图； 在读提交隔离级别下，每一个语句执行前都会重新算出一个新的视图。 注意，语句 start transaction with consistent snapshot; 在读提交隔离级别下，没有意义。 读提交时的状态图，注意是事务 C。 事务 A 查询语句返回的是 k=2，事务 B 查询结果 k=3。 ","date":"0001-01-01 00:00:00","objectID":"/ooooo-notes/old-notes/geektime/mysql-45/08/:2:1","tags":null,"title":"","uri":"/ooooo-notes/old-notes/geektime/mysql-45/08/"},{"categories":null,"content":"4、最重要的总结 一个数据版本，对于一个事务视图来说，除了自己的更新总是可见以外，有三种情况： 版本未提交，不可见； 版本已提交，但是是在视图创建后提交的，不可见； 版本已提交，而且是在视图创建前提交的，可见。 语句 update 、for update 、lock in share mode 是当前读。 ","date":"0001-01-01 00:00:00","objectID":"/ooooo-notes/old-notes/geektime/mysql-45/08/:3:0","tags":null,"title":"","uri":"/ooooo-notes/old-notes/geektime/mysql-45/08/"},{"categories":null,"content":"5、问题 我用下面的表结构和初始化语句作为试验环境，事务隔离级别是可重复读。现在，我要把所有\"字段c和id值相等的行\"的 c 值清零，但是却发现了一个“诡异”的、改不掉的情况。请你构造出这种情况，并说明其原理。 mysql\u003e CREATE TABLE `t` ( `id` int(11) NOT NULL, `c` int(11) DEFAULT NULL, PRIMARY KEY (`id`) ) ENGINE=InnoDB; insert into t(id, c) values(1,1),(2,2),(3,3),(4,4); 出现的情况： 第一种情况 2. 第二种情况 ","date":"0001-01-01 00:00:00","objectID":"/ooooo-notes/old-notes/geektime/mysql-45/08/:4:0","tags":null,"title":"","uri":"/ooooo-notes/old-notes/geektime/mysql-45/08/"},{"categories":null,"content":"1、查询过程 主键索引 ID 和 普通索引 K: 假设执行查询的语句是 select id from T where k=5, 使用二分法查询： 对于普通索引来说，查找到满足条件的第一个记录(5,500)后，需要查找下一个记录，直到碰到第一个不满足 k=5 条件的记录。 对于唯一索引来说，由于索引定义了唯一性，查找到第一个满足条件的记录后，就会停止继续检索。 两个索引的性能差别，微乎其微。 InnoDB 的数据是按数据页为单位来读写，也就是说，当找到 k=5 的记录的时候，它所在的数据页就都在内存里了，所以判断下一条记录是很快的。 ","date":"0001-01-01 00:00:00","objectID":"/ooooo-notes/old-notes/geektime/mysql-45/09/:1:0","tags":null,"title":"","uri":"/ooooo-notes/old-notes/geektime/mysql-45/09/"},{"categories":null,"content":"2、更新过程 当需要更新一个数据页时，如果数据页在内存中就直接更新，如果这个数据页还没有在内存中，在不影响数据一致性的前提下，InooDB 会将这些更新操作缓存在 change buffer 中，这样就不需要从磁盘中读入这个数据页了。在下次查询需要访问这个数据页的时候，将数据页读入内存，然后执行 change buffer 中与这个页有关的操作。通过这种方式就能保证这个数据逻辑的正确性。 将 change buffer 中的操作应用到原数据页，得到最新结果的过程称为 merge。除了访问这个数据页会触发 merge 外，系统有后台线程会定期 merge 。在数据库正常关闭（shutdown）的过程中，也会执行 merge 操作。 对于唯一索引来说，所有的更新操作都要先判断这个操作是否违反唯一性约束，所以必须要将数据页读入内存才能判断，这时 change buffer 不能使用了。 change buffer 用的是 buffer pool 里的内存，因此不能无限增大。change buffer 的大小，可以通过参数 innodb_change_buffer_max_size 来动态设置。这个参数设置为 50 的时候，表示 change buffer 的大小最多只能占用 buffer pool 的 50%。 ","date":"0001-01-01 00:00:00","objectID":"/ooooo-notes/old-notes/geektime/mysql-45/09/:2:0","tags":null,"title":"","uri":"/ooooo-notes/old-notes/geektime/mysql-45/09/"},{"categories":null,"content":"3、change buffer 的使用场景 change buffer 的主要目的就是将记录的变更动作缓存下来，所以在一个数据页做 merge 之前，change buffer 记录的变更越多（也就是这个页面上要更新的次数越多），收益就越大。 对于写多读少的业务来说， change buffer 的使用效果最好。这种业务模型常见的就是账单类、日志类的系统。 假设一个业务的更新模式是写入之后马上会做查询，那么即使满足了条件，将更新先记录在 change buffer，但之后由于马上要访问这个数据页，会立即触发 merge 过程。这样随机访问 IO 的次数不会减少，反而增加了 change buffer 的维护代价。 ","date":"0001-01-01 00:00:00","objectID":"/ooooo-notes/old-notes/geektime/mysql-45/09/:3:0","tags":null,"title":"","uri":"/ooooo-notes/old-notes/geektime/mysql-45/09/"},{"categories":null,"content":"4、索引选择和实践 在不影响业务的情况下，建议你尽量选择普通索引。 普通索引和 change buffer 的配合使用，对于数据量大的表的更新优化还是很明显的。 ","date":"0001-01-01 00:00:00","objectID":"/ooooo-notes/old-notes/geektime/mysql-45/09/:4:0","tags":null,"title":"","uri":"/ooooo-notes/old-notes/geektime/mysql-45/09/"},{"categories":null,"content":"5、change buffer 和 redo log 执行插入语句： mysql\u003e insert into t(id,k) values(id1,k1),(id2,k2); 我们假设\u0008当前 k 索引树的状态，查找到位置后，k1 所在的数据页在内存(InnoDB buffer pool)中，k2 所在的数据页不在内存中。下图是带 change buffer 的更新状态图。 这条插入语句做了如下的操作: Page 1 在内存中，直接更新内存。 Page 2 没有在内存中，就在内存的 change buffer 区域，记录下\"我要往 Page 2 插入一行\"这个信息。 将上述两个动作记入 redo log 中（图中3和4）。 图中的两个虚线箭头，是后台操作，不影响更新的响应时间。 现在要执行语句 select * from t where k in (k1, k2)，这两个读请求的流程图： 读 Page 1 的时候，直接从内存返回。 要读 Page 2 的时候，需要把 Page 2 从磁盘读入内存中，然后应用 change buffer 里面的操作日志，生成一个正确的版本并返回结果。 从上图可知，redo log 主要节省的是随机写磁盘的 IO 消耗（转成顺序写），而 change buffer 主要节省的则是随机读磁盘的 IO 消耗。 ","date":"0001-01-01 00:00:00","objectID":"/ooooo-notes/old-notes/geektime/mysql-45/09/:5:0","tags":null,"title":"","uri":"/ooooo-notes/old-notes/geektime/mysql-45/09/"},{"categories":null,"content":"6、问题 change buffer 一开始是写内存的，那么如果这个时候机器掉电重启，会不会导致 change buffer 丢失呢？change buffer 丢失可不是小事儿，再从磁盘读入数据可就没有了 merge 过程，就等于是数据丢失了。会不会出现这种情况呢？ 不会丢失。虽然是只更新内存，但是在事务提交的时候，我们把 change buffer 的操作也记录到 redo log 里了，所以崩溃恢复的时候，change buffer 也能找回来。 merge 的执行流程是这样的： 从磁盘读入数据页到内存（老版本的数据页）； 从 change buffer 里找出这个数据页的 change buffer 记录(可能有多个），依次应用，得到新版数据页； 写 redo log。这个 redo log 包含了数据的变更和 change buffer 的变更。 ","date":"0001-01-01 00:00:00","objectID":"/ooooo-notes/old-notes/geektime/mysql-45/09/:6:0","tags":null,"title":"","uri":"/ooooo-notes/old-notes/geektime/mysql-45/09/"},{"categories":null,"content":"1、选错索引 一个例子，建表语句如下： CREATE TABLE `x` ( `id` int(11) NOT NULL, `a` int(11) DEFAULT NULL, `b` int(11) DEFAULT NULL, PRIMARY KEY (`id`), KEY `a` (`a`), KEY `b` (`b`) ) ENGINE=InnoDB; 往表 x 中插入 10 万行记录，取值按整数递增，即：(1,1,1)，(2,2,2)，(3,3,3) 直到(100000,100000,100000)，存储过程如下： delimiter ;; create procedure idata() begin declare i int; set i=1; while(i\u003c=100000)do insert into x values(i, i, i); set i=i+1; end while; end;; delimiter ; call idata(); 分析一条 SQL 语句 select * from x where a between 10000 and 20000; 我们再做如下操作。 这时候，session B 的查询语句 select * from t where a between 10000 and 20000 就不会再选择索引 a 了。 查看慢查询： 临时开启慢查询： set global slow_query_log='ON'; set global slow_query_log_file='/var/lib/mysql/instance-1-slow.log'; 实验过程就是这三个语句： set long_query_time=0; # 记录所有的查询到慢查询日志中 select * from x where a between 10000 and 20000; # Q1 查询 select * from x force index(a) where a between 10000 and 20000; # Q2 强制使用索引 a 这三条 SQL 语句执行完成后的慢查询日志: Q1 扫描了 10 万行，显然是走了全表扫描，执行时间是 40 毫秒。Q2 扫描了 10001 行，执行了21毫秒，很显然 mysql 选错了索引。 ","date":"0001-01-01 00:00:00","objectID":"/ooooo-notes/old-notes/geektime/mysql-45/10/:1:0","tags":null,"title":"","uri":"/ooooo-notes/old-notes/geektime/mysql-45/10/"},{"categories":null,"content":"2、优化器的逻辑 在第一篇文章中，我们就提到过，选择索引是优化器的工作。 优化器选择索引会根据扫描行数、是否使用临时表、是否排序等因素进行综合判断。 当然，这个例子中只有扫描行数这个因素，对于扫描行数，MySQL 根据统计信息来估算记录数，抽样来得到索引的基数信息（区别度），如下图。 采样统计的时候，InnoDB 默认会选择 N 个数据页，统计这些页面上的不同值，得到一个平均值，然后乘以这个索引的页面数，就得到了这个索引的基数。 在 MySQL 中，有两种存储索引统计的方式，可以通过设置参数 innodb_stats_persistent 的值来选择： 设置为 on，表示统计信息会持久化存储。这时，默认的 N 是 20，M 是 10。 设置为 off，表示统计信息只存储在内存中。这时，默认的 N是 8，M 是 16。 MySQL 选错索引，是因为索引统计信息不准确，修正统计信息，执行 analyze table x; 命令。 另外一个语句： mysql\u003e select * from t where (a between 1 and 1000) and (b between 50000 and 100000) order by b limit 1; 从条件上看，这个查询没有符合条件的记录，因此会返回空集合。 如果优化器使用索引 a 的话，执行速度明显会快很多，执行 explain命令后： 返回结果中 key 字段显示，这次优化器选择了索引 b。 从这个结果中，你可以得到两个结论： 扫描行数的估计值依然不准确。 这个例子里 MySQL 又选错了索引。 ","date":"0001-01-01 00:00:00","objectID":"/ooooo-notes/old-notes/geektime/mysql-45/10/:2:0","tags":null,"title":"","uri":"/ooooo-notes/old-notes/geektime/mysql-45/10/"},{"categories":null,"content":"3、索引选择异常和处理 一种方法是，采用 force index 强行选择一个索引。 另一种方法是，我们可以考虑修改语句，引导 MySQL 使用我们期望的索引。 修改语句后： 之前优化器选择使用索引 b，是因为它认为使用索引 b 可以避免排序（ b 本身是索引，已经是有序的了，如果选择索引 b 的话，不需要再做排序，只需要遍历），所以即使扫描行数多，也判定为代价更小。 现在 order by b,a 这种写法，要求按照 b,a 排序，就意味着使用这两个索引都需要排序。因此，扫描行数成了影响决策的主要条件，于是此时优化器选了只需要扫描 1000 行的索引 a。 这种修改并不是通用的优化手段，只是刚好在这个语句里面有 limit 1,order by b limit 1 和 order by b,a limit 1 都会返回b 是最小的那一行，逻辑上一致，才可以这么做。 另一种修改语句： 在这个例子里，我们用 limit 100 让优化器意识到，使用 b 索引代价是很高的。其实是我们根据数据特征诱导了一下优化器，也不具备通用性。 第三种方法是，有些场景下，我们可以新建一个更合适的索引，来提供给优化器做选择，或删掉误用的索引。 第四种方法是，删掉索引 b。 ","date":"0001-01-01 00:00:00","objectID":"/ooooo-notes/old-notes/geektime/mysql-45/10/:3:0","tags":null,"title":"","uri":"/ooooo-notes/old-notes/geektime/mysql-45/10/"},{"categories":null,"content":"4、问题 前面我们在构造第一个例子的过程中，通过 session A 的配合，让 session B 删除数据后又重新插入了一遍数据，然后就发现 explain 结果中，rows 字段从 10001 变成 37000 多。 而如果没有 session A 的配合，只是单独执行 delete from t 、call idata()、explain 这三句话，会看到 rows 字段其实还是10000左右。 答案： delete 语句删掉了所有的数据，然后再通过 call idata() 插入了 10 万行数据，看上去是覆盖了原来的 10 万行。 但是，session A 开启了事务并没有提交，所以之前插入的 10 万行数据是不能删除的。这样，之前的数据每一行数据都有两个版本，旧版本是 delete 之前的数据，新版本是标记为 deleted 的数据。 这样，索引 a 上的数据其实就有两份。 表的行数，优化器直接用的是 show table status 的值。 ","date":"0001-01-01 00:00:00","objectID":"/ooooo-notes/old-notes/geektime/mysql-45/10/:4:0","tags":null,"title":"","uri":"/ooooo-notes/old-notes/geektime/mysql-45/10/"},{"categories":null,"content":"1、字符串索引 假设，你现在维护一个支持邮箱登录的系统，用户表是这么定义的 mysql\u003e create table SUser( ID bigint unsigned primary key, email varchar(64), ... )engine=innodb; 要使用邮箱登录，一定会出现类似下面的语句： mysql\u003e select f1, f2 from SUser where email='xxx'; email 这个字段上没有索引，那么这个语句就只能做全表扫描。 MySQL 是支持前缀索引的，你可以定义字符串的一部分作为索引，例如： mysql\u003e alter table SUser add index index1(email); ## 整个字符串 mysql\u003e alter table SUser add index index2(email(6)); ## 前 6 个字节 两个索引图差别如下： 使用前缀索引的优势，占用的空间会更小，但可能会增加额外的记录扫描次数。 例如分析执行语句 select id,name,email from SUser where email='zhangssxyz@xxx.com'; 的过程。 使用 index1 (整个字符串)： 这个过程中，只需要从 index1 索引树找到满足索引值是 ’zhangssxyz@xxx.com’ 的这条记录，再进行回表操作，就可以返回结果。 使用 index2 (前 6 个字节)： 这个过程中，需要从 index2 索引树找到满足索引值是 ’zhangs’ 的全部记录，再进行回表操作，逐一判断 email 是 ’zhangssxyz@xxx.com’，最后返回结果。可能有大量以 ’zhangs’ 开头的记录， 所以可能会增加额外的记录扫描次数。 使用前缀索引，定义好长度，就可以做到既节省空间，又不用额外增加太多的查询成本。 如果建立前缀索引，就要关注区分度，区分度越高越好。 可以使用如下语句，统计不同索引的个数： mysql\u003e select count(distinct email) as L from SUser; 例如，统计 4~7 个字节的前缀索引： mysql\u003e select count(distinct left(email,4)）as L4, count(distinct left(email,5)）as L5, count(distinct left(email,6)）as L6, count(distinct left(email,7)）as L7, from SUser; ","date":"0001-01-01 00:00:00","objectID":"/ooooo-notes/old-notes/geektime/mysql-45/11/:1:0","tags":null,"title":"","uri":"/ooooo-notes/old-notes/geektime/mysql-45/11/"},{"categories":null,"content":"2、其他方式 对于类似于邮箱这样的字段来说，使用前缀索引的效果可能还不错,遇到前缀的区分度不够好的情况时，我们要怎么办呢？ 例如对身份证号的解决方法。 第一种方式是使用倒序存储。 mysql\u003e select field_list from t where id_card = reverse('input_id_card_string'); 第二种方式是使用hash字段 ## 添加索引，用来存储 crc32。 mysql\u003e alter table t add id_card_crc int unsigned, add index(id_card_crc); # 查询时判断 crc32 值 mysql\u003e select field_list from t where id_card_crc=crc32('input_id_card_string') and id_card='input_id_card_string' 它们的相同点是，都不支持范围查询。 ","date":"0001-01-01 00:00:00","objectID":"/ooooo-notes/old-notes/geektime/mysql-45/11/:2:0","tags":null,"title":"","uri":"/ooooo-notes/old-notes/geektime/mysql-45/11/"},{"categories":null,"content":"3、问题 如果你在维护一个学校的学生信息数据库，学生登录名的统一格式是 ”学号@gmail.com\", 而学号的规则是：十五位的数字，其中前三位是所在城市编号、第四到第六位是学校编号、第七位到第十位是入学年份、最后五位是顺序编号。 系统登录的时候都需要学生输入登录名和密码，验证正确后才能继续使用系统。就只考虑登录验证这个行为的话，你会怎么设计这个登录名的索引呢？ 答案： 可以只存入学年份加顺序编号，它们的长度是 9 位。如果数字类型来存这 9 位数字。比如 201100001 ，这样只需要占 4 个字节。 ","date":"0001-01-01 00:00:00","objectID":"/ooooo-notes/old-notes/geektime/mysql-45/11/:3:0","tags":null,"title":"","uri":"/ooooo-notes/old-notes/geektime/mysql-45/11/"},{"categories":null,"content":"1、SQL语句为什么变\"慢\"了 在前面第 2 篇文章《日志系统：一条SQL更新语句是如何执行的？》中，InnoDB 在处理更新语句时，只做了写日志这一个磁盘操作，这个日志叫作 redo log。更新内存写完 redo log 后，就返回给客户端，本次更新成功。。 当内存数据页跟磁盘数据页内容不一致的时候，我们称这个内存页为\"脏页\"。内存数据写入到磁盘后，内存和磁盘上的数据页的内容就一致了，称为\"干净页\"。 MySQL 偶尔\"抖\"一下的那个瞬间，可能就是在刷脏页（flush）。 引发数据库的 flush 过程的几种情况： InnoDB 的 redo log 写满了。 系统内存不足。当需要新的内存页，而内存不够用的时候，就要淘汰一些数据页，空出内存给别的数据页使用。如果淘汰的是\"脏页\"，就要先将脏页写到磁盘。 MySQL 认为系统\"空闲\"的时候。 MySQL 正常关闭。 上面四种场景对性能的影响： 第 3 种情况和第 4 种场景是 MySQL 正常情况，不用太关心性能。 第 1 种是 “redo log 写满了，要 flush 脏页”，出现这种情况了，整个系统就不能再接受更新，如果你从监控上看，这时候更新数会跌为 0。 第 2 种是\"内存不够用了，要先将脏页写到磁盘\"，这种情况其实是常态。InnoDB用缓冲池（buffer pool）管理内存，缓冲池中的内存页有三种状态： 第一种是，还没有使用的； 第二种是，使用了并且是干净页； 第三种是，使用了并且是脏页。 InnoDB 的策略是尽量使用内存，因此对于一个长时间运行的库来说，未被使用的页面很少。刷脏页虽然是常态，但是出现以下这两种情况，都是会明显影响性能的： 一个查询要淘汰的脏页个数太多，会导致查询的响应时间明显变长。 日志写满，更新全部堵住，写性能跌为 0，这种情况对敏感业务来说，是不能接受的。 ","date":"0001-01-01 00:00:00","objectID":"/ooooo-notes/old-notes/geektime/mysql-45/12/:1:0","tags":null,"title":"","uri":"/ooooo-notes/old-notes/geektime/mysql-45/12/"},{"categories":null,"content":"2、InnoDB 刷脏页的控制策略 innodb_io_capacity 参数：告诉 InnoDB 所在主机的 IO 能力。 可以使用 fio 这个工具来测试 IO 能力： fio -filename=$filename -direct=1 -iodepth 1 -thread -rw=randrw -ioengine=psync -bs=16k -size=500M -numjobs=10 -runtime=10 -group_reporting -name=mytest InnoDB 的刷盘速度就是要参考这两个因素：一个是脏页比例，一个是redo log写盘速度。 参数 innodb_max_dirty_pages_pct 是脏页比例上限，默认值是 75%。 脏页比例是通过 Innodb_buffer_pool_pages_dirty/Innodb_buffer_pool_pages_total 得到的，具体的命令如下： select VARIABLE_VALUE into @a from global_status where VARIABLE_NAME = 'Innodb_buffer_pool_pages_dirty'; select VARIABLE_VALUE into @b from global_status where VARIABLE_NAME = 'Innodb_buffer_pool_pages_total'; select @a/@b; 要尽量避免刷脏页这种情况，你就要合理地设置 innodb_io_capacity 的值，并且平时要多关注脏页比例，不要让它经常接近 75%。 一个有趣的策略： 一旦一个查询请求需要在执行过程中先 flush 掉一个脏页时，这个查询就可能要比平时慢了。而 MySQL中 的一个机制，可能让你的查询会更慢：在准备刷一个脏页的时候，如果这个数据页旁边的数据页刚好是脏页，就会把这个\"邻居\"也带着一起刷掉；而且这个把\"邻居\"拖下水的逻辑还可以继续蔓延，也就是对于每个邻居数据页，如果跟它相邻的数据页也还是脏页的话，也会被放到一起刷。 在 InnoDB 中，innodb_flush_neighbors 参数就是用来控制这个行为的，值为 1 的时候会有上述的\"连坐\"机制，值为 0 时表示不找邻居，自己刷自己的。 如果是 SSD 这种， 建议 innodb_flush_neighbors = 0。 ","date":"0001-01-01 00:00:00","objectID":"/ooooo-notes/old-notes/geektime/mysql-45/12/:2:0","tags":null,"title":"","uri":"/ooooo-notes/old-notes/geektime/mysql-45/12/"},{"categories":null,"content":"3、问题 一个内存配置为 128GB、innodb_io_capacity设置为 20000 的大规格实例，正常会建议你将redo log 设置成 4 个 1GB 的文件。 但如果你在配置的时候不慎将 redo log 设置成了 1个 100M 的文件，会发生什么情况呢？又为什么会出现这样的情况呢？ 答案： redo log 太小，很快就会被写满，就必须要 flush，在这种情况下， change buffer 的优化也失效了，因为 flush 时，必须要进行 merge 操作。你看到的现象就是磁盘压力很小，但是数据库出现间歇性的性能下跌。 ","date":"0001-01-01 00:00:00","objectID":"/ooooo-notes/old-notes/geektime/mysql-45/12/:2:1","tags":null,"title":"","uri":"/ooooo-notes/old-notes/geektime/mysql-45/12/"},{"categories":null,"content":"MySQL 实战 45 讲 01、SQL 查询语句的执行过程 ","date":"0001-01-01 00:00:00","objectID":"/ooooo-notes/old-notes/geektime/mysql-45/readme/:1:0","tags":null,"title":"","uri":"/ooooo-notes/old-notes/geektime/mysql-45/readme/"}]